<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>点云自监督学习(MAE，持续更新)</title>
      <link href="/2023/12/25/mae/"/>
      <url>/2023/12/25/mae/</url>
      
        <content type="html"><![CDATA[<h1 id="点云自监督学习-MAE，持续更新"><a href="#点云自监督学习-MAE，持续更新" class="headerlink" title="点云自监督学习(MAE，持续更新)"></a>点云自监督学习(MAE，持续更新)</h1><p><a href="https://arxiv.org/abs/2203.06604">PointMAE论文链接</a></p><p><a href="https://github.com/Pang-Yatian/Point-MAE">PointMAE代码链接</a></p><p><a href="https://zhuanlan.zhihu.com/p/568827138">PointMAE讲解链接</a></p><p><a href="https://arxiv.org/abs/2111.14819">Point-BERT论文链接</a></p><p><a href="https://github.com/lulutang0608/Point-BERT">Point-BERT代码链接</a></p><p><a href="https://zhuanlan.zhihu.com/p/484336830">Point-BERT讲解链接</a></p><p> <img src="/pic/mae2.png" alt="目前点云分割的mae"></p><p>PointBERT pipeline（如下图）里的三个问题：</p><ul><li>需要训练一个基于DGCNN的dVAE用于生成点云的离散词表表示，整个pipeline比较复杂（引入了非Transformer的结构（DGCNN）来辅助Transformer训练）</li><li>依赖对比学习和数据增强去学习high-level语义特征（本文用调高mask ratio去解决）</li><li>PointBERT对点云做tokenize之后，被mask掉的token是以一个learnable token + positional embedding表示的，再与visible tokens一起输入Transformer Encoder，存在位置信息的泄漏（positional embedding里有mask token的位置信息，降低了reconstruction任务的难度）</li></ul><p><img src="/pic/mae1.png" alt="Point-BERT的pipeline。我们首先将输入点云划分为几个点pathes(子云)。然后使用一个迷你点网[34]来获得点嵌入序列。在预训练之前，通过基于dvae的点云重建(如图右图所示)学习Tokenizer，其中点云可以转换为一系列离散的点tokens;在预训练期间，我们对点嵌入的某些部分进行了掩码，并用掩码tokens替换它们。然后将掩蔽点嵌入到transformer中。在Tokenizer获得的点tokens的监督下，训练模型恢复原始点tokens。我们还添加了一个辅助的对比学习任务，以帮助transformer捕获高级语义知识。"></p><p>PointMAE pipeline:</p><ul><li>采用MAE的pipeline，在Encoder处仅输入visible tokens + visible token的positional embedding，在Decoder处才将visible tokens 和mask tokens 加上full-set的positional embedding 一起输入</li><li>采用非常高的mask ratio（60%）</li><li>直接对初始的点云xyz进行预测</li><li>整体网络仅由Transformer Blocks构成，没有其他结构</li><li>采用轻量级的decoder（encoder有12个Transformer blocks，decoder只有4个）</li></ul><p> <img src="/pic/mae3.png" alt="Point-MAE总体方案。在左边，我们展示了掩蔽和嵌入的过程。将输入云划分为多个点块，对点块进行随机掩码后嵌入。右侧为自动编码器预训练。编码器只处理可见的标记。将掩码tokens添加到解码器的输入序列中以重建掩码点补丁。"></p><p><strong>编码器仅处理visible tokens的好处：</strong></p><ul><li>可以让编码器更好地学习这些point patchs的high-level的语义特征，而不需要学习去区分visible tokens和mask tokens</li><li>避免了mask tokens的位置信息的泄漏</li><li>提高了网络的预训练效率（因为采用了高的mask ratio（60%），相当于编码器仅处理40%的tokens，自然就很快啦）</li></ul><p><strong>MAE pretrain效率高的原因:</strong></p><ul><li>编码器仅处理visible tokens</li><li>解码器虽然要处理全部tokens，但非常轻量级</li></ul>]]></content>
      
      
      <categories>
          
          <category> MAE </category>
          
      </categories>
      
      
        <tags>
            
            <tag> mae </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>终端神器tmux:多任务管理大师</title>
      <link href="/2023/12/21/tmux/"/>
      <url>/2023/12/21/tmux/</url>
      
        <content type="html"><![CDATA[<h1 id="终端神器tmux-多任务管理大师"><a href="#终端神器tmux-多任务管理大师" class="headerlink" title="终端神器tmux:多任务管理大师"></a>终端神器tmux:多任务管理大师</h1><h2 id="tumx是什么"><a href="#tumx是什么" class="headerlink" title="tumx是什么"></a>tumx是什么</h2><p>命令行的典型使用方式是，打开一个终端窗口（terminal window，以下简称”窗口”），在里面输入命令。用户与计算机的这种临时的交互，称为一次”会话”（session）。</p><p>会话的一个重要特点是，窗口与其中启动的进程是连在一起的。打开窗口，会话开始；关闭窗口，会话结束，会话内部的进程也会随之终止，不管有没有运行完。</p><p>一个典型的例子就是，SSH 登录远程计算机，打开一个远程窗口执行命令。这时，网络突然断线，再次登录的时候，是找不回上一次执行的命令的。因为上一次 SSH 会话已经终止了，里面的进程也随之消失了。</p><p>为了解决这个问题，会话与窗口可以”解绑”：窗口关闭时，会话并不终止，而是继续运行，等到以后需要的时候，再让会话”绑定”其他窗口。</p><p><strong>Tmux 就是会话与窗口的”解绑”工具，将它们彻底分离。</strong></p><blockquote><p>（1）它允许在单个窗口中，同时访问多个会话。这对于同时运行多个命令行程序很有用。<br>（2） 它可以让新窗口”接入”已经存在的会话。<br>（3）它允许每个会话有多个连接窗口，因此可以多人实时共享会话。<br>（4）它还支持窗口任意的垂直和水平拆分。</p></blockquote><h2 id="常用操作"><a href="#常用操作" class="headerlink" title="常用操作"></a>常用操作</h2><blockquote><p>1.新建会话tmux new -s my_session。<br>2.在 Tmux 窗口运行所需的程序。<br>3.按下快捷键Ctrl+b d将会话分离。<br>4.下次使用时，重新连接到会话tmux attach-session -t my_session。</p></blockquote><h2 id="基本用法"><a href="#基本用法" class="headerlink" title="基本用法"></a>基本用法</h2><h3 id="安装与进入"><a href="#安装与进入" class="headerlink" title="安装与进入"></a>安装与进入</h3><pre class="line-numbers language-none"><code class="language-none"># Ubuntu 或 Debian$ sudo apt-get install tmux# CentOS 或 Fedora$ sudo yum install tmux# Mac$ brew install tmux#键入tmux命令，就进入了 Tmux 窗口。$ tmux<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h3 id="前缀键Ctrl-b"><a href="#前缀键Ctrl-b" class="headerlink" title="前缀键Ctrl+b"></a>前缀键Ctrl+b</h3><p>Tmux 窗口有大量的快捷键。所有快捷键都要通过前缀键唤起。默认的前缀键是Ctrl+b，即先按下Ctrl+b，快捷键才会生效。</p><p>举例来说，帮助命令的快捷键是Ctrl+b ?。它的用法是，在 Tmux 窗口中，先按下Ctrl+b，再按下?，就会显示帮助信息。</p><p>然后，按下ESC 键或q键，就可以退出帮助。</p><h2 id="会话管理"><a href="#会话管理" class="headerlink" title="会话管理"></a>会话管理</h2><pre class="line-numbers language-none"><code class="language-none"># 新建一个指定名称的会话。$ tmux new -s &lt;session-name&gt;#分离会话$ tmux detach# 或者按下Ctrl+b d# 接入会话# 使用会话编号（attach 可以写成a）$ tmux attach -t 0$ tmux a -t 0# 使用会话名称$ tmux attach -t &lt;session-name&gt;$ tmux a -t &lt;session-name&gt;# 杀死会话# 使用会话编号$ tmux kill-session -t 0# 使用会话名称$ tmux kill-session -t &lt;session-name&gt;# 切换会话# 使用会话编号$ tmux switch -t 0# 使用会话名称$ tmux switch -t &lt;session-name&gt;# 重命名会话$ tmux rename-session -t 0 &lt;new-name&gt;<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h2 id="窗格快捷键"><a href="#窗格快捷键" class="headerlink" title="窗格快捷键"></a>窗格快捷键</h2><blockquote><p><strong>Ctrl+b d：分离当前会话。（后台继续运行）</strong><br>Ctrl+b s：列出所有会话。（tmux ls）<br>Ctrl+b $：重命名当前会话。<br><strong>Ctrl+b %：划分左右两个窗格。</strong><br><strong>Ctrl+b “：划分上下两个窗格。</strong><br>Ctrl+b <arrow key>：光标切换到其他窗格。是指向要切换到的窗格的方向键，比如切换到下方窗格，就按方向键↓。<br>Ctrl+b ;：光标切换到上一个窗格。<br>Ctrl+b o：光标切换到下一个窗格。<br>Ctrl+b {：当前窗格与上一个窗格交换位置。<br>Ctrl+b }：当前窗格与下一个窗格交换位置。<br>Ctrl+b Ctrl+o：所有窗格向前移动一个位置，第一个窗格变成最后一个窗格。<br>Ctrl+b Alt+o：所有窗格向后移动一个位置，最后一个窗格变成第一个窗格。<br><strong>Ctrl+b x：关闭当前窗格。</strong><br>Ctrl+b !：将当前窗格拆分为一个独立窗口。<br><strong>Ctrl+b z：当前窗格全屏显示，再使用一次会变回原来大小。</strong><br>Ctrl+b Ctrl+<arrow key>：按箭头方向调整窗格大小。<br><strong>Ctrl+b q：显示窗格编号。</strong></p></blockquote><h2 id="窗口快捷键"><a href="#窗口快捷键" class="headerlink" title="窗口快捷键"></a>窗口快捷键</h2><blockquote><p><strong>Ctrl+b c：创建一个新窗口，状态栏会显示多个窗口的信息。<br>Ctrl+b p：切换到上一个窗口（按照状态栏上的顺序）。<br>Ctrl+b n：切换到下一个窗口。<br>Ctrl+b <number>：切换到指定编号的窗口，其中的是状态栏上的窗口编号。<br>Ctrl+b w：从列表中选择窗口。<br>Ctrl+b ,：窗口重命名。</strong></p></blockquote><p>参考链接</p><p><a href="https://blog.csdn.net/Aibiabcheng/article/details/122482786">Tmux使用教程</a></p><p><a href="https://blog.csdn.net/qq_30883899/article/details/132871423">tmux的常用操作</a></p><p><a href="https://www.bilibili.com/video/BV1ML411h7tF/?spm_id_from=333.337.search-card.all.click&vd_source=c3ce2553ecbf3f37d2f4be6f466233fa">视频配置便捷tumx快捷键</a></p>]]></content>
      
      
      <categories>
          
          <category> tmux </category>
          
      </categories>
      
      
        <tags>
            
            <tag> tmux </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Learning Inter-Superpoint Affinity for Weakly Supervised 3D Instance Segmentation（未完待续）</title>
      <link href="/2023/12/04/superpoint3/"/>
      <url>/2023/12/04/superpoint3/</url>
      
        <content type="html"><![CDATA[<h1 id="Learning-Inter-Superpoint-Affinity-for-Weakly-Supervised-3D-Instance-Segmentation（未完待续）"><a href="#Learning-Inter-Superpoint-Affinity-for-Weakly-Supervised-3D-Instance-Segmentation（未完待续）" class="headerlink" title="Learning Inter-Superpoint Affinity for Weakly Supervised 3D Instance Segmentation（未完待续）"></a>Learning Inter-Superpoint Affinity for Weakly Supervised 3D Instance Segmentation（未完待续）</h1><h2 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h2><p>由于三维点云的标注很少，如何学习点云的区分特征来分割目标实例是一个具有挑战性的问题。在本文中，我们提出了一个简单而有效的3D实例分割框架，该框架可以通过对每个实例只标注一个点来获得良好的性能。具体地说，为了处理极少的标签例如分割，</p><ul><li>我们首先以无监督的方式将点云过分割成超点，并将点级注释扩展到超点级。</li><li>然后，在超点图的基础上，提出了一种考虑语义和空间关系的超点间亲和力挖掘模块，通过语义感知的随机游走自适应学习超点间亲和力，生成高质量的伪标签。</li><li>最后，我们提出了一个体积感知的实例求精模块，通过在超点图上的聚类中应用目标的体积约束来分割高质量的实例。</li></ul><p>在ScanNet-v2和S3DIS数据集上的大量实验表明，我们的方法在弱监督的点云实例分割任务中取得了最好的性能，甚至优于一些完全监督的方法。</p><h2 id="引言"><a href="#引言" class="headerlink" title="引言"></a>引言</h2><p>点云实例分割是三维计算机视觉中的一项经典任务，可应用于室内导航系统、增强现实、机器人等领域。全监督的实例分割方法[17，2，12]已经取得了令人印象深刻的结果，但它们依赖于大量的人工标记数据。然而，注释大量的点云是非常耗时和昂贵的。因此，以一种需要少量注记的半监督&#x2F;弱监督方式分割点云是有意义的。然而，如何充分利用有限的标签来提高实例分割的性能仍然是一个具有挑战性的问题。</p><p>很少有人致力于半监督&#x2F;弱监督点云实例分割。</p><ul><li>作为开拓者，廖添丁等人。[18]提出了一种以包围盒为监督的半监督点云实例分割方法，利用网络生成包围盒方案。实例分割是通过细化边界框内的点云来实现的。</li><li>此外，陶渊明等人也提出了自己的观点。[24]提出了一种两阶段分段监督3D实例和语义分割方法，该方法首先利用分段分组网络为整个场景生成伪标签，然后将生成的伪点级标签作为地面真值对网络进行训练。然而，这些简单的伪标签生成策略不能有效地生成高质量的伪标签，导致3D实例分割结果不佳。</li></ul><p>本文提出了一种简单而有效的弱监督3D实例分割框架，<strong>每个实例只需一个点的标注就能获得令人印象深刻的分割结果</strong>。对于标注较少的弱监督点云实例分割，我们的直觉体现在两个方面：</p><ul><li>(1)在稀有标注下，有效的标注传播是产生高质量伪标注的关键，尤其是在3D实例分割中。</li><li>(2)弱监督3D实例分割比弱监督3D语义分割更具挑战性，因此我们考虑引入目标体积约束来改善实例分割结果。具体地说，<ul><li>我们首先使用一种无监督的方法[14]将点云过度分割成超点并构建超点图。通过这种方式，点级别的标签可以扩展到超点级别的标签。</li><li>然后，我们提出了一个超点间亲和力挖掘模块，基于少量标注的超点级标签生成高质量的伪标签。在超点图的基础上，利用相邻超点的语义和空间信息自适应地学习超点间的亲和力，通过语义感知的随机游走将超点标记沿着超点图传播。</li><li>最后，我们提出了一个体积感知的实例精化模块来提高实例分割的性能。在超点传播训练模型的基础上，通过超点聚类得到粗略的实例分割结果，进而从实例分割结果中推断出目标的体积信息。目标体积信息包含体素的数量和目标的半径。将推断出的物体体积信息作为相应实例的地面真实，对网络进行再训练。在测试阶段，基于目标体积信息，利用预测的目标体积信息，提出了一种体积感知的实例聚类算法，用于分割高质量的实例。</li></ul></li></ul><p>在ScanNet-v2[6]和S3DIS[1]数据集上的大量实验证明了该方法的有效性。本论文的主要贡献如下：</p><ul><li>提出了一种考虑语义和空间关系的超点间亲和力挖掘模块，用于基于随机游走的标签传播自适应学习超点间亲和力。</li><li>提出了一种体积感知的实例求精模块，该模块利用对象体积信息指导超点图上的超点聚类进行实例分割。</li><li>我们简单而有效的框架在流行的数据集ScanNet-v2和S3DIS上实现了最先进的弱监督3D实例分割性能。</li></ul>]]></content>
      
      
      <categories>
          
          <category> DL </category>
          
      </categories>
      
      
        <tags>
            
            <tag> seg </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Superpoint Transformer for 3D Scene Instance Segmentation（未完待续）</title>
      <link href="/2023/12/03/superpoint2/"/>
      <url>/2023/12/03/superpoint2/</url>
      
        <content type="html"><![CDATA[<h1 id="Superpoint-Transformer-for-3D-Scene-Instance-Segmentation（未完待续）"><a href="#Superpoint-Transformer-for-3D-Scene-Instance-Segmentation（未完待续）" class="headerlink" title="Superpoint Transformer for 3D Scene Instance Segmentation（未完待续）"></a>Superpoint Transformer for 3D Scene Instance Segmentation（未完待续）</h1><p><a href="https://arxiv.org/abs/2211.15766">论文地址</a><br><a href="https://github.com/sunjiahao1999/SPFormer?utm_source=catalyzex.com">代码地址</a><br><a href="https://zhuanlan.zhihu.com/p/661437298">论文解读</a></p><h2 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h2><p>现有的大多数方法通过扩展用于3D对象检测或3D语义分割的模型来实现3D实例分割。然而，这些非直截了当的方法有两个缺点：</p><ul><li>1)不精确的边界框或不令人满意的语义预测限制了整体3D实例分割框架的性能。</li><li>2)现有方法需要耗时的聚合中间步骤。</li></ul><p>针对这些问题，本文提出了一种基于超点变换的端到端3D实例分割方法SPFormer。它将点云中的潜在特征分组为超点，并通过查询向量直接预测实例，而不依赖于对象检测或语义分割的结果。</p><ul><li>该框架的关键是设计了一种带transformer的查询解码器，通过超点交叉注意机制捕获实例信息，并生成实例的超点掩码。</li><li>通过基于超点掩码的二分图匹配，SPFormer无需中间聚合步骤即可实现网络训练，加快了网络的运行速度。</li><li>在ScanNetv2和S3DIS基准测试程序上的大量实验验证了该方法的简明性和有效性。值得注意的是，在ScanNetv2隐藏测试集上，SPFormer在MAP方面比现有的方法高出4.3%，同时保持了快速的推理速度(每帧247ms)。</li></ul><h2 id="引言"><a href="#引言" class="headerlink" title="引言"></a>引言</h2><p>3D场景理解被认为是许多应用的基本要素，包括增强&#x2F;虚拟现实(Park等人)。2020)、自动驾驶(周等人2020)和机器人导航(谢等人2021年)。通常情况下，实例分割是三维场景理解中的一项具有挑战性的任务，其目的不仅是检测稀疏点云上的实例，而且为每个实例提供清晰的掩码。</p><p><img src="/pic/Super1.png" alt="图1：不同方法的关键流程。(A)是输入点云。(B)基于proposal的方法首先检测对象。(C)基于分组的方法将点偏移到它们自己的实例中心和组点。(D)我们的方法通过超点交叉注意来突出感兴趣区域。"></p><p>现有的最先进的方法可以分为<strong>基于proposal的方法</strong>(Yang等人)。2019年；刘等人。2020)和<strong>基于分组</strong>(酱等人2020年；Chen等人。2021年；梁等人。2021年；Vu等人。2022年)。</p><ul><li>基于proposal的方法将3D实例分割视为一条自上而下的pipeline。它们首先生成区域proposals(即边界框)，如图1(B)所示，然后预测proposes区域中的实例掩码。这些方法受到MASK-RCNN巨大成功的鼓舞(他等人)。2017)在2D实例分段字段上。然而，由于域间隙的原因，这些方法在点云上遇到了困难。在三维领域中，包围盒具有更多的自由度(DoF)，增加了拟合的难度。此外，点通常只存在于物体表面的一部分，这导致无法检测到物体的几何中心。此外，低质量的区域proposal影响基于盒的二分图匹配(Yang等人。2019年)，并进一步降低了模型的性能。</li><li>相反，基于分组的方法采用自下而上的pipeline。它们学习逐点语义标签和实例中心偏移量。然后，它们使用偏移点和语义预测来聚集成实例，如图1(C)所示。在过去的两年中，基于分组的方法在3D实例分割任务中取得了很大的改进(梁等人)。2021年；Vu等人。2022年)。但也存在一些不足：<ul><li>(1)基于分组的方法依赖于它们的语义分割结果，这可能导致错误的预测。将这些错误预测传播到后续处理会抑制网络性能。</li><li>(2)这些方法需要一个中间的聚合步骤，增加了训练和推理时间。聚合步骤独立于网络训练，缺乏监督，需要额外的细化模块。</li></ul></li></ul><p>本文提出了一种基于超点变换的端到端两阶段3D实例分割方法SPFormer。SPFormer自下而上地将点云中的潜在特征分组到超点中，并通过查询向量将实例proposes作为自上而下的pipeline。</p><ul><li>在自下而上的分组阶段，利用稀疏的三维U-net提取自下而上的逐点特征。提出了一种简单的超点池化层，用于将潜在的逐点特征分组为超点。超点(Landrieu和Simonovsky 2018)可以利用几何规则来表示均匀的邻接点。与以前的方法(梁等人)不同2021)，<strong>我们的超点特征是潜在的</strong>，避免了通过非直截了当的语义和中心距离标签来监督特征。&#x3D;&#x3D;我们将超点作为3D场景潜在的中层表示，并直接使用实例标签来训练整个网络。&#x3D;&#x3D; </li><li>在自上而下的提proposal阶段，提出了一种新的带transformers的查询解码器。我们利用可学习的查询向量从潜在的超点特征中提出实例预测，作为自顶向下的pipeline。<strong>可学习查询向量通过超点交叉注意机制捕获实例信息</strong>。图1(D)示出了这样的过程，即椅子的部分越红，查询向量就越关注。利用携带实例信息和超点特征的查询向量，查询解码器直接生成实例类、得分和掩码预测。最后，通过基于超点掩码的二分图匹配，SPFormer可以实现端到端的训练，而不需要耗时的聚合步骤。此外，SPFormer没有像非最大值抑制(NMS)那样的后处理，进一步加快了网络速度。</li></ul><p>SPFormer在ScanNetv2和S3DIS基准测试中都达到了最先进的水平。特别是，SPFormer在定性和定量指标以及推理速度方面都超过了同类最先进的方法。SPFormer采用了一种新的流水线，可以作为3D实例分割的通用框架。总而言之，我们的贡献如下：</p><ul><li>我们提出了一种端到端的两阶段方法SPFormer，该方法不依赖于目标检测或语义分割的结果来表示具有潜在超文本特征的3D场景。</li><li>设计了一个带有transformers的查询解码器，其中可学习的查询向量可以通过超点交叉注意来捕获实例信息。通过查询向量，查询解码器可以直接生成实例预测。</li><li>通过基于超点掩码的二分图匹配，SPFormer可以实现网络训练，而不需要耗时的中间聚合步骤，也不需要复杂的推理后处理。</li></ul><h2 id="相关工作"><a href="#相关工作" class="headerlink" title="相关工作"></a>相关工作</h2><p><strong>以proposal为基础的方法。</strong></p><p>例如，基于proposal的方法采用自上而下的方法进行分割。以前的方法(Yi等人)2019年；侯、戴和尼埃纳2019年；成田等人。2019)专注于将2D图像要素和点云要素融合到一个体积网格中，并从该网格生成区域proposals。3D-Bonet(Yang et al.2019)使用PointNet++(齐等人2017a，b)从点云中提取特征，并将3D包围盒生成任务视为最优分配问题。GICN(Liu et al.2020)预测高斯热图以选择实例中心候选，并在提议的边界框内产生实例掩码。3D-MPA(Engelmann等人2020)样本预测质心和质心附近的聚集点以形成最终实例遮罩。大多数基于proposals的方法都基于3D边界框。然而，低质量的包围盒预测会影响实例分割模型的性能。</p><p><strong>基于分组的方法。</strong></p><p>基于分组的方法将3D实例分割视为一条自下而上的pipeline。MTML(Lahoud et al.2019)利用多任务策略学习特征嵌入。PointGroup(酱等人)2020)从原始和中心移动的点云中聚集点，并设计ScoreNet来评估聚集的质量。PE(Zhang And Wonka 2021)引入了一种新的概率嵌入空间。Dyco3D(何、沈和van den Hengel 2021)引入了动态卷积核。HAIS(Chen et al.2021)使用分层聚集来扩展PointGroup，并过滤实例预测内的噪声点。SSTNet(梁等人)2021)构建语义超点树，并通过拆分不相似节点获得实例预测。SoftGroup(Vu等人)2022)使用较低的聚类阈值来解决错误的语义硬预测，并使用微小的3D U-net来优化实例。尽管基于分组的方法可能有一个自上而下的精化模块，但它们仍然不可避免地依赖于中间聚合步骤。</p><p><strong>使用Transformer进行2D实例分割。</strong></p><p>最近，Transform(Vaswani et al.)2017)被引入到图像分类中(Dosovitski等人。2020年；Touvron等人。2021年；刘等人。2021)、目标检测(Carion等人2020年；戴相龙等人。2021)和分段(cheng，Schwing，and Kirillov，2021；cheng et al.2022A；郭某等人。2021年)。还有一些实例分割方法(Fang等人)。2021年；程等人。2022b)受到transformer的启发。Mask2Former(程等人)2022A)成功地应用tranformers构建了二维图像语义、实例和全景分割的通用网络。</p><p>受到Transform在2D分割任务中的成功应用的启发，我们有动力将Transform引入到3D实例分割中。然而，由于注意机制的复杂性，transformer不能简单地应用于稀疏卷积主干的输出，因为它会引入很高的计算开销。</p><p>本文中，我们将<strong>设计一种新的查询解码器用于3D实例分割，并使用超点在主干和查询解码器之间架起一座桥梁</strong>。</p><h2 id="方法"><a href="#方法" class="headerlink" title="方法"></a>方法</h2><p><img src="/pic/Super2.png" alt="图2"></p><p><img src="/pic/Super3.png"><br>SPFormer的体系结构如图2所示。首先，使用稀疏的3D U-net来提取自下而上的点状特征。提出了一种简单的超点池化层，用于将潜在的逐点特征分组为超点。其次，提出了一种新的带变换的查询解码器，其中可学习的查询向量通过超点交叉注意来获取实例信息。最后，通过基于超点掩码的二分图匹配，SPFormer可以实现端到端的训练，而不需要耗时的聚合步骤。</p><h3 id="Backbone-and-Superpoints"><a href="#Backbone-and-Superpoints" class="headerlink" title="Backbone and Superpoints"></a>Backbone and Superpoints</h3><p><strong>稀疏3D U-net。</strong></p><p>假设输入点云有N个点，输入可以表示为P∈RN×6。每个点都有颜色r，g，b和坐标x，y，z。在前面的实现(Graham，Engelcke和Van Der Maten 2018)之后，我们将点云体素化用于常规输入，并使用子流形稀疏卷积(SSC)或稀疏卷积(SC)组成的U-net骨干来提取点特征P∈RN×C。我们在补充材料中给出了稀疏3D U-net的细节。与常用的基于分组的方法不同，我们的方法不增加额外的语义分支和偏置分支。</p><p><strong>超点池化层。</strong></p><p>为了构建端到端框架，我们直接将逐点功能P∈RN×C馈送到基于预计算超点的超点池层(Landrieu和Simonovsky 2018年)。超点池化层通过对每个超点内部的逐点平均池化，简单地获得超点特征S∈Rm×C。在不失去一般性的情况下，我们假设从输入点云计算出M个超点。值得注意的是，超点池化层可靠地将输入点云下采样到数百个超点，这显著降低了后续处理的计算开销，并优化了整个网络的表示能力。</p><p><a href="https://blog.csdn.net/Dujing2019/article/details/104091750?spm=1001.2101.3001.6650.1&utm_medium=distribute.pc_relevant.none-task-blog-2~default~CTRLIST~Rate-1-104091750-blog-105473851.235%5Ev39%5Epc_relevant_3m_sort_dl_base4&depth_1-utm_source=distribute.pc_relevant.none-task-blog-2~default~CTRLIST~Rate-1-104091750-blog-105473851.235%5Ev39%5Epc_relevant_3m_sort_dl_base4&utm_relevant_index=2">Large-scale point cloud semantic segmentation with superpoint graphs.</a></p><h3 id="查询解码器"><a href="#查询解码器" class="headerlink" title="查询解码器"></a>查询解码器</h3><p>查询解码器由实例分支和掩码分支组成。在MASK分支中，一个简单的多层感知器旨在提取掩码感知特征SMASK∈Rm×D。实例分支由一系列transformer解码器层组成。它们通过超点交叉注意来解码可学习的查询向量。假设有K个可学习的查询向量。我们预定义了来自各transformer解码层的查询向量的特征为Z∈RK×D，D为嵌入维度，&#x3D;1，2，3…是层索引。</p>]]></content>
      
      
      <categories>
          
          <category> DL </category>
          
      </categories>
      
      
        <tags>
            
            <tag> seg </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Learning Superpoint Graph Cut for 3D Instance Segmentation （未完待续）</title>
      <link href="/2023/12/02/superpoint/"/>
      <url>/2023/12/02/superpoint/</url>
      
        <content type="html"><![CDATA[<h1 id="Learning-Superpoint-Graph-Cut-for-3D-Instance-Segmentation-（未完待续）"><a href="#Learning-Superpoint-Graph-Cut-for-3D-Instance-Segmentation-（未完待续）" class="headerlink" title="Learning Superpoint Graph Cut for 3D Instance Segmentation （未完待续）"></a>Learning Superpoint Graph Cut for 3D Instance Segmentation （未完待续）</h1><p><a href="https://proceedings.neurips.cc/paper_files/paper/2022/file/ef0af61ccfba2bf9fad4f4df6dfcb7c3-Paper-Conference.pdf">论文地址</a><br><a href="https://github.com/fpthink/GraphCut">代码地址(待发布)</a></p><h2 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h2><p>由于点云中目标的复杂局部几何结构，3D实例分割是一项具有挑战性的任务。在本文中，我们提出了一种基于学习的<strong>超点图割方法</strong>，它<strong>显式地</strong>学习点云的局部几何结构，用于3D实例分割。具体地说，</p><ul><li>我们首先将原始点云过分割成超点，并构造超点图。</li><li>然后，我们提出了一个边分数预测网络来预测超点图的边分数，其中通过交叉图注意力在坐标和特征空间中学习到的相邻节点的相似性向量被用来回归边分数。通过迫使同一实例的两个相邻节点在坐标和特征空间中靠近实例中心，我们提出了一种几何感知的边缘损失(Geometry-Aware Edge Loss)来训练边缘得分预测网络。</li><li>最后，我们开发了一个超点图割网络，该网络利用学习到的<strong>边分数和预测的节点语义类别</strong>来生成实例，其中提出了双边图注意力来提取坐标空间和特征空间上的区分特征来预测语义标签和实例分数。在ScanNet v2和S3DIS这两个具有挑战性的数据集上的大量实验表明，该方法在3D实例分割上达到了最新的性能。</li></ul><h2 id="1-引言"><a href="#1-引言" class="headerlink" title="1.引言"></a>1.引言</h2><p>近年来，随着激光雷达、Kinect摄像机等三维传感器的发展，各种三维计算机视觉任务受到越来越多的关注。3D场景分割是3D场景理解中的一项基本任务，在自动驾驶汽车、虚拟现实、机器人导航等领域有着广泛的应用。尽管近年来3D实例分割的进展令人鼓舞，但&#x3D;&#x3D;由于复杂几何结构的3D场景中3D点的不规则性和上下文不确定性&#x3D;&#x3D;，分割仍然是一项具有挑战性的任务。</p><p>许多人致力于3D实例分割，并取得了令人满意的性能。这些方法主要可以分为两类：基于检测的方法[44，45]和基于聚类的方法[40，18]。</p><ul><li>在基于检测的方法中，3D-Bonet[44]首先检测3D包围盒，然后使用掩码预测网络预测目标掩码以用于3D实例分割。然而，对于具有复杂几何结构的目标，基于检测的方法[45]无法获得准确的3D边界框，从而降低了实例分割的性能。</li><li>基于聚类的方法SGPN[40]基于语义分割对三维点进行聚类生成实例。与SGPN不同的是，酱等人。[18]提出了一种基于双重坐标空间(包括原始坐标空间和移动坐标空间)中的语义预测来聚类点的偏移量分支。此外，一些后续方法利用树结构[25]、层次聚集[3]和软语义分割[37]来提高3D实例分割的性能。然而，这些基于聚类的方法大多依赖于中心偏移量和语义来分割实例，不能有效地捕捉点云的几何上下文信息。因此，点云中具有复杂几何结构的目标往往限制了实例分割的性能。</li></ul><p>在本文中，我们提出了一种基于学习的超点图切割方法，<strong>该方法显式地学习点云的局部几何结构来分割3D实例</strong>。具体地说， &#x3D;&#x3D;我们构造超点图来学习超点的几何上下文相似性，并将实例分割转化为边的二进制分类&#x3D;&#x3D;。</p><p>我们的方法包括一个用于预测边缘分数的边缘分数预测网络和一个用于生成实例的超点图割网络。</p><ul><li>在我们的方法中，我们将原始点云过分割成超点，并通过链接坐标空间中k个最近的超点来构造超点图。</li><li>在边分数预测网络中，我们首先对两个相邻节点的局部邻域进行交叉图注意力，以提取局部几何特征来衡量节点的相似性。<ul><li>然后，基于从坐标空间和特征空间学习到的相似性向量，采用边缘分数分支来预测边缘分数。</li><li>此外，我们还提出了一种<strong>几何感知的边缘损失</strong>来训练边缘得分预测网络，该方法迫使同一实例的相邻节点在坐标空间和特征空间中都靠近实例中心。</li></ul></li><li>在超点图割网络中，我们使用学习到的边分数结合节点的语义类别来切割边，以形成object proposals。<ul><li>proposals是通过在超点图上应用广度优先搜索算法来聚合同一连通分量中的节点而得到的。</li><li>在每个proposals中，我们应用双边图注意力来聚合局部几何特征，以提取可区分的特征来预测类别和数十个proposals。</li><li>此外，我们采用掩码学习分支来过滤proposals中的低置信度超点以生成实例。</li></ul></li></ul><p>综上所述，我们提出了一种边缘分数预测网络，该网络学习相邻节点的局部几何特征来生成边缘分数。</p><ul><li>为了训练它，我们提出了一种几何感知的边缘损失，以同时在坐标空间和特征空间保持实例的紧凑性。</li><li>提出了一种超点图割网络，该网络通过在坐标和特征空间中利用双边图注意力来提取可区分的实例特征来生成准确的实例。在ScanNet v2[7]和S3DIS[1]数据集上的大量实验表明，该方法在3D实例分割上达到了最新的性能。在ScanNet v2的在线测试集上，我们的方法在MAP方面达到了55.2%的性能，比目前最好的结果高出4.6%[25]。对于S3DIS，我们的方法在MAP方面比当前最好的结果[37]要好2%以上。</li></ul><h2 id="2-相关工作"><a href="#2-相关工作" class="headerlink" title="2.相关工作"></a>2.相关工作</h2><h3 id="3D语义分割"><a href="#3D语义分割" class="headerlink" title="3D语义分割"></a>3D语义分割</h3><p>从不规则的三维点云中提取特征是3D语义分割的关键。</p><ul><li>qi等人[30]首先提出了<strong>PointNet</strong>，通过多层感知器网络从点集学习逐点特征进行语义分割。在此之后，人们提出了许多改进语义分割性能的努力[24，34，46，15，47，4，2]。早期的基于点的方法[36，43，31，14]设计了各种局部特征聚合策略来提取可区分的逐点特征用于语义分割。</li><li>受成功的2D卷积网络的启发，<strong>基于视图</strong>的方法[23，35，17，19]将点云投影到多个规则的2D视图中，在其中应用规则的2D卷积来提取特征。</li><li>除了基于视点的方法外，<strong>基于体积</strong>的方法[27，39，10，6]首先将点云体素化为规则的3D网格，然后应用3D卷积来提取点云的局部特征。</li><li>为了捕捉点云的局部几何结构，<strong>基于图</strong>的方法[42，22，38，5，16]在点云上构造图形，并利用图的卷积来聚集局部几何信息进行语义分割。</li></ul><h3 id="3D实例分割"><a href="#3D实例分割" class="headerlink" title="3D实例分割"></a>3D实例分割</h3><p>3D实例分割是一项更具挑战性的任务，它进一步需要识别每个实例。目前的方法大致可以分为两类：基于检测的方法和基于聚类的方法。</p><ul><li>基于检测的方法[45，13，26]首先检测点云中每个目标的3D包围盒，然后在每个包围盒上应用掩码预测网络来预测3D实例分割中的目标掩码。在[44]中，提出了一种称为3D-BoNet的3D实例分割框架，该框架直接回归所有实例的3D边界框，并预测每个实例的点级掩码。Yi等人的研究成果。[45]提出了一种生成式形状proposal网络，该网络通过从场景中的噪声观测重建形状来生成proposal，用于3D实例分割。此外，使用几何体和RGB输入，[13]开发了联合2D-3D特征学习网络，该网络结合2D和3D特征以回归3D目标边界框并预测实例masks。</li><li>基于聚类的方法通常使用点相似性[40]、语义地图[11，20]或几何移位[18，3，25，33]来将3D点聚类成对象实例。文献[40]中提出了一种相似度分组proposal网络，通过学习逐点相似度来聚类点以生成实例。[29]提出了一种多任务学习框架，该框架同时学习3D点的语义类别和高维嵌入，以将点聚类为对象实例。在[41]中，引入了一个分割框架来学习语义感知的逐点实例嵌入，用于关联分割点云实例和语义。Hanet al.[11]提出了一种占用率感知的方法来预测每个实例的占用体素数。PointGroup[18]通过使用预测的逐点中心偏移向量和逐点语义标签来聚类点。后续方法[3]采用分层聚集策略进行3D实例分割，首先进行点聚集，将点聚类成初步集合，然后再进行集合聚集，将集合聚类成实例。最近，Vu等人。[37]提出了一种软分组策略，通过将每个点与多个类别相关联来缓解语义预测错误的问题，从而在3D实例分割中获得显著的性能提升。此外，文献[25]还提出了一种语义超点树网络，称为SSTNet，用于分割实例中的点云。该算法首先对语义特征相似的超点进行分组，构建一棵二叉树，然后通过树的遍历和分裂生成实例。为了提高网络的效率，将动态卷积网络与小型变压器网络相结合，提出了一种轻量级的3D实例分割方法[12]。</li></ul><h2 id="3-方法"><a href="#3-方法" class="headerlink" title="3.方法"></a>3.方法</h2><p>基于学习的超点图割方法的概述如图1所示。基于超点图，边分数预测网络(SEC。3.1)从坐标和特征空间中提取边缘嵌入，用于预测边缘得分。之后，超点图割网络(SEC.3.2)通过学习区分实例特征来预测类别和实例分数，从而生成准确的对象实例。最后，在SEC3.3中，我们描述了如何从点云中训练我们的方法和推理实例。</p><p><img src="/pic/SuperPoint1.png" alt="图1：基于学习的超点图切割方法的框架。在给定超点图的情况下，我们首先**提取超点特征(图中省略)**。然后，我们构建了一个边缘分数预测网络，从坐标和特征空间中提取边缘嵌入，以预测边缘分数。最后，基于学习到的边分数，我们开发了一个超点图割网络来生成准确的实例。"></p><h3 id="3-1-边缘得分预测网络"><a href="#3-1-边缘得分预测网络" class="headerlink" title="3.1.边缘得分预测网络"></a>3.1.边缘得分预测网络</h3><p>给定一个原始点云，我们将其过分割成超点，并构造超点图G&#x3D;(V，E)，<strong>其中V表示超点的节点集，E表示边集</strong>。由于超点表示比点表示更粗糙，直接从超点表示学习特征不能有效地捕捉点云的局部几何结构。</p><ul><li>因此，我们在点云上应用子流形稀疏卷积[10]来提取点级特征，并使用点级特征通过平均汇集来初始化超点级特征。</li><li>之后，我们应用边缘条件卷积[32]来提取超点特征，记为F∈R|V|×C，其中C是特征维度。</li></ul><p><a href="https://blog.csdn.net/qq_53086461/article/details/129152257">子流形稀疏卷积解读</a><br><a href="https://blog.csdn.net/yuanmiyu6522/article/details/124943607">边缘条件卷积解读</a></p><h4 id="3-1-1-边缘特征嵌入-Edge-Feature-Embedding"><a href="#3-1-1-边缘特征嵌入-Edge-Feature-Embedding" class="headerlink" title="3.1.1.边缘特征嵌入(Edge Feature Embedding)"></a>3.1.1.边缘特征嵌入(Edge Feature Embedding)</h4><p>一旦我们获得超点特征，边缘得分预测网络就学习边缘嵌入(edge embeddings)来预测分割实例的边缘得分。给定相邻节点(u，v)∈E，期望学习边嵌入可以有效地识别节点u和v是否属于同一实例。为了解决这个问题，我们将<strong>交叉图注意力</strong>应用于双空间(坐标空间和特征空间)中的超点图，以学习超点相似性。学习到的节点u和v的相似度向量被用来形成用于预测边缘得分的边缘嵌入。</p><p><strong>在坐标空间中边缘嵌入。</strong></p><p>为了刻画节点u和v的相似性，</p><ul><li>我们首先将它们移向坐标空间中对应的实例质心。这里，多层感知器(MLP)网络对F进行编码以产生|V|偏移向量O&#x3D;{o1，.。。，o|V|}∈R|V|×3。给出原始超点坐标X&#x3D;{x1，.。。，x|V|}∈R|V|×3，移位的超点坐标Xˆ&#x3D;{ˆx1，.。。，ˆx|V|}由Xˆ&#x3D;X+O得到，这样就增加了属于不同实例的节点之间的几何距离，从而增强了对超点的区分。</li><li>然后，基于移动后的坐标空间，对于节点u，利用其k个最近的超点(即Nu)来构造局部k-NN图Gu。同样，我们可以得到节点v的图GV。</li><li>接着，我们对Gu和Gv进行交叉图注意力，通过学习的特征向量来表征节点的相似性，如图1所示。以节点u为例，交叉图注意力的权重α定义为：</li></ul><p> <img src="/pic/SuperPoint2.png" alt="公式1"></p><ul><li>其中ˆxi和ˆXu是移动后的坐标。请注意，我列举了两个图中的所有2*k个邻neighbors。因此，最终输出特征向量可表示为：</li></ul><p> <img src="/pic/SuperPoint3.png" alt="公式2"></p><ul><li>其中ˆαu，i是权重αu，i在Softmax之后，bi是可学习的偏差。学习的特征向量hu∈RC可以通过自适应地学习两个图上的几何差异来表征几何相似性。同样，我们可以得到另一个节点v的特征向量hv。我们将特征向量hv和hv组合为嵌入在坐标空间中的边：eu，v&#x3D;[hu，hv]。</li></ul><p><strong>特征空间中的边缘嵌入。</strong></p><p>除了考虑坐标空间外，我们还考虑了特征空间来提取可区分的边缘嵌入。</p><ul><li>首先，一个MLP网络对F进行编码以产生嵌入Z∈Rd的初始特征。通过将实例的特征嵌入相互推开，扩大了不同实例在特征空间中的距离。</li><li>给定一对结点(u，v)∈E，我们在特征空间中分别构造了k-NN图ˆGu和ˆGV。通过这种方式，可以预期每个图可以聚合同一实例中的超点。</li><li>然后，我们在ˆGU和ˆGV上执行交叉图注意力来刻画特征空间中节点的相似性。最后得到特征向量ˆhu∈Rc和ˆhv∈Rc。如果u和v属于同一实例，则它们在特征空间中共享相似的k-NN图，从而使得学习到的特征向量hˆu和ˆhv彼此相似。在这里，我们还结合特征向量来得到在特征空间中边缘嵌入：ˆeu，v&#x3D;[ˆhu，ˆhv]。</li></ul><p><strong>边缘分数预测。</strong><br>在获得坐标空间和特征空间中的边缘嵌入后，我们利用一个简单的MLP网络来生成边缘分数，其定义如下：</p><p><img src="/pic/SuperPoint4.png" alt="公式3"></p><p> 其中[·，···]表示串联运算，σ表示Sigmoid函数，du，v表示节点u和v在移位坐标空间中的几何距离。在实验中，如果边得分au，v&gt;0.5，则意味着节点u和v之间的边应该从超点图中截断。我们使用二进制交叉熵损失Ledge来最小化边缘得分。</p><h4 id="3-1-2-几何感知边缘损失（Geometry-Aware-Edge-Loss）"><a href="#3-1-2-几何感知边缘损失（Geometry-Aware-Edge-Loss）" class="headerlink" title="3.1.2.几何感知边缘损失（Geometry-Aware Edge Loss）"></a>3.1.2.几何感知边缘损失（Geometry-Aware Edge Loss）</h4><p>为了训练边分数预测网络，我们使用超点图的几何结构来形成几何感知边损失，如图2所示。</p><p><img src="/pic/SuperPoint5.png" alt="图2：几何感知边缘损失的详细信息。图(A)和(C)中的u和v属于同一实例，而图(B)和(D)中的u和v属于不同的实例。"></p><p>具体地说，给定节点u、v及其对应的实例质心cu和cv，我们通过最小化L2距离du、cu和dv、cv来向它们的实例质心绘制节点。此外，当(u，v)∈E属于同一实例时，期望它们可以通过最小化三角形UVC的面积来协作地移动到相同的实例质心c。如果(u，v)∈E属于不同的实例，但期望它们可以通过最小化三角形uvcu和uvcv的面积来协作地转移到它们自己的实例质心cu和cv。坐标空间中的面积约束写成：</p><p><img src="/pic/SuperPoint6.png" alt="公式4"></p><p>其中i(u，v)是指示函数，如果u和v属于同一实例，则i(u，v)等于1，否则等于0。请注意，“×”表示用于计算三角形面积的矢量的外积。对于来自同一实例的节点u和v，u和v同时靠近公共实例质心。因此，它们在坐标空间中拉得很近，这有助于将u和v组合到同一实例中。对于来自不同实例的节点u和v，u和v分别接近对应的实例质心。因此，它们在坐标空间中被推开，这有助于将u和v分成两个不同的实例。</p><p>同样，我们期望同一实例中的节点通过约束其特征嵌入而在特征空间中是紧凑的。对于(u，v)∈E属于同一实例，我们将u和v的嵌入画向实例的平均嵌入，并将它们拉到彼此之间。对于(u，v)∈E属于不同的实例，我们将u和v的嵌入相互推开。此外，通过增加实例自身的平均嵌入距离，实例彼此推开。因此，特征空间中的约束写成：</p><p><img src="/pic/SuperPoint7.png" alt="公式5"></p><p>其中zu∈Rd和zv∈Rd是特征嵌入。注意，gu∈Rd和gv∈Rd分别指示u和v所属的实例的平均特征嵌入。阈值δ和β设置为0.1和1.5，以确保实例间距离大于实例内距离。最后，几何感知边缘损失被定义为：</p><p><img src="/pic/SuperPoint9.png" alt="公式6"></p><h3 id="3-2-超点图割网络-Superpoint-Graph-Cut-Network"><a href="#3-2-超点图割网络-Superpoint-Graph-Cut-Network" class="headerlink" title="3.2.超点图割网络(Superpoint Graph Cut Network)"></a>3.2.超点图割网络(Superpoint Graph Cut Network)</h3><h4 id="3-2-1-基于超点图割的proposal生成方法"><a href="#3-2-1-基于超点图割的proposal生成方法" class="headerlink" title="3.2.1.基于超点图割的proposal生成方法"></a>3.2.1.基于超点图割的proposal生成方法</h4><p>给定边得分A&#x3D;{au，v}∈R|E|×1，我们提出了一种proposal生成算法，通过<strong>同时利用学习的边得分和预测的节点语义类别(即超点)来生成候选proposal</strong>。具体地说，</p><ul><li>为了减少语义预测误差，我们遵循[37]，采用soft阈值θ将节点与多个类别相关联。给定超点的语义分数S&#x3D;{s1，.。。，S|V||si∈RN，i&#x3D;1，.。。，|V|}，其中N是类的数目，如果si v&gt;θ，则第v个超点可以与第i个类相关联。这样，对于第i类，我们可以对超点图上的一个超点子集Ci进行切片，其中第i类索引上的超点的语义得分高于θ。</li><li>然后，在超点图上，对于边(u，v)∈E，如果节点u∈Ci和v∈Ci，边(u，v)将被保留，否则边将被删除。换句话说，我们去掉了两个语义不同的超点节点之间的边。</li><li>接着，对于超点图上保留的边(u，v)，我们利用边分数au，v来确定是否应该从超点图中切出边。在实验中，切割边缘的阈值设置为0.5。如果边得分高于0.5，则将从超点图中剪切该边。</li><li>最后，我们在超点图上应用<strong>广度优先搜索算法</strong>来聚集同一连通分支中的节点，以生成第i类的proposal。通过这种方式，我们可以通过迭代N个类来生成N个类的proposal。详细信息如算法1所示。</li></ul><p> <img src="/pic/SuperPoint8.png" alt="proposal生成算法"></p><h4 id="3-2-2-proposal嵌入中的双边图注意力"><a href="#3-2-2-proposal嵌入中的双边图注意力" class="headerlink" title="3.2.2.proposal嵌入中的双边图注意力"></a>3.2.2.proposal嵌入中的双边图注意力</h4><p>随着我们获得proposals I&#x3D;{I1，.。。从点云数据出发，通过在坐标空间和特征空间应用注意机制，我们提出了双边图注意来提取生成实例的proposal嵌入。具体地说，给定第i个proposal，我们首先通过平均移位的超点坐标来计算方proposal质心Ci。然后，我们采用相应超点的反距离加权平均来对proposals 质心的嵌入进行插补，其公式如下：</p>]]></content>
      
      
      <categories>
          
          <category> DL </category>
          
      </categories>
      
      
        <tags>
            
            <tag> seg </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Efficient LiDAR Point Cloud Oversegmentation Network</title>
      <link href="/2023/12/01/overseg/"/>
      <url>/2023/12/01/overseg/</url>
      
        <content type="html"><![CDATA[<h1 id="Efficient-LiDAR-Point-Cloud-Oversegmentation-Network"><a href="#Efficient-LiDAR-Point-Cloud-Oversegmentation-Network" class="headerlink" title="Efficient LiDAR Point Cloud Oversegmentation Network"></a>Efficient LiDAR Point Cloud Oversegmentation Network</h1><p><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Hui_Efficient_LiDAR_Point_Cloud_Oversegmentation_Network_ICCV_2023_paper.pdf">论文地址</a><br><a href="https://openaccess.thecvf.com/content/ICCV2023/supplemental/Hui_Efficient_LiDAR_Point_ICCV_2023_supplemental.pdf">补充材料</a><br><a href="https://github.com/fpthink/SuperLiDAR">代码地址(待发布)</a></p><h2 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h2><p>点云过分割是一项具有挑战性的任务，因为它需要产生具有感知意义的点云分区(即超点)。现有的大多数过分割方法都不能有效地从大规模LiDAR点云中生成超点，这是因为过程复杂且效率低下。本文提出了一种简单而高效的端到端LiDAR过分割网络，该网络通过基于低级点嵌入的分组来分割LiDAR点云中的超点。具体地说，</p><ul><li>我们首先从构建的局部邻域中学习点的相似性，通过<strong><strong>局部区分损失</strong></strong>来获得低级点嵌入。</li><li>然后，为了从稀疏的LiDAR点云中生成齐次超点，我们提出了一种同时考虑<strong>点嵌入相似性和点在三维空间中的欧几里德距离</strong>的LiDAR点分组算法。</li><li>最后，我们设计了一个超点求精模块，用于准确地将硬边界点分配到相应的超点。</li></ul><p>在两个大规模室外数据集SemancKITTI和nuScenes上的广泛结果表明，我们的方法在LiDAR过分割方面取得了新的进展。值得注意的是，我们的方法的推理时间比其他方法快100倍。此外，我们将学习到的超点应用到LiDAR语义分割任务中，结果表明，使用超点可以显著提高基线网络的LiDAR语义分割。</p><h2 id="介绍"><a href="#介绍" class="headerlink" title="介绍"></a>介绍</h2><p>在现代自动驾驶汽车中，3D LiDAR传感器可以获取周围物体及其表面特征的精确距离测量，以用于大规模户外场景理解。近年来，LiDAR语义分割[21]得到了广泛的研究，出现了各种方法并取得了令人印象深刻的结果。然而，激光雷达过分割在三维计算机视觉中很少被研究。与LiDAR语义分割不同，<strong>LiDAR过分割输出有感知意义的点云细分。得到的超点是一组点，这些点在对象的局部区域中在语义和几何上是同质的。超点表示法能够自适应、灵活地表示物体的局部几何结构。</strong>因此，研究激光雷达过分割对于基于激光雷达点云的应用是非常有意义的。然而，激光雷达点云的稀疏性、噪声和不规则性给激光雷达过分割带来了巨大的挑战。</p><p>早期的点云过分割方法通常是基于优化的方法。</p><ul><li>Lin等人的研究。[17]将超点分割问题归结为子集选择问题，提出了一种利用点云局部信息通过最小化能量函数来分割超点的启发式算法。</li><li>Guinard等人。[8]将点云过分割问题描述为一个结构化优化问题，并使用手工创建的局部描述符通过贪婪的图割算法生成几何简单的超点[14]。然而，由于LiDAR点云稀疏，计算的手工创建的特征的区分性较差，因此生成的超点无法在相似对象之间产生清晰的边界。</li><li>Landrieu等人。[13]引入了一种深度网络来提取点嵌入，用来代替[14]中的手工特征来分割超点。由于它是一个两阶段的方法，超点分割的处理过程复杂且耗时。</li><li>最近，Huy等人提出了一个新的观点。[11]提出了一种端到端的超点网络，该网络迭代地学习点-超点关联映射以聚类超点。然而，它需要后处理来滤除噪声点。总之，由于程序复杂和效率低下，上述方法不能有效地从大规模LiDAR点云生成超点。</li></ul><p>在本文中，我们提出了一种简单而高效的LiDAR过分割网络SuperLiDAR，它直接从LiDAR点云输出超点，而不需要任何额外的处理过程。超点分割的核心思想是基于<strong>低级点嵌入对点进行分组</strong>。具体地说，为了学习低级点嵌入，</p><ul><li>我们首先将其描述为一个由点云上定义的局部邻域构造的深度度量学习问题。引入局部判别损失，将3D点嵌入到同一物体的局部邻域内，从而保证了点的嵌入是相似的。</li><li>在获得低级点嵌入后，我们提出了一种LiDAR点分组算法，该算法通过广度优先搜索(BFS)对点进行分组以生成超点。利用点嵌入的相似性和三维点坐标的欧几里德距离，应用BFS算法生成紧致超点。</li><li>最后，我们提出了一个超点求精模块，该模块学习硬边界点与其k-最近邻候选超点之间的亲和力。通过将硬边界点与对应的相似度最高的超点进行赋值，可以得到高质量的超点。</li></ul><p>值得注意的是，我们的LiDAR过分割网络可以灵活地与下游任务集成，例如语义分割。为了评估学习的超点的有效性，我们引入了一个简单的<strong>多尺度超点聚合模型</strong>来进行LiDAR语义分割。为了验证该方法的有效性，我们在语义KITTI和nuScenes这两个大规模基准上进行了实验。激光雷达过分割实验表明，该方法不仅达到了最好的性能，而且比其他方法快100倍。此外，LiDAR语义分割实验表明，使用超点可以显著提高基线网络的性能。</p><p>本文的主要贡献如下：</p><ul><li>提出了一种高效的LiDAR过分割网络，用于学习大规模LiDAR点云的超点分割。</li><li>我们的方法在获得最先进的LiDAR过分割性能的同时，比现有的过分割方法快100倍。</li><li>我们证明了所提出的LiDAR过分割网络可以端到端的方式集成到LiDAR语义分割网络中，进一步提高了LiDAR语义分割的性能。</li></ul><h2 id="相关工作"><a href="#相关工作" class="headerlink" title="相关工作"></a>相关工作</h2><p><strong>点云过分割</strong></p><p>现有的点云过分割方法大致可以分为两类：基于优化的方法和基于深度学习的方法。</p><ul><li>基于优化的方法通常使用手工创建的描述符来提取点特征以用于超点分割。<ul><li>Papon等人。[21]是3D数据过分割领域的先驱。提出的体素云连通性分割方法利用颜色和深度信息以及三维几何关系将RGB-D数据分割成超点。</li><li>Lin等人的研究。[17]将超点分割问题描述为一个子集选择问题。基于手工生成的点云局部信息，通过直接最小化能量函数来分割超点。</li><li>Guinard等人。[8]将点云过分割问题视为一个结构化优化问题。他们使用贪婪的图割算法[14]来产生几何简单的超点，其中手工制作的局部描述符(如线性度、平面度、散射度和垂直度)被用来描述每个点的局部几何体。然而，当周围环境的点云中存在具有复杂局部几何结构的物体时，手工构造的特征通常不能提供区分特征来生成高质量的超点，特别是在稀疏的LiDAR点云中。</li></ul></li><li>基于深度学习的方法利用深度网络来提取区分点特征，以提高过分割性能。<ul><li>Guinard等人。[13]将点云过分割作为深度度量学习问题。提出了一种基于图结构的对比度损失来学习辨识点特征。通过使用学习的特征来代替贪婪的图割算法[14]中使用的手工特征，它可以从点云生成比[8]更高质量的超点。然而，由于两阶段超点分割策略，它不能被端到端训练。</li><li>最近，Huy等人提出了一个新的观点。[11]提出了一种端到端的超点网络，通过迭代学习点-超点关联图来聚类超点。尽管如此，聚集的超点在超点边界附近可能会有很多噪声，特别是在稀疏的LiDAR点云中。因此，需要对图像进行后处理来滤除噪声，增加了应用的复杂性和时间成本。</li></ul></li></ul><p>现有的点云过分割方法受到复杂处理过程的限制，不能灵活地应用于下游任务。因此，有必要设计一种高效的、具有可扩展性和灵活性的点云过分割</p><p><strong>激光雷达语义分割</strong></p><p>激光雷达语义分割[30，40，9，6，27，34，42，24，26，10]已经出现了基于不同点云表示的各种方法。</p><ul><li>为了利用传统的2D分割方案，将LiDAR点云投影到图像平面中的<strong>基于投影的方法</strong>，获得渲染图像[16]、快照[2]、切线图像[28]、距离图像[32，33]和鸟瞰图像[18]。虽然基于投影的方法是激光雷达分割的有效方法，但由于空间压缩，不可避免地会丢失空间几何信息。</li><li>为了直接利用3D几何信息，<strong>基于点的方法</strong>[22]通常使用多层感知器网络来处理3D LiDAR点，使用不同设计的局部描述符，包括最大汇集函数[23，31]、自适应加权[29，41]和非局部加权[38，4]。尽管与基于投影的方法相比，基于点的方法具有更高的性能，但由于大规模LiDAR点云中复杂的局部聚集操作和大量的点，因此<strong>基于点的方法通常是耗时的</strong>。</li><li>目前，最先进的技术是<strong>基于体素的方法</strong>，它们使用&#x3D;&#x3D;稀疏卷积[19]&#x3D;&#x3D;来处理大规模LiDAR点云。与传统的三维卷积算法相比，该算法只对非空体素进行卷积运算，大大降低了计算成本和内存消耗。<ul><li>基于SparseConv，不同的方法使用网络结构搜索[25]、多尺度特征聚集[5]、柱面分割和非对称3D卷积网络[43]、距离-点体素融合网络[35]和多模式网络[37]来提高分割性能。虽然基于体素的方法已经取得了令人印象深刻的结果，但体素的质量对其分辨率是敏感的。</li><li>体素大小越大，质量越低。这促使我们探索激光雷达点云的超点表示。</li></ul></li></ul><h2 id="方法"><a href="#方法" class="headerlink" title="方法"></a>方法</h2><h3 id="激光雷达过分割网络"><a href="#激光雷达过分割网络" class="headerlink" title="激光雷达过分割网络"></a>激光雷达过分割网络</h3><p><strong>低级点嵌入</strong></p><p>对于LiDAR过分割，我们使用点云的低级嵌入来分割超点。如图1所示，我们将其描述为一个由点云上定义的局部邻域构造的深度度量学习问题。</p><p><img src="/pic/Overseg1.png" alt="图1.我们的LiDAR过分割网络概述。给定一个LiDAR点云，在利用稀疏3D网络提取点特征后，我们首先学习低级点嵌入。在此基础上，提出了由点云生成超点的LiDAR点分组算法。最后，我们使用超点求精模块将未分配的空闲点分配给相应的超点。"></p><p>具体地说，给定LiDAR点云P&#x3D;{pi}Ni&#x3D;1，我们遵循[37]并使用稀疏3D网络来获得特征地图X∈RN×C，其中N是邻域的数量。我们使用MLP将X映射到低维嵌入F∈RN×D中，通过计算点与其周围点之间的欧几里德距离来构造每个点的局部邻域，表示为N&#x3D;{Ni}Ni&#x3D;1。每个局部邻域Ni是一组三维点，包含位于第i个局部邻域中的点。</p><p>在构造了LiDAR点云的局部邻域后，提出了一种局部判别损失LSP算法，将点特征映射到具有相似局部几何结构的低维嵌入空间。对于第i个局部邻域Ni，我们可以根据点标签，即基本真实语义标签(图1(A)中不同颜色的圆圈)来识别不同的语义部分。在有限范围内构造的局部邻域中，具有相同语义标签的点可视为具有相似的几何结构。局部区分性损失LSP由两个项组成：</p><p><img src="/pic/Overseg2.png" alt="公式1"></p><p>其中，相同项Lident将嵌入的点拉向相应语义部分的平均嵌入，而距离项Ldist将嵌入的点推离其他语义部分。具体表述如下：</p><p><img src="/pic/Overseg3.png" alt="公式2，3"></p><p>其中i(j，k)是指标函数。如果点j属于第k个语义部分，则i(j，k)等于1，否则等于0。Ti是第i个局部邻域Ni中语义部分的个数。此外，Fj∈Rd表示第i点的点嵌入，Zk∈Rd表示包含第i点的对应语义部分的平均值点嵌入。在实验中，阈值α和β分别被设置为0.01和0.2.</p><p><strong>激光雷达点分组</strong></p><p>为了提高激光雷达超点分割的推理速度，提出了一种简单而高效的激光雷达点分组算法。如图1所示，其关键思想是通过应用广度优先搜索(BFS)算法，基于学习的低级点嵌入对点进行分组。</p><p>具体地说，为了在3D空间中生成齐次紧致的超点，我们同时考虑了嵌入空间和3D坐标空间。</p><ul><li>在给定LiDAR点云的情况下，我们首先随机选择一个种子点作为BFS起点，该起点尚未分配给超点。</li><li>然后，根据起始点执行BFS算法，对周围的点进行分组，形成一个超点。程序如图1(B)所示。在BFS过程中，如果一个点可以被分配到相应的超点，那么它应该满足两个约束。<ul><li>对于一个约束，点嵌入之间的欧几里德距离应该小于阈值β，该阈值被认为是用来将点嵌入推开的余量如等式(3)。</li><li>对于另一个约束，点之间的欧几里德距离应该小于阈值γ，该阈值用于保持超点在3D坐标空间中的紧致性。</li><li>注意，在BFS期间，如果超点大小(即，超点内的点数)大于最大大小Nmax，则当前的超点增长过程将被终止。</li><li>在BFS之后，如果生成的超点大小小于最小大小Nmin，则会丢弃该超点。LiDAR点分组算法重复BFS过程，从LiDAR点云生成新的超点，直到它不能满足生成条件。具体步骤如算法1所示。</li></ul></li></ul><p> <img src="/pic/Overseg4.png" alt="激光雷达点分组算法"></p><p>在超点生成中，也可以采用基于聚类的算法，但它们会在超点上引入噪声点。为了消除噪声点，需要进行后处理，这会带来额外的时间成本。相比之下，BFS算法可以在O(N)的时间复杂度下生成没有噪声点的良好超点，其中N是LiDAR点数。请注意，**我们不是在稀疏的3D空间中执行BFS，而是将稀疏的LiDAR点云转换为密集的距离图像(dense range image)**。由于距离图像中邻接点的边缘是相邻像素网格的边缘，这可以大大节省边缘遍历的时间，因此对深度图像进行边界过滤可以有效地减少三维空间的边界过滤时间。因此，BFS的复杂度约为O(N)。</p><p><strong>超点精化</strong></p><p>在LiDAR点分组中，我们丢弃了生成的不符合超点最小尺寸的超点，因此一些LiDAR点仍然没有分配给任何超点。处理这些未指定的LiDAR点的简单方法是将点指定给坐标空间中最近的超点中心。但是，未指定的点大多是边界点。因此，很难根据距离将边界点准确地分配到相应的超点。为了保持超点分割的效率，我们提出了一种简单而有效的超点细化模块，通过学习该点与其K-近邻超点之间的亲和力来准确地将该点分配到相应的超点，如图1所示。</p><p>具体地说，给定一个未分配的LiDAR点i，我们首先根据该点与超点之间的欧几里德距离在3D坐标空间中找到K-近邻超点(Ki)。点i和超点j∈ki之间的亲和力定义为：</p><p><img src="/pic/Overseg5.png" alt="公式4"></p><p>其中，连接(·)表示连接操作。矢量fi−zj和pi−xj分别捕捉嵌入空间和坐标空间中的点和超点之间的差异。请注意，zj∈Rd和xj∈R3是超点的嵌入和坐标。它们是通过平均点坐标和嵌入而获得的。这样，我们就可以得到该点与其K-最近超点之间的亲和力矩阵Ai∈Rk×(4d+1)。之后，我们映射亲和度矩阵以获得亲和度分数，其定义为：</p><p><img src="/pic/Overseg6.png" alt="公式5"></p><p>其中W∈R1×(4d+1)是要学习的权重。最后，我们将softmax函数应用于亲和度得分YI∈RK，以获得亲和度概率。通过将点i分配给概率最高的超点，我们可以从LiDAR点云生成高质量的超点。请注意，第i点的基本事实是具有相同语义标签的最近超点。在训练期间，它由交叉熵损失监督，由LSR表示(见图1(C))。</p><h3 id="基于超点的LiDAR图像分割"><a href="#基于超点的LiDAR图像分割" class="headerlink" title="基于超点的LiDAR图像分割"></a>基于超点的LiDAR图像分割</h3><p>为了显示LiDAR过分割网络的可扩展性和灵活性，我们将学习到的超点应用到LiDAR语义分割任务中。具体地说，我们提出了一个简单的多尺度超点聚合模块，并将其与我们的LiDAR过度分割网络相集成，形成了一个端到端的LiDAR语义分割框架，如图2所示。</p><p><img src="/pic/Overseg7.png" alt="图2.我们的端到端LiDAR语义分割框架概述。"></p><p><strong>多尺度超点聚集</strong></p><p>在LiDAR过分割网络中，稀疏卷积[19]用于提取点的局部特征。然而，基于体素的表示方法不能很好地刻画点云的局部几何结构，从而产生粗特征图X∈RN×C。基于粗略的点特征，我们设计了一个简单的多尺度超点聚集模块，通过融合多尺度局部特征来增强点特征。具体地说，在超点生成过程中，</p><ul><li>我们首先通过调整超点的最小和最大尺寸来获得多尺度超点。对于第i个点，我们可以得到相应的L尺度的超点，用Si1，Si2，.SiL表示。</li><li>然后，我们对相应超点内的点特征采用最大合并函数来聚集超点特征(图2中的三个圆锥)。</li><li>在此之后，超点特征由一系列层组成的MLP独立处理，包括线性、RELU[20]和批归一化[12]。由此产生的超点特征由Ei 1，Ei 2，EiL。最后，融合L尺度的超点特征，聚集点云的多尺度几何信息。生成的第i个点的新特征写为：</li></ul><p><img src="/pic/Overseg8.png" alt="公式6"></p><p>与基于体素表示的粗点特征xi∈RC相比，新的点特征Xˆi∈RC∗(L+1)能够通过多尺度超点精细地刻画点的局部几何结构。在融合后的点特征的基础上，采用分类头来预测点标签。</p><p>本质上，<strong>超点提供了一个自适应邻域</strong>，该邻域是沿点云曲面的几何结构生成的。因此，与基于体素的邻域、基于球查询的邻域和基于k-NN的邻域相比，自适应邻域能够提取更具区分性的局部特征，从而提高了性能。</p><p>损失函数由于所提出的LiDAR语义分割框架是一个端到端的网络，因此可以通过联合损失函数直接进行优化。为了训练它，将联合损失函数定义为：</p><p><img src="/pic/Overseg9.png" alt="公式7"></p><p>其中，LSP是公式1中定义的训练超点的局部区分性损失。LSR和LSEM分别是超点求精和语义分割的交叉熵损失。</p><h2 id="实验"><a href="#实验" class="headerlink" title="实验"></a>实验</h2><h3 id="数据集和指标"><a href="#数据集和指标" class="headerlink" title="数据集和指标"></a>数据集和指标</h3><p>在本文中，对于LiDAR过分割和语义分割，我们使用了两个大规模的室外数据集，即SemancKITTI[1]和nuScenes[3]。数据集和评估指标的详细信息如下：</p><ul><li>SemancKITTI是一款拥有20个语义类别的大型自动驾驶汽车户外标杆。它包含由64波束LiDAR传感器收集的22个序列，其中序列00到10用于训练集(序列08用作验证集)，序列11到22用于在线隐藏测试集。序列00至10具有用于每次扫描的密集语义注释。</li><li>NuScenes包含由32光束LiDAR传感器收集的1000个场景。总共有1000个场景被分为训练(750)、验证(150)和测试(150)集。训练集和验证集被标注了17个语义类别，而测试集的标签被保存以用于在线盲测。</li><li>评估指标为了评估超点的质量，我们<strong>使用Oracle整体准确度(OOA)、边界查全率(BR)和边界精度(BP)作为[13]中定义的评估指标</strong>。OOA分析使用超点来衡量语义分割的理论上界，而BR和BP则衡量超点边界的质量。<ul><li>为了平衡BR和BP，我们报告F1分数，其公式为F1&#x3D;2BP·BR&#x2F;(BR+BP)。</li><li>为了评估LiDAR语义分割的性能，我们使用了均值交集对并集(MIoU)。</li></ul></li></ul><h3 id="网络结构"><a href="#网络结构" class="headerlink" title="网络结构"></a>网络结构</h3><p>我们使用与[2DPASS，37]中相同的稀疏3D网络作为我们的LiDAR过分割网络的主干。</p><ul><li>在激光雷达点分组算法中，将边界条件下距离约束的阈值γ设置为1.5。</li><li>在超点求精模块中，超参数K被设置为5。</li><li>在多尺度超点聚合模块中，我们使用了三种尺度的超点，它们由不同的超点大小阈值控制。</li></ul><h3 id="激光雷达过分割"><a href="#激光雷达过分割" class="headerlink" title="激光雷达过分割"></a>激光雷达过分割</h3><p>对于LiDAR过分割，我们将所提出的SuperLiDAR与SPG[15]、SSP[13]和SPNet[11]进行了比较。请注意，SPG使用手工制作的特征，而SSP和SPNet使用深度特征进行超点分割。我们运行官方代码从激光雷达点云生成超点。超点评估结果是分别在语义KITTI和nuScenes数据集的验证集上计算的。</p><p><strong>SemantiKITTI的结果。</strong></p><p><img src="/pic/Overseg10.png" alt="表1.语义KITTI和nuScenes验证集上生成的超点的比较结果。"></p><p>表1给出了基于语义KITTI数据集的超点分割的定量结果。为了进行公平的比较，不同方法生成的超点的数量保持相似(约1000个超点)。可以看出，我们的方法在四个指标上取得了最好的结果。</p><ul><li>特别是，我们的SuperLiDAR在OOA方面比其他方法高出约4%，这代表了语义分割性能的理论上限。因此，这意味着我们的方法能够以更高的精度生成高质量的超点。</li><li>此外，我们的方法具有较高的BR和BP，这表明我们的方法可以在稀疏的LiDAR点云中生成边界清晰的超点。<ul><li>由于稀疏的LiDAR点云，SPG[15]的手工制作特征的区分性较差，导致性能较差。</li><li>虽然SSP[13]使用了深度特征，但它通过基于优化的方法[14]生成超点，这限制了稀疏LiDAR点云的性能。</li><li>在稀疏点云中，基于聚类的方法SPNet[11]生成的超点很可能包含噪声，从而影响性能。</li></ul></li><li>在图3中，我们展示了不同方法在不同超点数目下的性能曲线。可以观察到，我们的方法的曲线在四个度量方面都高于其他方法。超点越多，超点越小，反之亦然。结果表明，该方法可以生成不同大小的高质量超点。</li></ul><p><img src="/pic/Overseg11.png" alt="图3.语义KITTI验证集上不同方法的性能"></p><p><strong>nuScenes的结果</strong></p><p>nuScenes数据集上超点分割的定量结果显示在表1中.请注意，不同方法生成的超点数量保持相同(<strong>约400个超点</strong>)。与SemancKITT的64波束点云相比，nuScenes的32波束点云更加稀疏。从表中可以看出，我们的方法在所有四个指标上仍然取得了最好的结果。在nuScenes数据集上的实验结果进一步表明，该方法可以有效地生成稀疏LiDAR点云中的高质量超点。</p><p><strong>可视化结果</strong></p><p>在图4中，我们展示了不同过分割方法的可视化结果。可以观察到，我们的SuperLiDAR生成的超点可以有效地区分道路和人行道。注意，<strong>道路和人行道几乎在同一水平面上，因此如果没有颜色信息</strong>，很难区分它们。尽管如此，我们的方法仍然能够学习区分点特征，利用局部区分性损失来分离它们。</p><p><img src="/pic/Overseg12.png" alt="图4.在语义KITTI(第一行)和nuScenes(第二行)的验证集上生成的超点的可视化"></p><p><strong>时间代价</strong></p><p>推理时间是超点分割的一个重要标准。</p><ul><li>对于基于优化的方法SPG[15]，我们使用单个Corei5 CPU来计算推理时间。</li><li>对于基于学习的方法SSP[13]、SPNet[11]和我们的SuperLiDAR，我们使用单个NVIDIA RTX 3090来运行基于PyTorch深度学习平台的代码。</li></ul><p>表2报告了不同方法在语义KITTI验证数据集的每次扫描上计算的平均推理时间。可以看出，我们的方法的推理时间仅为72ms，比其他方法快100倍。虽然SSP和SPNET使用网络来提取点特征，但它们也将手工制作的点特征馈送到网络中，导致数据处理时间较长。请注意，SSP和SPNET的数据处理相同，因此它们的数据处理时间相同。由于SPG和SSP采用相同的基于优化的方法来生成超点，所以它们的超点生成时间较长。由于sPNET的超点生成采用迭代策略，因此比我们的方法耗时更多。</p><p><img src="/pic/Overseg13.png" alt="表2.不同方法在语义KITTI验证集上的推理时间请注意，推理时间包括数据处理和超点生成。"></p><p><strong>消融研究</strong></p><p>在低级点嵌入学习中，我们在<strong>距离图像(range image)<strong>中而不是在3D空间中构造局部邻域，并在语义KITTI数据集上进行实验。表3展示了在三维空间中构建邻域的结果。可以观察到，建筑街区在3D空间中的表现(“Build nei.在3D空间中“)比在距离图像(”Default(SuperLiDAR)“)中差。与3D空间相比，距离像中的点更加均匀。由此得到的稠密邻域对应用</strong>局部区分损失</strong>学习好点嵌入是有益的。</p><p><img src="/pic/Overseg14.png" alt="表3.不同设置下的消融研究结果。"></p><p>为了解决LiDAR点分组后未分配点的问题，提出了一种超点细化模块。为了验证其有效性，我们通过将点分配到坐标空间中最近的超点中心来替换超点求精模块。表3结果(“无超点参考”)低于使用超点细化模块(“Default(SuperLiDAR)”)。由于简单的距离准则，硬边界点不能有效地分配给相应的超点。通过自适应地学习点和超点之间的亲和力，我们可以更准确地将点分配到相应的超点。</p><p>为了解决LiDAR点分组后未分配点的问题，提出了一种超点细化模块。为了验证其有效性，我们通过将点分配到坐标空间中最近的超点中心来替换超点求精模块。在选项卡中。3、结果(“无超点参考”)低于使用超点细化模块(“Default(SuperLiDAR)”)。由于简单的距离准则，硬边界点不能有效地分配给相应的超点。通过自适应地学习点和超点之间的亲和力，我们可以更准确地将点分配到相应的超点。</p><h3 id="LiDAR语义分割"><a href="#LiDAR语义分割" class="headerlink" title="LiDAR语义分割"></a>LiDAR语义分割</h3><p>对于LiDAR语义分割，我们在语义KITTI和nuScenes数据集上进行了实验，以验证学习超点的有效性。我们的LiDAR语义分割框架基于LiDAR过度分割网络，并使用多尺度超点聚合模块进行语义预测。我们使用LiDAR过分割网络的主干(即稀疏3D网络)作为基线。由于我们的基线是在[37]中使用的稀疏3D网络，因此我们直接将[37]的基线结果作为我们的方法在语义KITTI和nuScenes数据集上的基线结果。</p><p><img src="/pic/Overseg15.png" alt="表4.语义分割在语义KITTI和nuScenes测试集上的结果。结果在2022年11月11日之前进行了比较。“L”和“C”分别表示激光雷达和相机。请注意，我们只列出已发表作品的结果。"></p><p><strong>SemantiKITTI的结果。</strong></p><p>表4给出了语义分割在语义KITTI在线测试集上的实验结果。可以观察到，我们的SuperLiDAR的MIU值比基线高出2%。此外，图5显示了基线和我们的方法的可视化结果(第一行)。实验结果表明，利用学习的超点来增强点云的局部几何信息，可以进一步提高语义分割的性能。由于采用了复杂的点-体素或距离-点-体素融合框架，RPVNet[35]和AF2-S3Net[5]的性能要高于我们的基于体素的方法。此外，2DPASS[37]是一种多模式方法，它使用2D图像和知识提取来实现更高的性能和更短的推理时间。由于采用了简单有效的多尺度超点聚合模型，在不增加太多时间开销的情况下，有效地提高了基线的性能。</p><p><img src="/pic/Overseg16.png" alt="图5.语义分割在语义KITTI(第一行)和nuScenes(第二行)的验证集上的可视化"></p><p><strong>NuScenes的结果。</strong></p><p>表4给出了在nuScenes在线测试集上的语义分割结果。可以观察到，我们的SuperLiDAR的MIU值超过了基线约1%。此外，图5显示了基线和我们的方法的可视化结果(第二行)。由于nuScenes的点云比SemancKITTI的点云更稀疏，定量和可视化结果表明，学习的超点可以有效地提高分割性能。与仅使用LiDAR数据的方法相比，该方法可以在更短的推理时间内获得更高的性能。请注意，即使我们只使用LiDAR数据，我们的方法的性能也要好于使用LiDAR点云和相机图像的PMF[44]。此外，多模式方法2D3DNet[7]和2DPASS[37]采用复杂的多尺度融合网络以获得更高的性能。</p><h2 id="结论"><a href="#结论" class="headerlink" title="结论"></a>结论</h2><p>提出了一种高效的端到端LiDAR过分割网络，用于从LiDAR点云中分割超点。通过使用所提出的LiDAR点分组算法，我们的方法可以从稀疏的LiDAR点云中生成高质量的超点。值得注意的是，我们的方法在激光雷达过分割方面取得了新的进展，并且推理时间比现有方法快100倍。此外，在LiDAR语义分割上，通过使用超点，我们的方法可以显著提高两个大规模基准测试(即SemancKITTI和nuScenes)的基线结果。我们相信，提出的高效LiDAR过分割网络可以应用于更多的下游任务，如3D检测和跟踪。</p>]]></content>
      
      
      <categories>
          
          <category> DL </category>
          
      </categories>
      
      
        <tags>
            
            <tag> seg </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>BEVDet系列（持续更新）</title>
      <link href="/2023/11/30/bevdet-list/"/>
      <url>/2023/11/30/bevdet-list/</url>
      
        <content type="html"><![CDATA[<h1 id="BEVDet系列"><a href="#BEVDet系列" class="headerlink" title="BEVDet系列"></a>BEVDet系列</h1><p><a href="https://zhuanlan.zhihu.com/p/557613388">BEVDet系列源码解读</a><br><a href="https://blog.csdn.net/guangqianzhang/article/details/129615932">BEVDet网络结构</a><br><a href="https://zhuanlan.zhihu.com/p/492106899">BEVDet4D 强大而不失优雅的三维目标检测范式</a><br><a href="https://zhuanlan.zhihu.com/p/638452999">BEVDet4D讲解</a></p><h2 id="BEVDet4D-Exploit-Temporal-Cues-in-Multi-camera-3D-Object-Detection"><a href="#BEVDet4D-Exploit-Temporal-Cues-in-Multi-camera-3D-Object-Detection" class="headerlink" title="BEVDet4D: Exploit Temporal Cues in Multi-camera 3D Object Detection"></a>BEVDet4D: Exploit Temporal Cues in Multi-camera 3D Object Detection</h2><p>BEVDet4D首次尝试访问时间域中的丰富信息。它只是通过保留先前帧中的中间BEV特征来扩展朴素的BEVDet。然后通过空<strong>间对齐操作和拼接操作</strong>将保留的特征与当前帧中对应的特征进行融合。除此之外，论文保持了框架的大部分其他细节不变。这样，论文只在推理过程中投入了可以忽略的额外计算预算，<strong>同时使范例能够通过查询和比较两个候选特征来访问时间线索</strong>。虽然构建BEVDet4D的框架很简单，但构建其健壮的性能并不是一件容易的事情。BEVDet4D的空间对齐操作和学习目标应精心设计，以配合优雅的框架，从而<strong>简化速度预测任务</strong>，并获得优异的<strong>泛化性能</strong>。</p><p><strong>BEVFormer的速度精度是通过融合多个相邻帧(即总共4帧)的特征来实现的</strong>，这类似于大多数基于LiDAR的方法[2，46]，其中包含来自多次扫描的点。这与拟议的BEVDet4D有根本的不同，<strong>BEVDet4D只使用两个相邻的帧</strong>，并在更优雅的图案中实现了更高的速度精度。</p><p><img src="/pic/BEVDet1.png" alt="网络架构图"></p><p>如上图所示，BEVDet4D的总体框架建立在BEVDet基线之上，该基线由四种模块组成：<strong>图像-视图编码器、视图转换器、BEV编码器和特定于任务的头部</strong>（an image-view encoder, a view transformer, a BEV encoder, and a task-specific head）。</p><h3 id="网络架构图"><a href="#网络架构图" class="headerlink" title="网络架构图"></a>网络架构图</h3><p>为了利用时间线索，BEVDet4D通过保<strong>留由前一帧中的视图转换器生成的&#x3D;&#x3D;BEV特征&#x3D;&#x3D;来扩展基线</strong>。然后将保留的特征与当前帧中的特征合并。在此之前，将进行对齐操作以简化学习目标。论文应用一个简单的串联操作来合并这些特征，以验证BEVDet4D范例。更复杂的融合策略在本文中没有被开发出来。</p><p>此外，<strong>视图转换器生成的特征是稀疏的</strong>，对于后续模块来说过于粗糙，无法利用时间线索。因此，在时间融合之前，采用&#x3D;&#x3D;额外的BEV编码器&#x3D;&#x3D;来调整候选特征。实际上，额外的BEV编码器由<strong>两个朴素的残差单元</strong>[13]组成，其通道号设置为与输入特征相同。</p><h3 id="简化速度学习任务"><a href="#简化速度学习任务" class="headerlink" title="简化速度学习任务"></a>简化速度学习任务</h3><p><img src="/pic/BEVDet2.png" alt="说明对齐操作的效果。在没有对齐操作(即第一行)的情况下，需要以下模块来研究与自我运动相关的对象运动的更复杂的分布。通过在第二行中应用对齐操作，可以简化学习目标"></p><p>由于有多帧信息，不再直接预测目标速度，而是<strong>预测目标在两个连续帧中的位移</strong>。即将速度预测简化为移除了时间因素的，通过两个BEV特征间的差异来衡量的位置偏移预测。</p><p><img src="/pic/BEVDet3.png" alt="自车空间的BEV特征图"></p><p>此外，BEVDet4D使得位移与ego自身的运动无关进一步简化学习任务(因为ego车辆自身的运动会使目标运动复杂化)。因为ego的运动会使得在global坐标系中static的物体在ego坐标系中变为动态物体。由于BEV特征的感受野是围绕ego对称的，同样，ego运动会使连续两帧对应的BEV特征的global坐标系下的感受野变化。</p><p><img src="/pic/BEVDet4.png" alt="没有对齐下静止物体ego坐标下的位移差"></p><p>上图可以看到，如果直接concat两帧特征，则最后得到的两个特征的位移和ego运动相关(红色方框)，因此需要先将上一帧乘以(红色方框)的逆来消除自车(ego)运动的影响。如下图所示</p><p><img src="/pic/BEVDet5.png" alt="对齐下静止物体ego坐标下的位移差"></p><p>学习目标设为在ego坐标系下当前帧物体的运动，与ego运动无关。</p><h3 id="鲁棒性实验"><a href="#鲁棒性实验" class="headerlink" title="鲁棒性实验"></a>鲁棒性实验</h3><p><img src="/pic/BEVDet6.png" alt="表：nuScenes Val集合的消融研究结果。对齐操作包括旋转(R)和平移(T)。Extra表示额外的Bev编码器。Aug表示在选择相邻帧时在时间维度上的增大。"></p><p>如何建立BEVDet4D的鲁棒性能。即在view-transformer中通过调整伪点云来实现空间对齐操作。结果如上表所示，主要测试了以下几种空间对齐的方案。</p><ul><li>方案A，即直接将当前帧的特征与先前帧拼接。模型性能大幅降低，尤其是在速度和位移预测方面。推测可能是由于ego运动导致静态和动态目标更复杂导致，因此需要移除ego运动。</li><li>方案B，只进行平移对齐操作。模型可以利用位置对齐的候选特征来更好的感知静态目标，同时移除了ego运动简化了速度预测。此时，位移误差已经低于baseline，但速度误差仍高于baseline，这可能是由于相邻帧时间间隔不一致导致位移分布与速度分布相差较远。</li><li>方案C，进一步移除时间因素，让模型直接预测两帧间目标的位移。简化了学习目标，使训练更鲁棒。大幅降低速度预测误差。</li><li>方案D，在拼接两个候选特征前引入一个额外的BEV编码器。轻微地增加了大约2.8%的计算成本，推理速度基本保持不变，但是使得模型在C上有了更全面的提升。</li><li>方案E，在D的基础上通过调节速度预测损失的权重，进一步降低了速度误差。</li><li>方案F，在对齐操作时考虑ego位姿的旋转方差。进一步降低速度误差，表明更精确的对齐操作能够提高速度预测精度。</li><li>方案G，探索当前帧与参考帧之间最优的时间间隔。采用12Hz的数据，两帧时间间隔为T≈0.083s，训练时选择三个不同的时间间隔，并通过在测试比较它们来判断调整方向，这样可以避免超参搜索时的训练干扰。结果见下图，组价间隔为15T，设置为本文默认的测试时间间隔。训练时在[3T,27T]中随机采样来进行数据增强。这种方法进一步降低了速度误差。</li></ul><p><img src="/pic/BEVDet7.png" alt="当前帧和参考帧之间的时间间隔。以相同颜色绘制的点处于相同的训练配置中"></p><h3 id="代码分析"><a href="#代码分析" class="headerlink" title="代码分析"></a>代码分析</h3><p>以下是对BEVDet4D模型代码的详细介绍：</p><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token decorator annotation punctuation">@DETECTORS<span class="token punctuation">.</span>register_module</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token keyword">class</span> <span class="token class-name">BEVDet4D</span><span class="token punctuation">(</span>BEVDet<span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre><p>这是一个注册了BEVDet4D类的检测器模块。BEVDet4D继承自BEVDet，表示BEVDet4D是BEVDet的一种变体。</p><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span>             pre_process<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">,</span>             align_after_view_transfromation<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">,</span>             num_adj<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">,</span>             with_prev<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span>             <span class="token operator">**</span>kwargs<span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token builtin">super</span><span class="token punctuation">(</span>BEVDet4D<span class="token punctuation">,</span> self<span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token operator">**</span>kwargs<span class="token punctuation">)</span>    <span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>初始化方法接受多个参数，包括预处理网络的配置（pre_process），是否在视图变换后对齐BEV特征（align_after_view_transformation），相邻帧的数量（num_adj），以及其他超参数。此外，它调用了父类（BEVDet）的初始化方法。</p><pre class="line-numbers language-python" data-language="python"><code class="language-python">self<span class="token punctuation">.</span>align_after_view_transfromation <span class="token operator">=</span> align_after_view_transfromationself<span class="token punctuation">.</span>num_frame <span class="token operator">=</span> num_adj <span class="token operator">+</span> <span class="token number">1</span>self<span class="token punctuation">.</span>with_prev <span class="token operator">=</span> with_prev<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre><p>这里设置了BEV特征是否在视图变换后对齐（align_after_view_transformation）、相邻帧的数量（num_adj）以及是否使用前一帧的特征（with_prev）。</p><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token decorator annotation punctuation">@force_fp32</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token keyword">def</span> <span class="token function">shift_feature</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> <span class="token builtin">input</span><span class="token punctuation">,</span> trans<span class="token punctuation">,</span> rots<span class="token punctuation">,</span> bda<span class="token punctuation">,</span> bda_adj<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre><p><code>shift_feature</code> 方法用于将输入的特征进行位移，包括旋转、平移和BEV数据增强。</p><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">def</span> <span class="token function">prepare_bev_feat</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> img<span class="token punctuation">,</span> rot<span class="token punctuation">,</span> tran<span class="token punctuation">,</span> intrin<span class="token punctuation">,</span> post_rot<span class="token punctuation">,</span> post_tran<span class="token punctuation">,</span>                     bda<span class="token punctuation">,</span> mlp_input<span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre><p><code>prepare_bev_feat</code> 方法准备BEV特征，包括图像编码、视图变换和预处理。</p><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">def</span> <span class="token function">extract_img_feat_sequential</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> inputs<span class="token punctuation">,</span> feat_prev<span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><p><code>extract_img_feat_sequential</code> 方法提取顺序图像特征，包括对先前特征进行对齐和使用BEV编码器。</p><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">def</span> <span class="token function">prepare_inputs</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> inputs<span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><p><code>prepare_inputs</code> 方法准备输入数据，将输入图像和相关信息分割为每一帧的数据。</p><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">def</span> <span class="token function">extract_img_feat</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span>                     img<span class="token punctuation">,</span>                     img_metas<span class="token punctuation">,</span>                     pred_prev<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">,</span>                     sequential<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">,</span>                     <span class="token operator">**</span>kwargs<span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p><code>extract_img_feat</code> 方法是特征提取的主要入口，根据参数决定是提取顺序特征还是一般特征。</p><p>上述是对BEVDet4D模型代码的主要部分的详细介绍，包括初始化、特征提取等</p>]]></content>
      
      
      <categories>
          
          <category> DL </category>
          
      </categories>
      
      
        <tags>
            
            <tag> bev </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>弱监督下的BEV检测和占据预测(部分汇总)</title>
      <link href="/2023/11/29/weakly-supervised-det/"/>
      <url>/2023/11/29/weakly-supervised-det/</url>
      
        <content type="html"><![CDATA[<h1 id="弱监督下的BEV检测和占据预测-部分汇总"><a href="#弱监督下的BEV检测和占据预测-部分汇总" class="headerlink" title="弱监督下的BEV检测和占据预测(部分汇总)"></a>弱监督下的BEV检测和占据预测(部分汇总)</h1><h2 id="Weakly-Supervised-Class-agnostic-Motion-Prediction-for-Autonomous-Driving"><a href="#Weakly-Supervised-Class-agnostic-Motion-Prediction-for-Autonomous-Driving" class="headerlink" title="Weakly Supervised Class-agnostic Motion Prediction for Autonomous Driving"></a>Weakly Supervised Class-agnostic Motion Prediction for Autonomous Driving</h2><p><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Li_Weakly_Supervised_Class-Agnostic_Motion_Prediction_for_Autonomous_Driving_CVPR_2023_paper.pdf">论文链接</a></p><p><a href="%E5%BE%85%E5%85%AC%E5%B8%83">代码链接</a></p><p>了解动态环境中的运动行为对于自动驾驶至关重要，这使得激光雷达点云中与类别无关的运动预测受到越来越多的关注。室外场景通常可以分解为运动前景和静态背景，这使得我们能够将运动理解与场景解析联系起来。</p><p>基于这一观察结果，我们研究了一种新的弱监督运动预测范式，其中完全或部分(1%，0.1%)注释的前景&#x2F;背景二值掩模用于监督，而不是使用昂贵的运动注释。为此，我们提出了一种两阶段弱监督方法，</p><ul><li>其中在第一阶段用不完全二值掩码训练的分割模型将通过提前估计可能的运动前景来促进第二阶段运动预测网络的自监督学习。</li><li>此外，对于稳健的自监督运动学习，我们通过利用多帧信息并显式地抑制潜在的离群点来设计一致性感知的切角距离损失。</li></ul><p>综合实验表明，在完全或部分二值掩码作为监督的情况下，我们的弱监督模型比自监督模型有较大幅度的提高，性能与一些监督模型相当。</p><p> <img src="/pic/weakly4.png" alt="网络架构"></p><p>上图是两阶段弱监督运动预测方法概述。在阶段1中，我们使用部分注释的掩码训练前景&#x2F;背景(FG&#x2F;BG)分割网络PreSegNet。在阶段2中，我们训练了一个运动预测网络WeakMotionNet，它以一系列同步的Bev图作为输入，预测每个细胞的FG&#x2F;BG类别Xfb和未来的运动位移Xmot。在没有运动数据的情况下，我们从Stage1训练的PreSegNet生成FG&#x2F;BG点，并使用一致性感知切角损失来以自监督的方式训练WeakMotionNet的运动预测头部。</p><p> <img src="/pic/weakly5.png" alt="在nuScenes测试集上的运动预测评估结果"></p><p>在表1中，我们将我们的弱监督方法与各种基于nuScenes的SOTA运动预测方法进行了比较。PillarMotion是最好的自我监督方法，它利用现成的光流估计网络和额外的2D图像进行训练。在不使用图像或光流的任何知识的情况下，我们的模型通过1%或0.1%的注释FG&#x2F;BG面具训练，在所有评估指标上都比自我监督的PillarMotion高出约35%。比较我们的弱监督模型和全监督模型，我们观察到我们的模型在慢速和快速组上都比FlowNet3D[25]、HPLFlowNet[14]和PointRCNN[34]表现得更好。特别是，在快速组上，我们的模型比完全监督的场景流模型FlowNet3D和HPLFlowNet分别高出约70%和50%。比较表明，弱监督方法在标注工作量和性能之间取得了很好的折衷，缩小了与完全监督方法的差距。</p><h2 id="Weakly-Supervised-3D-Object-Detection-from-Point-Clouds"><a href="#Weakly-Supervised-3D-Object-Detection-from-Point-Clouds" class="headerlink" title="Weakly Supervised 3D Object Detection from Point Clouds"></a>Weakly Supervised 3D Object Detection from Point Clouds</h2><p><a href="https://arxiv.org/abs/2007.13970">论文链接</a><br><a href="https://github.com/Zengyi-Qin/Weakly-Supervised-3D-Object-Detection?utm_source=catalyzex.com">代码链接</a><br><a href="https://zhuanlan.zhihu.com/p/411282095">讲解链接</a><br><a href="https://zhuanlan.zhihu.com/p/411282095">讲解链接2</a></p><p> <img src="/pic/weakly1.png" alt="网络架构"></p><p>第一个关键组件是无监督的三维对象建议模块(UPM)，它基于归一化的点云密度选择三维锚点，生成潜在的3D框</p><p>第二个组件是一个跨模态转移学习模块，通过利用在图像数据集上预先训练的教师模型，它将信息，包括对象分类和旋转回归，从图像数据集转移到基于点云的三维物体检测器中，对建议进行分类和改进，以产生最终的预测 （能不能从Kitti到waymo？）</p><p>其中激光雷达扫描仪并不需要提供输入点云，而输入点云也可以从单目图像或一对立体图像中获得。假设每一帧的点云在训练集中都有一个成对的图像，但在只需要点云的测试时并不需要这一点</p><h2 id="Weakly-Supervised-3D-Object-Detection-from-Lidar-Point-Cloud"><a href="#Weakly-Supervised-3D-Object-Detection-from-Lidar-Point-Cloud" class="headerlink" title="Weakly Supervised 3D Object Detection from Lidar Point Cloud"></a>Weakly Supervised 3D Object Detection from Lidar Point Cloud</h2><p><a href="https://arxiv.org/abs/2007.11901">论文链接</a><br><a href="https://github.com/hlesmqh/WS3D">代码链接</a><br><a href="https://zhuanlan.zhihu.com/p/379025799">讲解链接</a></p><p>弱监督的3D点云目标检测，训练数据是少量的弱标签（BEV目标中心点）+少量的kitti-groundtruth，效果能与全监督性能相近，甚至更好。基于此，还做了一个自动标注器。</p><p> <img src="/pic/weakly2.png" alt="弱监管的数据注释策略"></p><p>原先的标注过程：标注者首先借助相机图像中的视觉内容在三维场景中查找对象，然后标注一个粗糙的立方体和方向箭头，最后，最佳标注（上图(c)）通过逐步调整正交视图中投影的二维框获得。尽管标注的结果质量很高，但是过程耗时又耗力。</p><p>作者的弱监督数据只包含带对象中心注释的BEV map，这可以很容易地获得。</p><p>具体来说，标注者首先粗略地点击相机前视图上的目标，然后放大BEV map，并显示初始点击周围的区域，以获得更准确的标注点。由于这个注释过程没有引用任何三维视图，所以它非常简单和快速；大多数注释只用通过点击两次按钮即可完成。这个标注包含的信息很弱，没有y轴中的高度和长方体的大小信息。</p><p> <img src="/pic/weakly3.png" alt="网络架构"></p><p> 整体结构如图所示。由两个阶段组成。第一阶段由圆柱形三维提案生成结果，第二阶段进行立方体预测。最后产生结果。</p><h2 id="FGR-Frustum-Aware-Geometric-Reasoning-for-Weakly-Supervised-3D-Vehicle-Detection"><a href="#FGR-Frustum-Aware-Geometric-Reasoning-for-Weakly-Supervised-3D-Vehicle-Detection" class="headerlink" title="FGR: Frustum-Aware Geometric Reasoning for Weakly Supervised 3D Vehicle Detection"></a>FGR: Frustum-Aware Geometric Reasoning for Weakly Supervised 3D Vehicle Detection</h2><p><a href="https://arxiv.org/pdf/2105.07647.pdf">论文链接</a><br><a href="https://github.com/weiyithu/FGR">代码链接</a></p><h2 id="WeakM3D-Towards-Weakly-Supervised-Monocular-3D-Object-Detection"><a href="#WeakM3D-Towards-Weakly-Supervised-Monocular-3D-Object-Detection" class="headerlink" title="WeakM3D: Towards Weakly Supervised Monocular 3D Object Detection"></a>WeakM3D: Towards Weakly Supervised Monocular 3D Object Detection</h2><p><a href="https://arxiv.org/abs/2203.08332">论文链接</a><br><a href="https://github.com/SPengLiang/WeakM3D?utm_source=catalyzex.com">代码链接</a></p><p>单目 3D 物体检测是 3D 场景理解中最具挑战性的任务之一。由于单目图像的不适定性质，现有的单目 3D 检测方法高度依赖于 LiDAR 点云上手动注释的 3D 框标签的训练。这个注释过程非常费力且昂贵。为了摆脱对 3D 框标签的依赖，在本文中，我们探索了弱监督的单目 3D 检测。具体来说，</p><ul><li>我们首先检测图像上的 2D 框。</li><li>然后，我们采用生成的2D框来选择相应的RoI LiDAR点作为弱监督。</li><li>最终，我们采用网络来预测 3D 框，它可以与相关的 RoI LiDAR 点紧密对齐。</li></ul><p>该网络是通过最小化我们新提出的 3D 框估计和相应的 RoI LiDAR 点之间的 3D 对齐损失来学习的。我们将说明上述学习问题的潜在挑战，并通过在我们的方法中引入几种有效的设计来解决这些挑战。</p>]]></content>
      
      
      <categories>
          
          <category> DL </category>
          
      </categories>
      
      
        <tags>
            
            <tag> bev </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>BEVFormer学习总结（结合代码）</title>
      <link href="/2023/11/29/bevformer-learning/"/>
      <url>/2023/11/29/bevformer-learning/</url>
      
        <content type="html"><![CDATA[<h1 id="BEVFormer学习总结（结合代码）"><a href="#BEVFormer学习总结（结合代码）" class="headerlink" title="BEVFormer学习总结（结合代码）"></a>BEVFormer学习总结（结合代码）</h1><p><a href="https://arxiv.org/pdf/2203.17270.pdf">论文链接</a><br><a href="https://drive.google.com/file/d/1dKnD6gUHhBXZ8gT733cIU_A7dHEEzNTP/view">中文论文链接</a><br><a href="https://github.com/fundamentalvision/BEVFormer">代码链接</a><br><a href="https://zhuanlan.zhihu.com/p/543335939">万字长文理解BEVFormer</a><br><a href="https://www.bilibili.com/video/BV1rK411o7PS/?spm_id_from=333.788&vd_source=2055b62125c0277a0d6878f41c89fec2">手撕BEVFormer视频</a><br><a href="https://blog.csdn.net/weixin_42108183/article/details/128426721?spm=1001.2014.3001.5501">BEVFormer代码流程梳理1</a><br><a href="https://blog.csdn.net/weixin_42108183/article/details/128433381?spm=1001.2014.3001.5501">BEVFormer代码流程梳理2</a><br><a href="https://ttxsai.blog.csdn.net/article/details/123450282?spm=1001.2101.3001.6650.3&utm_medium=distribute.pc_relevant.none-task-blog-2~default~CTRLIST~Rate-3-123450282-blog-122300131.235%5Ev38%5Epc_relevant_anti_vip_base&depth_1-utm_source=distribute.pc_relevant.none-task-blog-2~default~CTRLIST~Rate-3-123450282-blog-122300131.235%5Ev38%5Epc_relevant_anti_vip_base&utm_relevant_index=6">nuscenes数据集介绍</a><br><a href="https://zhuanlan.zhihu.com/p/48508221">Transformer学习笔记</a><br><a href="https://zhuanlan.zhihu.com/p/48508221">Vistion Transformer学习笔记</a><br><a href="https://zhuanlan.zhihu.com/p/657666107">Vistion Transformer学习笔记</a><br><a href="https://blog.csdn.net/weixin_42108183/article/details/128680716?spm=1001.2014.3001.5501">DeformableDETR原理+代码解析1</a><br><a href="https://zhuanlan.zhihu.com/p/612756240">DeformableDETR原理+代码解析2</a><br><a href="https://zhuanlan.zhihu.com/p/596303361">DeformableDETR原理+代码解析3</a><br><a href="https://blog.csdn.net/qq_52053775/article/details/126468394">DeformableDETR原理解析</a></p><h2 id="介绍"><a href="#介绍" class="headerlink" title="介绍"></a>介绍</h2><p>该篇论文提出了一个采用纯视觉（camera）做感知任务的算法模型 BEVFormer。BEVFormer 通过提取环视相机采集到的图像特征，并将提取的环视特征通过模型学习的方式转换到 BEV 空间（模型去学习如何将特征从图像坐标系转换到BEV坐标系），从而实现 3D 目标检测和地图分割任务，并取得了 SOTA 的效果， 利用询问向量来查找空间&#x2F;时间域,并相应地聚合时空信息,因此有利于更强的感知任务表征。</p><h2 id="官方的模型仓库"><a href="#官方的模型仓库" class="headerlink" title="官方的模型仓库"></a>官方的模型仓库</h2><table><thead><tr><th align="center">Backbone</th><th align="center">Method</th><th align="center">Lr Schd</th><th align="center">NDS</th><th align="center">mAP</th><th align="center">memroy</th><th align="center">Config</th><th align="center">Download</th></tr></thead><tbody><tr><td align="center">R50</td><td align="center">BEVFormer-tiny_fp16</td><td align="center">24ep</td><td align="center">35.9</td><td align="center">25.7</td><td align="center">-</td><td align="center"><a href="projects/configs/bevformer_fp16/bevformer_tiny_fp16.py">config</a></td><td align="center"><a href="https://github.com/zhiqi-li/storage/releases/download/v1.0/bevformer_tiny_fp16_epoch_24.pth">model</a>&#x2F;<a href="https://github.com/zhiqi-li/storage/releases/download/v1.0/bevformer_tiny_fp16_epoch_24.log">log</a></td></tr><tr><td align="center">R50</td><td align="center">BEVFormer-tiny</td><td align="center">24ep</td><td align="center">35.4</td><td align="center">25.2</td><td align="center">6500M</td><td align="center"><a href="projects/configs/bevformer/bevformer_tiny.py">config</a></td><td align="center"><a href="https://github.com/zhiqi-li/storage/releases/download/v1.0/bevformer_tiny_epoch_24.pth">model</a>&#x2F;<a href="https://github.com/zhiqi-li/storage/releases/download/v1.0/bevformer_tiny_epoch_24.log">log</a></td></tr><tr><td align="center"><a href="https://github.com/zhiqi-li/storage/releases/download/v1.0/r101_dcn_fcos3d_pretrain.pth">R101-DCN</a></td><td align="center">BEVFormer-small</td><td align="center">24ep</td><td align="center">47.9</td><td align="center">37.0</td><td align="center">10500M</td><td align="center"><a href="projects/configs/bevformer/bevformer_small.py">config</a></td><td align="center"><a href="https://github.com/zhiqi-li/storage/releases/download/v1.0/bevformer_small_epoch_24.pth">model</a>&#x2F;<a href="https://github.com/zhiqi-li/storage/releases/download/v1.0/bevformer_small_epoch_24.log">log</a></td></tr><tr><td align="center"><a href="https://github.com/zhiqi-li/storage/releases/download/v1.0/r101_dcn_fcos3d_pretrain.pth">R101-DCN</a></td><td align="center">BEVFormer-base</td><td align="center">24ep</td><td align="center">51.7</td><td align="center">41.6</td><td align="center">28500M</td><td align="center"><a href="projects/configs/bevformer/bevformer_base.py">config</a></td><td align="center"><a href="https://github.com/zhiqi-li/storage/releases/download/v1.0/bevformer_r101_dcn_24ep.pth">model</a>&#x2F;<a href="https://github.com/zhiqi-li/storage/releases/download/v1.0/bevformer_r101_dcn_24ep.log">log</a></td></tr><tr><td align="center"><a href="https://pan.baidu.com/s/1Jh5Aq2YwcD6tdj7Sl5BB3g?pwd=5rij">R50</a></td><td align="center">BEVformerV2-t1-base</td><td align="center">24ep</td><td align="center">42.6</td><td align="center">35.1</td><td align="center">23952M</td><td align="center"><a href="projects/configs/bevformerv2/bevformerv2-r50-t1-base-24ep.py">config</a></td><td align="center"><a href="https://pan.baidu.com/s/1ynzlAt1DQbH8NkqmisatTw?pwd=fdcv">model&#x2F;log</a></td></tr><tr><td align="center"><a href="https://pan.baidu.com/s/1Jh5Aq2YwcD6tdj7Sl5BB3g?pwd=5rij">R50</a></td><td align="center">BEVformerV2-t1-base</td><td align="center">48ep</td><td align="center">43.9</td><td align="center">35.9</td><td align="center">23952M</td><td align="center"><a href="projects/configs/bevformerv2/bevformerv2-r50-t1-base-48ep.py">config</a></td><td align="center"><a href="https://pan.baidu.com/s/1ynzlAt1DQbH8NkqmisatTw?pwd=fdcv">model&#x2F;log</a></td></tr><tr><td align="center"><a href="https://pan.baidu.com/s/1Jh5Aq2YwcD6tdj7Sl5BB3g?pwd=5rij">R50</a></td><td align="center">BEVformerV2-t1</td><td align="center">24ep</td><td align="center">45.3</td><td align="center">38.1</td><td align="center">37579M</td><td align="center"><a href="projects/configs/bevformerv2/bevformerv2-r50-t1-24ep.py">config</a></td><td align="center"><a href="https://pan.baidu.com/s/1ynzlAt1DQbH8NkqmisatTw?pwd=fdcv">model&#x2F;log</a></td></tr><tr><td align="center"><a href="https://pan.baidu.com/s/1Jh5Aq2YwcD6tdj7Sl5BB3g?pwd=5rij">R50</a></td><td align="center">BEVformerV2-t1</td><td align="center">48ep</td><td align="center">46.5</td><td align="center">39.5</td><td align="center">37579M</td><td align="center"><a href="projects/configs/bevformerv2/bevformerv2-r50-t1-48ep.py">config</a></td><td align="center"><a href="https://pan.baidu.com/s/1ynzlAt1DQbH8NkqmisatTw?pwd=fdcv">model&#x2F;log</a></td></tr><tr><td align="center"><a href="https://pan.baidu.com/s/1Jh5Aq2YwcD6tdj7Sl5BB3g?pwd=5rij">R50</a></td><td align="center">BEVformerV2-t2</td><td align="center">24ep</td><td align="center">51.8</td><td align="center">42.0</td><td align="center">38954M</td><td align="center"><a href="projects/configs/bevformerv2/bevformerv2-r50-t2-24ep.py">config</a></td><td align="center"><a href="https://pan.baidu.com/s/1ynzlAt1DQbH8NkqmisatTw?pwd=fdcv">model&#x2F;log</a></td></tr><tr><td align="center"><a href="https://pan.baidu.com/s/1Jh5Aq2YwcD6tdj7Sl5BB3g?pwd=5rij">R50</a></td><td align="center">BEVformerV2-t2</td><td align="center">48ep</td><td align="center">52.6</td><td align="center">43.1</td><td align="center">38954M</td><td align="center"><a href="projects/configs/bevformerv2/bevformerv2-r50-t2-48ep.py">config</a></td><td align="center"><a href="https://pan.baidu.com/s/1ynzlAt1DQbH8NkqmisatTw?pwd=fdcv">model&#x2F;log</a></td></tr><tr><td align="center"><a href="https://pan.baidu.com/s/1Jh5Aq2YwcD6tdj7Sl5BB3g?pwd=5rij">R50</a></td><td align="center">BEVformerV2-t8</td><td align="center">24ep</td><td align="center">55.3</td><td align="center">46.0</td><td align="center">40392M</td><td align="center"><a href="projects/configs/bevformerv2/bevformerv2-r50-t8-24ep.py">config</a></td><td align="center"><a href="https://pan.baidu.com/s/1ynzlAt1DQbH8NkqmisatTw?pwd=fdcv">model&#x2F;log</a></td></tr></tbody></table><p><strong>可以看到这里也有BEVFormerV2,后续会进行讲解，下面以BEVFormer-base为例进行讲解</strong></p><h2 id="Pipeline"><a href="#Pipeline" class="headerlink" title="Pipeline"></a>Pipeline</h2><ul><li>Backbone + Neck （ResNet-101-DCN + FPN）提取环视图像的多尺度特征；</li><li>论文提出的 Encoder 模块（包括 Temporal Self-Attention 模块和 Spatial Cross-Attention 模块）完成环视图像特征向 BEV 特征的建模；</li><li>Decoder 模块使用类似 Deformable DETR 的 完成 3D 目标检测的分类和定位任务；</li><li>正负样本的定义（采用 Transformer 中常用的匈牙利匹配算法，Focal Loss + L1 Loss 的总损失和最小）；</li><li>损失的计算（Focal Loss 分类损失 + L1 Loss 回归损失）；</li><li>反向传播，更新网络模型参数；</li></ul><p><img src="/pic/BEVFormer1.png" alt="制作的结构图"></p><p><strong>接下来将从输入数据格式，网络特征提取，BEV特征产生，BEV 特征解码完成 3D 框预测、正负样本定义、损失计算这六个方面完成 BEVFormer 的解析</strong></p><h2 id="输入的数据格式"><a href="#输入的数据格式" class="headerlink" title="输入的数据格式"></a>输入的数据格式</h2><p>对于 BEVFormer 网络模型而言，输入的数据是一个 6 维的张量：（bs，queue，cam，C，H，W）；</p><ul><li>bs：batch size 大小；</li><li>queue：连续帧的个数；由于 BEVFormer 采用了时序信息的思想（我认为加入时序信息后，可以一定程度上缓解遮挡问题），所以输入到网络模型中的数据要包含除当前帧之外，之前几帧的数据；</li><li>cam：每帧中包含的图像数量，对于nuScenes数据集而言，由于一辆车带有六个环视相机传感器，可以实现 360° 全场景的覆盖，所以一帧会包含六个环视相机拍摄到的六张环视图片；</li><li>C，H，W：图片的通道数，图片的高度，图片的宽度；</li></ul><h3 id="Nuscenes数据集简介"><a href="#Nuscenes数据集简介" class="headerlink" title="Nuscenes数据集简介"></a>Nuscenes数据集简介</h3><p>NuScenes数据集是一个包含两个城市1000个驾驶场景的大规模自动驾驶数据集。850个场景用于培训&#x2F;验证，150个场景用于测试。每一场戏都有20多秒长。它有40K个关键帧和整个传感器套件，包括6个摄像头（CAM）、1个激光雷达（LIDAR）、5个雷达（RADAR）、IMU和GPS。摄像机图像分辨率为1600×900。同时，发布了相应的HD-Map和CanBus数据，以探索多个输入的辅助。由于NuScenes提供了多样化的多传感器设置，因此它在学术文献中越来越受欢迎；数据规模没有Waymo的大，这使得在这个基准上快速验证想法变得高效。</p><p><strong>传感器在采集车上的布置如下图所示</strong></p><p> <img src="/pic/BEVFormer2.png" alt="采集车的结构图"></p><p>可以看出，相机（CAM）有六个，分别分布在前方（Front）、右前方（Front Right）、左前方（Front Left）、后方（Back）、右后方（Back Right）、左后方（Back Left）；激光雷达（LIDAR）有1个，放置在车顶（TOP）；毫米波雷达有五个，分别放置在前方（Front）、右前方（Front Right）、左前方（Front Left）、右后方（Back Right）、左后方（Back Left）。</p><h3 id="transformer的一些知识"><a href="#transformer的一些知识" class="headerlink" title="transformer的一些知识"></a>transformer的一些知识</h3><h4 id="自注意力机制（self-attention）"><a href="#自注意力机制（self-attention）" class="headerlink" title="自注意力机制（self-attention）"></a>自注意力机制（self-attention）</h4><p>自注意力机制是一种用于处理序列数据的机制，它能够在序列中的每个位置上计算该位置与其他位置之间的关联程度，并根据这些关联程度来加权组合序列中的信息。</p><p>概念：</p><ul><li>查询(Query)：查询是你想要了解的信息或者你想要从文本中提取的特征。它类似于你对文中的某个词语提出的问题或者你想要了解的内容。</li><li>键(Key)：键是文本中每个词语的表示。它类似于每个词语的标识符或者关键信息，用于帮助计算查询与其他词语之间的关联程度。</li><li>值(Value)：值是与每个词语相关的具体信息或特征。它类似于每个词语的具体含义或者特征向量。</li></ul><p>在自注意力机制中，具体步骤是：</p><ul><li>Stp1：从输入值a乘以矩阵Wq、Wk和Wv(这三个矩阵是模型参数，需要通过训练数据来学习)获取查询(Q)、键(K)、值(V),一般可以在输入a加上位置向量后再计算对应的Q、K、V</li><li>Step2:通过计算查询(Q)与键(K)之间的点积，来衡量查询与其他词语之间的关联程度，然后，通过对这些关联程度进行归一化处理（一般采用softma归一化），得到每个词语的注意力权重。</li><li>Step3:然后，根据这些注意力权重，对每个词语的值(V)进行加权求和，得到一个新的表示，该表示会更加关注与查询相关的信息。</li><li>Step4:最后，把Self Attention层的输出给全连接神经网络学习更多的信息</li></ul><p>以下是Transformer完整的架构，总体来看，它由**编码器(Encoder)<strong>和</strong>解码器(Decoder)**两部分组成。</p><p> <img src="/pic/transformer1.png" alt="完整架构图"></p><ul><li><strong>编码器(Encoder)</strong><ul><li>左边是编码器部分，主要作用是将输入数据编码成计算机能理解的高维抽象表示。</li><li>它的结构也就是前面一节我们所说的多头注意力机制+全连接神经网络的结构。此外，这里用了残差连接Q(Residual connection)的方式，将输入和多头注意力层或全连接神经网络的输出相加，再传递给下一层，避免梯度递减的问题。</li></ul></li><li><strong>解码器(Decoder)</strong><ul><li>右边是解码器的部分，主要作用是利用高维表示信息生成目标序列。它的结构大致与编码器相同，不同的有两点：<ul><li>第一，采用了<strong>掩码多头自注意力(Masked-Multi-.head self attention)</strong>,即在计算注意力得分时，模型只能关注生成内容的当前位置之前的信息，避免未来信息的泄漏。比如，这里计算输出b2时，就只使用了a1、a2两个位置的注意力得分进行加权。</li><li>第二，中间部分，利用了Encoder的输出结果计算交叉注意力(Cross Attention)。同之前的注意力机制类似，Cross Attention通过计算<strong>解码器当前位置(Q)的表示与编码器上下文表示(K)之间</strong>的注意力权重，将编码器上下文表示(V)加权，然后将该加权表示与解码器当前位置的表示进行融合。</li></ul></li></ul></li></ul><h4 id="Deformable-DETR"><a href="#Deformable-DETR" class="headerlink" title="Deformable DETR"></a>Deformable DETR</h4><p>在transformer中，特征点的特征向量Value可以由一个网络学习到，但是这个Value并不能表示全局的建模关系，于是就由另外两个网络为分别为每个特征点学习一个query和key，然后利用当前特征点的query与所有特征点的key做点乘，然后进行softmax，这样可以计算出每个特征点与其他特征点的权重关系，然后利用这个权重关系，将所有特征点的Value进行加权求和，得到每个特征点最终的Value。实时上，每个特征点并非需要与其他每个特征点做self-attention，比如图片上的左上角的特征点与右下角的特征点的关系是十分微弱的，甚至毫无关系。</p><ul><li>在Deformable DETR 中，每个特征点只与周围的几个特征点(默认为4)进行self-attention，也就是每个特征点的Value是由其周围4个特征点的的Value加权求和得到的。</li><li>相对于DETR，在Deformable DETR中，引入了多尺度的特征(能够同时兼顾大目标与小目标的识别)，因此每个特征点都能够在每个特征层上找到一个自己的采样点，然后在每个采样点周围采样4个偏移点作为self-attention的对象，即利用 4 * 4 &#x3D; 16 个偏移点特征向量Value来计算当前特征点的Value。</li><li>这里有个问题，在transformer中，当前特征点的Value加权求和时是将自己的Value包括在内的，而在deformable detr中，是将自己value除外的。</li></ul><p>已经基本明确在 Deformable DETR中， 特征点要与哪些偏移点怎么做self-attention了，那么后续可以分为两个部分:</p><ul><li>1、如何找到这些采偏移点，</li><li>2、这些偏移点的权重系数是多少。</li></ul><p>文章是利用两个网络来实现的，一个网络通过特征点的Value预测16个偏移点(四个特征层)的位置，另一个网络利用特征点的Value预测16个偏移点的权重系数，如下图所示。</p><p> <img src="/pic/BEVFormer3.png" alt="完整架构图"></p><p>图中左边所示为4种尺度的特征层，以最上方特征层中的一个特征点(0.3，0.3)为例，它在每个特征层上都有一个采样点(相对坐标一致)，正常来说每个采样点会与周围的四个点(绿色点)进行self-attention，但是这四个点最好的通过网络自己来学习，于是蓝色的点是网络学习到的偏移点，但是偏移点的坐标一般不会为整数，因此，蓝色特征点的Value就会有其附近的四个特征点(黄色)进行双线性差值得到，因此，一个特征点就采样到了16个偏移点，那么这个特征点的特征向量Value就由这16个偏移点的特征向量Value加权求和得到</p><h2 id="BEV特征的产生"><a href="#BEV特征的产生" class="headerlink" title="BEV特征的产生"></a>BEV特征的产生</h2><p>BEV 特征的产生用到的就是论文中最核心的部分 —— Encoder 模块</p><p>Encoder 模块包含两个子模块 <strong>Temporal Self-Attention模块 以及 Spatial Cross-Attention模块</strong>；接下来我会分别介绍一下这两个模块；</p><p>在梳理具体的代码实现之前，首先介绍下在 Temporal Self-Attention 模块和 Spatial Cross-Attention 模块中都要用到的一个组件 ——** 多尺度的可变形注意力模块**；这个模块是将 Transformer 的全局注意力变为局部注意力的一个非常关键的组件，用于减少训练时间，提高 Transformer 的收敛速度；（该思想最早出现在 Deformable DETR 中）</p><p>简单概括下多尺度的可变形注意力模块对数据处理的Pipeline，概括如下：</p><p> <img src="/pic/BEVFormer5.png" alt="Deformable Attention 模块 Pipeline"></p><p>通过流程图可知，输入到 Deformable Attention Module CUDA 扩展的变量主要有五个，分别是采样位置（Sample Location）、注意力权重（Attention Weights）、映射后的 Value 特征、多尺度特征每层特征起始索引位置、多尺度特征图的空间大小（便于将采样位置由归一化的值变成绝对位置）；</p><p>多尺度可变形注意力模块与 Transformer 中常见的先生成 Attention Map，再计算加权和的方式不同；常规而言 Attention Map &#x3D; Query 和 Key 做内积运算，将 Attention Map 再和 Value 做加权；但是由于这种方式计算量开销会比较大，所以在 Deformable DETR 中用局部注意力机制代替了全局注意力机制，只对几个采样点进行采样，而<strong>采样点的位置相对于参考点的偏移量和每个采样点在加权时的比重均是靠 Query 经过 Linear 层学习得到的</strong>。具体可以看下图</p><p> <img src="/pic/BEVFormer4.png" alt="Deformable Attention 模块 "></p><h3 id="Temporal-Self-Attention-模块"><a href="#Temporal-Self-Attention-模块" class="headerlink" title="Temporal Self-Attention 模块"></a>Temporal Self-Attention 模块</h3><h4 id="功能"><a href="#功能" class="headerlink" title="功能"></a>功能</h4><p>通过引入时序信息（插图中的 History BEV）与当前时刻的 BEV Query 进行融合，提高 BEV Query 的建模能力；</p><h4 id="代码实现"><a href="#代码实现" class="headerlink" title="代码实现"></a>代码实现</h4><p>对于 Temporal Self-Attention 模块而言，需要 bev_query、bev_pos、prev_bev、ref_point、value等参数（需要用到的参数参考 Deformable Attention Pipeline 图解）</p><ul><li>参数 bev_query<ul><li>一个完全 learnable parameter，通过 nn.Embedding() 函数得到，形状 shape &#x3D; (200 * 200，256)；200，200 分别代表 BEV 特征平面的长和宽；</li></ul></li><li>参数 bev_pose<ul><li>感觉也是一个完全 learnable parameter，与 2D 检测中常见的正余弦编码方式不同，感觉依旧是把不同的 grid 位置映射到一个高维的向量空间，shape &#x3D; （bs，256，200，200）代码如下：<pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token triple-quoted-string string">""" bev_pose 的生成过程 """</span><span class="token comment"># w, h 分别代表 bev 特征的空间尺寸 200 * 200</span>x <span class="token operator">=</span> torch<span class="token punctuation">.</span>arange<span class="token punctuation">(</span>w<span class="token punctuation">,</span> device<span class="token operator">=</span>mask<span class="token punctuation">.</span>device<span class="token punctuation">)</span>y <span class="token operator">=</span> torch<span class="token punctuation">.</span>arange<span class="token punctuation">(</span>h<span class="token punctuation">,</span> device<span class="token operator">=</span>mask<span class="token punctuation">.</span>device<span class="token punctuation">)</span><span class="token comment"># self.col_embed 和 self.row_embed 分别是两个 Linear 层，将(200, )的坐标向高维空间做映射</span>x_embed <span class="token operator">=</span> self<span class="token punctuation">.</span>col_embed<span class="token punctuation">(</span>x<span class="token punctuation">)</span>  <span class="token comment"># (200, 128)</span>y_embed <span class="token operator">=</span> self<span class="token punctuation">.</span>row_embed<span class="token punctuation">(</span>y<span class="token punctuation">)</span>  <span class="token comment"># (200, 128)</span><span class="token comment"># pos shape: (bs, 256, 200, 200)</span>pos <span class="token operator">=</span> torch<span class="token punctuation">.</span>cat<span class="token punctuation">(</span><span class="token punctuation">(</span>x_embed<span class="token punctuation">.</span>unsqueeze<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">)</span><span class="token punctuation">.</span>repeat<span class="token punctuation">(</span>h<span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">,</span> y_embed<span class="token punctuation">.</span>unsqueeze<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">.</span>repeat<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> w<span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">,</span> dim<span class="token operator">=</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">.</span>permute<span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">.</span>unsqueeze<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">)</span><span class="token punctuation">.</span>repeat<span class="token punctuation">(</span>mask<span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre></li></ul></li><li>参数 ref_point<ul><li>这个参数根据当前 Temporal Self-Attention 模块是否有 prev_bev 特征输入而言，会对应不同的情况，之所以会出现不同，是考虑到了前后时刻 BEV 特征存在特征不对齐的问题，BEV 特征不对齐主要体现在以下两个方面<ul><li>车自身是不断运动的。上一时刻和当前时刻，由于车自身的不断运动，两个时刻的 BEV 特征在空间上是不对齐的；针对这一问题，为了实现两个时刻特征的空间对齐，需要用到 can_bus 数据中有关车自身旋转角度和偏移的信息，从而对上一时刻的 BEV 特征与当前时刻的 BEV 特征在空间上实现特征对齐；</li><li>车周围的物体也在一定范围内运动。针对车周围的物体可能在不同时刻也有移动，这部分的特征对齐就是靠网络自身的注意力模块去学习实现修正了。</li></ul></li><li>综上，对于 Temporal Self-Attention 模块没有输入 prev_bev（第一帧没有前一时刻的 BEV 特征）的情况，其 ref_point &#x3D; ref_2d；对于存在输入 prev_bev 的情况，其 ref_point &#x3D; ref_2d + shift；</li><li>涉及到的ref_2d、shift参数，核心代码如下：</li></ul></li></ul> <pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token triple-quoted-string string">"""shift 参数的生成"""</span> <span class="token comment"># obtain rotation angle and shift with ego motion</span>delta_x <span class="token operator">=</span> kwargs<span class="token punctuation">[</span><span class="token string">'img_metas'</span><span class="token punctuation">]</span><span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">[</span><span class="token string">'can_bus'</span><span class="token punctuation">]</span><span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span>delta_y <span class="token operator">=</span> kwargs<span class="token punctuation">[</span><span class="token string">'img_metas'</span><span class="token punctuation">]</span><span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">[</span><span class="token string">'can_bus'</span><span class="token punctuation">]</span><span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span>ego_angle <span class="token operator">=</span> kwargs<span class="token punctuation">[</span><span class="token string">'img_metas'</span><span class="token punctuation">]</span><span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">[</span><span class="token string">'can_bus'</span><span class="token punctuation">]</span><span class="token punctuation">[</span><span class="token operator">-</span><span class="token number">2</span><span class="token punctuation">]</span> <span class="token operator">/</span> np<span class="token punctuation">.</span>pi <span class="token operator">*</span> <span class="token number">180</span>rotation_angle <span class="token operator">=</span> kwargs<span class="token punctuation">[</span><span class="token string">'img_metas'</span><span class="token punctuation">]</span><span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">[</span><span class="token string">'can_bus'</span><span class="token punctuation">]</span><span class="token punctuation">[</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">]</span>grid_length_y <span class="token operator">=</span> grid_length<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span>grid_length_x <span class="token operator">=</span> grid_length<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span>translation_length <span class="token operator">=</span> np<span class="token punctuation">.</span>sqrt<span class="token punctuation">(</span>delta_x <span class="token operator">**</span> <span class="token number">2</span> <span class="token operator">+</span> delta_y <span class="token operator">**</span> <span class="token number">2</span><span class="token punctuation">)</span>translation_angle <span class="token operator">=</span> np<span class="token punctuation">.</span>arctan2<span class="token punctuation">(</span>delta_y<span class="token punctuation">,</span> delta_x<span class="token punctuation">)</span> <span class="token operator">/</span> np<span class="token punctuation">.</span>pi <span class="token operator">*</span> <span class="token number">180</span><span class="token keyword">if</span> translation_angle <span class="token operator">&lt;</span> <span class="token number">0</span><span class="token punctuation">:</span>   translation_angle <span class="token operator">+=</span> <span class="token number">360</span>bev_angle <span class="token operator">=</span> ego_angle <span class="token operator">-</span> translation_angleshift_y <span class="token operator">=</span> translation_length <span class="token operator">*</span> \   np<span class="token punctuation">.</span>cos<span class="token punctuation">(</span>bev_angle <span class="token operator">/</span> <span class="token number">180</span> <span class="token operator">*</span> np<span class="token punctuation">.</span>pi<span class="token punctuation">)</span> <span class="token operator">/</span> grid_length_y <span class="token operator">/</span> bev_hshift_x <span class="token operator">=</span> translation_length <span class="token operator">*</span> \   np<span class="token punctuation">.</span>sin<span class="token punctuation">(</span>bev_angle <span class="token operator">/</span> <span class="token number">180</span> <span class="token operator">*</span> np<span class="token punctuation">.</span>pi<span class="token punctuation">)</span> <span class="token operator">/</span> grid_length_x <span class="token operator">/</span> bev_wshift_y <span class="token operator">=</span> shift_y <span class="token operator">*</span> self<span class="token punctuation">.</span>use_shiftshift_x <span class="token operator">=</span> shift_x <span class="token operator">*</span> self<span class="token punctuation">.</span>use_shiftshift <span class="token operator">=</span> bev_queries<span class="token punctuation">.</span>new_tensor<span class="token punctuation">(</span><span class="token punctuation">[</span>shift_x<span class="token punctuation">,</span> shift_y<span class="token punctuation">]</span><span class="token punctuation">)</span>  <span class="token comment"># shape (2,) </span><span class="token comment"># 通过`旋转`和`平移`变换实现 BEV 特征的对齐，对于平移部分是通过对参考点加上偏移量`shift`体现的</span><span class="token keyword">if</span> prev_bev <span class="token keyword">is</span> <span class="token keyword">not</span> <span class="token boolean">None</span><span class="token punctuation">:</span>   <span class="token keyword">if</span> prev_bev<span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span> <span class="token operator">==</span> bev_h <span class="token operator">*</span> bev_w<span class="token punctuation">:</span>      prev_bev <span class="token operator">=</span> prev_bev<span class="token punctuation">.</span>permute<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">)</span>   <span class="token keyword">if</span> self<span class="token punctuation">.</span>rotate_prev_bev<span class="token punctuation">:</span>      num_prev_bev <span class="token operator">=</span> prev_bev<span class="token punctuation">.</span>size<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">)</span>      prev_bev <span class="token operator">=</span> prev_bev<span class="token punctuation">.</span>reshape<span class="token punctuation">(</span>bev_h<span class="token punctuation">,</span> bev_w<span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">.</span>permute<span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span>  <span class="token comment"># sequence -> grid</span>      prev_bev <span class="token operator">=</span> rotate<span class="token punctuation">(</span>prev_bev<span class="token punctuation">,</span> rotation_angle<span class="token punctuation">,</span> center<span class="token operator">=</span>self<span class="token punctuation">.</span>rotate_center<span class="token punctuation">)</span>      prev_bev <span class="token operator">=</span> prev_bev<span class="token punctuation">.</span>permute<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">)</span><span class="token punctuation">.</span>reshape<span class="token punctuation">(</span>bev_h <span class="token operator">*</span> bev_w<span class="token punctuation">,</span> num_prev_bev<span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token triple-quoted-string string">"""ref_2d 参数的生成，常规的 2D 网格生成的规则坐标点"""</span>ref_y<span class="token punctuation">,</span> ref_x <span class="token operator">=</span> torch<span class="token punctuation">.</span>meshgrid<span class="token punctuation">(</span>torch<span class="token punctuation">.</span>linspace<span class="token punctuation">(</span><span class="token number">0.5</span><span class="token punctuation">,</span> H <span class="token operator">-</span> <span class="token number">0.5</span><span class="token punctuation">,</span> H<span class="token punctuation">,</span> dtype<span class="token operator">=</span>dtype<span class="token punctuation">,</span> device<span class="token operator">=</span>device<span class="token punctuation">)</span><span class="token punctuation">,</span>                              torch<span class="token punctuation">.</span>linspace<span class="token punctuation">(</span><span class="token number">0.5</span><span class="token punctuation">,</span> W <span class="token operator">-</span> <span class="token number">0.5</span><span class="token punctuation">,</span> W<span class="token punctuation">,</span> dtype<span class="token operator">=</span>dtype<span class="token punctuation">,</span> device<span class="token operator">=</span>device<span class="token punctuation">)</span><span class="token punctuation">)</span>ref_y <span class="token operator">=</span> ref_y<span class="token punctuation">.</span>reshape<span class="token punctuation">(</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">[</span><span class="token boolean">None</span><span class="token punctuation">]</span> <span class="token operator">/</span> Href_x <span class="token operator">=</span> ref_x<span class="token punctuation">.</span>reshape<span class="token punctuation">(</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">[</span><span class="token boolean">None</span><span class="token punctuation">]</span> <span class="token operator">/</span> Wref_2d <span class="token operator">=</span> torch<span class="token punctuation">.</span>stack<span class="token punctuation">(</span><span class="token punctuation">(</span>ref_x<span class="token punctuation">,</span> ref_y<span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span>ref_2d <span class="token operator">=</span> ref_2d<span class="token punctuation">.</span>repeat<span class="token punctuation">(</span>bs<span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">.</span>unsqueeze<span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><ul><li><p>参数 value</p><ul><li>对应着bev_query去查询的特征；<ul><li>对于 Temporal Self-Attention 模块输入包含 prev_bev时，value &#x3D; [prev_bev，bev_query]，对应的参考点 ref_point &#x3D; [ref_2d + shift，ref_2d]；如果输入不包含 prev_bev时，value &#x3D; [bev_query，bev_query]，对应的参考点ref_point &#x3D; [ref_2d，ref_2d]。</li><li>相应的，之前介绍的 bev_query 在输入包含 prev_bev时，bev_query &#x3D; [value[0]，bev_query]；输入不包含 prev_bev时，value &#x3D; [bev_query，bev_query]；</li><li>整体的思路还是计算在计算 self-attention，无论是否存在prev_bev，都是在计算prev_bev以及bev_query自身的相似性，最后将两组计算得到的bev_query结果做一下平均。</li></ul></li></ul></li><li><p>内部参数 Offset、Weights、 Sample Location</p><ul><li>参数Offset的计算是同时考虑了value[0]和bev_query的信息，在映射空间的维度上进行了concat，并基于 concat 后的特征，去计算 Offset以及attention weights ，涉及到的核心代码如下，这里解释一下为什么 level &#x3D; 1，由于 BEV 特征只有一层，所以只会对一层 200 * 200 空间大小的 BEV 特征，基于每个位置采样四个点，重新构造新的 BEV 特征；</li></ul><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token triple-quoted-string string">""" bev_query 按照通道维度进行 concat """</span>query <span class="token operator">=</span> torch<span class="token punctuation">.</span>cat<span class="token punctuation">(</span><span class="token punctuation">[</span>value<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">:</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">,</span> query<span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span>  <span class="token comment"># (bs, 40000, 512)</span><span class="token triple-quoted-string string">""" value 经过 Linear 做映射 """</span>value <span class="token operator">=</span> self<span class="token punctuation">.</span>value_proj<span class="token punctuation">(</span>value<span class="token punctuation">)</span><span class="token triple-quoted-string string">""" offsets 以及 attention weights 的生成过程 """</span><span class="token comment"># sampling_offsets: shape = (bs, num_query, 8, 1, 4, 2)</span><span class="token comment"># 对 query 进行维度映射得到采样点的偏移量</span>sampling_offsets <span class="token operator">=</span> self<span class="token punctuation">.</span>sampling_offsets<span class="token punctuation">(</span>query<span class="token punctuation">)</span><span class="token punctuation">.</span>view<span class="token punctuation">(</span>bs<span class="token punctuation">,</span> num_query<span class="token punctuation">,</span> self<span class="token punctuation">.</span>num_heads<span class="token punctuation">,</span> self<span class="token punctuation">.</span>num_levels<span class="token punctuation">,</span> self<span class="token punctuation">.</span>num_points<span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">)</span><span class="token comment"># 对 query 进行维度映射得到注意力权重</span>attention_weights <span class="token operator">=</span> self<span class="token punctuation">.</span>attention_weights<span class="token punctuation">(</span>query<span class="token punctuation">)</span><span class="token punctuation">.</span>view<span class="token punctuation">(</span>bs<span class="token punctuation">,</span> num_query<span class="token punctuation">,</span> self<span class="token punctuation">.</span>num_heads<span class="token punctuation">,</span> self<span class="token punctuation">.</span>num_levels <span class="token operator">*</span> self<span class="token punctuation">.</span>num_points<span class="token punctuation">)</span>  attention_weights <span class="token operator">=</span> attention_weights<span class="token punctuation">.</span>softmax<span class="token punctuation">(</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token comment"># attention_weights: shape = (bs, num_query, 8, 1, 4)</span>attention_weights <span class="token operator">=</span> attention_weights<span class="token punctuation">.</span>view<span class="token punctuation">(</span>bs<span class="token punctuation">,</span> num_query<span class="token punctuation">,</span> self<span class="token punctuation">.</span>num_heads<span class="token punctuation">,</span> self<span class="token punctuation">.</span>num_levels<span class="token punctuation">,</span> self<span class="token punctuation">.</span>num_points<span class="token punctuation">)</span> <span class="token triple-quoted-string string">""" sample location 的生成过程 通过代码可以观察到两点：1. 通过 query 学到的 sampling_offsets 偏移量是一个绝对量，不是相对量，所以需要做 normalize；2. 最终生成的 sampling_locations 是一个相对量；"""</span>offset_normalizer <span class="token operator">=</span> torch<span class="token punctuation">.</span>stack<span class="token punctuation">(</span><span class="token punctuation">[</span>spatial_shapes<span class="token punctuation">[</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">,</span> spatial_shapes<span class="token punctuation">[</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span>sampling_locations <span class="token operator">=</span> reference_points<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span> <span class="token punctuation">:</span><span class="token punctuation">,</span> <span class="token boolean">None</span><span class="token punctuation">,</span> <span class="token punctuation">:</span><span class="token punctuation">,</span> <span class="token boolean">None</span><span class="token punctuation">,</span> <span class="token punctuation">:</span><span class="token punctuation">]</span> \               <span class="token operator">+</span> sampling_offsets <span class="token operator">/</span> offset_normalizer<span class="token punctuation">[</span><span class="token boolean">None</span><span class="token punctuation">,</span> <span class="token boolean">None</span><span class="token punctuation">,</span> <span class="token boolean">None</span><span class="token punctuation">,</span> <span class="token punctuation">:</span><span class="token punctuation">,</span> <span class="token boolean">None</span><span class="token punctuation">,</span> <span class="token punctuation">:</span><span class="token punctuation">]</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre></li><li><p>输出bev query</p><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token triple-quoted-string string">""" 各个参数的 shape 情况 1. value: (2，40000，8，32） # 2: 代表前一时刻的 BEV 特征和后一时刻的 BEV 特征，两个特征在计算的过程中是互不干扰的，                           # 40000: 代表 bev_query 200 * 200 空间大小的每个位置                           # 8: 代表8个头，# 32: 每个头表示为 32 维的特征2. spatial_shapes: (200, 200) # 方便将归一化的 sampling_locations 反归一化3. level_start_index: 0 # BEV 特征只有一层4. sampling_locations: (2, 40000, 8, 1, 4, 2)5. attention_weights: (2, 40000, 8, 1, 4)6. output: (2, 40000, 8, 32)"""</span>output <span class="token operator">=</span> MultiScaleDeformableAttnFunction<span class="token punctuation">.</span><span class="token builtin">apply</span><span class="token punctuation">(</span>value<span class="token punctuation">,</span>                                                 spatial_shapes<span class="token punctuation">,</span>                                                 level_start_index<span class="token punctuation">,</span>                                                 sampling_locations<span class="token punctuation">,</span>                                                attention_weights<span class="token punctuation">,</span>                                                 self<span class="token punctuation">.</span>im2col_step<span class="token punctuation">)</span><span class="token string">""</span>" 最后将前一时刻的 bev_query 与当前时刻的 bev_query 做平均output <span class="token operator">=</span> output<span class="token punctuation">.</span>permute<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">)</span>output <span class="token operator">=</span> <span class="token punctuation">(</span>output<span class="token punctuation">[</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token punctuation">:</span>bs<span class="token punctuation">]</span> <span class="token operator">+</span> output<span class="token punctuation">[</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">,</span> bs<span class="token punctuation">:</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token operator">/</span>self<span class="token punctuation">.</span>num_bev_queue<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre></li></ul><p>至此，Temporal Self-Attention 模块的逻辑到此结束，将生成的 bev_query 送入到后面的 Spatial Cross-Attention 模块中。</p><h3 id="Spatial-Cross-Attention-模块"><a href="#Spatial-Cross-Attention-模块" class="headerlink" title="Spatial Cross-Attention 模块"></a>Spatial Cross-Attention 模块</h3><h4 id="功能-1"><a href="#功能-1" class="headerlink" title="功能"></a>功能</h4><p>利用 Temporal Self-Attention 模块输出的 bev_query， 对主干网络和 Neck 网络提取到的多尺度环视图像特征进行查询，生成 BEV 空间下的BEV Embedding特征；</p><h4 id="代码实现-1"><a href="#代码实现-1" class="headerlink" title="代码实现"></a>代码实现</h4><p>对于 Spatial Cross-Attention 模块而言，与 Temporal Self-Attention 模块需要的参数很类似，但是并不需要 bev_pos 参数，只需要 bev_query、ref_point、value（就是 concat 到一起的多尺度特征）；虽不需要 bev_pose，但是整体流程与 Deformable Attention Pipeline 图解类似</p><ul><li><p>参数bev_query</p><ul><li>bev_query参数来自于 Temporal Self-Attention 模块的输出；</li></ul></li><li><p>参数value</p><ul><li>对于 Transformer 而言，由于其本身是处理文本序列的模型，而文本序列都是一组组一维的数据，所以需要将前面提取的多尺度特征做 flatten() 处理，并将所有层的特征汇聚到一起，方便之后做查询；对应的核心代码如下：<pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token triple-quoted-string string">""" 首先将多尺度的特征每一层都进行 flatten() """</span><span class="token keyword">for</span> lvl<span class="token punctuation">,</span> feat <span class="token keyword">in</span> <span class="token builtin">enumerate</span><span class="token punctuation">(</span>mlvl_feats<span class="token punctuation">)</span><span class="token punctuation">:</span>   bs<span class="token punctuation">,</span> num_cam<span class="token punctuation">,</span> c<span class="token punctuation">,</span> h<span class="token punctuation">,</span> w <span class="token operator">=</span> feat<span class="token punctuation">.</span>shape   spatial_shape <span class="token operator">=</span> <span class="token punctuation">(</span>h<span class="token punctuation">,</span> w<span class="token punctuation">)</span>   feat <span class="token operator">=</span> feat<span class="token punctuation">.</span>flatten<span class="token punctuation">(</span><span class="token number">3</span><span class="token punctuation">)</span><span class="token punctuation">.</span>permute<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">)</span>     <span class="token keyword">if</span> self<span class="token punctuation">.</span>use_cams_embeds<span class="token punctuation">:</span>      feat <span class="token operator">=</span> feat <span class="token operator">+</span> self<span class="token punctuation">.</span>cams_embeds<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span> <span class="token boolean">None</span><span class="token punctuation">,</span> <span class="token boolean">None</span><span class="token punctuation">,</span> <span class="token punctuation">:</span><span class="token punctuation">]</span><span class="token punctuation">.</span>to<span class="token punctuation">(</span>feat<span class="token punctuation">.</span>dtype<span class="token punctuation">)</span>      feat <span class="token operator">=</span> feat <span class="token operator">+</span> self<span class="token punctuation">.</span>level_embeds<span class="token punctuation">[</span><span class="token boolean">None</span><span class="token punctuation">,</span> <span class="token boolean">None</span><span class="token punctuation">,</span> lvl<span class="token punctuation">:</span>lvl <span class="token operator">+</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token punctuation">:</span><span class="token punctuation">]</span><span class="token punctuation">.</span>to<span class="token punctuation">(</span>feat<span class="token punctuation">.</span>dtype<span class="token punctuation">)</span>      spatial_shapes<span class="token punctuation">.</span>append<span class="token punctuation">(</span>spatial_shape<span class="token punctuation">)</span>      feat_flatten<span class="token punctuation">.</span>append<span class="token punctuation">(</span>feat<span class="token punctuation">)</span><span class="token triple-quoted-string string">""" 对每个 camera 的所有层级特征进行汇聚 """</span>feat_flatten <span class="token operator">=</span> torch<span class="token punctuation">.</span>cat<span class="token punctuation">(</span>feat_flatten<span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">)</span>  <span class="token comment"># (cam, bs, sum(h*w), 256)</span>spatial_shapes <span class="token operator">=</span> torch<span class="token punctuation">.</span>as_tensor<span class="token punctuation">(</span>spatial_shapes<span class="token punctuation">,</span> dtype<span class="token operator">=</span>torch<span class="token punctuation">.</span><span class="token builtin">long</span><span class="token punctuation">,</span> device<span class="token operator">=</span>bev_pos<span class="token punctuation">.</span>device<span class="token punctuation">)</span><span class="token comment"># 计算每层特征的起始索引位置</span>level_start_index <span class="token operator">=</span> torch<span class="token punctuation">.</span>cat<span class="token punctuation">(</span><span class="token punctuation">(</span>spatial_shapes<span class="token punctuation">.</span>new_zeros<span class="token punctuation">(</span><span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">,</span> spatial_shapes<span class="token punctuation">.</span>prod<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">.</span>cumsum<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">)</span><span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token comment"># 维度变换</span>feat_flatten <span class="token operator">=</span> feat_flatten<span class="token punctuation">.</span>permute<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">)</span>  <span class="token comment"># (num_cam, sum(H*W), bs, embed_dims)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre></li></ul></li><li><p>参数ref_point</p><ul><li><p>首先说一下ref_3d坐标点，这个ref_3d是基于 BEV 空间产生的三维空间规则网格点，同时在 z 轴方向上人为的选择了 4 个坐标点。</p></li><li><p>这里要使用 z 轴，并在 z 轴方向上采样的物理意义，我的理解是为了提取每个 BEV 位置处不同高度的特征；可以理解一下，假如对于 BEV 平面上的（x，y）处有一辆汽车，它所对应的特征应该由车底、车身、车顶处等位置的特征汇聚而成，但是这些位置对应的高度是不一致的，而为了更好的获取在 BEV 空间下的（x，y）处的特征，就将（x，y）的坐标进行了 lift ，从而将 BEV 坐标系下的三维点映射回图像平面后可以去查询并融合更加准确的特征；</p></li><li><p>而在映射的过程中，论文中也提到，由于每个参考点映射回图像坐标系后，不会落到六个图像上，只可能落在其中的某些图像的某些位置上，所以只对这些参考点附近的位置进行采样，可以提高模型的收敛速度（借鉴了 Deformable DETR 的思想）如下图所示：<br><img src="/pic/BEVFormer6.png"></p></li><li><p>ref_3d参数生成、3D 坐标向图像平面转换等过程的核心代码如下，真正用在 Spatial Cross-Attention 模块中的参考点是下面代码段中的reference_points_cam 。</p><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token triple-quoted-string string">""" ref_3d 坐标生成 """</span>zs <span class="token operator">=</span> torch<span class="token punctuation">.</span>linspace<span class="token punctuation">(</span><span class="token number">0.5</span><span class="token punctuation">,</span> Z <span class="token operator">-</span> <span class="token number">0.5</span><span class="token punctuation">,</span> num_points_in_pillar<span class="token punctuation">,</span> dtype<span class="token operator">=</span>dtype<span class="token punctuation">,</span> device<span class="token operator">=</span>device<span class="token punctuation">)</span><span class="token punctuation">.</span>view<span class="token punctuation">(</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">.</span>expand<span class="token punctuation">(</span>num_points_in_pillar<span class="token punctuation">,</span> H<span class="token punctuation">,</span> W<span class="token punctuation">)</span> <span class="token operator">/</span> Zxs <span class="token operator">=</span> torch<span class="token punctuation">.</span>linspace<span class="token punctuation">(</span><span class="token number">0.5</span><span class="token punctuation">,</span> W <span class="token operator">-</span> <span class="token number">0.5</span><span class="token punctuation">,</span> W<span class="token punctuation">,</span> dtype<span class="token operator">=</span>dtype<span class="token punctuation">,</span> device<span class="token operator">=</span>device<span class="token punctuation">)</span><span class="token punctuation">.</span>view<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> W<span class="token punctuation">)</span><span class="token punctuation">.</span>expand<span class="token punctuation">(</span>num_points_in_pillar<span class="token punctuation">,</span> H<span class="token punctuation">,</span> W<span class="token punctuation">)</span> <span class="token operator">/</span> Wys <span class="token operator">=</span> torch<span class="token punctuation">.</span>linspace<span class="token punctuation">(</span><span class="token number">0.5</span><span class="token punctuation">,</span> H <span class="token operator">-</span> <span class="token number">0.5</span><span class="token punctuation">,</span> H<span class="token punctuation">,</span> dtype<span class="token operator">=</span>dtype<span class="token punctuation">,</span> device<span class="token operator">=</span>device<span class="token punctuation">)</span><span class="token punctuation">.</span>view<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> H<span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">.</span>expand<span class="token punctuation">(</span>num_points_in_pillar<span class="token punctuation">,</span> H<span class="token punctuation">,</span> W<span class="token punctuation">)</span> <span class="token operator">/</span> Href_3d <span class="token operator">=</span> torch<span class="token punctuation">.</span>stack<span class="token punctuation">(</span><span class="token punctuation">(</span>xs<span class="token punctuation">,</span> ys<span class="token punctuation">,</span> zs<span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span>  <span class="token comment"># (4, 200, 200, 3)  (level, bev_h, bev_w, 3) 3代表 x,y,z 坐标值</span>ref_3d <span class="token operator">=</span> ref_3d<span class="token punctuation">.</span>permute<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">.</span>flatten<span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">.</span>permute<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span>  <span class="token comment"># (4, 200 * 200, 3)</span>ref_3d <span class="token operator">=</span> ref_3d<span class="token punctuation">[</span><span class="token boolean">None</span><span class="token punctuation">]</span><span class="token punctuation">.</span>repeat<span class="token punctuation">(</span>bs<span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span>  <span class="token comment"># (1, 4, 200 * 200, 3)</span><span class="token triple-quoted-string string">""" BEV 空间下的三维坐标点向图像空间转换的过程代码中的`lidar2img`需要有两点需要注意1. BEV 坐标系 这里指 lidar 坐标系2. 这里提到的`lidar2img`是经过坐标变换的，一般分成三步   第一步：lidar 坐标系 -> ego vehicle 坐标系   第二步：ego vehicle 坐标系 -> camera 坐标系   第三部：camera 坐标系 通过相机内参 得到像素坐标系   以上这三步用到的所有平移和旋转矩阵都合并到了一起，形成了 `lidar2img` 旋转平移矩阵同时需要注意：再与`lidar2img`矩阵乘完，还需要经过下面两步坐标系转换，才是得到了三维坐标点在二维图像平面上的点"""</span><span class="token comment"># (level, bs, cam, num_query, 4)</span>坐标系转换第一步：reference_points_cam <span class="token operator">=</span> torch<span class="token punctuation">.</span>matmul<span class="token punctuation">(</span>lidar2img<span class="token punctuation">.</span>to<span class="token punctuation">(</span>torch<span class="token punctuation">.</span>float32<span class="token punctuation">)</span><span class="token punctuation">,</span> reference_points<span class="token punctuation">.</span>to<span class="token punctuation">(</span>torch<span class="token punctuation">.</span>float32<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">.</span>squeeze<span class="token punctuation">(</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span>  eps <span class="token operator">=</span> <span class="token number">1e-5</span>bev_mask <span class="token operator">=</span> <span class="token punctuation">(</span>reference_points_cam<span class="token punctuation">[</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">:</span><span class="token number">3</span><span class="token punctuation">]</span> <span class="token operator">></span> eps<span class="token punctuation">)</span>  <span class="token comment"># (level, bs, cam, num_query, 1)</span>坐标系转换第二步：reference_points_cam <span class="token operator">=</span> reference_points_cam<span class="token punctuation">[</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">:</span><span class="token number">2</span><span class="token punctuation">]</span> <span class="token operator">/</span> torch<span class="token punctuation">.</span>maximum<span class="token punctuation">(</span>reference_points_cam<span class="token punctuation">[</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">:</span><span class="token number">3</span><span class="token punctuation">]</span><span class="token punctuation">,</span> torch<span class="token punctuation">.</span>ones_like<span class="token punctuation">(</span>reference_points_cam<span class="token punctuation">[</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">:</span><span class="token number">3</span><span class="token punctuation">]</span><span class="token punctuation">)</span> <span class="token operator">*</span> eps<span class="token punctuation">)</span><span class="token comment"># reference_points_cam = (bs, cam = 6, 40000, level = 4, xy = 2)</span>reference_points_cam<span class="token punctuation">[</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">]</span> <span class="token operator">/=</span> img_metas<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">[</span><span class="token string">'img_shape'</span><span class="token punctuation">]</span><span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span>  <span class="token comment"># 坐标归一化</span>reference_points_cam<span class="token punctuation">[</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">]</span> <span class="token operator">/=</span> img_metas<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">[</span><span class="token string">'img_shape'</span><span class="token punctuation">]</span><span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span>  <span class="token comment"># 坐标归一化</span><span class="token comment"># bev_mask 用于评判某一 三维坐标点 是否落在了 二维坐标平面上</span><span class="token comment"># bev_mask = (bs, cam = 6, 40000, level = 4)</span>bev_mask <span class="token operator">=</span> <span class="token punctuation">(</span>bev_mask <span class="token operator">&amp;</span> <span class="token punctuation">(</span>reference_points_cam<span class="token punctuation">[</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">:</span><span class="token number">2</span><span class="token punctuation">]</span> <span class="token operator">></span> <span class="token number">0.0</span><span class="token punctuation">)</span>                     <span class="token operator">&amp;</span> <span class="token punctuation">(</span>reference_points_cam<span class="token punctuation">[</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">:</span><span class="token number">2</span><span class="token punctuation">]</span> <span class="token operator">&lt;</span> <span class="token number">1.0</span><span class="token punctuation">)</span>                     <span class="token operator">&amp;</span> <span class="token punctuation">(</span>reference_points_cam<span class="token punctuation">[</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">:</span><span class="token number">1</span><span class="token punctuation">]</span> <span class="token operator">&lt;</span> <span class="token number">1.0</span><span class="token punctuation">)</span>                     <span class="token operator">&amp;</span> <span class="token punctuation">(</span>reference_points_cam<span class="token punctuation">[</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">:</span><span class="token number">1</span><span class="token punctuation">]</span> <span class="token operator">></span> <span class="token number">0.0</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre></li></ul></li></ul><p>需要注意的是，上述得到的bev_query以及reference_points_cam参数并不是直接用在了 Spatial Cross-Attention 模块中，而是选择了有用的部分进行使用（减少模型的计算量，提高训练过程的收敛速度），这里还是根据 Deformable Attention Pipeline 中涉及的参数进行说明：</p><ul><li><p>参数queries_rebatch</p><ul><li>之前也有提到，并不是 BEV 坐标系下的每个三维坐标都会映射到环视相机的所有图像上，而只会映射到其中的某几张图片上，所以使用所有来自 Temporal Self-Attention 模块的所有bev_query会消耗很大的计算量，所以这里是对bev_query进行了重新的整合，涉及的核心代码如下：<pre class="line-numbers language-python" data-language="python"><code class="language-python">   indexes <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span><span class="token comment"># 根据每张图片对应的`bev_mask`结果，获取有效query的index</span><span class="token keyword">for</span> i<span class="token punctuation">,</span> mask_per_img <span class="token keyword">in</span> <span class="token builtin">enumerate</span><span class="token punctuation">(</span>bev_mask<span class="token punctuation">)</span><span class="token punctuation">:</span>   index_query_per_img <span class="token operator">=</span> mask_per_img<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">.</span><span class="token builtin">sum</span><span class="token punctuation">(</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">.</span>nonzero<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>squeeze<span class="token punctuation">(</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span>   indexes<span class="token punctuation">.</span>append<span class="token punctuation">(</span>index_query_per_img<span class="token punctuation">)</span>queries_rebatch <span class="token operator">=</span> query<span class="token punctuation">.</span>new_zeros<span class="token punctuation">(</span><span class="token punctuation">[</span>bs <span class="token operator">*</span> self<span class="token punctuation">.</span>num_cams<span class="token punctuation">,</span> max_len<span class="token punctuation">,</span> self<span class="token punctuation">.</span>embed_dims<span class="token punctuation">]</span><span class="token punctuation">)</span>reference_points_rebatch <span class="token operator">=</span> reference_points_cam<span class="token punctuation">.</span>new_zeros<span class="token punctuation">(</span><span class="token punctuation">[</span>bs <span class="token operator">*</span> self<span class="token punctuation">.</span>num_cams<span class="token punctuation">,</span> max_len<span class="token punctuation">,</span> D<span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">]</span><span class="token punctuation">)</span> <span class="token keyword">for</span> i<span class="token punctuation">,</span> reference_points_per_img <span class="token keyword">in</span> <span class="token builtin">enumerate</span><span class="token punctuation">(</span>reference_points_cam<span class="token punctuation">)</span><span class="token punctuation">:</span>   <span class="token keyword">for</span> j <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span>bs<span class="token punctuation">)</span><span class="token punctuation">:</span>      index_query_per_img <span class="token operator">=</span> indexes<span class="token punctuation">[</span>i<span class="token punctuation">]</span>      <span class="token comment"># 重新整合 `bev_query` 特征，记作 `query_rebatch</span>      queries_rebatch<span class="token punctuation">[</span>j <span class="token operator">*</span> self<span class="token punctuation">.</span>num_cams <span class="token operator">+</span> i<span class="token punctuation">,</span> <span class="token punctuation">:</span><span class="token builtin">len</span><span class="token punctuation">(</span>index_query_per_img<span class="token punctuation">)</span><span class="token punctuation">]</span> <span class="token operator">=</span> query<span class="token punctuation">[</span>j<span class="token punctuation">,</span> index_query_per_img<span class="token punctuation">]</span>      <span class="token comment"># 重新整合 `reference_point`采样位置，记作`reference_points_rebatch`</span>      reference_points_rebatch<span class="token punctuation">[</span>j <span class="token operator">*</span> self<span class="token punctuation">.</span>num_cams <span class="token operator">+</span> i<span class="token punctuation">,</span> <span class="token punctuation">:</span><span class="token builtin">len</span><span class="token punctuation">(</span>index_query_per_img<span class="token punctuation">)</span><span class="token punctuation">]</span> <span class="token operator">=</span> reference_points_per_img<span class="token punctuation">[</span>j<span class="token punctuation">,</span> index_query_per_img<span class="token punctuation">]</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre></li></ul></li><li><p>参数reference_points_rebatch</p><ul><li>与产生query_rebatch的原因相同，获得映射到二维图像后的有效位置，对原有的reference_points进行重新的整合reference_points_rebatch。</li></ul></li><li><p>内部参数Offset、Weights、Sample Locations</p></li></ul> <pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token triple-quoted-string string">""" 获取 sampling_offsets，依旧是对 query 做 Linear 做维度的映射，但是需要注意的是这里的 query 指代的是上面提到的 `quries_rebatch` """</span><span class="token comment"># sample 8 points for single ref point in each level.</span><span class="token comment"># sampling_offsets: shape = (bs, max_len, 8, 4, 8, 2)</span>sampling_offsets <span class="token operator">=</span> self<span class="token punctuation">.</span>sampling_offsets<span class="token punctuation">(</span>query<span class="token punctuation">)</span><span class="token punctuation">.</span>view<span class="token punctuation">(</span>bs<span class="token punctuation">,</span> num_query<span class="token punctuation">,</span> self<span class="token punctuation">.</span>num_heads<span class="token punctuation">,</span> self<span class="token punctuation">.</span>num_levels<span class="token punctuation">,</span> self<span class="token punctuation">.</span>num_points<span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">)</span>attention_weights <span class="token operator">=</span> self<span class="token punctuation">.</span>attention_weights<span class="token punctuation">(</span>query<span class="token punctuation">)</span><span class="token punctuation">.</span>view<span class="token punctuation">(</span>bs<span class="token punctuation">,</span> num_query<span class="token punctuation">,</span> self<span class="token punctuation">.</span>num_heads<span class="token punctuation">,</span> self<span class="token punctuation">.</span>num_levels <span class="token operator">*</span> self<span class="token punctuation">.</span>num_points<span class="token punctuation">)</span>attention_weights <span class="token operator">=</span> attention_weights<span class="token punctuation">.</span>softmax<span class="token punctuation">(</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token comment"># attention_weights: shape = (bs, max_len, 8, 4, 8)</span>attention_weights <span class="token operator">=</span> attention_weights<span class="token punctuation">.</span>view<span class="token punctuation">(</span>bs<span class="token punctuation">,</span> num_query<span class="token punctuation">,</span>                                          self<span class="token punctuation">.</span>num_heads<span class="token punctuation">,</span>                                          self<span class="token punctuation">.</span>num_levels<span class="token punctuation">,</span>                                          self<span class="token punctuation">.</span>num_points<span class="token punctuation">)</span><span class="token triple-quoted-string string">""" 生成 sampling location """</span>offset_normalizer <span class="token operator">=</span> torch<span class="token punctuation">.</span>stack<span class="token punctuation">(</span><span class="token punctuation">[</span>spatial_shapes<span class="token punctuation">[</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">,</span> spatial_shapes<span class="token punctuation">[</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span>reference_points <span class="token operator">=</span> reference_points<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span> <span class="token punctuation">:</span><span class="token punctuation">,</span> <span class="token boolean">None</span><span class="token punctuation">,</span> <span class="token boolean">None</span><span class="token punctuation">,</span> <span class="token boolean">None</span><span class="token punctuation">,</span> <span class="token punctuation">:</span><span class="token punctuation">,</span> <span class="token punctuation">:</span><span class="token punctuation">]</span>sampling_offsets <span class="token operator">=</span> sampling_offsets <span class="token operator">/</span> offset_normalizer<span class="token punctuation">[</span><span class="token boolean">None</span><span class="token punctuation">,</span> <span class="token boolean">None</span><span class="token punctuation">,</span> <span class="token boolean">None</span><span class="token punctuation">,</span> <span class="token punctuation">:</span><span class="token punctuation">,</span> <span class="token boolean">None</span><span class="token punctuation">,</span> <span class="token punctuation">:</span><span class="token punctuation">]</span>sampling_locations <span class="token operator">=</span> reference_points <span class="token operator">+</span> sampling_offsets<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><ul><li>输出bev_embedding</li><li>将上述处理好的参数，送入到多尺度可变形注意力模块中生成bev_embedding特征；</li></ul>  <pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token triple-quoted-string string">"""1. value: shape = (cam = 6, sum(h_i * w_i) = 30825, head = 8, dim = 32)2. spatial_shapes = ([[116, 200], [58, 100], [29,  50], [15,  25]])3. level_start_index= [0, 23200, 29000, 30450]4. sampling_locations = (cam, max_len, 8, 4, 8, 2)5. attention_weights = (cam, max_len, 8, 4, 8)6. output = (cam, max_len, 8, 32)"""</span>output <span class="token operator">=</span> MultiScaleDeformableAttnFunction<span class="token punctuation">.</span><span class="token builtin">apply</span><span class="token punctuation">(</span>value<span class="token punctuation">,</span> spatial_shapes<span class="token punctuation">,</span> level_start_index<span class="token punctuation">,</span> sampling_locations<span class="token punctuation">,</span>               attention_weights<span class="token punctuation">,</span> self<span class="token punctuation">.</span>im2col_step<span class="token punctuation">)</span><span class="token triple-quoted-string string">"""最后再将六个环视相机查询到的特征整合到一起，再求一个平均值 """</span><span class="token keyword">for</span> i<span class="token punctuation">,</span> index_query_per_img <span class="token keyword">in</span> <span class="token builtin">enumerate</span><span class="token punctuation">(</span>indexes<span class="token punctuation">)</span><span class="token punctuation">:</span>   <span class="token keyword">for</span> j <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span>bs<span class="token punctuation">)</span><span class="token punctuation">:</span>  <span class="token comment"># slots: (bs, 40000, 256)</span>      slots<span class="token punctuation">[</span>j<span class="token punctuation">,</span> index_query_per_img<span class="token punctuation">]</span> <span class="token operator">+=</span> queries<span class="token punctuation">[</span>j <span class="token operator">*</span> self<span class="token punctuation">.</span>num_cams <span class="token operator">+</span> i<span class="token punctuation">,</span> <span class="token punctuation">:</span><span class="token builtin">len</span><span class="token punctuation">(</span>index_query_per_img<span class="token punctuation">)</span><span class="token punctuation">]</span>count <span class="token operator">=</span> bev_mask<span class="token punctuation">.</span><span class="token builtin">sum</span><span class="token punctuation">(</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span> <span class="token operator">></span> <span class="token number">0</span>count <span class="token operator">=</span> count<span class="token punctuation">.</span>permute<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">)</span><span class="token punctuation">.</span><span class="token builtin">sum</span><span class="token punctuation">(</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span>count <span class="token operator">=</span> torch<span class="token punctuation">.</span>clamp<span class="token punctuation">(</span>count<span class="token punctuation">,</span> <span class="token builtin">min</span><span class="token operator">=</span><span class="token number">1.0</span><span class="token punctuation">)</span>slots <span class="token operator">=</span> slots <span class="token operator">/</span> count<span class="token punctuation">[</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token boolean">None</span><span class="token punctuation">]</span>  <span class="token comment"># maybe normalize.</span>slots <span class="token operator">=</span> self<span class="token punctuation">.</span>output_proj<span class="token punctuation">(</span>slots<span class="token punctuation">)</span>  <span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>以上就是 Spatial Cross-Attention 模块的整体逻辑。</p><p>将 Temporal Self-Attetion 模块和 Spatial Cross-Attention 模块堆叠在一起，并重复六次，最终得到的 BEV Embedding 特征作为下游 3D 目标检测和道路分割任务的 BEV 空间特征。</p><h2 id="Decoder模块"><a href="#Decoder模块" class="headerlink" title="Decoder模块"></a>Decoder模块</h2><p>上述产生 BEV 特征的过程是用了当前输入到网络模型中除当前帧外，之前所有帧的特征去迭代修正去获得prev_bev的特征；所以在利用 Decoder 模块进行解码之前，<strong>需要对当前时刻环视的 6 张图片同样利用 Backbone + Neck 提取多尺度的特征，然后利用上述的 Temporal Self-Attention 模块和 Spatial Cross-Attention 模块的逻辑生成当前时刻的bev_embedding</strong>，然后将这部分特征送入到 Decoder 中进行 3D 目标检测。</p><p>下面分析 Decoder 模块是如何获得预测框和分类得分的。</p><ul><li><p>query、query_pos </p><ul><li>首先是object_query_embed参数，该参数同样是沿用了 2D 目标检测中的 Deformable DETR 的思想。query和query_pose 全都是可学习的。模型直接用 nn.Embedding() 生成一组（900，512）维的张量。然后将 512 维的张量分成两组，分别构成了query &#x3D; (900，256)和query_pos &#x3D; (900，256) 。</li></ul></li><li><p>referece_points</p><ul><li>之前介绍过，对于多尺度可变形注意力模块是需要参考点的，但是在预测过程中是没有参考点的，这就需要网络学习出来，网络是靠 query_pos学习得到的，核心代码如下：<pre class="line-numbers language-python" data-language="python"><code class="language-python">reference_points <span class="token operator">=</span> self<span class="token punctuation">.</span>reference_points<span class="token punctuation">(</span>query_pos<span class="token punctuation">)</span>  <span class="token comment"># (bs, 900, 3)  3 代表 (x, y, z) 坐标</span>reference_points <span class="token operator">=</span> reference_points<span class="token punctuation">.</span>sigmoid<span class="token punctuation">(</span><span class="token punctuation">)</span>  <span class="token comment"># absolute -> relative</span>init_reference_out <span class="token operator">=</span> reference_points <span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre></li></ul></li><li><p>Decoder 逻辑</p><ul><li>在获取到需要用到的query、query_pos、reference_points参数后，后面的逻辑有些类似 Deformabe DETR 的 Decoder 过程，简单概括如下几点：<ul><li>利用query和query_pos去做常规的 Self-Attention 运算更新query；</li><li>利用 Self-Attention 得到的 query，之前获得的 bev_embedding作为value，query_pos，由 query生成的reference_points（虽然生成的x，y，z参考点位置，但是 BEV Embedding 是二维的，所以参考点只选择了前两维）仿照 Deformable Attention Module 的 pipeline 做可变形注意力；</li></ul></li><li>可变形注意力核心代码如下：</li></ul><pre class="line-numbers language-python" data-language="python"><code class="language-python">   <span class="token triple-quoted-string string">""" 由 query 生成 sampling_offsets 和 attention_weights """</span>sampling_offsets <span class="token operator">=</span> self<span class="token punctuation">.</span>sampling_offsets<span class="token punctuation">(</span>query<span class="token punctuation">)</span><span class="token punctuation">.</span>view<span class="token punctuation">(</span>            bs<span class="token punctuation">,</span> num_query<span class="token punctuation">,</span> self<span class="token punctuation">.</span>num_heads<span class="token punctuation">,</span> self<span class="token punctuation">.</span>num_levels<span class="token punctuation">,</span> self<span class="token punctuation">.</span>num_points<span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">)</span>  <span class="token comment"># (bs, 900, 8, 1, 4, 2)</span>attention_weights <span class="token operator">=</span> self<span class="token punctuation">.</span>attention_weights<span class="token punctuation">(</span>query<span class="token punctuation">)</span><span class="token punctuation">.</span>view<span class="token punctuation">(</span>            bs<span class="token punctuation">,</span> num_query<span class="token punctuation">,</span> self<span class="token punctuation">.</span>num_heads<span class="token punctuation">,</span> self<span class="token punctuation">.</span>num_levels <span class="token operator">*</span> self<span class="token punctuation">.</span>num_points<span class="token punctuation">)</span>  <span class="token comment"># (bs, 900, 8, 4)</span>attention_weights <span class="token operator">=</span> attention_weights<span class="token punctuation">.</span>softmax<span class="token punctuation">(</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span>attention_weights <span class="token operator">=</span> attention_weights<span class="token punctuation">.</span>view<span class="token punctuation">(</span>bs<span class="token punctuation">,</span> num_query<span class="token punctuation">,</span>                                                   self<span class="token punctuation">.</span>num_heads<span class="token punctuation">,</span>                                                   self<span class="token punctuation">.</span>num_levels<span class="token punctuation">,</span>                                                   self<span class="token punctuation">.</span>num_points<span class="token punctuation">)</span>  <span class="token comment"># (bs, 900, 8, 1, 4)</span><span class="token triple-quoted-string string">""" sampling_offsets 和 reference_points 得到 sampling_locations """</span>offset_normalizer <span class="token operator">=</span> torch<span class="token punctuation">.</span>stack<span class="token punctuation">(</span>               <span class="token punctuation">[</span>spatial_shapes<span class="token punctuation">[</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">,</span> spatial_shapes<span class="token punctuation">[</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span>sampling_locations <span class="token operator">=</span> reference_points<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span> <span class="token punctuation">:</span><span class="token punctuation">,</span> <span class="token boolean">None</span><span class="token punctuation">,</span> <span class="token punctuation">:</span><span class="token punctuation">,</span> <span class="token boolean">None</span><span class="token punctuation">,</span> <span class="token punctuation">:</span><span class="token punctuation">]</span> \               <span class="token operator">+</span> sampling_offsets \               <span class="token operator">/</span> offset_normalizer<span class="token punctuation">[</span><span class="token boolean">None</span><span class="token punctuation">,</span> <span class="token boolean">None</span><span class="token punctuation">,</span> <span class="token boolean">None</span><span class="token punctuation">,</span> <span class="token punctuation">:</span><span class="token punctuation">,</span> <span class="token boolean">None</span><span class="token punctuation">,</span> <span class="token punctuation">:</span><span class="token punctuation">]</span><span class="token triple-quoted-string string">""" 多尺度可变形注意力模块 """</span><span class="token comment"># value: shape = (bs, 40000, 8, 32)</span><span class="token comment"># spatial_shapes = (200, 200)</span><span class="token comment"># level_start_index = 0</span><span class="token comment"># sampling_locations = (bs, 900, 8, 1, 4, 2)</span><span class="token comment"># attention_weights = (bs, 900, 8, 1, 4)</span><span class="token comment"># output = (bs, 900, 256)</span>output <span class="token operator">=</span> MultiScaleDeformableAttnFunction<span class="token punctuation">.</span><span class="token builtin">apply</span><span class="token punctuation">(</span>value<span class="token punctuation">,</span> spatial_shapes<span class="token punctuation">,</span> level_start_index<span class="token punctuation">,</span> sampling_locations<span class="token punctuation">,</span>               attention_weights<span class="token punctuation">,</span> self<span class="token punctuation">.</span>im2col_step<span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><ul><li><p>在获得查询到的特征后，会利用回归分支（FFN 网络）对提取的特征计算回归结果，预测 10 个输出；</p><ul><li>这 10 个维度的含义为：[xc，yc，w，l，zc，h，rot.sin()，rot.cos()，vx，vy]；</li><li>[预测框中心位置的x方向偏移，预测框中心位置的y方向偏移，预测框的宽，预测框的长，预测框中心位置的z方向偏移，预测框的高，旋转角的正弦值，旋转角的余弦值，x方向速度，y方向速度]；</li></ul></li><li><p>然后根据预测的偏移量，对参考点的位置进行更新，为级联的下一个 Decoder 提高精修过的参考点位置，核心代码如下：</p></li></ul><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">if</span> reg_branches <span class="token keyword">is</span> <span class="token keyword">not</span> <span class="token boolean">None</span><span class="token punctuation">:</span>  <span class="token comment"># update the reference point.</span> tmp <span class="token operator">=</span> reg_branches<span class="token punctuation">[</span>lid<span class="token punctuation">]</span><span class="token punctuation">(</span>output<span class="token punctuation">)</span>  <span class="token comment"># (bs, 900, 256) -> (bs, 900, 10) 回归分支的预测输出</span> <span class="token keyword">assert</span> reference_points<span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">]</span> <span class="token operator">==</span> <span class="token number">3</span> new_reference_points <span class="token operator">=</span> torch<span class="token punctuation">.</span>zeros_like<span class="token punctuation">(</span>reference_points<span class="token punctuation">)</span> <span class="token comment"># 预测出来的偏移量是绝对量</span> new_reference_points<span class="token punctuation">[</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token punctuation">:</span><span class="token number">2</span><span class="token punctuation">]</span> <span class="token operator">=</span> tmp<span class="token punctuation">[</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token punctuation">:</span><span class="token number">2</span><span class="token punctuation">]</span> <span class="token operator">+</span> inverse_sigmoid<span class="token punctuation">(</span>reference_points<span class="token punctuation">[</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token punctuation">:</span><span class="token number">2</span><span class="token punctuation">]</span><span class="token punctuation">)</span>  <span class="token comment"># 框中心处的 x, y 坐标</span> new_reference_points<span class="token punctuation">[</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">:</span><span class="token number">3</span><span class="token punctuation">]</span> <span class="token operator">=</span> tmp<span class="token punctuation">[</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token number">4</span><span class="token punctuation">:</span><span class="token number">5</span><span class="token punctuation">]</span> <span class="token operator">+</span> inverse_sigmoid<span class="token punctuation">(</span>reference_points<span class="token punctuation">[</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">:</span><span class="token number">3</span><span class="token punctuation">]</span><span class="token punctuation">)</span>  <span class="token comment"># 框中心处的 z 坐标</span> <span class="token comment"># 参考点坐标是一个归一化的坐标</span> new_reference_points <span class="token operator">=</span> new_reference_points<span class="token punctuation">.</span>sigmoid<span class="token punctuation">(</span><span class="token punctuation">)</span> reference_points <span class="token operator">=</span> new_reference_points<span class="token punctuation">.</span>detach<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token triple-quoted-string string">""" 最后将每层 Decoder 产生的特征 = (bs, 900, 256)，以及参考点坐标 = (bs, 900, 3) 保存下来。"""</span><span class="token keyword">if</span> self<span class="token punctuation">.</span>return_intermediate<span class="token punctuation">:</span>   intermediate<span class="token punctuation">.</span>append<span class="token punctuation">(</span>output<span class="token punctuation">)</span>   intermediate_reference_points<span class="token punctuation">.</span>append<span class="token punctuation">(</span>reference_points<span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><ul><li>然后将层级的 bev_embedding特征以及参考点通过 for loop 的形式，一次计算每个 Decoder 层的分类和回归结果：</li></ul><pre class="line-numbers language-python" data-language="python"><code class="language-python">bev_embed<span class="token punctuation">,</span> hs<span class="token punctuation">,</span> init_reference<span class="token punctuation">,</span> inter_references <span class="token operator">=</span> outputshs <span class="token operator">=</span> hs<span class="token punctuation">.</span>permute<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">)</span>  <span class="token comment"># (decoder_level, bs, 900, 256)</span>outputs_classes <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>outputs_coords <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span><span class="token keyword">for</span> lvl <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span>hs<span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">:</span>   <span class="token keyword">if</span> lvl <span class="token operator">==</span> <span class="token number">0</span><span class="token punctuation">:</span>      reference <span class="token operator">=</span> init_reference   <span class="token keyword">else</span><span class="token punctuation">:</span>      reference <span class="token operator">=</span> inter_references<span class="token punctuation">[</span>lvl <span class="token operator">-</span> <span class="token number">1</span><span class="token punctuation">]</span>   reference <span class="token operator">=</span> inverse_sigmoid<span class="token punctuation">(</span>reference<span class="token punctuation">)</span>   outputs_class <span class="token operator">=</span> self<span class="token punctuation">.</span>cls_branches<span class="token punctuation">[</span>lvl<span class="token punctuation">]</span><span class="token punctuation">(</span>hs<span class="token punctuation">[</span>lvl<span class="token punctuation">]</span><span class="token punctuation">)</span>  <span class="token comment"># (bs, 900, num_classes)</span>   tmp <span class="token operator">=</span> self<span class="token punctuation">.</span>reg_branches<span class="token punctuation">[</span>lvl<span class="token punctuation">]</span><span class="token punctuation">(</span>hs<span class="token punctuation">[</span>lvl<span class="token punctuation">]</span><span class="token punctuation">)</span>  <span class="token comment"># (bs, 900, 10)</span>   <span class="token keyword">assert</span> reference<span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">]</span> <span class="token operator">==</span> <span class="token number">3</span>   tmp<span class="token punctuation">[</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">:</span><span class="token number">2</span><span class="token punctuation">]</span> <span class="token operator">+=</span> reference<span class="token punctuation">[</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">:</span><span class="token number">2</span><span class="token punctuation">]</span>  <span class="token comment"># (x, y)</span>   tmp<span class="token punctuation">[</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">:</span><span class="token number">2</span><span class="token punctuation">]</span> <span class="token operator">=</span> tmp<span class="token punctuation">[</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">:</span><span class="token number">2</span><span class="token punctuation">]</span><span class="token punctuation">.</span>sigmoid<span class="token punctuation">(</span><span class="token punctuation">)</span>   tmp<span class="token punctuation">[</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token number">4</span><span class="token punctuation">:</span><span class="token number">5</span><span class="token punctuation">]</span> <span class="token operator">+=</span> reference<span class="token punctuation">[</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">:</span><span class="token number">3</span><span class="token punctuation">]</span>   tmp<span class="token punctuation">[</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token number">4</span><span class="token punctuation">:</span><span class="token number">5</span><span class="token punctuation">]</span> <span class="token operator">=</span> tmp<span class="token punctuation">[</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token number">4</span><span class="token punctuation">:</span><span class="token number">5</span><span class="token punctuation">]</span><span class="token punctuation">.</span>sigmoid<span class="token punctuation">(</span><span class="token punctuation">)</span>   tmp<span class="token punctuation">[</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">:</span><span class="token number">1</span><span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token punctuation">(</span>tmp<span class="token punctuation">[</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">:</span><span class="token number">1</span><span class="token punctuation">]</span> <span class="token operator">*</span> <span class="token punctuation">(</span>self<span class="token punctuation">.</span>pc_range<span class="token punctuation">[</span><span class="token number">3</span><span class="token punctuation">]</span> <span class="token operator">-</span> self<span class="token punctuation">.</span>pc_range<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">)</span> <span class="token operator">+</span> self<span class="token punctuation">.</span>pc_range<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">)</span>   tmp<span class="token punctuation">[</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">:</span><span class="token number">2</span><span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token punctuation">(</span>tmp<span class="token punctuation">[</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">:</span><span class="token number">2</span><span class="token punctuation">]</span> <span class="token operator">*</span> <span class="token punctuation">(</span>self<span class="token punctuation">.</span>pc_range<span class="token punctuation">[</span><span class="token number">4</span><span class="token punctuation">]</span> <span class="token operator">-</span> self<span class="token punctuation">.</span>pc_range<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">)</span> <span class="token operator">+</span> self<span class="token punctuation">.</span>pc_range<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">)</span>   tmp<span class="token punctuation">[</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token number">4</span><span class="token punctuation">:</span><span class="token number">5</span><span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token punctuation">(</span>tmp<span class="token punctuation">[</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token number">4</span><span class="token punctuation">:</span><span class="token number">5</span><span class="token punctuation">]</span> <span class="token operator">*</span> <span class="token punctuation">(</span>self<span class="token punctuation">.</span>pc_range<span class="token punctuation">[</span><span class="token number">5</span><span class="token punctuation">]</span> <span class="token operator">-</span> self<span class="token punctuation">.</span>pc_range<span class="token punctuation">[</span><span class="token number">2</span><span class="token punctuation">]</span><span class="token punctuation">)</span> <span class="token operator">+</span> self<span class="token punctuation">.</span>pc_range<span class="token punctuation">[</span><span class="token number">2</span><span class="token punctuation">]</span><span class="token punctuation">)</span>   outputs_coord <span class="token operator">=</span> tmp   outputs_classes<span class="token punctuation">.</span>append<span class="token punctuation">(</span>outputs_class<span class="token punctuation">)</span>   outputs_coords<span class="token punctuation">.</span>append<span class="token punctuation">(</span>outputs_coord<span class="token punctuation">)</span> <span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><ul><li>分类分支的网络结构：</li></ul><pre class="line-numbers language-python" data-language="python"><code class="language-python">Sequential<span class="token punctuation">(</span><span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">)</span><span class="token punctuation">:</span> Linear<span class="token punctuation">(</span>in_features<span class="token operator">=</span><span class="token number">256</span><span class="token punctuation">,</span> out_features<span class="token operator">=</span><span class="token number">256</span><span class="token punctuation">,</span> bias<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span><span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">:</span> LayerNorm<span class="token punctuation">(</span><span class="token punctuation">(</span><span class="token number">256</span><span class="token punctuation">,</span><span class="token punctuation">)</span><span class="token punctuation">,</span> eps<span class="token operator">=</span><span class="token number">1e-05</span><span class="token punctuation">,</span> elementwise_affine<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span><span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">:</span> ReLU<span class="token punctuation">(</span>inplace<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span><span class="token punctuation">(</span><span class="token number">3</span><span class="token punctuation">)</span><span class="token punctuation">:</span> Linear<span class="token punctuation">(</span>in_features<span class="token operator">=</span><span class="token number">256</span><span class="token punctuation">,</span> out_features<span class="token operator">=</span><span class="token number">256</span><span class="token punctuation">,</span> bias<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span><span class="token punctuation">(</span><span class="token number">4</span><span class="token punctuation">)</span><span class="token punctuation">:</span> LayerNorm<span class="token punctuation">(</span><span class="token punctuation">(</span><span class="token number">256</span><span class="token punctuation">,</span><span class="token punctuation">)</span><span class="token punctuation">,</span> eps<span class="token operator">=</span><span class="token number">1e-05</span><span class="token punctuation">,</span> elementwise_affine<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span><span class="token punctuation">(</span><span class="token number">5</span><span class="token punctuation">)</span><span class="token punctuation">:</span> ReLU<span class="token punctuation">(</span>inplace<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span><span class="token punctuation">(</span><span class="token number">6</span><span class="token punctuation">)</span><span class="token punctuation">:</span> Linear<span class="token punctuation">(</span>in_features<span class="token operator">=</span><span class="token number">256</span><span class="token punctuation">,</span> out_features<span class="token operator">=</span><span class="token number">10</span><span class="token punctuation">,</span> bias<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><ul><li>回归分支的网络结构<pre class="line-numbers language-python" data-language="python"><code class="language-python">Sequential<span class="token punctuation">(</span><span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">)</span><span class="token punctuation">:</span> Linear<span class="token punctuation">(</span>in_features<span class="token operator">=</span><span class="token number">256</span><span class="token punctuation">,</span> out_features<span class="token operator">=</span><span class="token number">256</span><span class="token punctuation">,</span> bias<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span><span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">:</span> ReLU<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">:</span> Linear<span class="token punctuation">(</span>in_features<span class="token operator">=</span><span class="token number">256</span><span class="token punctuation">,</span> out_features<span class="token operator">=</span><span class="token number">256</span><span class="token punctuation">,</span> bias<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span><span class="token punctuation">(</span><span class="token number">3</span><span class="token punctuation">)</span><span class="token punctuation">:</span> ReLU<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">(</span><span class="token number">4</span><span class="token punctuation">)</span><span class="token punctuation">:</span> Linear<span class="token punctuation">(</span>in_features<span class="token operator">=</span><span class="token number">256</span><span class="token punctuation">,</span> out_features<span class="token operator">=</span><span class="token number">10</span><span class="token punctuation">,</span> bias<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre></li></ul></li></ul><h2 id="正负样本的定义"><a href="#正负样本的定义" class="headerlink" title="正负样本的定义"></a>正负样本的定义</h2><p>正负样本的定义用到的就是匈牙利匹配算法，分类损失和类似回归损失的总损失和最小；</p><ul><li><p>分类损失的计算代码如下：</p> <pre class="line-numbers language-python" data-language="python"><code class="language-python">cls_pred <span class="token operator">=</span> cls_pred<span class="token punctuation">.</span>sigmoid<span class="token punctuation">(</span><span class="token punctuation">)</span>  <span class="token comment"># calculate the neg_cost and pos_cost by focal loss.</span>neg_cost <span class="token operator">=</span> <span class="token operator">-</span><span class="token punctuation">(</span><span class="token number">1</span> <span class="token operator">-</span> cls_pred <span class="token operator">+</span> self<span class="token punctuation">.</span>eps<span class="token punctuation">)</span><span class="token punctuation">.</span>log<span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token operator">*</span> <span class="token punctuation">(</span><span class="token number">1</span> <span class="token operator">-</span> self<span class="token punctuation">.</span>alpha<span class="token punctuation">)</span> <span class="token operator">*</span> cls_pred<span class="token punctuation">.</span><span class="token builtin">pow</span><span class="token punctuation">(</span>self<span class="token punctuation">.</span>gamma<span class="token punctuation">)</span>pos_cost <span class="token operator">=</span> <span class="token operator">-</span><span class="token punctuation">(</span>cls_pred <span class="token operator">+</span> self<span class="token punctuation">.</span>eps<span class="token punctuation">)</span><span class="token punctuation">.</span>log<span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token operator">*</span> self<span class="token punctuation">.</span>alpha <span class="token operator">*</span> <span class="token punctuation">(</span><span class="token number">1</span> <span class="token operator">-</span> cls_pred<span class="token punctuation">)</span><span class="token punctuation">.</span><span class="token builtin">pow</span><span class="token punctuation">(</span>self<span class="token punctuation">.</span>gamma<span class="token punctuation">)</span>cls_cost <span class="token operator">=</span> pos_cost<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span> gt_labels<span class="token punctuation">]</span> <span class="token operator">-</span> neg_cost<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span> gt_labels<span class="token punctuation">]</span>cls_cost <span class="token operator">=</span> cls_cost <span class="token operator">*</span> self<span class="token punctuation">.</span>weight<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span></span></code></pre></li><li><p>类回归损失的计算代码如下：</p><ul><li>这里介绍一下，gt_box 的表示方式，gt_box 的维度是九维的，分别是 [xc，yc，zc，w，l，h，rot，vx，vy]；而预测结果框的维度是十维的，所以要对 gt_box 的维度进行转换，转换为的维度表示为 [xc，yc，w，l，cz，h，rot.sin()，rot.cos()，vx，vy]</li><li>对应代码如下：</li></ul><pre class="line-numbers language-python" data-language="python"><code class="language-python">cx <span class="token operator">=</span> bboxes<span class="token punctuation">[</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">:</span><span class="token number">1</span><span class="token punctuation">]</span>cy <span class="token operator">=</span> bboxes<span class="token punctuation">[</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">:</span><span class="token number">2</span><span class="token punctuation">]</span>cz <span class="token operator">=</span> bboxes<span class="token punctuation">[</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">:</span><span class="token number">3</span><span class="token punctuation">]</span>w <span class="token operator">=</span> bboxes<span class="token punctuation">[</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">:</span><span class="token number">4</span><span class="token punctuation">]</span><span class="token punctuation">.</span>log<span class="token punctuation">(</span><span class="token punctuation">)</span>l <span class="token operator">=</span> bboxes<span class="token punctuation">[</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token number">4</span><span class="token punctuation">:</span><span class="token number">5</span><span class="token punctuation">]</span><span class="token punctuation">.</span>log<span class="token punctuation">(</span><span class="token punctuation">)</span>h <span class="token operator">=</span> bboxes<span class="token punctuation">[</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token number">5</span><span class="token punctuation">:</span><span class="token number">6</span><span class="token punctuation">]</span><span class="token punctuation">.</span>log<span class="token punctuation">(</span><span class="token punctuation">)</span>rot <span class="token operator">=</span> bboxes<span class="token punctuation">[</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token number">6</span><span class="token punctuation">:</span><span class="token number">7</span><span class="token punctuation">]</span>vx <span class="token operator">=</span> bboxes<span class="token punctuation">[</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token number">7</span><span class="token punctuation">:</span><span class="token number">8</span><span class="token punctuation">]</span> vy <span class="token operator">=</span> bboxes<span class="token punctuation">[</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token number">8</span><span class="token punctuation">:</span><span class="token number">9</span><span class="token punctuation">]</span>normalized_bboxes <span class="token operator">=</span> torch<span class="token punctuation">.</span>cat<span class="token punctuation">(</span><span class="token punctuation">(</span>cx<span class="token punctuation">,</span> cy<span class="token punctuation">,</span> w<span class="token punctuation">,</span> l<span class="token punctuation">,</span> cz<span class="token punctuation">,</span> h<span class="token punctuation">,</span> rot<span class="token punctuation">.</span>sin<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span> rot<span class="token punctuation">.</span>cos<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span> vx<span class="token punctuation">,</span> vy<span class="token punctuation">)</span><span class="token punctuation">,</span> dim<span class="token operator">=</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre></li><li><p>计算类回归损失（L1 Loss）</p><ul><li>这里有一点需要注意的是，在正负样本定义中计算 L1 Loss 的时候，只对前预测框和真值框的前 8 维计算损失：<pre class="line-numbers language-python" data-language="python"><code class="language-python">self<span class="token punctuation">.</span>reg_cost<span class="token punctuation">(</span>bbox_pred<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span> <span class="token punctuation">:</span><span class="token number">8</span><span class="token punctuation">]</span><span class="token punctuation">,</span> normalized_gt_bboxes<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span> <span class="token punctuation">:</span><span class="token number">8</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre></li></ul></li></ul><h2 id="损失的计算"><a href="#损失的计算" class="headerlink" title="损失的计算"></a>损失的计算</h2><p>损失的计算就是分类损失以及 L1 Loss，这里的 L1 Loss 就是对真值框和预测框的10个维度计算 L1 Loss了，计算出来损失，反向传播更新模型的参数。</p>]]></content>
      
      
      <categories>
          
          <category> DL </category>
          
      </categories>
      
      
        <tags>
            
            <tag> bev </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>BEV感知中的时序融合方法（部分待更新）</title>
      <link href="/2023/11/28/bev-time-fusion/"/>
      <url>/2023/11/28/bev-time-fusion/</url>
      
        <content type="html"><![CDATA[<h1 id="BEV感知中的时序融合方法（部分待更新）"><a href="#BEV感知中的时序融合方法（部分待更新）" class="headerlink" title="BEV感知中的时序融合方法（部分待更新）"></a>BEV感知中的时序融合方法（部分待更新）</h1><p><a href="https://zhuanlan.zhihu.com/p/583682754">参考链接</a></p><p>首先展示目前的融合时序的BEV感知算法</p><p> <img src="/pic/time2.jpg" alt="BEV感知模型时序融合方法精简汇总"></p><p>在传统感知算法中,时序融合是提高感知算法准确性和连续性的关键,可以弥补单帧感知的局限性,增加感受野,改善目标检测(Object Detection)帧间跳变和目标遮挡问题,更加准确地判断目标运动速度,同时也对目标预测(Prediction)和跟踪(Tracking)有重要作用.在BEV感知中,时序融合同样可以发挥相应的作用.同时,由于前序帧相关信息可以直接从缓存中读取,并不会带来性能上的大幅下降.下图直观地展示了时序融合的工作原理:</p><p> <img src="/pic/time1.png" alt="参考BEVFormer论文"></p><ul><li>传统的时序融合主要是在后处理中使用RNN或卡尔曼滤波等方式进行融合,这种方式由于要增加额外的开销,影响模型的性能,所以近年来大量采用的是特征级融合.</li><li>特征级融合是继前融合,后融合新提出来的方法,不仅可以用在多传感器融合,也可以用在时序融合,具有跨模态,跨时空的特点.</li><li>而BEV感知由于自身的特点,存在两个特征域:图像域(自车camera图像坐标系)和BEV域(自车lidar坐标系),这一点可以区别于传统感知算法只有图像域特征,从而BEV感知的时序融合可以在两个特征域任意一个进行,具体融合的方法也有两种:<ul><li>基于CNN的方式，其中基于CNN的方式又可使用2D卷积和3D卷积</li><li>基于Transformer的方式,</li><li>基于CNN和Transformer结合的方法.</li></ul></li><li>本文对BEV时序方法的分类主要基于以上几个方面,论文来源基本是2022年的工作.另外由于本文篇幅较长,文末提供精简归纳表格,欢迎阅读下篇获取.</li></ul><p>在具体的时序融合方法上,我们主要关注以下几个对融合结果影响较大的方面:</p><ul><li>一是如何选择前序帧,这个决定了时序融合的有效范围,</li><li>二是如何进行时空对齐(alignment),即将前序帧特征通过ego-motion进行转换,使之与当前帧特征处于同一个坐标系下,这样才可以进行准确的融合,</li><li>三是融合的具体方法,</li><li>最后是融合的分辨率,是融合效果和性能的折中选择.</li><li>数据集方面,以下大部分模型都使用nuscenes数据集,该数据集有1000个场景(scenes),每个scene包括20+精细标注的关键帧(key frame),间隔0.5秒,每两个关键帧之间存在若干无精细标注的非关键帧(sweeps).</li></ul><h2 id="基于Transformer的BEV特征融合"><a href="#基于Transformer的BEV特征融合" class="headerlink" title="基于Transformer的BEV特征融合"></a>基于Transformer的BEV特征融合</h2><h3 id="BEVFormer"><a href="#BEVFormer" class="headerlink" title="BEVFormer"></a>BEVFormer</h3><p>BEVFormer是相对比较早的一个经典BEV感知模型,主体框架是基于transformer生成bev feature,再做基于DETR的目标检测,在之前博客里有详细介绍,主要motion是针对DETR3D的改进,</p><ul><li>一是DETR3D只有基于稀疏的object query的decoder, BEVFormer增加了基于稠密的bev query的encoder,可以生成稠密的bev feature,</li><li>二是由于有了bev feature,方便进行稠密的任务,如语义分割等,也方便进行时序融合.时序融合在encoder中的Temporal Self-Attention中实现,这个模块本质上就是deformable attention(来自于deformable DETR),只是<strong>query做了前序帧和当前帧的拼接</strong>.</li></ul><p>BEVformer在前序帧的选择上,是在<strong>前面4帧中随机选3帧(只包括关键帧)</strong>,所以时序范围为2秒,这3帧不是一次性输入,而是<strong>迭代地进行两两融合</strong>,第一帧由于没有前序帧,只与自己本身融合,也就是每个iteration需要跑<strong>4次前向传播和1次反向传播</strong>.前序bev feature在缓存中直接读取,不会降低推理的效率.</p><p>时空对齐方面,由于是<strong>BEV特征域融合</strong>,而两帧的bev特征分别在两帧的自车lidar坐标系下,所以<strong>需要将前序帧的lidar坐标通过ego-motion转换到当前帧的lidar坐标</strong>.这里面又包括两种方式:</p><ul><li>变换bev feature和变换reference_points(即密集query对应的坐标值),两种方法需要做的变换略有不同.论文中的做法是旋转feature,平移reference_points,</li><li>这里存在一个问题就是论文中旋转feature的方式会产生全0的黑边,不利于后续的融合,而变换reference_points在后续的grid_sample环节会有插值作用,会更加准确.</li></ul><p>具体的融合方式上,论文中是在Temporal Self-Attention模块中把时空对齐后的前序bev feature和当前bev feature分别做deformable attention,再在h*w平面做算术平均进行融合.这里算术平均有点简单粗暴,也可以修改为自适应的融合方式.融合分辨率也就是encoder中query的数量，论文中用的比较大,是200*200,而decoder的query数量与与之类似的经典bev3D模型DETR3D相同,为900.其中,DETR3D无时序版本,并且只有decoder.</p><h3 id="PolarDETR-华中科大-地平线机器人"><a href="#PolarDETR-华中科大-地平线机器人" class="headerlink" title="PolarDETR(华中科大,地平线机器人)"></a>PolarDETR(华中科大,地平线机器人)</h3><p>[Polar Parametrization for Vision-based Surround-View 3D Detection](<a href="https://arxiv.org/abs/">https://arxiv.org/abs/</a><br>2206.10965)<br><a href="https://github.com/hustvl/PolarDETR">代码链接</a></p><p> <img src="/pic/time3.png" alt="网络框架图"></p><p>PolarDETR在整体框架上接近于DETR3D,主要不同点一是bev特征和目标位置的表征和从笛卡尔坐标系转换到了极坐标系,即由半径r,方位角α, 高度z进行表征,二是加入了时序融合.</p><p>为什么使用极坐标系的解释,如下图,假设目标At1和At2由于位置和朝向刚好匹配,在两个2D视角内的呈现是完全相同的,bev有效检测范围是d,这时候在笛卡尔坐标系中At1将被过滤掉,而At2会被保留下来,这对于模型训练来讲显然是不利于收敛的,问题就在于笛卡尔坐标系的各个边界点距中心的距离不一致.而如果使用极坐标系,只要两个目标距自车距离相等,就将被同等对待.</p><p> <img src="/pic/time4.png"></p><p>关于时序融合,本文采用的方式和BEVFormer类似,也是<strong>基于transformer的bev特征融合</strong>,只是这里融合的是</p><ul><li>&#x3D;&#x3D;代表目标的object query,而不是代表bev feature的bev query&#x3D;&#x3D;.</li><li>时序对齐的方法是在极坐标bev下,把当前帧采样点投影到前序帧获取特征,类似于变换reference_points.</li><li>融合方法是<strong>所有帧channel维度拼接后做self attention</strong>,区别于BEVFormer的两两迭代融合,但具体用了多少帧由于代码还未公开所以不确定.</li></ul><p> <img src="/pic/time5.png" alt="性能对比表"></p><h2 id="基于CNN-2D-x2F-3D-conv-的BEV特征融合"><a href="#基于CNN-2D-x2F-3D-conv-的BEV特征融合" class="headerlink" title="基于CNN(2D&#x2F;3D conv)的BEV特征融合"></a>基于CNN(2D&#x2F;3D conv)的BEV特征融合</h2><h3 id="BEVDet4D-x2F-BEVDepth4D-鉴智机器人"><a href="#BEVDet4D-x2F-BEVDepth4D-鉴智机器人" class="headerlink" title="BEVDet4D&#x2F;BEVDepth4D(鉴智机器人)"></a>BEVDet4D&#x2F;BEVDepth4D(鉴智机器人)</h3><p> <img src="/pic/time6.png" alt="网络结构图"></p><p>BEVDet4D和BEVDepth4D是基于BEVDet和BEVDepth增加时序融合的版本.二者框架非常类似,与BEVFormer属于不同的两种bev表征的方式。BEVDet系列属于基于LSS思想,多视角特征先通过深度估计网络进行像素级的深度估计,再投影到bev空间,通过基于CNN的bev encoder进行编码后连接Centerpoint检测头.BEVDepth4D的主要改进是增加了对深度估计网络的监督,使结果更准确.</p><p>具体来说，BEVDet4D的时序融合<strong>发生在投影到bev空间得到bev feature后</strong>,与前序帧先经过时空对齐,在channel维度拼接,再送入bev encoder进行融合.这里的</p><ul><li>时空对齐是使用grid_sample把前序帧特征warp到当前帧,和直接旋转平移feature或reference_points本质上相同,但博主认为,如果后续还要做deformable self attention进行融合的话,这样处理效率较低,因为还需要再做一次grid_sample来取相应的value,还是直接对reference_points进行变换可以获得较高的效率.不过</li><li>这里后续是使用CNN进行融合,影响不大.CNN这里用的是2D卷积.因为这种架构需要额外的深度估计网络,所以bev feature分辨率不能太大,文中采用了16倍下采样.</li><li>在前序帧的选择上,训练阶段是在前3帧或后3帧随机选1帧,推理阶段只在前3帧随机选一帧.训练阶段把后续帧也加进来可以提高鲁棒性.</li></ul><h3 id="BEVerse"><a href="#BEVerse" class="headerlink" title="BEVerse"></a>BEVerse</h3><p><a href="https://arxiv.org/abs/2205.09743">论文链接</a><br><a href="https://github.com/zhangyp15/BEVerse">代码链接</a><br><a href="https://zhuanlan.zhihu.com/p/518147623">学习文章</a></p><p> <img src="/pic/time7.png" alt="网络结构图"></p><p>BEVerse是一个感知预测一体化模型,</p><ul><li>主体基于LSS生成bev feature,</li><li>再经过spatial-temporal bev encoder进行时空编码,-</li><li>再进行下游的检测分割和预测任务.</li></ul><p>由于需要做预测,时序融合成为重要部分,并且需要前序帧和后续帧都要加入训练,选择的帧数相应也会比较多.</p><ul><li>时序对齐的方法仍然类似于BEVDet4D,只是BEVDet4D只使用1帧前序帧,BEVerse使用的是前2帧+后4帧,每帧都用grid_sample warp到当前帧再进行channel维度的拼接.</li><li>拼接完成后,模型设计了Temporal3DConvModel进行时序的融合,和上文两个基于CNN融合的模型不同,BEVerse由于使用的帧数比较多,采用3D卷积和3D池化对所有帧进行融合.3D卷积是处理连续帧信息的一个重要方式.分辨率使用的是128*128.</li></ul><h2 id="基于transformer的图像特征融合"><a href="#基于transformer的图像特征融合" class="headerlink" title="基于transformer的图像特征融合"></a>基于transformer的图像特征融合</h2><p>前文所介绍的模型有一个共同点,即都是在bev空间下对bev feature做时序融合.由于每一帧的bev feature只有一个,所以bev空间下的时序融合比较简单直接,可直接通过warp的方式将前序帧与当前帧融合,而且需要的缓存空间也比较小.但这种方法也有不足之处,</p><ul><li>一是会带来可融合区域的浪费,丢失有用信息,</li><li>二是在融合过程中只能使用固定权重,无法自适应地调整前序帧权重,</li><li>三是可用的时序区间也比较短,因为时序过长,可融合区域会更小,难以起到加强作用.<strong>BEVFormer的实验中,融合3帧,也就是2s的时序区间效果达到了峰值.</strong></li></ul><h3 id="UniFusion-浙江大学-大疆-上海AI-lab"><a href="#UniFusion-浙江大学-大疆-上海AI-lab" class="headerlink" title="UniFusion(浙江大学,大疆,上海AI lab)"></a>UniFusion(浙江大学,大疆,上海AI lab)</h3><p><a href="https://arxiv.org/abs/2207.08536">论文链接</a><br><a href="https://github.com/cfzd/UniFusion">代码链接</a></p><p> <img src="/pic/time10.png" alt="网络结构图"></p><p>Uniformer解释了基于warp的融合方式为什么会带来信息丢失.如下图所示,图(b)的灰色部分是连续两帧实际可融合区域,图(a)的灰色部分是生成一定范围内的矩形的bev feature后实际融合的区域,可见融合范围大大缩小,所以很多有用的信息被浪费了.</p><p> <img src="/pic/time8.png" alt="不同视角"></p><p>为了更好地融合时序信息,可以不在bev空间通过warp的方式进行融合,而是把这一过程提前到<strong>图像空间</strong>,通过缓存前序帧的图像特征,并把前序帧的lidar2img参数,也就是相机外参转换到当前帧,那就<strong>等同于当前帧又多了很多个相机视角</strong>,同时可以看到更大范围的信息,图上图(c)所示.在这种架构下,多帧时间的融合和多视角空间的融合被统一起来了,所以模型命名为Uniformer.下图更加直观地展示了两种方法的区别:</p><p>  <img src="/pic/time9.png"></p><p>Uniformer架构可以解决上述warp方法的全部缺陷.</p><ul><li>第一点,它不造成信息浪费,可以融合当前帧和前序帧相机视角所能覆盖的所有区域,</li><li>第二点,它可以自适应地学习每个视角的权重,不区分当前和前序帧,</li><li>第三,只要缓存空间允许,它可以融合很长的时序区间.当然这种方法的<strong>劣势是需要缓存多视角特征,无法使用较大的分辨率</strong>,一般需要高倍下采样,最后再进行上采样.Uniformer为这种方法取名为”virtual views”即虚拟视角方法.</li></ul><p>在具体实现上,Uniformer的前序帧选取前6帧,时序对齐的方式如上文所述,通过外参转换的方式将前序帧变为当前帧的虚拟视角,然后做基于transformer的融合,包括self attention和cross attention,只是cross attention同时融合了时间和空间信息.最后还设计了self-regression自回归模块来融合多层transformer结果,最后得到bev feature,并指出这种方法也能达到类似于BEVFormer将前序帧和当前帧bev feature进行concate再融合的提升效果.bev feature分辨率采用50*50再用4倍上采样.实验效果对比如下图所示:</p>]]></content>
      
      
      <categories>
          
          <category> DL </category>
          
      </categories>
      
      
        <tags>
            
            <tag> bev </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>BEV感知总结</title>
      <link href="/2023/11/28/bev-reserch/"/>
      <url>/2023/11/28/bev-reserch/</url>
      
        <content type="html"><![CDATA[<h1 id="BEV感知总结"><a href="#BEV感知总结" class="headerlink" title="BEV感知总结"></a>BEV感知总结</h1><h2 id="参考论文和好用的工具箱："><a href="#参考论文和好用的工具箱：" class="headerlink" title="参考论文和好用的工具箱："></a>参考论文和好用的工具箱：</h2><p><a href="https://arxiv.org/abs/2209.05324">Delving into the Devils of Bird’s-eye-view Perception: A Review， Evaluation and Recipe(深入研究鸟瞰感知的魔咒：综述、评估和秘诀)</a><br><a href="https://github.com/OpenDriveLab/Birds-eye-view-Perception">BEV 感知论文和工具箱</a><br><a href="https://www.youtube.com/watch?v=j0z4FweCy4M">(2021) Tesla AI Day. Online</a><br><a href="https://zhuanlan.zhihu.com/p/597554089?utm_id=0">自动驾驶中的BEV感知与建图小记</a></p><p><a href="https://zhuanlan.zhihu.com/p/598164131">CaDDN论文+源码解读</a><br><a href="https://zhuanlan.zhihu.com/p/546194237?utm_id=0">GitNet: 基于几何先验的转换用于BEV分割</a><br><a href="https://blog.csdn.net/qq_39967751/article/details/126290811">Monocular 3D Object Detection with Depth from Motion (DfM)</a><br><a href="https://zhuanlan.zhihu.com/p/508794328">理解DD3D目标检测</a><br><a href="https://blog.csdn.net/weixin_41610241/article/details/126404303">CenterPoint:Center-based 3D Object Detection and Tracking (Based: KITTI)</a><br><a href="https://zhuanlan.zhihu.com/p/445975451">SPVCNN:使用稀疏点体素卷积搜索高效 3D 架构</a><br><a href="https://zhuanlan.zhihu.com/p/432149359">Multi-Level Fusion (CVPR 2018)：解耦单目3D任务的早期尝试</a><br><a href="https://zhuanlan.zhihu.com/p/597394576">UVTR:Unifying Voxel-based Representation with Transformer for 3D Object Detection</a><br><a href="https://blog.csdn.net/m0_63604019/article/details/126878682">PointPainting: Sequential Fusion for 3D Object Detection(3D物体检测的顺序融合)</a><br><a href="https://blog.csdn.net/qq_43456497/article/details/129112072">PointAugmenting: Cross-Modal Augmentation for 3D Object Detection总结</a><br><a href="https://zhuanlan.zhihu.com/p/416084515">MVFuseNet : Improving End-to-End Object Detection and Motion Forecasting through Multi-View Fusion of LiDAR Data</a><br><a href="http://681314.com/A/5A3VK5CZoJ">学习笔记之BEV模型学习小结</a><br><a href="https://zhuanlan.zhihu.com/p/597554089">自动驾驶中的BEV感知与建图小记</a></p><p>在感知任务的鸟瞰(BEV)中学习强大的表征是一种趋势，并引起了工业界和学术界的广泛关注。大多数自动驾驶算法的常规方法在前视或透视角中执行检测、分割、跟踪等。随着传感器配置变得越来越复杂，集成来自不同传感器的多源信息并在统一的视角中表示特征变得至关重要。BEV感知继承了几个优点，因为在BEV中表示周围场景是直观和融合友好的；并且在BEV中表示目标最适合于后续模块，如在规划和&#x2F;或控制中。</p><p>BEV感知的核心问题在于：</p><ul><li>(A)如何通过从透视角到BEV的视角转换来重建丢失的3D信息；</li><li>(B)如何在BEV网格中获取注释的真值 ；</li><li>(C)如何融合来自不同传感器和视角下的特征；</li><li>(D)如何在不同场景下适应和泛化算法。</li></ul><p>接下来回顾了关于Bev感知的最新工作，并对不同的解决方案进行了深入的分析。此外，还描述了业内几种BEV方法的系统设计。此外，还介绍了一整套实用指南，以提高Bev感知任务的性能，包括Camera、LiDAR和融合输入。最后，指出了该领域未来的研究方向。</p><h2 id="引言"><a href="#引言" class="headerlink" title="引言"></a>引言</h2><p>与2D视域中被广泛研究的前视角或透视角相比，Bev表示具有几个固有的优点。</p><ul><li>首先，它没有2D任务中常见的<strong>遮挡或缩放</strong>问题，识别有遮挡或交叉交通的车辆可以更好地解决。</li><li>此外，以BEV的形式表示目标或道路元素将有利于后续模块(例如规划、控制)的开发和部署。</li></ul><h2 id="概述"><a href="#概述" class="headerlink" title="概述"></a>概述</h2><p>根据输入的数据，将Bev感知研究主要分为三个部分–<strong>BevCamera、Bev LiDAR和Bev融合</strong>。图1描绘了Bev感知的总体情况。具体而言，</p><ul><li>Bev Camera指的是从周围多个Camera中检测或分割3D目标的纯视觉或以视觉为中心的算法；</li><li>Bev LiDAR描述了从点云输入进行检测或分割的任务；</li><li>Bev Fusion描述了来自Camera、LiDAR、GNSS、里程计、HD-Map、CAN-Bus等多个传感器输入的融合机制。</li></ul><p> <img src="/pic/BEV1.png" alt="图1"></p><p>如图1所示，对基本感知算法(分类、检测、分割、跟踪等)进行分组和分类。将自动驾驶任务分为三个层次，其中Bev感知的概念位于中间。基于传感器输入层、基础任务和产品场景的不同组合，某一种BEV感知算法可以相应地指示。</p><ul><li>例如，M2BEV和BEVFormer属于来自多个Camera的BEVCamera轨迹，以执行包括3D目标检测和BEV地图分割在内的多个任务。</li><li>BEVFusion在Bev空间设计了一种融合策略，可以同时从Camera和LiDAR输入执行3D检测和跟踪。</li><li>特斯拉发布了其系统pipeline，用于在L2高速公路导航和智能召唤中检测矢量空间(BEV)中的目标和车道线。</li></ul><h2 id="BEV感知的动机研究"><a href="#BEV感知的动机研究" class="headerlink" title="BEV感知的动机研究"></a>BEV感知的动机研究</h2><h3 id="意义重大"><a href="#意义重大" class="headerlink" title="意义重大"></a><strong>意义重大</strong></h3><p>众所周知，仅摄像解决方案和LiDAR解决方案之间存在巨大的性能差距。例如，截至2022年8月提交，一流的纯Camera和LiDAR方法在nuScenes数据集上的差距超过20%，在Waymo基准上的差距超过30%。这自然促使调查只有Camera的解决方案是否可以超越或与LiDAR方法平起平坐。</p><ul><li>从学术角度来看，设计基于Camera的pipeline以使其优于LiDAR的本质是<strong>更好地理解从2D外观输入到3D几何图形输出的视角转换过程</strong>。如何将Camera特征转换为几何表示，就像在点云中所做的那样，给学术界留下了一个有意义的影响。</li><li>在工业方面，将一套LiDAR设备安装到SDV(Software Defined Vehicles，软件定义汽车)中的成本很高；OEM(原始设备制造商，如福特、宝马等)更喜欢廉价且准确的软件算法部署。将仅有Camera的算法改进到LiDAR自然就属于这个目标，因为Camera的成本通常是LiDAR的十分之一。<ul><li>此外，基于摄像头的pipeline可以识别远距离目标和基于颜色的道路元素(例如红绿灯)，而这两者都是LiDAR方法所不能做到的。</li><li>基于Lidar的方法，BEV是最好的方案之一；不过最近的研究也表示，对于对Camera的输入，BEV仍有很大的进步空间。无论是Camera还是Lidar的数据都能够很好的映射到BEV空间，而且也能够更好的以一种统一的方式进行数据融合。</li></ul></li></ul><h3 id="空间-努力方向"><a href="#空间-努力方向" class="headerlink" title="空间(努力方向)"></a><strong>空间(努力方向)</strong></h3><ul><li>一个问题，Bev感知背后的主旨是从Camera和LiDAR输入中学习稳健和可泛化的特征表示。这在LiDAR分支中很容易，因为输入(点云)具有这样的3D属性。在Camera分支中，从单目或多视设置中学习3D空间信息是困难的。看到有一些尝试通过姿势估计[EPro-PnP]或时间运动[Dfm]来学习更好的2D-3D对应，但Bev感知背后的核心问题需要对原始传感器输入的&#x3D;&#x3D;深度估计进行实质性的创新&#x3D;&#x3D;。</li><li>另一个关键问题是如何在pipeline的早期或中期进行<strong>特征融合</strong>。大多数传感器融合算法将该问题视为简单的<strong>目标级融合或沿着通道的朴素特征拼接</strong>。如何&#x3D;&#x3D;从多通道输入中对齐和集成特征&#x3D;&#x3D;起着至关重要的作用，从而留下了广泛的创新空间。</li></ul><h3 id="准备工作"><a href="#准备工作" class="headerlink" title="准备工作"></a><strong>准备工作</strong></h3><p>由于Bev感知同时需要Camera和LiDAR，因此<strong>高质量的注释以及2D和3D目标之间的准确对齐</strong>是此类基准的两个关键评估。</p><ul><li>虽然Kitti[11]是全面的，在早期的自动驾驶研究中吸引了很多关注，但大规模和多样化的基准测试，如**Waymo[8]、nuScenes[7]、ArgoVerse[12]**，为验证Bev感知想法提供了坚实的平台。这些新提出的基准通常具有高质量的标签；场景多样性和数据量也在很大程度上扩大了。</li><li>至于算法方面，近年来见证了普通视觉的巨大发展，其中**Transformer[14]、VIT[15，16]、Masked Auto-encoders(MAE)[17]和CLIP[18]**等方法比传统方法获得了令人印象深刻的收益。相信，这些工作将有益于并激励BEV感知研究的伟大。</li></ul><p>基于以上三个方面的讨论，得出结论：<strong>BEV感知研究具有巨大的潜在影响，值得学术界和产业界长期大力关注。</strong></p><h2 id="3D感知中的背景"><a href="#3D感知中的背景" class="headerlink" title="3D感知中的背景"></a>3D感知中的背景</h2><p>接下来将回顾执行感知任务的传统方法，包括基于单目Camera的3D目标检测、基于LiDAR的3D目标检测和分割以及传感器融合策略。还介绍了3D感知中的主要数据集，如Kitti数据集[11]、nuScenes数据集[7]和Waymo Open数据集[8]。</p><h3 id="任务定义及相关工作"><a href="#任务定义及相关工作" class="headerlink" title="任务定义及相关工作"></a>任务定义及相关工作</h3><ul><li><p>基于单目Camera的3D目标检测。</p><ul><li>基于单目Camera的方法<strong>将RGB图像作为输入</strong>，并尝试预测每个目标的3D位置和类别。</li><li>单目3D检测的主 要挑战是RGB图像<strong>缺乏深度信息</strong>，因此这类方法需要对深度进行预测。由于从单幅图像估计深度是一个不适定的问题，通常基于单目Camera的方法的性能低于基于LiDAR的方法。</li></ul></li><li><p>激光雷达检测与分割。</p><ul><li>激光雷达使用3D空间中的<strong>一组点</strong>来描述周围环境，</li><li>这些点捕获了目标的几何信息。尽管缺乏颜色和纹理信息，感知范围有限，但由于<strong>深度先验</strong>，基于LiDAR的方法比基于Camera的方法有很大的优势。</li></ul></li><li><p>传感器融合。</p><ul><li>现代自动驾驶汽车配备了不同的传感器，如摄像头、激光雷达和雷达。每种传感器都有优缺点。Camera数据包含密集的颜色和纹理信息，但无法捕获深度信息。激光雷达提供了准确的深度和结构信息，但存在范围有限和稀疏性的问题。雷达比LiDAR更稀疏，但感知范围更长，可以捕获运动物体的信息。</li><li>理想情况下，传感器融合将推动感知系统的<strong>性能上限</strong>，然而如何融合来自不同模式的数据仍然是一个具有挑战性的问题。</li></ul></li></ul><h3 id="数据集和指标"><a href="#数据集和指标" class="headerlink" title="数据集和指标"></a>数据集和指标</h3><p>接下来介绍一些流行的自动驾驶数据集和常用的评估指标。表1总结了现行BEV感知基准的主要统计数据。通常，一个数据集由各种场景组成，每个场景在不同的数据集中具有不同的长度。总持续时间从几十分钟到数百小时不等。</p><p>对于Bev感知任务，3D包围框标注和3D分割标注是必不可少的，高清地图配置已成为主流趋势。它们中的大多数都可以在不同的任务中采用。达成共识，即需要具有多个模态和各种注释的传感器。发布更多类型的数据[7、12、24、25、33、39]，如IMU&#x2F;GPS和CAN-BUS。与Kaggle和EvalAI排行榜类似，公布了每个数据集的提交总数，以表明某个数据集的受欢迎程度。</p><p><img src="/pic/BEV2.png" alt="表1"></p><h4 id="数据集"><a href="#数据集" class="headerlink" title="数据集"></a>数据集</h4><ul><li><p>Kitti数据集。</p><ul><li>KITTI是2012年提出的一个开创性的自动驾驶数据集。它拥有7481张训练图像和7518张测试图像，用于3D目标检测任务。它也有从Velodyne激光扫描仪捕捉到的相应的点云。测试集分为3个部分：简单、中等和困难，主要根据边界框大小和遮挡级别。目标检测评价分为两类：三维目标检测评价和鸟瞰评价。Kitti是第一个针对多个自动驾驶任务的全面数据集，它吸引了社区的大量关注。</li></ul></li><li><p>Waymo数据集。</p><ul><li>Waymo Open DataSet v1.3分别在训练、验证和测试集中包含798、202和80个视频序列。每个序列有5个LiDAR和5个左、左、前、右、右、侧5个视角，图像分辨率为1920×1280像素或1920×886像素。Waymo规模庞大，形式多样。它随着数据集版本的不断更新而不断发展。每年，Waymo公开赛都会定义新的任务，并鼓励社区努力解决这些问题。</li></ul></li><li><p>NuScenes数据集。</p><ul><li>NuScenes数据集是一个包含两个城市1000个驾驶场景的大规模自动驾驶数据集。850个场景用于培训&#x2F;验证，150个场景用于测试。每一场戏都有20多秒长。它有40K个关键帧和整个传感器套件，包括6个摄像头，1个激光雷达和5个雷达。摄像机图像分辨率为1600×900。同时，发布了相应的HD-Map和CanBus数据，以探索多个输入的辅助。由于NuScenes提供了多样化的多传感器设置，因此它在学术文献中越来越受欢迎；数据规模没有Waymo的大，这使得在这个基准上快速验证想法变得高效。</li></ul></li></ul><h4 id="评估指标"><a href="#评估指标" class="headerlink" title="评估指标"></a>评估指标</h4><ul><li><p>Let-3D-APL</p><ul><li>在仅有摄像头的3D检测中，使用LET-3D-APL(Longitudinal Error Tolerant 3D Average Precision with Longitudinal Affinity Weight，具有纵向亲和权重的纵向误差容错 3D 平均精度)代替3D-AP作为度量。与基于并集的3D交集(IOU)相比，LET-3D-APL允许预测包围框在给定容差范围内的纵向定位误差。LET-3D-APL通过使用定位亲和力来衡量精度来惩罚纵向定位误差。LET-3D-APL的定义在数学上定义为：</li></ul><p>  <img src="/pic/BEV3.png" alt="公式"><br> 其中，PL(R)表示纵向亲和度加权精确值，p(r)表示调用r时的精确值，乘数al是被视为Tp(真正)的所有匹配预测的平均纵向亲和度。</p></li><li><p>mAP</p><ul><li>在二维目标检测中，平均平均精度(MAP)类似于著名的AP度量，但匹配策略被从IOU替换为BEV平面上的2D中心距离。AP在不同的距离阈值下计算：0.5米、1米、2米和4米。MAP是通过在上述阈值中对AP求平均来计算的。</li></ul></li><li><p>NDS</p><ul><li>NuScenes检测分数(NDS)是几个度量的组合：MAP、Mate(平均平移误差)、MASE(平均比例误差)、MAOE(平均方向误差)、MAVE(平均速度误差)和MAAE(平均属性误差)。通过使用上述指标的加权和来计算NDS。MAP的权重为5，其余的权重为1。在第一步中，TPerror被转换为TPcore，NDS定义：</li></ul><p>  <img src="/pic/BEV4.png" alt="公式"></p></li></ul><h2 id="BEV感知方法"><a href="#BEV感知方法" class="headerlink" title="BEV感知方法"></a>BEV感知方法</h2><p>主要从学术界和工业界对BEV认知的不同角度进行了详细的描述。根据输入方式的不同将BEV区分为三种设置，BEV Camera(仅摄像头3D感知)、Bev LiDAR和Bev Fusion，并总结了BEV感知的工业设计。</p><p>表2总结了基于输入数据和任务类型的BEV知觉文献分类。表3描述了多年来流行排行榜上3D目标检测和分割的性能收益。</p><p><img src="/pic/BEV5.png" alt="表2"></p><p><img src="/pic/BEV6.png" alt="表3"></p><h3 id="BEV-Camera"><a href="#BEV-Camera" class="headerlink" title="BEV Camera"></a>BEV Camera</h3><h4 id="通用pipeline"><a href="#通用pipeline" class="headerlink" title="通用pipeline"></a>通用pipeline</h4><p>如图2所示，一般的只有Camera的3D感知系统可以分为三个部分：2D特征提取模块、视角转换模块(可选)和3D解码器。</p><p><img src="/pic/BEV7.png" alt="图2"></p><ul><li>特征提取<ul><li>在2D特征提取中，2D感知中存在着大量的经验，可以在3D感知中考虑，以骨干预训练的形式。</li></ul></li><li>视角转换模块<ul><li>与2D感知系统有很大的不同。注意，并不是所有的3D感知方法都具有视角转换模块，并且一些方法直接根据2D空间中的特征来检测3D空间中的目标，如FCOS3D这样的单目3D检测其是直接从2D特征中预测3D信息。</li></ul></li><li>3D解码器<ul><li>接收2D&#x2F;3D空间中的要素，并输出3D边界框、Bev地图分割、3D车道关键点等3D感知结果。</li><li>大多数3D解码器来自基于LiDAR的方法[Voxelnet，SECOND]，其在Voxel空间&#x2F;Bev空间执行检测，但仍有一些Camera3D解码器利用2D空间[FCOS3D，SMOKE，DETR3D]的特征并直接回归3D目标的定位。</li></ul></li></ul><h4 id="View-Transformation-视角转换模块"><a href="#View-Transformation-视角转换模块" class="headerlink" title="View Transformation(视角转换模块)"></a>View Transformation(视角转换模块)</h4><p>在仅有Camera的3D感知中，视角转换模块是关键，因为它是构建3D数据和编码3D先验假设。最近的研究[M2BEV，BEVFormer，Mono3D，PersFormer，BEVDet，PETR，BEVDepth，Polarformer，3d-lanenet]集中于增强这一模块。将视角转换技术分为三大主流。</p><ul><li>第一种方法称为“2D-3D方法”，从2D图像特征开始，通过深度估计将2D特征“提升”到3D空间。</li><li>第二种是3D-2D方法，它起源于3D空间，通过3D到2D的投影映射将2D特征编码到3D空间。前两个流显式建模几何变换关系。</li><li>第三种方法被称为“纯网络方法”，它利用神经网络隐式地获取几何变换。图3给出了执行视角转换的概要路线图，下面将对其进行详细分析。</li></ul><p><img src="/pic/BEV8.png" alt="图3：视角转换的分类。在2D-3D方法中，基于LSS的方法[BEVFusion，CaDDN，BEVDet，BEVDepth，LSS，BEVDet4d，BEVFusion2]根据2D特征预测每个像素的深度分布。在3D-2D方法的基础上，基于单应矩阵的方法[BEVFormer，PersFormer，GitNet]假定稀疏的3D样本点，并通过摄像机参数将其投影到2D平面上。基于纯网络的方法[94，Fishing net，NEAT，97，98]采用MLP或变换来隐式建模从3D空间到2D平面的投影。"></p><ul><li><p>2D-3D方法</p><ul><li>首先由LSS提出，它预测2D特征上的网格深度分布，然后基于深度将2D特征提升到Voxel空间，并执行类似于基于LiDAR的方法的下游任务。这一过程可以表述为：</li></ul><p> <img src="/pic/BEV9.png" alt="公式"></p><ul><li>伪LiDAR方法[Pseudo-lidar，Pseudo-lidar++]从预先训练的深度估计模型中提取深度信息，并且提升过程发生在2D特征提取之前。</li><li>在LSS之后，还有另一项工作遵循了与面元分布相同的思想，即CaDDN(Categorical depth distribution network for monocular 3d object detection)。CADDN使用类似的网络来预测分类深度分布，将Voxel空间特征压缩到BEV空间，并在最后执行3D检测。</li><li>LSS和CaDDN的主要区别在于，<strong>CaDDN使用深度真值信息来指导其分类深度分布预测</strong>，从而具有从2D空间提取3D信息的优越的深度网络。后续工作，例如**BEVDet及其时间版本BEVDet4D、BEVDepth、BEVFusion和其他[Dsgn，DD3D，LIGA-Stereo]**。</li></ul></li><li><p>3D-2D方法</p><ul><li>逆透视映射(IPM)有条件地提出了从3D空间到2D空间的投影，假设3D空间中的对应点位于水平面上。这样的变换矩阵可以从Camera的内部和外部参数数学推导出来。一系列的工作是应用IPM将元素从透视角转换为鸟瞰图，无论是前处理还是后处理。在视角转换方面，OFT-Net首次提出了从3D到2D的特征投影方法。OFT-Net形成一个均匀分布的3DVoxel特征网格，通过聚集来自相应投影区域的图像特征来填充Voxel。然后，通过对Voxel特征进行垂直求和来获得正交Bev特征地图。</li><li>最近，受特斯拉感知系统技术路线图的启发，3D-2D几何投影和神经网络的组合流行起来[BEVFormer，PersFormer，DETR3D，GitNet]。请注意，变压器体系结构中的交叉注意机制在概念上满足了这种几何投影的需要，如以下所示：</li></ul><p> <img src="/pic/BEV10.png" alt="公式"></p><ul><li>其中q、k、v表示查询、键和值，Pxyz是Voxel空间中的预定义锚点，利用Camera参数将Pxyz投影到图像平面，以实现模型的快速收敛。为了获得稳健的检测结果，BEVFormer[4]利用Transformer中的交叉注意机制来增强3D-2D视角转换的建模。其他人Imvoxelnet和MVFCOS3D++减轻了网格取样器的压力，以有效地加速这一过程。尽管如此，<em>这些方法在很大程度上依赖于Camera参数的精确度，这些参数很容易受到长时间驾驶的波动的影响。</em></li></ul></li><li><p>基于纯网络的方法</p><ul><li>无论是2D方法还是3D-2D方法，这两种方法都引入了几何投影中包含的遗传感应偏差。相反，一些方法倾向于利用神经网络来隐式表示Camera投影关系。许多BEV地图分割工作[Hdmapnet，Translating images into maps，CVT]使用多层感知器或Transformer[Attention is all you need]体系结构来隐式地建模3D-2D投影。<strong>VPN引入了视角关系模块–多层感知器(MLP)，用于通过处理来自所有视角的输入来产生地图-视角特征，从而实现了跨多个视角的共享特征表示的获取。HDMapNet采用MLP架构来执行特征地图的视角转换</strong>。BEVSegFormer构建密集的BEV查询，通过MLP从查询特征中直接预测其二维投影点，然后使用可变形注意力更新查询嵌入。CVT将图像特征与基于摄像机内外参数的摄像机感知位置嵌入相结合，并引入了交叉视角注意模块来产生地图视角表示。某些方法不显式构建BEV特征。PETR将源自摄像机参数的3D位置嵌入集成到2D多视角特征中。这一集成使稀疏查询能够通过普通的交叉关注直接与3D位置感知图像功能交互。</li></ul></li></ul><h4 id="关于BEV和透视方法的讨论"><a href="#关于BEV和透视方法的讨论" class="headerlink" title="关于BEV和透视方法的讨论"></a>关于BEV和透视方法的讨论</h4><p>在纯相机3D感知的最初阶段，主要关注的是如何从透视图（也称为2D空间）预测3D对象的定位。这是因为2D感知在那个阶段发展得很好[Faster R-CNN，Fast R-CNN，Mask R-CNN，FCOS]，如何使2D检测器具有感知3D场景的能力成为主流方法[Probabilistic and geometric depth: Detecting objects in perspective，FCOS3D，SMOCK，Multi-Level Fusion based 3D Object Detection from Monocular Images]。后来，一些研究涉及到Bev表示，因为在这种观点下，很容易解决3D空间中相同大小的目标由于距离Camera的距离而在图像平面上具有非常不同的大小的问题。这一系列工作要么预测深度信息，要么利用3D先验假设来补偿Camera输入中3D信息的损失<br>虽然最近基于BEV的方法席卷了3D感知界，但值得注意的是，这种成功主要得益于三个方面。</p><ul><li>第一个原因是流行的nuScenes数据集[7]，它具有多摄像机设置，非常适合在BEV下应用多视角特征聚合。</li><li>第二个原因是，大多数纯Camera的Bev感知方法都从基于LiDAR的方法[Voxelnet，Pointpillars，CenterPoint，SPVCNN，SECOND，PointNet，PointNet++]中获得了很多帮助，表现在检测头的形式和相应的损耗设计上。</li><li>第三个原因是单目方法[FCOS3D，SMOKE，Multi-level fusion based 3d object detection from monocular images]的长期发展使基于BEV的方法蓬勃发展，这是处理透视视角中特征表示形式的一个很好的起点。核心问题是如何从2D图像中重建丢失的3D信息。为此，基于BEV的方法和透视方法是解决同一问题的两种不同方式，它们并不相互排斥。</li></ul><h3 id="BEV-LiDAR"><a href="#BEV-LiDAR" class="headerlink" title="BEV LiDAR"></a>BEV LiDAR</h3><h4 id="通用pipeline-1"><a href="#通用pipeline-1" class="headerlink" title="通用pipeline"></a>通用pipeline</h4><p>图4描绘了Bev LiDAR感知的一般pipeline。</p><ul><li>提取的点云特征被转换为BEV特征地图。常见的检测头产生3D预测结果。</li><li>在特征提取部分，主要有两个分支将点云数据转换为BEV表示。根据pipeline的顺序，将这两个选项分别称为pre-BEV and post-BEV，以表明骨干网络的输入是来自3D表示还是来自BEV表示。</li></ul><p><img src="/pic/BEV11.png" alt="图4"></p><h4 id="Pre-BEV特征提取"><a href="#Pre-BEV特征提取" class="headerlink" title="Pre-BEV特征提取"></a>Pre-BEV特征提取</h4><ul><li>除了基于点的方法对原始点云进行处理外，基于Voxel的方法将点Voxel化为离散的网格，通过离散化连续的三维坐标来提供更高效的表示。基于离散Voxel表示，3D卷积或3D稀疏卷积可用于提取点云特征。</li><li>相关方法。<ul><li>VoxelNet堆叠多个Voxel特征编码(VFE)层，以将Voxel中的点云分布编码为Voxel特征。</li><li>SECOND在处理Voxel表示时引入了稀疏卷积，大大降低了训练和推理速度。</li><li>CenterPoint是一款功能强大的基于中心的无锚3D检测器。PV-RCNN结合了点和Voxel分支来学习更具区分性的点云特征。</li><li>PV-RCNN结合了点和Voxel分支来学习更具区分性的点云特征。</li><li>SA-SSD设计了一种辅助网络，将骨干网络中的Voxel特征转换回点级表示，以显式地利用三维点云的结构信息，减少下采样的损失。</li><li>Voxel R-CNN采用三维卷积主干提取点云特征。然后在BEV上应用2D网络来提供object proposals，这些object proposals通过提取的特征进行细化。它获得了与基于点的方法相当的性能。</li><li>Object DGCNN将3D目标检测任务建模为BEV中动态图上的消息传递。在将点云转换为BEV特征地图后，预测查询点迭代地从关键点采集BEV特征。</li><li>Votr引入了局部注意、扩展注意和快速Voxel查询，以实现对大背景信息的多个Voxel的注意机制。</li><li>SST将提取的Voxel特征视为Query，然后在非重叠区域应用稀疏区域注意和区域摆动，以避免对基于Voxel的网络进行下采样。</li><li>AFDetV2通过引入KeyPoint辅助监控和多任务头部，形成了单级无锚点网络。</li></ul></li></ul><h4 id="Post-BEV特征提取"><a href="#Post-BEV特征提取" class="headerlink" title="Post-BEV特征提取"></a>Post-BEV特征提取</h4><p>由于3D空间中的Voxel稀疏且不规则，应用3D卷积的效率很低。对于工业应用，可能不支持诸如3D卷积之类的运算符；需要合适且高效的3D检测网络。</p><ul><li>MV3D是第一种将点云数据转换为BEV表示的方法。在将点离散到BEV网格中后，根据网格中的点获得高度、强度和密度特征来表示网格特征。由于Bev网格中的点很多，在此处理过程中，信息的损失是相当大的。</li><li>其他作品PIXOR、Hdnet、BirdNet、Rt3d、Yolo3d、Complex-YOLO遵循类似的模式，使用Bev网格中的统计数据来表示点云，例如最大高度和平均强度。</li><li>PointPillars首先引入了柱的概念，柱是一种高度不受限制的特殊Voxel。它利用PointNet的简化版本来学习柱子中点的表示。然后，编码后的特征可以由标准2D卷积网络和检测头处理。虽然PointPillars的性能不如其他3D主干，但它及其变体具有很高的效率，因此适合工业应用。</li></ul><h4 id="总结讨论"><a href="#总结讨论" class="headerlink" title="总结讨论"></a>总结讨论</h4><p>将点云数据转换为任何形式的表示不可避免地会导致信息丢失。</p><ul><li>在Bev前特征提取中，最先进的方法利用细粒度的Voxel，保留了点云数据中的大部分3D信息，从而有利于3D检测。作为权衡，它需要较高的内存消耗和计算成本。</li><li>在Bev后特征提取中，将点云数据直接转换为BEV表示，避免了在3D空间中的复杂操作。随着高度维度的压缩，不可避免地会产生巨大的信息损失。最有效的方法是使用统计方法来表示BEV特征图，但其结果较差。基于PointPillars的方法[45]平衡了性能和成本，成为工业应用的流行选择。如何处理性能和效率之间的权衡成为基于LiDAR的应用面临的重大挑战。</li></ul><h3 id="BEV-Fusion"><a href="#BEV-Fusion" class="headerlink" title="BEV Fusion"></a>BEV Fusion</h3><h4 id="通用pipeline-2"><a href="#通用pipeline-2" class="headerlink" title="通用pipeline"></a>通用pipeline</h4><p>逆透视映射(IPM)提出了利用摄像机内部和外部矩阵的几何约束将像素映射到Bev平面上的方法。尽管它由于平地假设而不准确，但它提供了在BEV中统一图像和点云的可能性。Lift-Splat-Shots(LSS)是第一个预测图像特征深度分布的方法，它引入神经网络来学习Camera到激光雷达的不适定变换问题。其他工作[BEVFormer，UVTR]发展了不同的方法来进行视角转换。给定从透视角到BEV的视角转换方法，</p><p>图5展示了两种典型的BEV融合算法pipeline设计，适用于学术界和工业界。主要区别在于2D到3D的转换和融合模块。在透视变换pipeline(A)中，不同算法的结果首先被转换到3D空间，然后使用先验规则或手工规则进行融合。BEV感知pipeline(B)首先将PV特征转换为BEV，然后融合特征以获得最终预测，从而保留大多数原始信息并避免手工设计，在转换为BEV表示后，将来自不同传感器的特征映射进行融合。在BEV表示中也可以引入时间和自我运动信息。</p><p><img src="/pic/BEV12.png" alt="图5"></p><h4 id="LiDAR-camera-Fusion"><a href="#LiDAR-camera-Fusion" class="headerlink" title="LiDAR-camera Fusion"></a>LiDAR-camera Fusion</h4><ul><li>两个同名的BEVFusion[BEVFusion: Multi-task multi-sensor fusion withunified bird’s-eye view representation，BEVFusion: A simple and robust lidar-camera fusion framework]从不同的方向探索了Bev中的融合。由于Camera到激光雷达的投影[PointPainting，PointAugmenting]抛弃了Camera特征的语义密度，前者BEVFusion设计了一种高效的Camera到Bev转换方法，该方法高效地将Camera特征投影到Bev中，<strong>然后使用卷积层将其与激光雷达Bev特征融合</strong>。后者BEVFusion将BEV融合作为保持感知系统稳定性的鲁棒性主题，它<strong>将摄像头和激光雷达功能编码到同一BEV中，以确保摄像头和激光雷达流的独立</strong>，这种设计使感知系统能够在传感器故障时保持稳定性。</li><li>除了BEVFusion之外，UVTR在没有高度压缩的特定于模式的Voxel空间中表示不同的输入模式，以避免语义歧义并实现进一步的交互。通过将每个视角的图像特征变换到为每个图像生成深度分布的预定义空间来构造图像Voxel空间。点Voxel空间是使用常见的3D卷积网络来构造的。然后在两个Voxel空间之间进行跨通道交互，以增强特定于通道的信息。</li></ul><h4 id="Temporal-Fusion-时域融合"><a href="#Temporal-Fusion-时域融合" class="headerlink" title="Temporal Fusion(时域融合)"></a>Temporal Fusion(时域融合)</h4><ul><li>时间信息在推断物体的运动状态和识别遮挡方面起着重要作用。BEV提供了连接不同时间戳中的场景表示的理想桥梁，因为BEV特征地图的中心位置持续到EGO CAR。</li><li>MVFuseNet同时利用Bev和Range视角进行时间特征提取。其他文献FIERY、BEVerse、BEVDet4D使用自运动将先前的BEV特征与当前坐标对齐，然后融合当前BEV特征以获得时间特征。</li><li>BEVDet4D使用空间对齐操作将先前的特征图与当前帧进行融合，然后连接多个特征图。</li><li>BEVFormer和UniFormer采用了一种软方法来融合时间信息。注意力模块用于分别从先前的BEV特征图和先前的帧中融合时间信息。</li><li>关于自我汽车的运动，注意模块在不同时间戳的表示中的位置也被自我运动信息校正。</li></ul><h4 id="总结与讨论"><a href="#总结与讨论" class="headerlink" title="总结与讨论"></a>总结与讨论</h4><p>由于图像在透视坐标系中，而点云在三维坐标系中，两种模式之间的空间对齐成为一个至关重要的问题。</p><ul><li>虽然利用几何投影关系将点云数据投影到图像坐标上很容易，但点云数据的稀疏性使得提取信息丰富的特征变得困难。相反，由于透视角中缺乏深度信息，将透视角中的图像转换到3D空间将是一个不适定的问题。基于先验知识，前人的工作，如IPM和LSS，使得将透视角中的信息转换为BEV成为可能，为多传感器和时间融合提供了统一的表示。</li><li>在Bev空间融合激光雷达和Camera数据为3D检测任务提供了令人满意的性能。这种方法还保持了不同模式的独立性，这为构建更稳健的感知系统提供了机会。</li><li>对于时间融合，通过考虑自我运动信息，不同时间戳中的表示可以直接在BEV空间中融合。由于Bev坐标与3D坐标一致，通过监控控制和运动信息可以很容易地获得对自我运动的补偿。考虑到鲁棒性和一致性，BEV是多传感器和时间融合的理想表示。</li></ul>]]></content>
      
      
      <categories>
          
          <category> DL </category>
          
      </categories>
      
      
        <tags>
            
            <tag> bev </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>BEV感知经验性的评估和可能改进的方法</title>
      <link href="/2023/11/26/bev-evalution/"/>
      <url>/2023/11/26/bev-evalution/</url>
      
        <content type="html"><![CDATA[<h1 id="BEV感知经验性的评估和可能改进的方法"><a href="#BEV感知经验性的评估和可能改进的方法" class="headerlink" title="BEV感知经验性的评估和可能改进的方法"></a>BEV感知经验性的评估和可能改进的方法</h1><p>总结了在各种基准上实现最佳结果的技巧和最有用的实践。建立在BEVFormer之上的BEVFormer++，用于仅用于相机检测，以及从SPVCNN派生的Voxel-SPVCNN用于LiDAR分割。这些实践经验可以作为无缝集成到其他BEV感知模型和评估效果的参考。基于Camera和基于LiDAR的环境下的数据增强，高效的BEV编码器设计，感知头和损失函数族，有用的测试时间增强(TTA)和集成策略等。</p><h2 id="数据增强"><a href="#数据增强" class="headerlink" title="数据增强"></a>数据增强</h2><p>数据增强在增强感知模型的稳健性和泛化能力方面起着至关重要的作用。通过综合扩展训练数据集的多样性，可以提高模型处理真实场景变化的能力。某些非仿射变换很难应用于图像或BEV空间的数据增强。例如，copy-paste和Mosaic等方法存在问题，因为它们可能会导致图像语义和3D空间信息之间的不一致。</p><h3 id="BEVCamera-Camera-only-Detection"><a href="#BEVCamera-Camera-only-Detection" class="headerlink" title="BEVCamera (Camera-only) Detection"></a>BEVCamera (Camera-only) Detection</h3><p>用于二维识别任务的常见图像数据增强适用于基于相机的Bev感知任务。一般来说，可以将增强分为仅涉及颜色变化的静态增强和移动像素的空间变换。</p><ul><li>基于颜色变化的增强效果可以直接应用。</li><li>对于涉及空间变换的增强，除了相应地变换ground truth之外，相机参数的校准也是必要的。最近仅使用相机的方法中采用的常见增强方法有颜色抖动、翻转、调整大小、旋转、裁剪和栅格蒙版。</li></ul><p>在Bev感知中有两种翻转图像的方法，称之为ImageLevel翻转和Bev-Level翻转。在图像级翻转期间，翻转图像并调整相机内部参数，而相机外部参数和GT框保持不变。这将保留3D到2D的投影关系。请注意，图像级翻转仅增强了2D特征提取，不会对与BEV关联的后续模块产生任何影响。</p><ul><li>在Bev级翻转中，翻转图像并对称地重新排列多视角图像，例如将左前摄像头切换到右前。保留了重叠区域的相干性。在调整外部参数时，摄影机内部参数保持不变，并且翻转Bev平面上的GT长方体。Bev级翻转提升了整个Bev感知模型。</li></ul><p>在BEVFormer++中，使用了颜色抖动、翻转、多尺度调整和网格掩码。输入图像按0.5到1.2之间的系数缩放，按0.5的比率翻转；使用正方形蒙版随机遮罩总面积的最大30%。值得注意的是，BEVFormer++采用BEV级翻转。相关的见表4 ID6实验，表明数据增强对提高3D模型的性能起着至关重要的作用。由于BEVFormer使用序列输入，因此它确保输入增强后序列的每一帧的转换是一致的。</p><h3 id="LiDAR-Segmentaion"><a href="#LiDAR-Segmentaion" class="headerlink" title="LiDAR Segmentaion"></a>LiDAR Segmentaion</h3><p>与检测任务不同，重数据增强可以应用于分割任务，包括随机旋转、缩放、翻转和点平移。</p><ul><li>对于随机旋转，从[0，2π)的范围中拾取一个角度，将旋转应用于x-y平面上的每个点。</li><li>缩放从[0.9，1.1]的范围中选择比例因子，然后在点云坐标上相乘。</li><li>随机翻转沿X轴、Y轴或同时沿X和Y轴进行。</li><li>对于随机平移，每个轴的偏移与正态分布分开采样，平均值为0，标准差为0.1。</li></ul><p>除了坐标和反射率之外，还可以利用额外的信息来提高模型的性能。</p><ul><li>Painting[PointPainting，PointAugmenting]是利用图像信息增强点云数据的常用技术。对于未标注的图像数据，通过将点云标签投影到相应的图像上并对稀疏标注进行加密化，从带标注的点云数据中获得图像上的语义标签。训练图像模型以提供2D语义分割结果。然后，将预测的语义标签绘制为一个热点向量来指向云数据，作为表示图像语义信息的附加通道。</li><li>此外，还可以使用时间信息，因为自动驾驶中的数据集通常是按顺序收集的。过去的连续帧与当前帧串联。附加一个通道来表示不同帧的相对时间信息。为了减少点数，应用了一个小的体素化网络。然后，将体素作为点作为的模型的输入。</li></ul><h2 id="BEV-Encoder"><a href="#BEV-Encoder" class="headerlink" title="BEV Encoder"></a>BEV Encoder</h2><h3 id="BEVCamera-BEVFormer"><a href="#BEVCamera-BEVFormer" class="headerlink" title="BEVCamera: BEVFormer++"></a>BEVCamera: BEVFormer++</h3><p>BEVFormer++有多个编码层，每个编码层都遵循传统的transformers结构，除了三个定制设计，即BEV查询、空间交叉注意和时间自我注意(BEV queries, spatial cross-attention, and temporal self-attention)。具体来说，BEV查询是网格形状的可学习参数，旨在通过注意机制从多camera视图查询BEV空间中的特征。空间交叉注意和时间自我注意是与BEV查询一起工作的关注层，用于从多camera图像中查找和聚集空间特征，以及从历史BEV特征中查找和聚集时间特征。</p><ul><li>在推断过程中，在时间戳t，将多个摄像头图像馈送到骨干网络(例如，ResNet-101)，并获得不同摄像头视图的特征Ft。同时，保留了之前时间戳t−1处的BEV特征Bt−1。<ul><li>在每个编码层中，首先使用BEV查询Q通过时间自注意从先前的BEV特征Bt−1中查询时间信息。</li><li>然后，使用BEV查询Q通过空间交叉注意从多camera特征Ft中查询空间信息。</li><li>在前馈网络[transformer]之后，编码层生成精细化的BEV特征作为下一个编码层的输入。</li><li>在六个堆叠编码层之后，生成当前时间戳t处的统一BEV特征Bt。</li><li>3D检测头和地图分割头以BEV特征Bt为输入，对3D包围盒和语义地图等感知结果进行预测。</li></ul></li></ul><p>为了提高BEV编码器的特征质量，主要从以下三个方面进行了讨论。</p><p> <img src="/pic/BEV21.png" alt="表4"></p><ul><li>(A)2D Feature Extractor(2D特征抽取器)。<ul><li>在2D感知任务中改进主干网络特征表征质量也最有可能提高BEV任务的质量。<ul><li>在图像主干中，采用了在大多数2D感知任务中广泛使用的特征金字塔。如表4所示二维特征抽取器的结构设计，如最先进的图像特征抽取器[Swin transformer]、全局信息交互[Cornernet]、多层特征融合[FPN,Deformable detr]等都有助于更好地表示Bev感知的特征。除了结构设计，<strong>监督主干的辅助任务对BEV感知的性能也很重要</strong>，将在下面讨论。</li></ul></li></ul></li><li>(B)View transformation(视图变换)。<ul><li>该变换接受图像特征并将它们重新组织到BEV空间。超参数，包括<strong>图像特征的采样范围和频率，以及Bev分辨率</strong>，对于Bev感知的性能是至关重要的。<ul><li>采样范围决定图像后面的视锥的多少将被采样到Bev空间。默认情况下，此范围等于LiDAR注释的有效范围。当效率具有更高的优先级时，由于在大多数情况下它只包含不重要的信息，例如天空，所以观察锥体的上部z轴部分可能会受到损害。</li><li>采样频率决定了图像特征的效用。较高的频率确保了该模型以较高的计算成本准确地对每个BEV位置的相应图像特征进行采样。</li><li>Bev分辨率决定了Bev特征的表示颗粒度，其中每个特征都可以精确地追溯到世界坐标中的栅格。为了更好地表示交通灯和行人等小尺度对象，需要高分辨率。相关实验在表格4 ID 2&amp;3.</li><li>在视图变换中，许多BEV感知网络中也存在特征提取操作，例如卷积块或变换器块。在BEV空间中加入更好的特征提取子网络也可以提高BEV感知性能。</li></ul></li></ul></li><li>(C)Bev时序融合(Temporal BEV fusion)。<ul><li>在给定Bev特征结构的情况下，Bev空间中的时间融合通常利用自我姿态信息来对齐时间Bev特征。然而，在这个对准过程中，其他代理的运动没有显式建模，需要通过模型进行额外的学习。</li><li>因此，为了加强对其他运动主体特征的融合，在进行时间融合时增加交叉注意的感知范围是合理的。例如，可以在可变形的注意力模块中放大注意力偏移量的核大小，或者使用全局注意力。在表4中可以观察到相关的改进ID%1。</li></ul></li></ul><h3 id="BEVLiDAR-Voxel-SPVCNN"><a href="#BEVLiDAR-Voxel-SPVCNN" class="headerlink" title="BEVLiDAR: Voxel-SPVCNN"></a>BEVLiDAR: Voxel-SPVCNN</h3><p>由于粗体素化和激进的下采样，现有的3D感知模型对于识别小实例并不理想。</p><ul><li>SPVCNN[83]在基于体素的分支中使用Minkowski UNET[119]。为了保持点云的分辨率，使用了额外的基于点的分支，而不需要进行下采样。基于点的分支和基于体素的分支的特征将在网络的不同阶段彼此传播。</li><li>通过对原始SPVCNN[83]进行两个有效的修改而提出了Voxel-SPVCNN。<ul><li>与简单地对原始输入特征进行体素化相比，采用轻量级的3层MLP来提取点特征，然后进行体素化处理。</li><li>此外，基于点的分支的输入被voxel-as-point分支所代替。该分支的网络结构仍然是MLP；但输入被替换为体素。</li><li>Voxel-SPVCNN的效率更高，因为基于点的分支的计算量大大减少，特别是在输入是多扫描点云的情况下。模型体系结构的改变带来了1.1Mou的改进(见表5 ID 7)。</li></ul></li></ul><h2 id="Loss"><a href="#Loss" class="headerlink" title="Loss"></a>Loss</h2><h3 id="BEVCamera-only-Detection"><a href="#BEVCamera-only-Detection" class="headerlink" title="BEVCamera-only Detection"></a>BEVCamera-only Detection</h3><ul><li><p>BEV特征表示的一个多用途的好处是使模型能够在2D和3D目标检测中提出损失来进行训练。</p><ul><li>如表4 ID 16-20，当利用不同的检测头设计时，可以通过最小的修改来转移相应的损失，例如调整损失权重。</li></ul></li><li><p>除了3D目标的训练损失外，辅助损失在纯camera的Bev检测中也起着重要作用。</p><ul><li>一种类型的辅助损失是在2D特征提取器之上添加2D检测损失。这种监督增强了二维图像特征的局部化，进而有助于通过视觉变换在BEV感知中提供3D表示。在表4 ID 4中可以观察到利用这种辅助损耗的一个例子。</li><li>另一种辅助损失是深度监督[BEVDepth]。当利用LiDAR系统产生的地面真实深度时，可以提高Bev感知的隐含深度估计能力，以获得准确的三维目标定位。</li><li>这两项辅助任务都可以在训练期间应用，以提高成绩。通常采用2D检测或深度预训练骨干作为初始权重[BEVformer，DETR3D]。</li></ul></li></ul><h3 id="LiDAR-segmentation"><a href="#LiDAR-segmentation" class="headerlink" title="LiDAR segmentation"></a>LiDAR segmentation</h3><p>与传统的交叉熵损失不同，Geo损失[139]和Lovasz损失[140]被用来训练所有模型。为了更好地区分不同类别的边界，Geo Lost对细节丰富的体素有很强的响应性。Lovasz Lost是一种可区分的联合交集(IOU)损失，以缓解阶级失衡问题。它将模型性能提高了0.6 Mou，如表5 ID 2。</p><h2 id="展望"><a href="#展望" class="headerlink" title="展望"></a>展望</h2><p>最大的挑战和未来的努力可能是：</p><ul><li>(A)如何设计更准确的深度估计器；</li><li>(B)如何在一种新的融合机制中更好地对齐来自多个传感器的特征表示；</li><li>(C)如何设计一个无参数网络，使得算法性能可以自由地进行姿势变化或传感器位置，从而在不同的场景中实现更好的泛化能力；</li><li>(D)如何结合基础模型的成功知识来促进BEV感知。</li></ul><p>下面详细分开讨论</p><h3 id="Depth-Estimation-深度估计"><a href="#Depth-Estimation-深度估计" class="headerlink" title="Depth Estimation(深度估计)"></a>Depth Estimation(深度估计)</h3><p>基于视觉的Bev感知的核心问题在于准确的深度估计，因为该任务是在3D环境中执行的。目前解决深度预测的方法是</p><ul><li>(A)伪LiDAR生成；</li><li>(B)将特征从2D提升到3D对应；</li><li>(C)LiDAR相机蒸馏；</li><li>(D)双目视差或时间运动。<br>这些方向中的任何一个或组合都是有希望的。为了保证更好的性能，大量的监控数据也是至关重要的[80]。</li></ul><p>另一个有趣而重要的方向是如何在训练过程中利用LiDAR信息(例如，作为深度监控)，而在推理过程中只输入视觉输入。这对OEM(制造商)来说是非常有利的，因为我们经常从多个来源获得方便的大量培训数据，但出于部署考虑，发货产品上只提供摄像头输入。</p><h3 id="Fusion-Mechanism-融合机制"><a href="#Fusion-Mechanism-融合机制" class="headerlink" title="Fusion Mechanism(融合机制)"></a>Fusion Mechanism(融合机制)</h3><p>根据融合模块在pipeline中的位置，到目前为止，大多数融合方法可以分为早期融合、中期融合或晚期融合。</p><ul><li>传感器融合算法最简单的设计是将来自相机和激光雷达的两组特征分别连接起来。然而，如何“对齐”来自不同来源的特性是至关重要的。<ul><li>这意味着：(A)相机的特征表示在3D几何空间而不是2D环境中被适当地描述；</li><li>(B)3D空间中的点云与2D域中的对应点云具有准确的对应关系，这意味着LiDAR和相机之间的软和&#x2F;或硬同步得到了精细的保证。</li></ul></li><li>在上述前提条件的基础上，如何设计出优雅的融合方案需要社区更多的关注。<ul><li>这一部分的未来努力可能是：(A)利用自我和&#x2F;或交叉注意在Transformer中整合来自各种模式的特征表示；</li><li>(B)从一般的多模式文献中获得的知识也可能是有利的，例如，CLIP formulation中的文本-图像对的哲学[18]可以启发自动驾驶领域不同传感器的信息集成。</li></ul></li></ul><h3 id="Parameter-free-Design-to-Improve-Generalization-提高泛化能力的无参数设计"><a href="#Parameter-free-Design-to-Improve-Generalization-提高泛化能力的无参数设计" class="headerlink" title="Parameter-free Design to Improve Generalization(提高泛化能力的无参数设计)"></a>Parameter-free Design to Improve Generalization(提高泛化能力的无参数设计)</h3><p>BEV感知中最大的挑战之一是领域适应。一个数据集中的训练模型在另一个数据集中的表现和泛化程度。人们负担不起高昂的成本(培训、数据、注释等)在每一个数据集中的算法。</p><ul><li>由于Bev感知本质上是对物理世界的3D重建，我们认为一个<strong>好的探测器必须与相机参数捆绑在一起</strong>，例如，外参矩阵。不同的基准有不同的摄像机&#x2F;传感器设置，对应于物理位置、重叠区域、视场(视场)、失真参数等。这些因素都会导致将良好性能从一个场景转移到另一个域的(极端)困难。<ul><li>为此，它敦促我们<strong>将网络与摄像机参数分离</strong>，使特征学习独立于外部和&#x2F;或内部矩阵。学术界(extrinsic free，[190])和产业界(rectify module修正模块，[6])在这个方向上都有一些有趣的工作。尽管如此，这并不是一件微不足道的事情，作为未来的工作，最好从社区中进行更多的调查。<strong>无参数设计对于解决实际应用中由于道路颠簸和摄像机不稳定造成的检测不准确具有很强的鲁棒性。</strong></li></ul></li></ul><h3 id="Foundation-Models-to-Facilitate-BEV-Perception-促进BEV感知的基础模型"><a href="#Foundation-Models-to-Facilitate-BEV-Perception-促进BEV感知的基础模型" class="headerlink" title="Foundation Models to Facilitate BEV Perception(促进BEV感知的基础模型)"></a>Foundation Models to Facilitate BEV Perception(促进BEV感知的基础模型)</h3><p>近年来，在一般视觉社区中，大型或基础模型[14、15、18、191、192]取得了令人印象深刻的性能，并在许多领域和任务中超越了最先进的技术。</p><ul><li>对于Bev的感知，至少有两个方面值得研究。<ul><li>一种是应用大型预先训练模型中的丰富知识，并提供更好的初始检查点来进行微调。然而，像上一节所暗示的那样，某些2D基础模型的直接自适应在3D BEV意义上可能不能很好地工作。如何设计和选择基础车型，以更好地适应自动驾驶任务，是我们可以接受的长期研究问题。</li><li>另一个是如何发展多任务学习的想法，就像在Bev感知的基础模型(通才)中那样。在一般的视觉文献中有一些有趣的工作，其中OFA[193]、Uni-PerceiverMoE[194]、Gato[195]等将执行多个复杂的任务并获得令人满意的结果。我们能否将类似的理念应用到BEV感知中，并将多项任务统一在一个框架中？这是有意义的，因为自动驾驶中的感知和认知领域需要协作来处理复杂的场景，以实现最终的L5目标。</li></ul></li></ul>]]></content>
      
      
      <categories>
          
          <category> DL </category>
          
      </categories>
      
      
        <tags>
            
            <tag> bev </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>BEVDet训练记录</title>
      <link href="/2023/11/18/bevdet/"/>
      <url>/2023/11/18/bevdet/</url>
      
        <content type="html"><![CDATA[<h1 id="BEVDet在服务器上环境搭建以及运行记录"><a href="#BEVDet在服务器上环境搭建以及运行记录" class="headerlink" title="BEVDet在服务器上环境搭建以及运行记录"></a>BEVDet在服务器上环境搭建以及运行记录</h1><h2 id="流程"><a href="#流程" class="headerlink" title="流程"></a>流程</h2><ul><li><p>创建conda虚拟环境：</p><pre class="line-numbers language-python" data-language="python"><code class="language-python">conda create <span class="token operator">-</span><span class="token operator">-</span>name BEVDet python<span class="token operator">=</span><span class="token number">3.8</span> <span class="token operator">-</span>y<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre></li><li><p>在虚拟环境里安装torch：</p><pre class="line-numbers language-python" data-language="python"><code class="language-python">conda install pytorch<span class="token operator">==</span><span class="token number">1.10</span><span class="token number">.0</span> torchvision<span class="token operator">==</span><span class="token number">0.11</span><span class="token number">.0</span> cudatoolkit<span class="token operator">=</span><span class="token number">11.3</span> <span class="token operator">-</span>c https<span class="token punctuation">:</span><span class="token operator">//</span>mirrors<span class="token punctuation">.</span>tuna<span class="token punctuation">.</span>tsinghua<span class="token punctuation">.</span>edu<span class="token punctuation">.</span>cn<span class="token operator">/</span>anaconda<span class="token operator">/</span>cloud<span class="token operator">/</span>pytorch<span class="token operator">/</span>linux<span class="token operator">-</span><span class="token number">64</span><span class="token operator">/</span><span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre></li><li><p>安装mmcv-full:</p><pre class="line-numbers language-python" data-language="python"><code class="language-python">pip install mmcv<span class="token operator">-</span>full<span class="token operator">==</span><span class="token number">1.5</span><span class="token number">.3</span> <span class="token operator">-</span>f https<span class="token punctuation">:</span><span class="token operator">//</span>download<span class="token punctuation">.</span>openmmlab<span class="token punctuation">.</span>com<span class="token operator">/</span>mmcv<span class="token operator">/</span>dist<span class="token operator">/</span>cu113<span class="token operator">/</span>torch1<span class="token punctuation">.</span><span class="token number">10.0</span><span class="token operator">/</span>index<span class="token punctuation">.</span>html  <span class="token operator">-</span>i https<span class="token punctuation">:</span><span class="token operator">//</span>pypi<span class="token punctuation">.</span>tuna<span class="token punctuation">.</span>tsinghua<span class="token punctuation">.</span>edu<span class="token punctuation">.</span>cn<span class="token operator">/</span>simple<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre></li><li><p>安装mmdet：</p><pre class="line-numbers language-python" data-language="python"><code class="language-python">pip install mmdet<span class="token operator">==</span><span class="token number">2.25</span><span class="token number">.1</span> mmsegmentation<span class="token operator">==</span><span class="token number">0.25</span><span class="token number">.0</span>  <span class="token operator">-</span>i https<span class="token punctuation">:</span><span class="token operator">//</span>pypi<span class="token punctuation">.</span>tuna<span class="token punctuation">.</span>tsinghua<span class="token punctuation">.</span>edu<span class="token punctuation">.</span>cn<span class="token operator">/</span>simple<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre></li><li><p>安装其他：</p><pre class="line-numbers language-python" data-language="python"><code class="language-python">pip install pycuda \    lyft_dataset_sdk \    networkx<span class="token operator">==</span><span class="token number">2.2</span> \    numba<span class="token operator">==</span><span class="token number">0.53</span><span class="token number">.0</span> \    numpy<span class="token operator">==</span><span class="token number">1.23</span><span class="token number">.5</span> \    nuscenes<span class="token operator">-</span>devkit \    yapf<span class="token operator">==</span><span class="token number">0.40</span><span class="token number">.1</span>\    setuptools<span class="token operator">==</span><span class="token number">59.5</span><span class="token number">.0</span>\    plyfile \    scikit<span class="token operator">-</span>image \    tensorboard \    trimesh<span class="token operator">==</span><span class="token number">2.35</span><span class="token number">.39</span> <span class="token operator">-</span>i https<span class="token punctuation">:</span><span class="token operator">//</span>pypi<span class="token punctuation">.</span>tuna<span class="token punctuation">.</span>tsinghua<span class="token punctuation">.</span>edu<span class="token punctuation">.</span>cn<span class="token operator">/</span>simple<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre></li><li><p>最后安装整个项目：</p><pre class="line-numbers language-python" data-language="python"><code class="language-python">pip install <span class="token operator">-</span>e <span class="token punctuation">.</span><span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre></li><li><p>多机训练：</p><pre class="line-numbers language-python" data-language="python"><code class="language-python">CUDA_VISIBLE_DEVICES<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">,</span><span class="token number">1</span><span class="token punctuation">,</span><span class="token number">2</span><span class="token punctuation">,</span><span class="token number">3</span> PORT<span class="token operator">=</span><span class="token number">29500</span> <span class="token punctuation">.</span><span class="token operator">/</span>tools<span class="token operator">/</span>dist_train<span class="token punctuation">.</span>sh $<span class="token punctuation">&#123;</span>CONFIG_FILE<span class="token punctuation">&#125;</span> <span class="token number">4</span><span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre></li></ul><pre class="line-numbers language-python" data-language="python"><code class="language-python">CUDA_VISIBLE_DEVICES<span class="token operator">=</span><span class="token number">7</span> PORT<span class="token operator">=</span><span class="token number">29510</span> <span class="token punctuation">.</span><span class="token operator">/</span>tools<span class="token operator">/</span>dist_train<span class="token punctuation">.</span>sh configs<span class="token operator">/</span>bevdet<span class="token operator">/</span>bevdet<span class="token operator">-</span>r50<span class="token punctuation">.</span>py <span class="token number">1</span> <span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><br/><h3 id="报错-TypeError-FormatCode-got-an-unexpected-keyword-argument-‘verify’"><a href="#报错-TypeError-FormatCode-got-an-unexpected-keyword-argument-‘verify’" class="headerlink" title="报错 TypeError: FormatCode() got an unexpected keyword argument ‘verify’"></a>报错 TypeError: FormatCode() got an unexpected keyword argument ‘verify’</h3><p>原因：yapf版本过高<br>由0.40.2 切换成 0.40.1问题解决</p><p>pip install yapf&#x3D;&#x3D;0.40.1</p><h3 id="警告’Creating-a-tensor-from-a-list-of-numpy-ndarrays-is-extremely-slow’"><a href="#警告’Creating-a-tensor-from-a-list-of-numpy-ndarrays-is-extremely-slow’" class="headerlink" title="警告’Creating a tensor from a list of numpy.ndarrays is extremely slow’"></a>警告’Creating a tensor from a list of numpy.ndarrays is extremely slow’</h3><p>总结<br>(1) 对于不含numpy.ndarrays的list而言，list-&gt;tensor明显快于list-&gt;numpy.ndarrays-&gt;tensor (1.7s&lt;2.5s);</p><p>(2) 对于含有numpy.ndarrays的list而言，list-&gt;numpy.ndarrays-&gt;tensor明显快于list-&gt;tensor (18.8s&lt;41.2s).</p><p><a href="https://zhuanlan.zhihu.com/p/429901066">参考链接</a></p><h3 id="报错：-AttributeError-module-‘distutils’-has-no-attribute-‘version’"><a href="#报错：-AttributeError-module-‘distutils’-has-no-attribute-‘version’" class="headerlink" title="报错： AttributeError: module ‘distutils’ has no attribute ‘version’."></a>报错： AttributeError: module ‘distutils’ has no attribute ‘version’.</h3><p>解决： setuptools版本问题”，版本过高导致的问题；setuptools版本</p><p>第一步： pip uninstall setuptools【使用pip，不能使用 conda uninstall setuptools ; 【不能使用conda的命令，原因是，conda在卸载的时候，会自动分析与其相关的库，然后全部删除，如果y的话，整个环境都需要重新配置。</p><p>第二步： pip或者conda install setuptools&#x3D;&#x3D;59.5.0【现在最新的版本已经到了68了，之前的老版本只是部分保留，找不到的版本不行</p><h3 id="Ubuntu安装ninja"><a href="#Ubuntu安装ninja" class="headerlink" title="Ubuntu安装ninja"></a>Ubuntu安装ninja</h3><p>Ninja是一个比Make更快速的小型构建系统。其github地址为：<a href="https://ninja-build.org/">https://ninja-build.org/</a></p><p><a href="https://blog.csdn.net/SHH_1064994894/article/details/129268006">不同安装方式</a></p><p><a href="https://blog.csdn.net/qq_36287943/article/details/105343192?spm=1001.2101.3001.6650.2&utm_medium=distribute.pc_relevant.none-task-blog-2~default~CTRLIST~Rate-2-105343192-blog-129268006.235%5Ev38%5Epc_relevant_anti_vip_base&depth_1-utm_source=distribute.pc_relevant.none-task-blog-2~default~CTRLIST~Rate-2-105343192-blog-129268006.235%5Ev38%5Epc_relevant_anti_vip_base&utm_relevant_index=5">Ninja安装和基本使用</a></p><p>pip3 install ninja</p><h2 id="BEVDet系列源码解读"><a href="#BEVDet系列源码解读" class="headerlink" title="BEVDet系列源码解读"></a><a href="https://zhuanlan.zhihu.com/p/557613388">BEVDet系列源码解读</a></h2><p><img src="/pic/fb9cef23d1dd44e2f67fad259113ad1b.png"></p><p><strong>BEVDet方法:关键是其中的第二步View Transformer将图像特征转化为BEV特征的过程并使用cuda实现了高效的voxel_pooling_v2在后处理中也提出了scale-NMS可以针对不同尺度的物体进行缩放然后进行过滤。</strong></p><p><a href="https://www.zhihu.com/column/c_1502288547017617408">系列解读链接</a></p><h2 id="nuScenes使用介绍以及BEVDet进行训练"><a href="#nuScenes使用介绍以及BEVDet进行训练" class="headerlink" title="nuScenes使用介绍以及BEVDet进行训练"></a>nuScenes使用介绍以及BEVDet进行训练</h2><p><a href="https://blog.csdn.net/weixin_44596312/article/details/122300131">csdn</a></p><p><a href="https://mmdetection3d.readthedocs.io/zh-cn/latest/advanced_guides/datasets/nuscenes.html">mmdet3d解说</a></p><h3 id="在mini-v1-0训练测试"><a href="#在mini-v1-0训练测试" class="headerlink" title="在mini-v1.0训练测试"></a>在mini-v1.0训练测试</h3><p>mini-v1.0里面有三类物体（C.V.、Trailer、Barrier）不包含，其他物体数量也比较少，所以最后的性能上会比较低。</p><p><img src="/pic/6a46792dc50f7f77a616b9c0fc84113c.png"></p><p><img src="/pic/4e597a394f9c1492958cc248049f4fe3.png"></p><p><img src="/pic/e77bbdc1106b3b95edfee3277d314dd3.png" alt="1699529096094.png"></p><h3 id="相关检测参数指标"><a href="#相关检测参数指标" class="headerlink" title="相关检测参数指标"></a>相关检测参数指标</h3><p><a href="https://blog.csdn.net/weixin_45097875/article/details/125846945">参考链接</a></p><h4 id="mAP"><a href="#mAP" class="headerlink" title="mAP:"></a>mAP:</h4><p>在评测时依旧使用目标检测中常用的的AP，不过AP的阈值匹配不使用IoU来计算，而使用在地平面上的2D中心距离d来计算。这样解耦了物体的尺寸和方向对AP计算的影响。d设置为{0.5,1,2,4}米。在计算AP时，去除了低于0.1的recall和precision并用0来代替这些区域。不同类以及不同难度D用来计算mAP：</p><h4 id="mATE："><a href="#mATE：" class="headerlink" title="mATE："></a>mATE：</h4><p>Average Translation Error,平均平移误差(ATE) 是二维欧几里德中心距离(单位为米).</p><h4 id="mASE："><a href="#mASE：" class="headerlink" title="mASE："></a>mASE：</h4><p>Average Scale Error, 平均尺度误差(ASE) 是1 - IoU, 其中IoU 是角度对齐后的三维交并比</p><h4 id="mAOE："><a href="#mAOE：" class="headerlink" title="mAOE："></a>mAOE：</h4><p>Average Orientation Error.平均角度误差(AOE) 是预测值和真实值之间最小的偏航角差。(所有的类别角度偏差都在360∘度内, 除了障碍物这个类别的角度偏差在180∘ 内)</p><h4 id="mAVE："><a href="#mAVE：" class="headerlink" title="mAVE："></a>mAVE：</h4><p> Average Velocity Error.平均速度误差(AVE) 是二维速度差的L2 范数(m&#x2F;s)。</p><h4 id="mAAE："><a href="#mAAE：" class="headerlink" title="mAAE："></a>mAAE：</h4><p>Average Attribute Error,平均属性错误(AAE) 被定义为1−acc, 其中acc 为类别分类准确度。</p><p><img src="/pic/bd1a7e7e4cdf242c91e2ecd6e1f7eabb.png"></p><p><img src="/pic/6a46792dc50f7f77a616b9c0fc84113c.png"></p><br/><p>可以看出每个参数的性能指标</p><h2 id="BEVDet中nuscenes数据集处理"><a href="#BEVDet中nuscenes数据集处理" class="headerlink" title="BEVDet中nuscenes数据集处理"></a>BEVDet中nuscenes数据集处理</h2><p>处理前的结构</p><pre class="line-numbers language-python" data-language="python"><code class="language-python">mmdetection3d├── mmdet3d├── tools├── configs├── data│   ├── nuscenes│   │   ├── maps│   │   ├── samples│   │   ├── sweeps│   │   ├── v1<span class="token punctuation">.</span><span class="token number">0</span><span class="token operator">-</span>test<span class="token operator">|</span>   <span class="token operator">|</span>   ├── v1<span class="token punctuation">.</span><span class="token number">0</span><span class="token operator">-</span>trainval<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>处理使用命令</p><pre class="line-numbers language-python" data-language="python"><code class="language-python">python tools<span class="token operator">/</span>create_data<span class="token punctuation">.</span>py nuscenes <span class="token operator">-</span><span class="token operator">-</span>root<span class="token operator">-</span>path <span class="token punctuation">.</span><span class="token operator">/</span>data<span class="token operator">/</span>nuscenes <span class="token operator">-</span><span class="token operator">-</span>out<span class="token operator">-</span><span class="token builtin">dir</span> <span class="token punctuation">.</span><span class="token operator">/</span>data<span class="token operator">/</span>nuscenes <span class="token operator">-</span><span class="token operator">-</span>extra<span class="token operator">-</span>tag nuscenes<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>处理后的结构</p><pre class="line-numbers language-python" data-language="python"><code class="language-python">mmdetection3d├── mmdet3d├── tools├── configs├── data│   ├── nuscenes│   │   ├── maps│   │   ├── samples│   │   ├── sweeps│   │   ├── v1<span class="token punctuation">.</span><span class="token number">0</span><span class="token operator">-</span>test<span class="token operator">|</span>   <span class="token operator">|</span>   ├── v1<span class="token punctuation">.</span><span class="token number">0</span><span class="token operator">-</span>trainval│   │   ├── nuscenes_database│   │   ├── nuscenes_infos_train<span class="token punctuation">.</span>pkl│   │   ├── nuscenes_infos_val<span class="token punctuation">.</span>pkl│   │   ├── nuscenes_infos_test<span class="token punctuation">.</span>pkl│   │   ├── nuscenes_dbinfos_train<span class="token punctuation">.</span>pkl│   │   ├── nuscenes_infos_train_mono3d<span class="token punctuation">.</span>coco<span class="token punctuation">.</span>json│   │   ├── nuscenes_infos_val_mono3d<span class="token punctuation">.</span>coco<span class="token punctuation">.</span>json│   │   ├── nuscenes_infos_test_mono3d<span class="token punctuation">.</span>coco<span class="token punctuation">.</span>json<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p><img src="/pic/3965332f9620a419a12dff851c3b6de2.png"></p><p><img src="/pic/23ecfbfcf5e0597b8c2a808e2a138b0b.png"></p>]]></content>
      
      
      <categories>
          
          <category> DL </category>
          
      </categories>
      
      
        <tags>
            
            <tag> bev </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>MonoScene训练记录</title>
      <link href="/2023/11/17/monoscene/"/>
      <url>/2023/11/17/monoscene/</url>
      
        <content type="html"><![CDATA[<h1 id="MonoScene在服务器上环境搭建以及运行记录"><a href="#MonoScene在服务器上环境搭建以及运行记录" class="headerlink" title="MonoScene在服务器上环境搭建以及运行记录"></a>MonoScene在服务器上环境搭建以及运行记录</h1><h2 id="conda环境"><a href="#conda环境" class="headerlink" title="conda环境"></a>conda环境</h2><pre class="line-numbers language-python" data-language="python"><code class="language-python">pytorch<span class="token operator">-</span>lightning_libgcc_mutex             <span class="token number">0.1</span>                        main    defaults_openmp_mutex             <span class="token number">5.1</span>                       1_gnu    defaultsabsl<span class="token operator">-</span>py                   <span class="token number">2.0</span><span class="token number">.0</span>                    pypi_0    pypiaiohttp                   <span class="token number">3.8</span><span class="token number">.6</span>                    pypi_0    pypiaiosignal                 <span class="token number">1.3</span><span class="token number">.1</span>                    pypi_0    pypiantlr4<span class="token operator">-</span>python3<span class="token operator">-</span>runtime    <span class="token number">4.8</span>                      pypi_0    pypi<span class="token keyword">async</span><span class="token operator">-</span>timeout             <span class="token number">4.0</span><span class="token number">.3</span>                    pypi_0    pypiasynctest                 <span class="token number">0.13</span><span class="token number">.0</span>                   pypi_0    pypiattrs                     <span class="token number">23.1</span><span class="token number">.0</span>                   pypi_0    pypiblas                      <span class="token number">1.0</span>                         mkl    defaultsca<span class="token operator">-</span>certificates           <span class="token number">2023.08</span><span class="token number">.22</span>           h06a4308_0    defaultscachetools                <span class="token number">5.3</span><span class="token number">.2</span>                    pypi_0    pypicertifi                   <span class="token number">2022.12</span><span class="token number">.7</span>        py37h06a4308_0    defaultscharset<span class="token operator">-</span>normalizer        <span class="token number">3.3</span><span class="token number">.1</span>                    pypi_0    pypicudatoolkit               <span class="token number">10.2</span><span class="token number">.89</span>              hfd86e86_1    defaultscycler                    <span class="token number">0.11</span><span class="token number">.0</span>                   pypi_0    pypifonttools                 <span class="token number">4.38</span><span class="token number">.0</span>                   pypi_0    pypifreetype                  <span class="token number">2.12</span><span class="token number">.1</span>               h4a9f257_0    defaultsfrozenlist                <span class="token number">1.3</span><span class="token number">.3</span>                    pypi_0    pypifsspec                    <span class="token number">2023.1</span><span class="token number">.0</span>                 pypi_0    pypifuture                    <span class="token number">0.18</span><span class="token number">.3</span>                   pypi_0    pypigiflib                    <span class="token number">5.2</span><span class="token number">.1</span>                h5eee18b_3    defaultsgoogle<span class="token operator">-</span>auth               <span class="token number">2.23</span><span class="token number">.3</span>                   pypi_0    pypigoogle<span class="token operator">-</span>auth<span class="token operator">-</span>oauthlib      <span class="token number">0.4</span><span class="token number">.6</span>                    pypi_0    pypigrpcio                    <span class="token number">1.59</span><span class="token number">.0</span>                   pypi_0    pypihydra<span class="token operator">-</span>core                <span class="token number">1.0</span><span class="token number">.5</span>                    pypi_0    pypiidna                      <span class="token number">3.4</span>                      pypi_0    pypiimageio                   <span class="token number">2.31</span><span class="token number">.2</span>                   pypi_0    pypiimportlib<span class="token operator">-</span>metadata        <span class="token number">6.7</span><span class="token number">.0</span>                    pypi_0    pypiimportlib<span class="token operator">-</span>resources       <span class="token number">5.12</span><span class="token number">.0</span>                   pypi_0    pypiintel<span class="token operator">-</span>openmp              <span class="token number">2021.4</span><span class="token number">.0</span>          h06a4308_3561    defaultsjoblib                    <span class="token number">1.3</span><span class="token number">.2</span>                    pypi_0    pypijpeg                      9b                   h024ee3a_2    defaultskiwisolver                <span class="token number">1.4</span><span class="token number">.5</span>                    pypi_0    pypilcms2                     <span class="token number">2.12</span>                 h3be6417_0    defaultsld_impl_linux<span class="token operator">-</span><span class="token number">64</span>          <span class="token number">2.38</span>                 h1181459_1    defaultslibffi                    <span class="token number">3.4</span><span class="token number">.4</span>                h6a678d5_0    defaultslibgcc<span class="token operator">-</span>ng                 <span class="token number">11.2</span><span class="token number">.0</span>               h1234567_1    defaultslibgomp                   <span class="token number">11.2</span><span class="token number">.0</span>               h1234567_1    defaultslibpng                    <span class="token number">1.6</span><span class="token number">.39</span>               h5eee18b_0    defaultslibstdcxx<span class="token operator">-</span>ng              <span class="token number">11.2</span><span class="token number">.0</span>               h1234567_1    defaultslibtiff                   <span class="token number">4.1</span><span class="token number">.0</span>                h2733197_1    defaultslibuv                     <span class="token number">1.44</span><span class="token number">.2</span>               h5eee18b_0    defaultslibwebp                   <span class="token number">1.2</span><span class="token number">.0</span>                h89dd481_0    defaultsllvmlite                  <span class="token number">0.36</span><span class="token number">.0</span>                   pypi_0    pypilz4<span class="token operator">-</span>c                     <span class="token number">1.9</span><span class="token number">.4</span>                h6a678d5_0    defaultsmarkdown                  <span class="token number">3.4</span><span class="token number">.4</span>                    pypi_0    pypimarkupsafe                <span class="token number">2.1</span><span class="token number">.3</span>                    pypi_0    pypimatplotlib                <span class="token number">3.5</span><span class="token number">.3</span>                    pypi_0    pypimkl                       <span class="token number">2021.4</span><span class="token number">.0</span>           h06a4308_640    defaultsmkl<span class="token operator">-</span>service               <span class="token number">2.4</span><span class="token number">.0</span>            py37h7f8727e_0    defaultsmkl_fft                   <span class="token number">1.3</span><span class="token number">.1</span>            py37hd3c417c_0    defaultsmkl_random                <span class="token number">1.2</span><span class="token number">.2</span>            py37h51133e4_0    defaultsmonoscene                 <span class="token number">0.0</span><span class="token number">.0</span>                     dev_0    <span class="token operator">&lt;</span>develop<span class="token operator">></span>multidict                 <span class="token number">6.0</span><span class="token number">.4</span>                    pypi_0    pypincurses                   <span class="token number">6.4</span>                  h6a678d5_0    defaultsnetworkx                  <span class="token number">2.6</span><span class="token number">.3</span>                    pypi_0    pypininja                     <span class="token number">1.10</span><span class="token number">.2</span>               h06a4308_5    defaultsninja<span class="token operator">-</span>base                <span class="token number">1.10</span><span class="token number">.2</span>               hd09550d_5    defaultsnumba                     <span class="token number">0.53</span><span class="token number">.0</span>                   pypi_0    pypinumpy                     <span class="token number">1.20</span><span class="token number">.3</span>                   pypi_0    pypinvidia<span class="token operator">-</span>cublas<span class="token operator">-</span>cu11        <span class="token number">11.10</span><span class="token number">.3</span><span class="token number">.66</span>               pypi_0    pypinvidia<span class="token operator">-</span>cuda<span class="token operator">-</span>nvrtc<span class="token operator">-</span>cu11    <span class="token number">11.7</span><span class="token number">.99</span>                  pypi_0    pypinvidia<span class="token operator">-</span>cuda<span class="token operator">-</span>runtime<span class="token operator">-</span>cu11  <span class="token number">11.7</span><span class="token number">.99</span>                  pypi_0    pypinvidia<span class="token operator">-</span>cudnn<span class="token operator">-</span>cu11         <span class="token number">8.5</span><span class="token number">.0</span><span class="token number">.96</span>                 pypi_0    pypioauthlib                  <span class="token number">3.2</span><span class="token number">.2</span>                    pypi_0    pypiomegaconf                 <span class="token number">2.0</span><span class="token number">.6</span>                    pypi_0    pypiopencv<span class="token operator">-</span>python             <span class="token number">4.5</span><span class="token number">.1</span><span class="token number">.48</span>                 pypi_0    pypiopenssl                   <span class="token number">1.1</span><span class="token punctuation">.</span>1w               h7f8727e_0    defaultspackaging                 <span class="token number">23.2</span>                     pypi_0    pypipillow                    <span class="token number">9.3</span><span class="token number">.0</span>            py37hace64e9_1    defaultspip                       <span class="token number">22.3</span><span class="token number">.1</span>           py37h06a4308_0    defaultsprotobuf                  <span class="token number">3.19</span><span class="token number">.6</span>                   pypi_0    pypipyasn1                    <span class="token number">0.5</span><span class="token number">.0</span>                    pypi_0    pypipyasn1<span class="token operator">-</span>modules            <span class="token number">0.3</span><span class="token number">.0</span>                    pypi_0    pypipydeprecate               <span class="token number">0.3</span><span class="token number">.1</span>                    pypi_0    pypipyparsing                 <span class="token number">3.1</span><span class="token number">.1</span>                    pypi_0    pypipython                    <span class="token number">3.7</span><span class="token number">.16</span>               h7a1cb2a_0    defaultspython<span class="token operator">-</span>dateutil           <span class="token number">2.8</span><span class="token number">.2</span>                    pypi_0    pypipytorch<span class="token operator">-</span>lightning         <span class="token number">1.4</span><span class="token number">.9</span>                    pypi_0    pypipywavelets                <span class="token number">1.3</span><span class="token number">.0</span>                    pypi_0    pypipyyaml                    <span class="token number">5.3</span><span class="token number">.1</span>                    pypi_0    pypireadline                  <span class="token number">8.2</span>                  h5eee18b_0    defaultsrequests                  <span class="token number">2.31</span><span class="token number">.0</span>                   pypi_0    pypirequests<span class="token operator">-</span>oauthlib         <span class="token number">1.3</span><span class="token number">.1</span>                    pypi_0    pypirsa                       <span class="token number">4.9</span>                      pypi_0    pypiscikit<span class="token operator">-</span>image              <span class="token number">0.18</span><span class="token number">.1</span>                   pypi_0    pypiscikit<span class="token operator">-</span>learn              <span class="token number">0.24</span><span class="token number">.0</span>                   pypi_0    pypiscipy                     <span class="token number">1.7</span><span class="token number">.3</span>                    pypi_0    pypisetuptools                <span class="token number">65.6</span><span class="token number">.3</span>           py37h06a4308_0    defaultssix                       <span class="token number">1.16</span><span class="token number">.0</span>             pyhd3eb1b0_1    defaultssqlite                    <span class="token number">3.41</span><span class="token number">.2</span>               h5eee18b_0    defaultstbb                       <span class="token number">2020.2</span>               hff7bd54_0    defaultstensorboard               <span class="token number">2.11</span><span class="token number">.2</span>                   pypi_0    pypitensorboard<span class="token operator">-</span>data<span class="token operator">-</span>server   <span class="token number">0.6</span><span class="token number">.1</span>                    pypi_0    pypitensorboard<span class="token operator">-</span>plugin<span class="token operator">-</span>wit    <span class="token number">1.8</span><span class="token number">.1</span>                    pypi_0    pypithreadpoolctl             <span class="token number">3.1</span><span class="token number">.0</span>                    pypi_0    pypitifffile                  <span class="token number">2021.11</span><span class="token number">.2</span>                pypi_0    pypitk                        <span class="token number">8.6</span><span class="token number">.12</span>               h1ccaba5_0    defaultstorch                     <span class="token number">1.13</span><span class="token number">.1</span>                   pypi_0    pypitorchaudio                <span class="token number">0.7</span><span class="token number">.2</span>                      py37    pytorchtorchmetrics              <span class="token number">0.6</span><span class="token number">.0</span>                    pypi_0    pypitorchvision               <span class="token number">0.8</span><span class="token number">.2</span>                py37_cu102    pytorchtqdm                      <span class="token number">4.49</span><span class="token number">.0</span>                   pypi_0    pypityping_extensions         <span class="token number">4.3</span><span class="token number">.0</span>            py37h06a4308_0    defaultsurllib3                   <span class="token number">2.0</span><span class="token number">.7</span>                    pypi_0    pypiwerkzeug                  <span class="token number">2.2</span><span class="token number">.3</span>                    pypi_0    pypiwheel                     <span class="token number">0.38</span><span class="token number">.4</span>           py37h06a4308_0    defaultsxz                        <span class="token number">5.4</span><span class="token number">.2</span>                h5eee18b_0    defaultsyarl                      <span class="token number">1.9</span><span class="token number">.2</span>                    pypi_0    pypizipp                      <span class="token number">3.15</span><span class="token number">.0</span>                   pypi_0    pypizlib                      <span class="token number">1.2</span><span class="token number">.13</span>               h5eee18b_0    defaultszstd                      <span class="token number">1.4</span><span class="token number">.9</span>                haebb681_0    defaults<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p><strong>需要注意的是这里使用了pytorch-lightning，版本对应安装</strong></p><p><a href="https://lightning.ai/docs/pytorch/latest/starter/introduction.html">官方文档连接</a></p><p><img src="/pic/a93ac42b74810deb39e5ea6dc4f5a66e.png"></p><h2 id="数据集使用SemanticKITTI以及NYUv2"><a href="#数据集使用SemanticKITTI以及NYUv2" class="headerlink" title="数据集使用SemanticKITTI以及NYUv2"></a>数据集使用SemanticKITTI以及NYUv2</h2><p><a href="http://www.semantic-kitti.org/dataset.html#download">Semantic Scene Completion dataset v1.1下载地址</a></p><p><a href="https://www.cvlibs.net/datasets/kitti/eval_odometry.php"> KITTI Odometry Benchmark calibration data (Download odometry data set (calibration files, 1 MB)) and the RGB images (Download odometry data set (color, 65 GB))</a></p><pre class="line-numbers language-python" data-language="python"><code class="language-python">$ export KITTI_PREPROCESS<span class="token operator">=</span><span class="token operator">/</span>home3<span class="token operator">/</span>dataset<span class="token operator">/</span>kitti<span class="token operator">/</span>preprocess$ export KITTI_ROOT<span class="token operator">=</span><span class="token operator">/</span>home3<span class="token operator">/</span>dataset<span class="token operator">/</span>kitti<span class="token operator">/</span>semantic_kitti$ export KITTI_LOG<span class="token operator">=</span><span class="token operator">/</span>home3<span class="token operator">/</span>chenhaiyang<span class="token operator">/</span>MonoScene<span class="token operator">/</span>monoscene<span class="token operator">/</span>data<span class="token operator">/</span>semantic_kitti<span class="token operator">/</span>logdir<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre><ul><li><p>KITTI_PREPROCESS保存的是未处理的数据</p></li><li><p>KITTI_ROOT保存的是处理后的数据</p></li><li><p>KITTI_LOG保存的是日志文件</p></li></ul><h3 id="kitti数据集标注解释"><a href="#kitti数据集标注解释" class="headerlink" title="kitti数据集标注解释"></a>kitti数据集标注解释</h3><p><a href="https://zhuanlan.zhihu.com/p/452672948">参考链接1</a></p><p><a href="https://blog.csdn.net/yangziluomu/article/details/78339575">参考链接2</a></p><pre class="line-numbers language-txt" data-language="txt"><code class="language-txt">1惯性导航系统（GPS / IMU）：OXTS RT 30031台激光雷达：Velodyne HDL-64E2台灰度相机，1.4百万像素：Point Grey Flea 2（FL2-14S3M-C）2个彩色摄像头，1.4百万像素：Point Grey Flea 2（FL2-14S3C-C）4个变焦镜头，4-8毫米：Edmund Optics NT59-917<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span></span></code></pre><p><img src="/pic/9dd8f665b331552caace5d45f362b48e.png"></p><h4 id="calib-txt-标定文件解读"><a href="#calib-txt-标定文件解读" class="headerlink" title="calib.txt 标定文件解读"></a>calib.txt 标定文件解读</h4><p>在calib文件中，有sequence 00-21序列，包括calib.txt 和 times.txt文件。<br>在sequence calib.txt 中，</p><pre class="line-numbers language-python" data-language="python"><code class="language-python">P0<span class="token punctuation">:</span> <span class="token number">7.188560000000e+02</span> <span class="token number">0.000000000000e+00</span> <span class="token number">6.071928000000e+02</span> <span class="token number">0.000000000000e+00</span> <span class="token number">0.000000000000e+00</span> <span class="token number">7.188560000000e+02</span> <span class="token number">1.852157000000e+02</span> <span class="token number">0.000000000000e+00</span> <span class="token number">0.000000000000e+00</span> <span class="token number">0.000000000000e+00</span> <span class="token number">1.000000000000e+00</span> <span class="token number">0.000000000000e+00</span>P1<span class="token punctuation">:</span> <span class="token number">7.188560000000e+02</span> <span class="token number">0.000000000000e+00</span> <span class="token number">6.071928000000e+02</span> <span class="token operator">-</span><span class="token number">3.861448000000e+02</span> <span class="token number">0.000000000000e+00</span> <span class="token number">7.188560000000e+02</span> <span class="token number">1.852157000000e+02</span> <span class="token number">0.000000000000e+00</span> <span class="token number">0.000000000000e+00</span> <span class="token number">0.000000000000e+00</span> <span class="token number">1.000000000000e+00</span> <span class="token number">0.000000000000e+00</span>P2<span class="token punctuation">:</span> <span class="token number">7.188560000000e+02</span> <span class="token number">0.000000000000e+00</span> <span class="token number">6.071928000000e+02</span> <span class="token number">4.538225000000e+01</span> <span class="token number">0.000000000000e+00</span> <span class="token number">7.188560000000e+02</span> <span class="token number">1.852157000000e+02</span> <span class="token operator">-</span><span class="token number">1.130887000000e-01</span> <span class="token number">0.000000000000e+00</span> <span class="token number">0.000000000000e+00</span> <span class="token number">1.000000000000e+00</span> <span class="token number">3.779761000000e-03</span>P3<span class="token punctuation">:</span> <span class="token number">7.188560000000e+02</span> <span class="token number">0.000000000000e+00</span> <span class="token number">6.071928000000e+02</span> <span class="token operator">-</span><span class="token number">3.372877000000e+02</span> <span class="token number">0.000000000000e+00</span> <span class="token number">7.188560000000e+02</span> <span class="token number">1.852157000000e+02</span> <span class="token number">2.369057000000e+00</span> <span class="token number">0.000000000000e+00</span> <span class="token number">0.000000000000e+00</span> <span class="token number">1.000000000000e+00</span> <span class="token number">4.915215000000e-03</span>Tr<span class="token punctuation">:</span> <span class="token number">4.276802385584e-04</span> <span class="token operator">-</span><span class="token number">9.999672484946e-01</span> <span class="token operator">-</span><span class="token number">8.084491683471e-03</span> <span class="token operator">-</span><span class="token number">1.198459927713e-02</span> <span class="token operator">-</span><span class="token number">7.210626507497e-03</span> <span class="token number">8.081198471645e-03</span> <span class="token operator">-</span><span class="token number">9.999413164504e-01</span> <span class="token operator">-</span><span class="token number">5.403984729748e-02</span> <span class="token number">9.999738645903e-01</span> <span class="token number">4.859485810390e-04</span> <span class="token operator">-</span><span class="token number">7.206933692422e-03</span> <span class="token operator">-</span><span class="token number">2.921968648686e-01</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>0,1,2,3 代表相机的编号，0表示左边灰度相机，1右边灰度相机，2左边彩色相机，3右边彩色相机。Tr表示将velodyne坐标系转换到左边相机系统坐标。</p><p>根据calib.txt相机投影矩阵可以得到相机内参。</p><p><img src="/pic/9b517ca2a52c3bcf686f3d2b598f234e.png"></p><p><img src="/pic/c7a26d5ae792b4f518aa79e835c820fa.png"></p><h2 id="网络训练"><a href="#网络训练" class="headerlink" title="网络训练"></a>网络训练</h2><pre class="line-numbers language-python" data-language="python"><code class="language-python">python monoscene<span class="token operator">/</span>scripts<span class="token operator">/</span>train_monoscene<span class="token punctuation">.</span>py \    dataset<span class="token operator">=</span>kitti \    enable_log<span class="token operator">=</span>true \    kitti_root<span class="token operator">=</span>$KITTI_ROOT \    kitti_preprocess_root<span class="token operator">=</span>$KITTI_PREPROCESS\    kitti_logdir<span class="token operator">=</span>$KITTI_LOG \    n_gpus<span class="token operator">=</span><span class="token number">3</span> batch_size<span class="token operator">=</span><span class="token number">3</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>服务器上的显存不够，训练不了</p><p>为了能够训练，可能采取的方法：</p><ul><li>更改2D解码器的特征尺寸</li><li>缩小输入图像：还需要调整投影矩阵</li><li>通过更改basemodel_name 和 num_features来使用较小的 2D 主干。</li><li>减少3D网络的特征维度</li><li>尝试先禁用或减小上下文的大小</li></ul><p><a href="https://github.com/astra-vision/MonoScene/issues/25">参考</a></p><h3 id="记录一下代码的构造"><a href="#记录一下代码的构造" class="headerlink" title="记录一下代码的构造"></a>记录一下代码的构造</h3><ul><li><p>2d unet结构的创建 编码器使用rwightman&#x2F;gen-efficientnet-pytorch下的tf_efficientnet_b7_ns作为backbone,解码器自己创建的上采样层</p></li><li><p>FLoSP结构将2d特征提升至三维的</p></li><li><p>3D UNets：2层 encoder-decoder 结构，用于提取 3d 特征</p></li><li><p>completion head：3D ASPP 结构和 softmax 层，用于处理 3D UNet 输出得到3d场景 completion 结果</p></li></ul><h4 id="介绍一下efficientnet"><a href="#介绍一下efficientnet" class="headerlink" title="介绍一下efficientnet"></a>介绍一下efficientnet</h4><p><a href="https://blog.csdn.net/qq_37541097/article/details/114434046">参考链接</a></p><p><img src="/pic/a221d02adcef4934cb9c8a3f909d0b52.png"></p><p>在之前的一些论文中，有的会通过增加网络的width即增加卷积核的个数（增加特征矩阵的channels）来提升网络的性能如图(b)所示，有的会通过增加网络的深度即使用更多的层结构来提升网络的性能如图(c)所示，有的会通过增加输入网络的分辨率来提升网络的性能如图(d)所示。而在本篇论文中会同时增加网络的width、网络的深度以及输入网络的分辨率来提升网络的性能如图(e)所示：</p><p><img src="/pic/a060b74b18a9239af2a8fb22663a2feb.png"></p><p><img src="/pic/2f6cb84d74f69886fa0d803d7702118a.png"></p><br/><p><strong>未完待续~</strong></p>]]></content>
      
      
      <categories>
          
          <category> DL </category>
          
      </categories>
      
      
        <tags>
            
            <tag> ssc </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>三维检测与占据预测PPT记录</title>
      <link href="/2023/11/16/3det-occ/"/>
      <url>/2023/11/16/3det-occ/</url>
      
        <content type="html"><![CDATA[<h1 id="三维检测与占据预测PPT记录"><a href="#三维检测与占据预测PPT记录" class="headerlink" title="三维检测与占据预测PPT记录"></a>三维检测与占据预测PPT记录</h1><p>主要对前期制作的三维目标检测和占据预测的PPT进行记录</p><blockquote><p>其中的三维目标检测主要从分类方法上展示不同的检测网络方法与结构，占据预测主要从BEV检测的两条主线开始讲起，再到特斯拉的占据预测网络，再到CVPR2023关于占据预测的模型，再到ICCV的占据预测的模型，以及最后的CVPR2023 占据预测比赛的前几名的解决方案。</p></blockquote><p>下面展示三维目标检测网络的PPT</p><iframe src="https://onedrive.live.com/embed?resid=E13F4D665106CFB4%212317&authkey=!AJirAp4km13Oq6Y&em=2" width="402" height="327" frameborder="0" scrolling="no"></iframe><p>接着展示占据预测的PPT</p><iframe src="https://onedrive.live.com/embed?resid=E13F4D665106CFB4%212316&authkey=!AIpr6wK938OwxXI&em=2" width="402" height="327" frameborder="0" scrolling="no"></iframe>]]></content>
      
      
      <categories>
          
          <category> DL </category>
          
      </categories>
      
      
        <tags>
            
            <tag> occ </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>PON:利用金字塔占用网络预测图像的语义地图表示</title>
      <link href="/2023/11/08/pon/"/>
      <url>/2023/11/08/pon/</url>
      
        <content type="html"><![CDATA[<h1 id="PON-Predicting-Semantic-Map-Representations-from-Images-using-Pyramid-Occupancy-Networks"><a href="#PON-Predicting-Semantic-Map-Representations-from-Images-using-Pyramid-Occupancy-Networks" class="headerlink" title="PON:Predicting Semantic Map Representations from Images using Pyramid Occupancy Networks"></a>PON:Predicting Semantic Map Representations from Images using Pyramid Occupancy Networks</h1><p>论文链接：<a href="https://arxiv.org/pdf/2003.13402.pdf">PON-pdf</a><br>代码链接：<a href="https://github.com/tom-roddick/mono-semantic-maps">PON-py</a></p><p>PON:利用金字塔占用网络预测图像的语义地图表示</p><p>提出了一个dense transformer（并非self attention的transformer， 只是MLP结构）的网络结构用于将2D图转换成BEV</p><p>我们的贡献如下:</p><ul><li>提出了一种新的密集变换层，它将基于图像的特征图映射到鸟瞰图空间。</li><li>设计了一个深度卷积神经网络架构，其中包括在多个图像尺度上运行的变压器金字塔，以从单眼图像预测准确的鸟瞰图。</li><li>我们在两个大规模自动驾驶数据集上评估我们的方法，并表明我们能够显着提高文献中领先作品的性能。<br>我们还定性地展示了如何使用贝叶斯语义占用网格框架来累积跨多个相机和时间步长的地图预测，以构建完整的场景模型。 该方法足够快，可用于实时应用程序，在单个 GeForce RTX 2080 Ti 显卡上每秒处理 23.2 帧</li></ul><p>一些参考链接</p><p><a href="https://blog.csdn.net/weixin_43889128/article/details/122301675?ops_request_misc=%257B%2522request%255Fid%2522%253A%2522169927967316800213076409%2522%252C%2522scm%2522%253A%252220140713.130102334..%2522%257D&request_id=169927967316800213076409&biz_id=0&utm_medium=distribute.pc_search_result.none-task-blog-2~all~sobaiduend~default-2-122301675-null-null.142%5Ev96%5Epc_search_result_base3&utm_term=Predicting%20Semantic%20Map%20Representations%20from%20Images%20using%20Pyramid%20Occupancy%20Networks&spm=1018.2226.3001.4187">翻译</a></p><p><a href="https://blog.csdn.net/Never__Say__No/article/details/120958739?ops_request_misc=%257B%2522request%255Fid%2522%253A%2522169927967316800213076409%2522%252C%2522scm%2522%253A%252220140713.130102334..%2522%257D&request_id=169927967316800213076409&biz_id=0&utm_medium=distribute.pc_search_result.none-task-blog-2~all~sobaiduend~default-4-120958739-null-null.142%5Ev96%5Epc_search_result_base3&utm_term=Predicting%20Semantic%20Map%20Representations%20from%20Images%20using%20Pyramid%20Occupancy%20Networks&spm=1018.2226.3001.4187">图像到BEV转换</a></p>]]></content>
      
      
      <categories>
          
          <category> DL </category>
          
          <category> bev </category>
          
      </categories>
      
      
        <tags>
            
            <tag> bev </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>语义场景补全（SSC）代码篇（二）</title>
      <link href="/2023/11/05/code-ssc2/"/>
      <url>/2023/11/05/code-ssc2/</url>
      
        <content type="html"><![CDATA[<h1 id="主要记录SSC中的理论与代码实现（第二章）"><a href="#主要记录SSC中的理论与代码实现（第二章）" class="headerlink" title="主要记录SSC中的理论与代码实现（第二章）"></a>主要记录SSC中的理论与代码实现（第二章）</h1><p>主要从开篇之作SSCNet（2017）开始讲起，然后按激光雷达点云输入（LMSCNet，2020；S3CNet，2021a；JS3C-Net，2021）,单双目以及两者融合的顺序</p><p>紧接上回，双目视觉的解决方案</p><h2 id="OccDepth"><a href="#OccDepth" class="headerlink" title="OccDepth"></a>OccDepth</h2><p>第一个只输入视觉的立体3D SSC方法。为了有效地利用深度信息，提出了一种立体软特征分配（Stereo-SFA）模块，通过隐式学习立体图像之间的相关性来更好地融合3D深度感知特征。占用感知深度（OAD）模块将知识提取与预先训练的深度模型明确地用于生成感知深度的3D特征。此外，为了更有效地验证立体输入场景，我们提出了基于TartanAir的SemanticTartanAir数据集。这是一个虚拟场景，可以使用真实的地面实况来更好地评估该方法的有效性。</p><p>代码链接如下: <a href="https://github.com/megvii-research/OccDepth">OccDepth: A Depth-aware Method for 3D Semantic Occupancy Network</a>, arXiv 2023.</p><p><img src="/pic/OccDepth.png"><br>3D SSC是通过桥接立体SFA模块来从立体图像中推断出来的，该模块用于将特征提升到3D空间，OAD模块用于增强深度预测，3D U-Net用于提取几何结构和语义。立体深度网络仅在训练中用于提供深度监督。</p><h3 id="代码介绍"><a href="#代码介绍" class="headerlink" title="代码介绍"></a>代码介绍</h3><p>代码方面类似monoscene，都使用了使用PyTorch Lightning，训练代码<a href="https://github.com/megvii-research/OccDepth/blob/main/occdepth/scripts/train.py">train.py</a>介绍如下：</p><ol><li><p><strong>导入模块</strong>: 脚本以各种导入语句开始，包括数据模块、模型、实用程序和配置。</p></li><li><p><strong>Hydra配置</strong>: <code>main</code> 函数被 <code>@hydra.main</code> 装饰，这表明可以使用Hydra进行配置，Hydra是一个强大的配置管理工具。配置路径根据 <code>config_path</code> 环境变量确定。</p></li><li><p><strong>主要函数</strong>: <code>main</code> 函数是脚本的入口点。它加载使用Hydra指定的配置。</p></li><li><p><strong>实验名称</strong>: 基于各种配置设置，创建了 <code>exp_name</code> 变量，用于命名实验。</p></li><li><p><strong>数据模块设置</strong>: 根据选择的数据集（例如 “kitti”、”NYU” 或 “tartanair”），实例化了相应的数据模块。数据模块处理数据加载和预处理。</p></li><li><p><strong>模型初始化</strong>: 创建了“OccDepth”模型的实例。模型接受各种参数，包括类别名称、类别权重和配置设置。</p></li><li><p><strong>日志记录和回调</strong>: 根据配置，脚本设置了使用TensorBoard的日志记录，并定义了各种回调，包括模型检查点和学习率监视器。</p></li><li><p><strong>从上次继续或从头开始训练</strong>: 脚本检查模型检查点文件（例如 “last.ckpt”）是否存在。如果存在，则继续从那个点训练。否则，从头开始训练模型。</p></li><li><p><strong>训练</strong>: 脚本初始化PyTorch Lightning Trainer，并调用 <code>fit</code> 方法，使用指定的数据模块来训练模型。训练过程由各种配置设置控制，包括GPU数量、梯度裁剪和其他超参数。</p></li><li><p><strong>随机种子初始化</strong>: 使用 <code>seed_everything</code> 设置随机种子。</p></li><li><p><strong>执行</strong>: 当运行脚本时，将调用主函数。</p></li></ol><p>总之，该脚本旨在训练”OccDepth”模型，可用于不同的数据集，并允许使用Hydra进行灵活的配置。它设置数据加载、模型和训练流程，并且如果有可用的检查点，可以从那个点继续训练。训练结果和日志将保存在配置中指定的目录中。</p><p><strong>接着看一下<a href="https://github.com/megvii-research/OccDepth/blob/main/occdepth/models/OccDepth.py">OccDepth.py</a>的模型</strong></p><p>“OccDepth” 是一个PyTorch模型，用于语义分割和深度预测的任务。这个模型主要由以下组件构成：</p><ol><li><p>**构造函数 <code>__init__</code>**：构造函数初始化模型的各种参数和组件。以下是一些重要的参数和配置项：</p><ul><li><code>class_names</code>：类别名称列表，用于分类问题。</li><li><code>class_weights</code>：类别权重，用于处理类别不平衡的情况。</li><li><code>class_weights_occ</code>：用于处理类别不平衡的另一组类别权重。</li><li><code>full_scene_size</code>：场景的完整尺寸。</li><li><code>project_res</code>：2D特征到3D特征的投影分辨率。</li><li><code>config</code>：模型的配置参数，包括超参数等。</li><li><code>infer_mode</code>：是否为推理模式，如果是，则不使用上下文先验（context prior）。</li></ul></li><li><p><strong>特征提取器</strong>：使用UNet结构进行特征提取。这些提取到的特征被用于语义分割。</p></li><li><p><strong>2D-3D投影层</strong>：这部分用于将2D特征映射到3D特征。包括两种投影方法：”flosp” 和 “flosp_depth”。</p><ul><li>“flosp” 模型对2D特征进行投影以生成3D场景特征。</li><li>“flosp_depth” 模型不仅对2D特征进行投影，还用深度信息进行额外的处理。</li></ul></li><li><p><strong>3D语义分割头部</strong>：这一部分负责将3D特征用于语义分割。具体的网络结构可能取决于不同的数据集（”NYU” 或 “kitti”）。</p></li><li><p><strong>2D语义分割头部</strong>：用于2D语义分割的头部网络。</p></li><li><p><strong>深度预测头部</strong>：根据需求，模型可以生成深度预测。</p></li><li><p><strong>训练、验证和测试步骤</strong>：这些步骤在不同的数据集和任务上运行，计算损失、评估性能，并记录指标。这包括分类损失、语义分割IoU、精度和召回。</p></li><li><p><strong>优化器和学习率调度器</strong>：在 <code>configure_optimizers</code> 方法中配置了优化器和学习率调度器。通常使用AdamW优化器和学习率衰减策略。</p></li><li><p><strong>模型的输入数据预处理</strong>：输入数据是图像，通过 <code>process_rgbs</code> 方法处理，生成特征图。</p></li><li><p><strong>模型的前向传播</strong>：通过 <code>forward</code> 方法执行模型的前向传播操作，包括特征提取、2D-3D投影、3D语义分割等。</p></li><li><p><strong>损失计算</strong>：根据任务类型和模型预测，计算不同的损失，包括分类损失、语义分割损失、深度损失等。</p></li><li><p><strong>评估指标</strong>：评估模型性能，包括IoU、精度、召回等。</p></li><li><p><strong>超参数配置</strong>：模型的超参数（如学习率、权重衰减等）在构造函数中进行了设置。</p></li><li><p><strong>导出模型和计算FLOPs和参数数量</strong>：通过条件选择，模型可以导出为ONNX格式，并计算模型的浮点运算数（FLOPs）和参数数量。</p></li></ol><p>总体而言，”OccDepth” 模型用于处理多视图数据的语义分割和深度预测任务，具有丰富的配置选项和评估指标，以满足不同数据集和任务的需求。这个模型的复杂性和功能强大，适用于一系列3D场景理解任务。</p><p><strong><a href="https://github.com/megvii-research/OccDepth/blob/main/occdepth/models/SFA.py">SFA.py</a>用于执行2D到3D的稀疏特征聚合。以下是该模块的功能和工作原理的简要说明：</strong></p><ul><li><p><code>SFA</code> 类继承自<code>nn.Module</code>，它包含了一个用于2D到3D投影的聚合过程。</p></li><li><p><code>__init__</code> 函数接受以下参数：</p><ul><li><code>scene_size</code>：3D场景的大小，通常表示为一个包含三个维度大小的元组或列表。</li><li><code>dataset</code>：表示使用的数据集的名称，例如”NYU”或”kitti”。</li><li><code>project_scale</code>：投影尺度，用于将2D特征映射到3D场景中。</li></ul></li><li><p><code>forward</code> 函数执行2D到3D的稀疏特征聚合操作：</p><ul><li>输入 <code>x2d</code> 是包含多个视图的2D特征的张量。</li><li><code>projected_pix</code> 是每个像素在3D场景中的投影坐标。</li><li><code>fov_mask</code> 是表示视野范围的掩码。</li></ul><p>  主要的工作步骤包括：</p><ol><li>对于每个视图，将2D特征映射到3D场景中。这是通过计算权重和聚合来实现的。</li><li>对不同视图之间的特征进行加权平均。</li><li>根据数据集类型（”NYU”或”kitti”）重塑3D场景特征的形状。</li></ol></li></ul><p>这个模块的关键思想是将多个2D视图的特征信息聚合到3D场景中，以增强3D场景的表示。这对于处理多视图或多帧输入的问题（如3D物体检测或重建）非常有用。模块的实现中包含了一系列的数学运算，包括加权平均和角度余弦相似度等。这有助于捕捉不同视图之间的相关性和信息融合。</p><p><a href="https://github.com/megvii-research/OccDepth/blob/main/occdepth/models/flosp_depth/flosp_depth.py">flosp_depth.py</a></p><p>这是一个名为 <code>FlospDepth</code> 的PyTorch模块，通常用于深度预测和点云处理。以下是该模块的功能和工作原理的简要说明：</p><ol><li><p>初始化和配置：</p><ul><li><code>__init__</code> 函数接受多个参数，包括场景边界、深度范围、输出通道数等，用于初始化模型的各个部分和配置。</li><li>可以选择不同的深度网络（<code>DepthNet</code>）配置。</li></ul></li><li><p>深度网络：</p><ul><li><code>DepthNet</code> 是一个用于预测深度图的子模块。</li><li>它接受输入特征图，相机内参矩阵等信息，并返回深度图。</li><li>深度图是在该模块中预测的，并且经过 softmax 处理。</li></ul></li><li><p>Voxel 特征聚合：</p><ul><li>从不同视角的深度图生成体素特征，这是通过采样视锥体积并将视锥体积中的深度信息投影到3D体素网格中实现的。</li><li>选择了不同的体素聚合方式，包括”mean”和”sum”，以聚合多个视角的信息。</li></ul></li><li><p>配置和参数：</p><ul><li>一些初始化参数，如体素大小、坐标、数量等，被存储在模块的缓冲区中，以便后续使用。</li><li>还有一些配置参数，如深度范围和体素网格大小等。</li></ul></li><li><p>推理模式和训练模式：</p><ul><li>模块支持两种模式，即推理模式和训练模式。</li><li>在推理模式中，模块可以接受缩放后的像素大小（<code>scaled_pixel_size</code>），以适应不同的尺度。</li><li>在训练模式中，模块接受相机内参矩阵、相机到世界坐标的变换矩阵等信息，用于生成体素网格。</li></ul></li></ol><p>这个模块的关键思想是从多个视角的深度图生成3D体素网格，然后通过合适的聚合方式将这些体素特征合并在一起。这对于许多3D场景理解任务，如点云分割、物体检测和语义分割，都是有用的。</p><h2 id="VoxFormer"><a href="#VoxFormer" class="headerlink" title="VoxFormer"></a>VoxFormer</h2><p>代码链接如下：<a href="https://github.com/NVlabs/VoxFormer">VoxFormer: a Cutting-edge Baseline for 3D Semantic Occupancy Prediction</a>, CVPR 2023</p><p>一种基于 Transformer 的语义场景完成框架，可以<strong>仅从 2D 图像输出完整的 3D 体积语义</strong>。我们的框架采用两阶段设计，从深度估计中一组稀疏的可见和占用体素查询开始，然后是从稀疏体素生成密集 3D 体素的致密化阶段。这种设计的一个关键思想是，2D 图像上的视觉特征仅对应于可见的场景结构，而不对应于被遮挡或空白的空间。因此，从可见结构的特征化和预测开始更为可靠。一旦我们获得了稀疏查询集，我们就应用屏蔽自动编码器设计，通过自注意力将信息传播到所有体素。</p><p><img src="/pic/VoxFormer.png"></p><p>VoxFormer的总体框架如上图所示。给定 RGB 图像，由 ResNet50 提取 2D 特征，并由现成的深度预测器估计深度。校正后的估计深度启用了与类别无关的查询建议阶段：将选择位于占用位置的查询来与图像特征进行可变形交叉注意。之后，将添加掩模标记以通过可变形自注意力来完成体素特征。精炼后的体素特征将被上采样并投影到输出空间以进行每体素语义分割。请注意，我们的框架支持单个或多个图像的输入。</p><p>VoxFormer由类不可知的查询建议（阶段-1）和类特定的语义分割（阶段-2）组成，其中阶段-1提出了一组稀疏的占用体素，阶段-2完成了从阶段-1给出的建议开始的场景表示。具体而言，阶段1具有轻量级的基于2D CNN的查询建议网络，该网络使用图像深度来重建场景几何结构。然后，它从整个视场上预定义的可学习体素查询中提出了一组稀疏的体素。</p><h3 id="代码分析"><a href="#代码分析" class="headerlink" title="代码分析"></a>代码分析</h3><p>第一阶段的查询建议网络(QPN)，根据深度决定要查询哪些体素:被占用的体素值得仔细关注，而空的体素可以从组中分离出来。给出二维RGB观测，首先基于深度估计得到场景的2.5D表示。然后，通过占用率预测获得三维查询位置，从而纠正图像深度不准确的问题。</p><p>代码<a href="https://github.com/NVlabs/VoxFormer/blob/main/projects/configs/voxformer/qpn.py">qpn.py</a>，基于mmdet3d</p><p>以下是对配置文件的更详细分析：</p><ol><li><p><code>_gamma_</code> 和 <code>_alpha_</code>：这些参数可能是用于模型的超参数调整，但配置文件中没有提供它们的具体用途。通常，<code>_gamma_</code> 和 <code>_alpha_</code> 可能会在训练过程中被动态地调整以提高模型性能。</p></li><li><p><code>_nsweep_</code>：指定了数据集中采样的扫描次数，这可能与数据集的采样策略和数据增强有关。在一些场景下，多次扫描可以提供更多的信息，例如在激光雷达数据中。</p></li><li><p><code>_depthmodel_</code>：这个参数指定了深度模型的名称或配置，定义了一个名为 MSNet3D 的三维立体匹配网络模型。这个模型主要用于深度估计和立体匹配任务。</p></li><li><p><code>point_cloud_range</code> 和 <code>voxel_size</code>：定义了点云的范围和体素的大小。这些参数用于将点云数据转化为体素表示，以便于模型的处理。<code>point_cloud_range</code> 指定了点云数据的范围，<code>voxel_size</code> 指定了体素的大小。</p></li><li><p><code>img_norm_cfg</code>：定义了图像的归一化配置，包括均值、标准差和是否将图像转化为RGB格式。这些配置通常用于对图像进行预处理，以便输入到深度学习模型中。</p></li><li><p><code>class_names</code>：包含目标类别的列表，用于指定模型需要检测或分类的物体类别。在这个配置中，包含了诸如’car’、’truck’、’pedestrian’等物体类别。</p></li><li><p><code>input_modality</code>：定义了数据输入模态，包括激光雷达、摄像机、雷达、地图和外部信息。这些模态可以用于训练和测试模型，根据任务需要选择合适的输入模态。</p></li><li><p><code>model</code>：指定了要训练的深度学习模型的配置。这个模型被命名为 <code>LMSCNet_SS</code>，并包括类别数、输入维度等信息。</p></li><li><p><code>train_cfg</code>：包含了训练配置，如点云的网格大小、体素大小、分配器配置等。这些配置参数影响了训练过程中的数据处理和损失计算。</p></li><li><p><code>dataset_type</code> 和 <code>data_root</code>：定义了数据集的类型和根目录，这将用于加载训练、验证和测试数据。</p></li><li><p><code>test_pipeline</code>：定义了测试数据的预处理管道，这里加载了多视角图像。预处理管道用于对输入数据进行处理以供模型使用。</p></li><li><p><code>data</code>：包含了数据集的设置，包括训练、验证和测试数据的类型、数据根目录、预处理根目录等。数据采样器也在这里配置。</p></li><li><p><code>optimizer</code> 和 <code>optimizer_config</code>：定义了优化器的类型、学习率、权重衰减等参数。<code>AdamW</code> 优化器用于模型的权重更新。</p></li><li><p><code>lr_config</code>：指定了学习率的调整策略，包括余弦退火、预热等。这些策略用于在训练过程中调整学习率。</p></li><li><p><code>total_epochs</code>：指定了总的训练轮数，模型将在这些轮数内进行训练。</p></li><li><p><code>evaluation</code>：定义了评估的间隔和评估的管道，用于在训练过程中定期评估模型的性能。</p></li><li><p><code>runner</code>：指定了训练器的类型和最大训练轮数，以及其他训练参数。</p></li><li><p><code>log_config</code>：包含了日志的配置，包括日志的记录间隔和日志类型。这些日志用于跟踪训练过程中的性能。</p></li><li><p><code>checkpoint_config</code>：定义了模型检查点的保存间隔，即模型在训练过程中的保存频率。</p></li></ol><p>分析一下_depthmodel_指定的MSNet3D和model指定的LMSCNet_SS模型</p><p>首先逐步分析代码<code>MSNet3D</code>的关键部分：</p><ol><li><p><code>hourglass3D</code> 类：这是一个用于定义三维卷积层的类，由多个 MobileV2_Residual_3D 模块组成。MobileV2_Residual_3D 模块用于构建深度神经网络的基本构建块。这个类定义了前向传播方法，通过堆叠卷积和反卷积层来构建一个”U”形网络。</p></li><li><p><code>MSNet3D</code> 类：这是主要的网络模型，包含了特征提取、特征匹配、立体匹配和深度估计等部分。它包括以下关键组件：</p><ul><li><code>feature_extraction</code>：特征提取模块。</li><li><code>dres0</code> 和 <code>dres1</code>：用于处理立体匹配代价体积的 MobileV2_Residual_3D 模块。</li><li><code>encoder_decoder1</code>、<code>encoder_decoder2</code> 和 <code>encoder_decoder3</code>：使用 <code>hourglass3D</code> 模块实现的编码器-解码器结构。</li><li><code>classif0</code>、<code>classif1</code>、<code>classif2</code> 和 <code>classif3</code>：用于执行立体匹配和深度估计的模块。</li></ul><p>在前向传播方法中，通过输入左视图和右视图的图像数据 <code>L</code> 和 <code>R</code>，首先提取特征，然后构建立体匹配代价体积。接下来，对代价体积进行多尺度编码器-解码器处理，最终产生深度估计。</p></li></ol><p>再好好分析一下<code>LMSCNet_SS</code>这个模型的主要组件和架构摘要：</p><ol><li><p>这个模型在<code>mmdet3d</code>框架中注册为自定义检测器。</p></li><li><p>它继承自<code>MVXTwoStageDetector</code>，这是<code>mmdet3d</code>框架中用于3D物体检测的基础类。</p></li><li><p>模型包括多个组件，包括编码器、解码器和分割头。它以3D占用网格数据作为输入，用于预测语义标签。</p></li><li><p>编码器通过多个2D卷积层处理输入数据，将特征图下采样到较小的尺寸。</p></li><li><p>解码器由一系列反卷积层组成，将特征图上采样到所需的输出比例。</p></li><li><p>分割头进一步处理特征图，以生成语义分割预测。</p></li><li><p>训练时计算损失，其中包括二元交叉熵损失（BCE）和其他特定于语义分割的损失。</p></li><li><p><code>forward</code> 方法用于处理训练和测试模式，训练时返回损失，测试时返回预测。</p></li><li><p>代码包括用于存储的二进制数据的打包和解包函数。</p></li><li><p>使用<code>auto_fp16</code>装饰器可以自动应用混合精度训练。</p></li></ol><p>第2阶段基于一种新颖的稀疏到密集类MAE架构，如图（a）所示。它首先通过允许所提出的体素关注图像观察来加强其特征化。接下来，未提出的体素将与可学习的掩码标记相关联，并且将通过自注意来处理全套体素，以完成每体素语义分割的场景表示。</p><p><img src="/pic/VoxFormer2.png"></p><p>请注意，这里提供了两个版本的VoxFormer，一个只以当前图像作为输入(<a href="https://github.com/NVlabs/VoxFormer/blob/main/projects/configs/voxformer/voxformer-S.py">VoxFormer- S</a>)，另一个以当前图像和前4个图像作为输入(<a href="https://github.com/NVlabs/VoxFormer/blob/main/projects/configs/voxformer/voxformer-T.py">VoxFormer- T</a>)。</p><p><strong>VoxFormer-S是一个用于3D物体检测的模型，该模型使用多通道LiDAR数据和图像数据进行训练。以下是对这个配置文件的主要部分进行的详细分析：</strong></p><ol><li><p><code>work_dir</code> 定义了模型训练期间的工作目录，训练日志和模型检查点将在此目录中保存。在这里，工作目录被设置为’result&#x2F;voxformer-S’。</p></li><li><p><code>_base_</code> 包含了其他配置文件的路径，这些文件会被合并到当前配置中。在这里，使用了默认的运行时配置文件’default_runtime.py’。</p></li><li><p><code>plugin</code> 和 <code>plugin_dir</code> 是用于配置MMDet3D插件的选项。<code>plugin</code> 设置为True，表示启用插件，而<code>plugin_dir</code> 指定了插件的目录。</p></li><li><p><code>_num_layers_cross_</code> 和 <code>_num_points_cross_</code> 分别定义了交叉变换器（cross_transformer）中的层数和点数。这些参数用于模型的 Transformer 层设置。</p></li><li><p><code>_dim_</code> 定义了模型的嵌入维度。在这里，它被设置为128。</p></li><li><p><code>_pos_dim_</code> 定义了位置编码的维度，通常是嵌入维度的一半。这里的值由_dim_除以2计算。</p></li><li><p><code>_ffn_dim_</code> 定义了Feedforward网络的维度，通常是嵌入维度的两倍。在这里，它被设置为256。</p></li><li><p><code>_num_levels_</code> 定义了输出级别的数量。这里设置为1，表示只有一个输出级别。</p></li><li><p><code>_labels_tag_</code> 和 <code>_num_cams_</code> 用于配置数据集标签和相机的数量。这些参数是与数据集相关的配置。</p></li><li><p><code>point_cloud_range</code> 和 <code>voxel_size</code> 分别定义了点云范围和体素大小。这些参数也是与数据集相关的配置。</p></li><li><p><code>_sem_scal_loss_</code> 和 <code>_geo_scal_loss_</code> 用于配置语义分割损失和几何损失的选项。这些参数控制是否启用语义分割损失和几何损失。</p></li><li><p><code>_depthmodel_</code> 是深度模型的选择。它在数据集中用于深度估计。</p></li><li><p><code>_nsweep_</code> 定义了查询建议网络的扫描次数。这是一个与数据集和模型训练相关的参数。</p></li><li><p><code>model</code> 部分定义了VoxFormer模型的结构，包括图像骨干网络、特征金字塔网络、以及交叉和自注意力变换器。模型结构的配置包括不同的组件和层，用于处理图像和点云数据。</p></li><li><p><code>dataset_type</code> 和 <code>data</code> 部分配置了数据集的类型、数据根目录以及训练、验证和测试数据集的设置。这些设置与数据集的加载和处理相关。</p></li><li><p><code>optimizer</code>、<code>optimizer_config</code> 和 <code>lr_config</code> 用于配置优化器、学习率衰减策略以及其他优化选项。这些设置用于模型训练的优化配置。</p></li><li><p><code>total_epochs</code> 定义了总的训练周期数。在这里，设置为20个周期。</p></li><li><p><code>evaluation</code> 部分配置了评估的间隔。这指定了多少个周期后进行一次模型评估。</p></li><li><p><code>runner</code> 部分定义了训练过程的运行器。这包括如何管理训练循环以及模型的保存和加载。</p></li><li><p><code>log_config</code> 部分配置了训练日志的记录方式。在这里，设置了文本记录和Tensorboard记录。</p></li><li><p><code>checkpoint_config</code> 用于配置模型检查点的保存间隔。在这里，设置为None，表示没有指定检查点的保存间隔。</p></li></ol><p><strong>VoxFormer- T一个用于3D物体检测的模型，结合了LiDAR点云数据和图像数据。以下是对这个配置文件的主要部分进行的详细分析：</strong></p><ol><li><p><code>work_dir</code> 定义了模型训练期间的工作目录，训练日志和模型检查点将在此目录中保存。在这里，工作目录被设置为’result&#x2F;voxformer-T’。</p></li><li><p><code>_base_</code> 包含了其他配置文件的路径，这些文件会被合并到当前配置中。在这里，使用了默认的运行时配置文件’default_runtime.py’。</p></li><li><p><code>plugin</code> 和 <code>plugin_dir</code> 是用于配置MMDet3D插件的选项。<code>plugin</code> 设置为True，表示启用插件，而<code>plugin_dir</code> 指定了插件的目录。</p></li><li><p><code>_num_layers_cross_</code> 和 <code>_num_points_cross_</code> 分别定义了交叉变换器（cross_transformer）中的层数和点数。这些参数用于模型的 Transformer 层设置。</p></li><li><p><code>_num_layers_self_</code> 和 <code>_num_points_self_</code> 分别定义了自注意力变换器（self_transformer）中的层数和点数。这些参数也用于模型的 Transformer 层设置。</p></li><li><p><code>_dim_</code> 定义了模型的嵌入维度。在这里，它被设置为128。</p></li><li><p><code>_pos_dim_</code> 定义了位置编码的维度，通常是嵌入维度的一半。这里的值由_dim_除以2计算。</p></li><li><p><code>_ffn_dim_</code> 定义了Feedforward网络的维度，通常是嵌入维度的两倍。在这里，它被设置为256。</p></li><li><p><code>_num_levels_</code> 定义了输出级别的数量。这里设置为1，表示只有一个输出级别。</p></li><li><p><code>_labels_tag_</code> 和 <code>_num_cams_</code> 用于配置数据集标签和相机的数量。这些参数是与数据集相关的配置。</p></li><li><p><code>_temporal_</code> 定义了用于数据集的时间轴信息。在这里，使用了一个包含-12、-9、-6和-3的列表，这表示了不同时间步长的输入数据。</p></li><li><p><code>point_cloud_range</code> 和 <code>voxel_size</code> 分别定义了点云范围和体素大小。这些参数也是与数据集相关的配置。</p></li><li><p><code>_sem_scal_loss_</code> 和 <code>_geo_scal_loss_</code> 用于配置语义分割损失和几何损失的选项。这些参数控制是否启用语义分割损失和几何损失。</p></li><li><p><code>_depthmodel_</code> 是深度模型的选择。它在数据集中用于深度估计。</p></li><li><p><code>_nsweep_</code> 定义了查询建议网络的扫描次数。这是一个与数据集和模型训练相关的参数。</p></li><li><p><code>model</code> 部分定义了VoxFormer-T模型的结构，包括图像骨干网络、特征金字塔网络、以及交叉和自注意力变换器。模型结构的配置包括不同的组件和层，用于处理图像和点云数据。</p></li><li><p><code>dataset_type</code> 和 <code>data</code> 部分配置了数据集的类型、数据根目录以及训练、验证和测试数据集的设置。这些设置与数据集的加载和处理相关。</p></li><li><p><code>optimizer</code>、<code>optimizer_config</code> 和 <code>lr_config</code> 用于配置优化器、学习率衰减策略以及其他优化选项。这些设置用于模型训练的优化配置。</p></li><li><p><code>total_epochs</code> 定义了总的训练周期数。在这里，设置为20个周期。</p></li><li><p><code>evaluation</code> 部分配置了评估的间隔。这指定了多少个周期后进行一次模型评估。</p></li><li><p><code>runner</code> 部分定义了训练过程的运行器。这包括如何管理训练循环以及模型的保存和加载。</p></li><li><p><code>log_config</code> 部分配置了训练日志的记录方式。在这里，设置了文本记录和Tensorboard记录。</p></li><li><p><code>checkpoint_config</code> 用于配置模型检查点的保存间隔。在这里，设置为None，表示没有指定检查点的保存间隔。</p></li></ol><p><strong>这两份代码之间的主要区别：</strong></p><ol><li><p><strong>时间轴差异</strong>:</p><ul><li><code>voxformer-S.py</code> 使用了一个空的时间轴，<code>_temporal_</code> 参数为空列表。</li><li><code>voxformer-T.py</code> 使用了包含多个时间步长的时间轴，<code>_temporal_</code> 参数设置为包含了[-12, -9, -6, -3]的列表。</li></ul></li><li><p><strong>相机数量差异</strong>:</p><ul><li><code>voxformer-S.py</code> 中的 <code>_num_cams_</code> 参数设置为1，表示只使用单个相机。</li><li><code>voxformer-T.py</code> 中的 <code>_num_cams_</code> 参数设置为5，表示使用5个相机。</li></ul></li><li><p><strong>输出级别的数量</strong>:</p><ul><li>两者中的 <code>model</code> 部分都设置了 <code>num_outs</code> 参数 <code>_num_levels_</code>，但两者都将其设置为1，表示只有一个输出级别。</li></ul></li><li><p>**<a href="https://github.com/zhangyp15/OccFormer">OccFormer: Dual-path Transformer for Vision-based 3D Semantic Occupancy Prediction</a>, ICCV 2023</p><ul><li><code>voxformer-S.py</code> 和 <code>voxformer-T.py</code> 都配置了相同的数据集类型 <code>dataset_type</code>，数据根目录 <code>data_root</code>，以及训练、验证和测试数据集的相关设置。</li></ul></li></ol><p>总的来说，主要的区别在于时间轴配置、相机数量以及数据集的数据加载。<code>voxformer-S.py</code> 使用了一个静态的时间轴和单个相机，而 <code>voxformer-T.py</code> 使用了多个时间步长和多个相机。这些差异可能是用于处理不同类型数据集或任务的配置。要选择合适的配置，需要考虑你的数据集和任务的特定需求。</p><h2 id="OccFormer"><a href="#OccFormer" class="headerlink" title="OccFormer"></a>OccFormer</h2><p>代码链接：<a href="https://github.com/zhangyp15/OccFormer">OccFormer: Dual-path Transformer for Vision-based 3D Semantic Occupancy Prediction</a>, ICCV 2023</p><p>自动驾驶的视觉感知经历了从鸟瞰图（BEV）表示到 3D 语义占用的转变。与BEV平面相比，3D语义占用进一步提供了沿垂直方向的结构信息。本文提出了 OccFormer，一种双路径transformer网络，可有效处理 3D 体积以进行语义占用预测。OccFormer 实现了对相机生成的 3D 体素特征的远程、动态且高效的编码。它是通过将繁重的 3D 处理分解为沿水平面的局部和全局transformer路径而获得的。对于占用解码器，我们通过提出保留池和类引导采样来适应 3D 语义占用的普通 Mask2Former，这显着减轻了稀疏性和类不平衡。实验结果表明，OccFormer 显着优于 SemanticKITTI 数据集上的语义场景完成和 nuScenes 数据集上的 LiDAR 语义分割的现有方法。</p><p><img src="/pic/OccFormer.png"></p><p>该Pipeline由用于提取多尺度2D特征的图像编码器、用于将2D特征提升到3D体积的图像到3D变换以及用于获得3D语义特征并预测3D语义占用的基于变换器的编码器-解码器组成。</p><h3 id="代码分析-1"><a href="#代码分析-1" class="headerlink" title="代码分析"></a>代码分析</h3><p><a href="https://github.com/zhangyp15/OccFormer/blob/main/projects/configs/occformer_kitti/occformer_kitti.py">occformer_kitti.py</a>这段代码是关于 <code>OccFormer</code> 模型的kitti数据集的配置，以下是对主要部分的详细分析：</p><ol><li><p><strong>基础配置</strong>:</p><ul><li><code>_base_</code> 参数包括两个配置文件，一个是 <code>custom_nus-3d.py</code>，另一个是 <code>default_runtime.py</code>。</li><li><code>sync_bn</code> 和 <code>plugin</code> 分别设置为 <code>True</code>，启用了同步批处理规范化（Batch Normalization）和插件（plugin）功能。</li></ul></li><li><p><strong>数据设置</strong>:</p><ul><li><code>class_names</code> 定义了类别的名称，包括一些物体类别（如汽车、自行车等）和一些特殊类别（如未标记、道路、人等），总共有20个类别。</li><li><code>point_cloud_range</code> 指定了点云的范围。</li><li><code>occ_size</code> 定义了3D体素的尺寸。</li><li><code>lss_downsample</code> 指定了生成3D体素（Local Semantic Segmentation, LSS）时的下采样比率。</li><li><code>voxel_size</code> 是通过计算 <code>point_cloud_range</code> 和 <code>occ_size</code> 得出的体素大小。</li></ul></li><li><p><strong>模型配置</strong>:</p><ul><li><code>model</code> 部分配置了 <code>OccFormer</code> 模型的架构，包括以下组件：<ul><li><code>img_backbone</code> 使用自定义的EfficientNet骨干网络。</li><li><code>img_neck</code> 使用SECONDFPN的颈部结构。</li><li><code>img_view_transformer</code> 使用 <code>ViewTransformerLiftSplatShootVoxel</code> 进行图像到点云的转换。</li><li><code>img_bev_encoder_backbone</code> 使用 <code>OccupancyEncoder</code> 进行BEV编码。</li><li><code>img_bev_encoder_neck</code> 使用多层Transformer解码器。</li><li><code>pts_bbox_head</code> 配置了 <code>Mask2FormerOccHead</code> 头部，用于形状转换、损失计算等。</li></ul></li></ul></li><li><p><strong>数据集配置</strong>:</p><ul><li>数据集类型 <code>dataset_type</code> 设置为 <code>CustomSemanticKITTILssDataset</code>。</li><li><code>data</code> 部分配置了数据加载和处理的设置，包括训练、验证和测试数据集。</li></ul></li><li><p><strong>优化器和学习率策略</strong>:</p><ul><li><code>optimizer</code> 部分配置了AdamW优化器，包括学习率、权重衰减等设置。</li><li><code>lr_config</code> 配置了学习率策略，采用阶段性的学习率衰减策略。</li></ul></li><li><p><strong>检查点和日志配置</strong>:</p><ul><li><code>checkpoint_config</code> 配置了检查点的保存策略。</li><li><code>runner</code> 部分定义了运行的类型和最大训练轮数。</li><li><code>evaluation</code> 部分配置了模型的评估策略。</li></ul></li></ol><p>总结，这段代码配置了 <code>OccFormer</code> 模型，包括图像和点云的处理、模型架构、数据加载、优化器、学习率策略和训练设置等。这个模型用于语义分割任务，能够将点云数据映射到3D体素空间，然后使用Transformer结构进行处理，最终输出语义分割结果。这是一个复杂的3D视觉模型，适用于点云数据的语义分割任务。</p><p><a href="https://github.com/zhangyp15/OccFormer/blob/main/projects/configs/occformer_nusc/occformer_nusc_r50_256x704.py">occformer_nusc_r50_256x704.py</a>这段代码是关于 <code>OccFormer</code> 模型的nuScenes数据集的配置，以下是对主要部分的详细分析：</p><ol><li><p><strong>基础配置</strong>:</p><ul><li><code>_base_</code> 参数包括两个配置文件：<code>custom_nus-3d.py</code> 和 <code>default_runtime.py</code>。</li><li><code>sync_bn</code> 和 <code>plugin</code> 分别设置为 <code>True</code>，启用了同步批处理规范化（Batch Normalization）和插件（plugin）功能。</li><li><code>plugin_dir</code> 指定插件的目录路径。</li><li><code>img_norm_cfg</code> 包含了图像的标准化配置，包括均值、标准差和是否转换为RGB格式。</li></ul></li><li><p><strong>类别和点云范围</strong>:</p><ul><li><code>class_names</code> 包含了物体类别的名称，共17个类别。</li><li><code>num_class</code> 表示类别的总数。</li><li><code>point_cloud_range</code> 定义了点云的范围。</li><li><code>occ_size</code> 指定了3D体素的尺寸。</li><li><code>lss_downsample</code> 是用于生成3D体素（Local Semantic Segmentation, LSS）时的下采样比率。</li><li><code>voxel_size</code> 是通过计算 <code>point_cloud_range</code> 和 <code>occ_size</code> 得出的体素大小。</li></ul></li><li><p><strong>数据配置</strong>:</p><ul><li><code>data_config</code> 包含了数据加载和处理的设置，包括相机信息、输入图像大小、数据增强参数等。</li><li><code>grid_config</code> 包含了3D卷积网格的配置，定义了xyz范围和体素大小等信息。</li></ul></li><li><p><strong>模型配置</strong>:</p><ul><li><code>model</code> 部分配置了 <code>OccupancyFormer</code> 模型，包括以下组件：<ul><li><code>img_backbone</code> 使用了ResNet-50骨干网络。</li><li><code>img_neck</code> 使用SECONDFPN的颈部结构。</li><li><code>img_view_transformer</code> 用于将图像信息转换为3D体素。</li><li><code>img_bev_encoder_backbone</code> 使用 <code>OccupancyEncoder</code> 进行BEV编码。</li><li><code>img_bev_encoder_neck</code> 使用多层Transformer解码器。</li><li><code>pts_bbox_head</code> 配置了 <code>Mask2FormerNuscOccHead</code> 头部，用于形状转换、损失计算等。</li></ul></li></ul></li><li><p><strong>数据集配置</strong>:</p><ul><li><code>dataset_type</code> 设置为 <code>CustomNuScenesOccLSSDataset</code>，指定了数据集类型。</li><li><code>data_root</code> 指定了数据集的根目录。</li><li><code>train_pipeline</code> 和 <code>test_pipeline</code> 分别配置了训练和测试数据的处理流程。</li></ul></li><li><p><strong>优化器和学习率策略</strong>:</p><ul><li><code>optimizer</code> 部分配置了AdamW优化器，包括学习率、权重衰减等设置。</li><li><code>optimizer_config</code> 包含了梯度剪切的设置。</li><li><code>lr_config</code> 配置了学习率策略，采用阶段性的学习率衰减策略。</li></ul></li><li><p><strong>检查点和日志配置</strong>:</p><ul><li><code>checkpoint_config</code> 配置了检查点的保存策略。</li><li><code>runner</code> 部分定义了运行的类型和最大训练轮数。</li><li><code>evaluation</code> 部分配置了模型评估的策略，包括评估间隔、评估指标等。</li></ul></li></ol><p><strong>这两段代码是针对不同的任务和数据集的配置文件，它们的主要区别在于以下几个方面：</strong></p><ol><li><p><strong>任务和数据集</strong>:</p><ul><li>第一个代码段是针对SemanticKITTI数据集的，主要用于3D语义分割任务。</li><li>第二个代码段是为了处理NuScenes数据集的，用于3D物体检测和语义分割任务。</li></ul></li><li><p><strong>类别和数据范围</strong>:</p><ul><li>第一个代码段中的 <code>class_names</code> 包含了SemanticKITTI数据集的类别，而第二个代码段的 <code>class_names</code> 包含了NuScenes数据集的类别，因此它们的类别列表是不同的。</li><li><code>point_cloud_range</code> 和 <code>occ_size</code> 也在两个代码段中有所不同，因为它们对应不同数据集的点云范围和3D体素尺寸。</li></ul></li><li><p><strong>数据处理和数据加载</strong>:</p><ul><li>两个代码段中的数据加载和处理流程是不同的。第一个代码段包含了用于加载SemanticKITTI数据的处理流程，而第二个代码段包含了用于加载NuScenes数据的处理流程。</li></ul></li><li><p><strong>模型配置</strong>:</p><ul><li>模型配置在两个代码段中也存在差异。虽然它们都使用了Transformer结构，但底层的图像骨干网络、特征提取过程、解码器结构等都可能有所不同。</li></ul></li><li><p><strong>数据路径和检查点路径</strong>:</p><ul><li>数据路径和检查点路径在两个配置中也不同。它们指定了数据集的根目录和用于预训练模型的检查点文件。</li></ul></li></ol><p>总的来说，这两段代码主要区别在于它们的应用领域、数据集和任务的不同。第一个代码段适用于SemanticKITTI数据集的3D语义分割任务，而第二个代码段则适用于NuScenes数据集的3D物体检测和语义分割任务。每个配置都经过仔细调整，以满足其特定的数据和任务需求。</p><h2 id="TPVFormer"><a href="#TPVFormer" class="headerlink" title="TPVFormer"></a>TPVFormer</h2><p>代码链接： <a href="https://github.com/wzzheng/TPVFormer">TPVFormer: An academic alternative to Tesla’s Occupancy Network</a>, CVPR2023</p><p>以视觉为中心的自动驾驶感知的现代方法广泛采用鸟瞰图（BEV）表示来描述 3D 场景。尽管它比体素表示效率更高，但它很难用单个平面描述场景的细粒度 3D 结构。为了解决这个问题，我们提出了一种三透视图（TPV）表示法，它与 BEV 一起提供了两个额外的垂直平面。我们通过对三个平面上的投影特征求和来对 3D 空间中的每个点进行建模。为了将图像特征提升到3D TPV空间，我们进一步提出了一种基于transformer的TPV编码器（TPVFormer）以有效地获得TPV特征。我们采用注意力机制来聚合每个 TPV 平面中每个查询对应的图像特征。实验表明，我们用稀疏监督训练的模型可以有效地预测所有体素的语义占用率。我们首次证明，在 nuScenes 上的 LiDAR 分割任务中，仅使用相机输入就可以实现与基于 LiDAR 的方法相当的性能。</p><p><img src="/pic/TPVFormer.png"></p><p>三维语义占用预测TPVFormer框架。我们采用一个图像骨干网来提取多摄像机图像的多尺度特征。然后通过交叉注意自适应提升二维特征到TPV空间，并利用交叉视图混合注意实现TPV平面之间的交互。为了预测三维空间中一个点的语义占用情况，我们在三个TPV平面上的投影特征的总和上应用一个轻量级预测头。</p><h3 id="代码分析-2"><a href="#代码分析-2" class="headerlink" title="代码分析"></a>代码分析</h3><p>这里我们只看TPVFormer 进行 3D 语义占用预测任务代码<a href="https://github.com/wzzheng/TPVFormer/blob/main/config/tpv04_occupancy.py">tpv04_occupancy.py</a></p><p>这段代码是一个配置文件，用于定义一个名为 “TPVFormer” 的模型，它的主要组成部分和参数如下：</p><ol><li><p><strong>基础配置</strong>:</p><ul><li><code>_base_</code> 中包含了用于数据集、优化器和学习率策略的配置文件路径。</li></ul></li><li><p><strong>数据集参数</strong>:</p><ul><li><code>dataset_params</code> 包含了数据集相关的参数，如数据版本、标签映射、数据空间的范围等。</li></ul></li><li><p><strong>模型类型</strong>:</p><ul><li><code>TPVFormer</code> 是所使用的模型类型。</li></ul></li><li><p><strong>TPV Aggregator</strong>:</p><ul><li><code>tpv_aggregator</code> 部分定义了一个名为 “TPVAggregator” 的模块，用于聚合TPV（Top-View Pillar）特征。</li><li>这个模块包括了输入和输出维度、TPV空间的尺寸、类别数等参数。</li></ul></li><li><p><strong>图像骨干网络</strong>:</p><ul><li><code>img_backbone</code> 定义了一个ResNet类型的图像骨干网络，用于从输入图像中提取特征。</li><li>这个网络的参数包括网络深度、输出特征层级、冻结的阶段、Batch Normalization等。</li></ul></li><li><p><strong>图像颈部（FPN）</strong>:</p><ul><li><code>img_neck</code> 定义了一个FPN类型的颈部网络，用于生成不同层级的特征金字塔。</li><li>这个网络将图像骨干网络的输出特征进行特征金字塔处理。</li></ul></li><li><p><strong>TPV Head</strong>:</p><ul><li><code>tpv_head</code> 定义了模型的头部，用于处理TPV特征。</li><li>这个部分包括了TPV空间的参数、特征维度、位置编码等。</li><li>它还包含了一个编码器，用于处理TPV特征。编码器中包括了多层的TPVFormerLayer，每一层包括了自注意力、跨视图注意力等组件。</li></ul></li></ol><p>这里我们主要看一下使用的<a href="https://github.com/wzzheng/TPVFormer/blob/main/tpvformer04/tpvformer.py">TPVFormer模型</a>的代码以及为 <a href="https://github.com/wzzheng/TPVFormer/blob/main/tpvformer04/tpv_aggregator.py">TPVAggregator</a>的模块，用于聚合TPV（Top-View Pillar）特征</p><p><strong>下面是对 TPVFormer 模型的代码的分析：</strong></p><ol><li><p>TPVFormer 继承了 <code>BaseModule</code>，这是一个基础的模型类，它是 mmcv（MMLab Computer Vision）和 mmseg（MMSegmentation）框架中的模型定义类。</p></li><li><p><code>__init__</code> 函数初始化 TPVFormer 模型，接受多个参数：</p><ul><li><code>use_grid_mask</code>: 控制是否使用 Grid Mask 数据增强。</li><li><code>img_backbone</code>: 图像骨干网络，用于提取图像特征。</li><li><code>img_neck</code>: 图像颈部网络，可选的，用于进一步处理图像特征。</li><li><code>tpv_head</code>: TPVFormer 头部网络，用于执行语义分割任务。</li><li><code>pretrained</code>: 预训练模型的配置。</li><li><code>tpv_aggregator</code>: 用于聚合 TPV 特征的头部网络。</li></ul></li><li><p>在 <code>__init__</code> 函数中，根据传入的参数初始化 TPVFormer 模型的各个组件，包括图像骨干网络、图像颈部网络、TPVFormer 头部网络以及 TPV 聚合器。如果提供了预训练模型，则将其配置传递给图像骨干网络。</p></li><li><p>TPVFormer 模型支持使用 Grid Mask 数据增强。Grid Mask 是一种数据增强方法，可以随机遮挡输入图像的一部分，从而增加模型的鲁棒性。</p></li><li><p><code>extract_img_feat</code> 函数用于提取图像特征，接受图像数据 <code>img</code> 作为输入。在函数内部，它首先对输入的图像进行形状变换，然后应用 Grid Mask 数据增强（如果启用），接着通过图像骨干网络提取图像特征，最后通过图像颈部网络进行进一步处理。</p></li><li><p><code>forward</code> 函数是 TPVFormer 模型的前向传播函数。它接受图像数据 <code>img</code> 和其他输入参数，调用 <code>extract_img_feat</code> 函数提取图像特征，然后将这些特征传递给 TPVFormer 头部网络 <code>tpv_head</code> 执行语义分割任务。最后，模型将输出结果传递给 TPV 聚合器 <code>tpv_aggregator</code> 进行进一步处理。</p></li></ol><p>总之，TPVFormer 是一个用于图像语义分割任务的模型，它包括图像骨干网络、图像颈部网络以及 TPVFormer 头部网络，可以处理输入的图像数据和点云数据，通过 TPV 聚合器聚合特征，以生成最终的语义分割结果。模型还支持 Grid Mask 数据增强以提高模型的鲁棒性。</p><p><strong>下面是对 TPVAggregator 模块的代码的分析：</strong></p><ol><li><p>TPVAggregator 继承了 <code>BaseModule</code>，这是 mmseg（MMSegmentation）框架中的头部模块类。</p></li><li><p><code>__init__</code> 函数初始化 TPVAggregator 模块，接受多个参数：</p><ul><li><code>tpv_h</code>, <code>tpv_w</code>, <code>tpv_z</code>: TPV 的高度、宽度和深度。</li><li><code>nbr_classes</code>: 类别数，即目标类别的数量。</li><li><code>in_dims</code>, <code>hidden_dims</code>, <code>out_dims</code>: 输入、隐藏和输出维度。</li><li><code>scale_h</code>, <code>scale_w</code>, <code>scale_z</code>: 高度、宽度和深度的缩放因子。</li><li><code>use_checkpoint</code>: 是否使用 PyTorch 的 checkpoint 功能。</li></ul></li><li><p>TPVAggregator 模块包括一个简单的神经网络，其中包含线性层（Linear）和 Softplus 激活函数用于从输入特征中提取特征，然后通过线性分类层进行目标分类。</p></li><li><p><code>forward</code> 函数用于执行前向传播操作，接受 TPV 特征列表 <code>tpv_list</code> 和点云数据 <code>points</code>（可选）。在函数内部，首先对 TPV 特征进行形状变换和插值操作，以匹配点云数据的尺寸和位置。然后，如果提供了点云数据，将点云数据映射到 TPV 特征上，执行点云与 TPV 特征的融合。最后，将融合后的特征传递给线性层和分类器进行分类，并返回分类结果。</p></li><li><p>如果未提供点云数据，则仅将 TPV 特征进行插值和融合，并执行分类。最终的分类结果以形状 <code>(batch_size, num_classes, scale_w * tpv_w, scale_h * tpv_h, scale_z * tpv_z)</code> 返回。</p></li></ol><p>总之，TPVAggregator 模块用于聚合 TPV 特征和点云数据，以执行语义分割任务。它包含线性层和分类器，用于将输入特征映射到目标类别的概率分布。这个模块的设计旨在结合点云数据和 TPV 特征，以提高语义分割的性能。</p>]]></content>
      
      
      <categories>
          
          <category> DL </category>
          
      </categories>
      
      
        <tags>
            
            <tag> ssc </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>OCC-VO:生成面向自动驾驶的基于3D占用栅格的视觉里程计稠密地图</title>
      <link href="/2023/11/04/occ-vo/"/>
      <url>/2023/11/04/occ-vo/</url>
      
        <content type="html"><![CDATA[<h1 id="OCC-VO-Dense-Mapping-via-3D-Occupancy-Based-Visual-Odometry-for-Autonomous-Driving"><a href="#OCC-VO-Dense-Mapping-via-3D-Occupancy-Based-Visual-Odometry-for-Autonomous-Driving" class="headerlink" title="OCC-VO: Dense Mapping via 3D Occupancy-Based Visual Odometry for Autonomous Driving"></a>OCC-VO: Dense Mapping via 3D Occupancy-Based Visual Odometry for Autonomous Driving</h1><p>论文链接：<a href="https://arxiv.org/pdf/2309.11011.pdf">OCC-VO-pdf</a><br>代码链接：<a href="https://github.com/USTCLH/OCC-VO">OCC-VO-py</a></p><p>生成面向自动驾驶的基于3D占用栅格的视觉里程计稠密地图</p><p>主要贡献：</p><ol><li><p><strong>OCC-VO框架的设计和开发：</strong> 作者设计并开发了OCC-VO框架，该框架接受环视相机图像作为输入，并生成密集语义地图，有助于增强场景理解，从而支持感知和导航等下游任务。</p></li><li><p><strong>3D语义占用预测模块：</strong> 在OCC-VO框架中，使用了名为TPV-Former的开源3D语义占用预测模块，用于将环视相机图像转化为3D语义占用栅格，实现对环境的语义理解。</p></li><li><p><strong>位姿估计和地图算法的定制：</strong> 为了解决3D语义占用地图的配准问题，作者设计了一种专为此任务定制的位姿估计和地图算法。该算法以GICP算法为基础，结合语义约束，以更好地对齐点云数据，特别适用于处理具有相似几何结构但不同语义的场景，如自动驾驶道路表面。</p></li><li><p><strong>全局语义地图的构建：</strong> 在地图创建阶段，借鉴了PFilter中的思想，消除了不可靠的点，从而创建了一个更稳健的全局语义地图。</p></li><li><p><strong>精准的地图和姿态估计：</strong> 最终的成果是经过精心调整的姿态估计和高度准确的地图，为感知和导航任务提供了可靠的基础。</p></li></ol><p>这些贡献共同构成了这项工作的核心，旨在提高环境理解和导航系统的性能，特别是在自动驾驶等领域中。</p><p>这段文本提供了关于OCC-VO框架的详细信息，包括作者、代码地址以及主要贡献。以下是以Markdown形式的总结：</p><h2 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h2><p>OCC-VO是一个新颖的框架，充分利用深度学习技术，将2D相机图像转化为3D语义占用，以解决自主系统中的视觉里程计（VO）挑战。它使用TPV-Former模块将环视相机图像转化为3D语义占用，经过特定设计的位姿估计和地图算法，包括语义标签滤波器、动态对象滤波器以及Voxel PFilter，生成密集的全局语义地图。在Occ3D-nuScenes数据集上的评估结果表明，OCC-VO相较于ORB-SLAM3取得了更高的成功率和轨迹准确性，成功率提高了20.6%，轨迹准确性提高了29.6%。</p><h2 id="主要贡献"><a href="#主要贡献" class="headerlink" title="主要贡献"></a>主要贡献</h2><ol><li><p>设计和开发OCC-VO框架，将环视相机图像转化为密集语义地图，用于增强场景理解和支持感知和导航任务。</p></li><li><p>使用TPV-Former模块，将环视相机图像转化为3D语义占用栅格。</p></li><li><p>定制位姿估计和地图算法，包括语义标签滤波器、动态对象滤波器，以及Voxel PFilter，以提高配准和全局地图一致性。</p></li><li><p>在Occ3D-nuScenes数据集上的评估，显示OCC-VO在自动驾驶场景中取得了较高的准确性和稳健性，尤其相较于ORB-SLAM3。</p></li></ol><p><img src="/pic/OCC-VO.png"></p><p>如上图所示，将同时被环绕视图摄像机捕获的6幅图像，由TPV-Former转换为3D语义占用。将得到的三维语义占用率作为点云，通过与全局语义图的配准估计出每一帧的姿态。具体来说，我们使用了GICP算法两次。首先，建立点之间的粗糙对应关系。在此之后，我们使用语义标签过滤器和动态对象过滤器来丢弃错误的匹配，从而细化第二个GICP应用程序的准确性。一旦确定了一个精确的姿势，我们利用Voxel PFilter将数据的框架合并到全局语义地图中，为全局一致性纠正TPV-Former对地图的推断中的错误。</p><h2 id="内容概述"><a href="#内容概述" class="headerlink" title="内容概述"></a>内容概述</h2><ul><li><p>系统概述：介绍了OCC-VO的工作流程，包括图像转化、姿态估计、过滤器应用，以及全局地图维护。</p></li><li><p>语义标签过滤器：介绍了基于语义标签的对象过滤方法，以提高配准准确性。有效地防止了各种物体或曲面之间的错误点匹配对优化解的影响。</p></li><li><p>动态对象过滤器：解释了如何处理动态对象，以平衡准确性和场景恶化。具体来说，使用语义标签将具有运动潜能的对象从三维语义占用和全局语义地图中分离出来。然后对每个对象进行点云聚类，这些聚类随后被视为统一的对象。利用第一个GICP的转换结果，我们比较了每个物体的位置。这让我们能够识别相对位移，<strong>并决定一个物体是否是动态的</strong>，是否应该被移除。利用每个输入的静态部分，进行了精细化配准，从而提高了姿态估计的精度。</p></li><li><p>Voxel PFilter：介绍了引入Voxel PFilter来维护全局地图的一致性，校正网络干扰噪音。</p></li><li><p>实验：使用Occ3D-nuScenes数据集评估OCC-VO，比较其性能和执行时间分析。</p></li></ul><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>OCC-VO是一个新颖的VO框架，结合3D语义占用栅格，用于自动驾驶场景中的密集地图生成。通过设计的滤波器，OCC-VO在自动驾驶环境中取得了更高的准确性和稳健性。未来的工作将包括将环路闭合检测等模块整合到OCC-VO中，以发展成为一个完整的SLAM系统。</p><p><strong><a href="https://mp.weixin.qq.com/s/q_YpZSS2mDoa2SNhUCAOTg">来源</a></strong></p><h2 id="详细阅读细节"><a href="#详细阅读细节" class="headerlink" title="详细阅读细节"></a>详细阅读细节</h2><p>挑战一：由于3D语义占用不同于场景结构的原始捕获，例如激光雷达扫描，因此出现了挑战。因此，使用这些数据来执行点云配准带来了几个问题。</p><ul><li>问题一：三维语义占用的粗分辨率导致地标位置估计的不确定性，进而影响配准的准确性。</li><li>问题二：由于神经网络模型的不完善，可能导致路标构造不准确甚至部分缺失</li><li>问题三：区分静止环境和动态对象非常重要，特别是在自动驾驶等应用中，因为它们的匹配可能导致姿态估计不太准确。</li></ul><p>解决一：</p><ul><li>方法一：设计了一种适合三维语义占用的姿态估计和映射算法。以Lidarbased SLAM中常用的GICP算法作为我们的基线。该算法通过特征匹配和迭代优化来实现点云的对齐。</li><li>方法二：在配准过程中引入语义约束，类似于ColorICP在几何结构相似但语义不同的情况下，例如自动驾驶环境中的路面，这些语义约束非常有效</li><li>方法三：实现了一个动态目标滤波器，以提高地图精度和姿态估计精度</li><li>方法四：在映射阶段，我们利用PFilter中的思想消除不可靠点，构建更健壮的全局语义映射。最终结果是一个微调的姿态估计和一个高度精确的地图</li></ul>]]></content>
      
      
      <categories>
          
          <category> DL </category>
          
          <category> slam </category>
          
      </categories>
      
      
        <tags>
            
            <tag> ssc </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>语义场景补全（SSC）代码篇（一）</title>
      <link href="/2023/11/03/code-ssc/"/>
      <url>/2023/11/03/code-ssc/</url>
      
        <content type="html"><![CDATA[<h1 id="主要记录SSC中的理论与代码实现（第一章）"><a href="#主要记录SSC中的理论与代码实现（第一章）" class="headerlink" title="主要记录SSC中的理论与代码实现（第一章）"></a>主要记录SSC中的理论与代码实现（第一章）</h1><p>主要从开篇之作SSCNet（2017）开始讲起，然后按激光雷达点云输入（LMSCNet，2020；S3CNet，2021a；JS3C-Net，2021）,单双目以及两者融合的顺序</p><h2 id="首先是开篇之作SSCNet"><a href="#首先是开篇之作SSCNet" class="headerlink" title="首先是开篇之作SSCNet"></a>首先是开篇之作SSCNet</h2><p>官方代码实在caffe框架上搭建的，第一个将语义分割和场景完成与 3D CNN 端到端结合的工作，从单视图深度地图观察生成一个完整的三维体素表示的体积占用和语义标签的场景，利用这两个任务的耦合特性，引入了语义场景完成网络(SSCNet)</p><p>代码链接： <a href="https://github.com/shurans/sscnet">SSCNet: Semantic Scene Completion from a Single Depth Image</a>, 2017</p><p><img src="/pic/SSCNet.png"></p><p>代码：<br>使用Caffe框架的网络定义txt文件,主要包括下面几个部分:</p><ul><li>数据层 (layer { name: “data” … })： 这是数据输入层，它从SUNCG数据集中加载数据。数据集包括深度信息（可能是三维数据）以及标签信息（seg_label）。还包括一些参数，如体素大小、裁剪大小、类别映射等。</li><li>卷积层 (layer { name: “conv1_1” … })： 这是一个卷积层，用于提取特征。它定义了卷积核的数量、大小、填充等参数。</li><li>ReLU 层 (layer { name: “relu1_1” … })： 这是激活函数层，通常用于引入非线性性。ReLU（Rectified Linear Unit）是一种常见的激活函数。</li><li>池化层 (layer { name: “pool2” … })： 这是一个池化层，用于降低特征图的分辨率，通常用于减少计算复杂度。</li><li>Eltwise 层 (layer { name: “res3_2” … })： 这是一个元素级操作层，通常用于残差网络（ResNet）中，用于连接不同层的输出。</li><li>全连接层 (layer { name: “fc12” … })： 这是一个全连接层，用于将卷积层的输出映射到最终的分类标签或预测。</li><li>SoftmaxWithLoss 层 (layer { name: “loss” … })： 这是损失层，通常与Softmax函数一起使用，用于计算损失函数，用于模型训练。</li></ul><h3 id="数据层配置"><a href="#数据层配置" class="headerlink" title="数据层配置"></a>数据层配置</h3><p>数据层在深度学习模型中是一个关键组成部分，用于加载和处理输入数据。在你提供的代码中，数据层的定义如下：</p><pre class="line-numbers language-protobuf" data-language="protobuf"><code class="language-protobuf">layer <span class="token punctuation">&#123;</span>  name<span class="token punctuation">:</span> <span class="token string">"data"</span>  type<span class="token punctuation">:</span> <span class="token string">"SuncgData"</span>  top<span class="token punctuation">:</span> <span class="token string">"data"</span>  top<span class="token punctuation">:</span> <span class="token string">"seg_label"</span>  top<span class="token punctuation">:</span> <span class="token string">"seg_weight"</span>  suncg_data_param <span class="token punctuation">&#123;</span>    file_data<span class="token punctuation">:</span> <span class="token string">"../../../../data/depthbin/SUNCGtrain_1_500"</span>    file_data<span class="token punctuation">:</span> <span class="token string">"../../../../data/depthbin/SUNCGtrain_501_1000"</span>    file_data<span class="token punctuation">:</span> <span class="token string">"../../../../data/depthbin/SUNCGtrain_1001_2000"</span>    file_data<span class="token punctuation">:</span> <span class="token string">"../../../../data/depthbin/SUNCGtrain_1001_3000"</span>    file_data<span class="token punctuation">:</span> <span class="token string">"../../../../data/depthbin/SUNCGtrain_3001_5000"</span>    file_data<span class="token punctuation">:</span> <span class="token string">"../../../../data/depthbin/SUNCGtrain_5001_7000"</span>    file_list<span class="token punctuation">:</span> <span class="token string">""</span>    vox_unit<span class="token punctuation">:</span> <span class="token number">0.02</span>    vox_margin<span class="token punctuation">:</span> <span class="token number">0.24</span>    vox_size<span class="token punctuation">:</span> <span class="token number">240</span>    vox_size<span class="token punctuation">:</span> <span class="token number">144</span>    vox_size<span class="token punctuation">:</span> <span class="token number">240</span>    crop_size<span class="token punctuation">:</span> <span class="token number">240</span>    crop_size<span class="token punctuation">:</span> <span class="token number">144</span>    crop_size<span class="token punctuation">:</span> <span class="token number">240</span>    label_size<span class="token punctuation">:</span> <span class="token number">60</span>    label_size<span class="token punctuation">:</span> <span class="token number">36</span>    label_size<span class="token punctuation">:</span> <span class="token number">60</span>    seg_classes<span class="token punctuation">:</span> <span class="token number">11</span>    shuffle<span class="token punctuation">:</span> <span class="token boolean">true</span>    occ_empty_only<span class="token punctuation">:</span> <span class="token boolean">true</span>    neg_obj_sample_ratio<span class="token punctuation">:</span> <span class="token number">2</span>    seg_class_map<span class="token punctuation">:</span> <span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span>    seg_class_weight<span class="token punctuation">:</span> <span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span>    occ_class_weight<span class="token punctuation">:</span> <span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span>    with_projection_tsdf<span class="token punctuation">:</span> <span class="token boolean">false</span>    batch_size<span class="token punctuation">:</span> <span class="token number">1</span>    tsdf_type<span class="token punctuation">:</span> <span class="token number">1</span>    data_type<span class="token punctuation">:</span> TSDF    surf_only<span class="token punctuation">:</span> <span class="token boolean">false</span>  <span class="token punctuation">&#125;</span><span class="token punctuation">&#125;</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>主要参数如下</p><ul><li><p><strong>name: “data”：</strong> 数据层的名称，用于在模型中引用该层。</p></li><li><p><strong>type: “SuncgData”：</strong> 数据层的类型，指定了如何处理数据。</p></li><li><p><strong>top: “data”, top: “seg_label”, top: “seg_weight”：</strong> 数据层的输出，模型中其他层可以引用这些输出。</p></li><li><p><strong>suncg_data_param：</strong> 数据层的参数配置块。</p></li><li><p><strong>file_data：</strong> 数据文件的路径，用于加载数据。这里看起来是从SUNCG数据集中加载深度信息。</p></li><li><p><strong>vox_unit, vox_margin, vox_size：</strong> 与体素（Voxel）的设置相关。<code>vox_unit</code>是体素的大小，<code>vox_margin</code>是体素的边缘大小，<code>vox_size</code>是体素在每个维度的数量。这些参数定义了体素网格的离散化空间。</p></li><li><p><strong>crop_size 和 label_size：</strong> 数据的裁剪大小和标签大小。<code>crop_size</code>表示输入数据的裁剪尺寸，<code>label_size</code>表示标签的大小。</p></li><li><p><strong>seg_classes：</strong> 指定了分类任务中的类别数量。</p></li><li><p><strong>shuffle：</strong> 一个布尔值，表示是否在加载数据时进行洗牌（打乱数据顺序）。</p></li><li><p><strong>occ_empty_only：</strong> 一个布尔值，可能表示只关注占用（occupied）和空闲（empty）之间的区别。</p></li><li><p><strong>neg_obj_sample_ratio：</strong> 负对象采样比例。</p></li><li><p><strong>seg_class_map, seg_class_weight, occ_class_weight：</strong> 用于定义不同类别的映射和权重。<code>seg_class_map</code>用于将一些类别映射到其他类别，<code>seg_class_weight</code>和<code>occ_class_weight</code>用于类别的权重设置。</p></li><li><p><strong>with_projection_tsdf 和 surf_only：</strong> 控制是否使用投影TSDF和是否仅考虑表面信息。投影TSDF通常用于三维重建，而<code>surf_only</code>可能用于指示仅关注表面的信息。</p></li><li><p><strong>batch_size：</strong> 训练时的批处理大小，影响每次模型权重更新时使用的样本数量。</p></li><li><p><strong>tsdf_type 和 data_type：</strong> 与数据类型相关的参数，例如TSDF数据类型。</p></li><li><p><strong>surf_only：</strong> 一个布尔值，可能表示是否仅考虑表面数据。</p></li></ul><p>这些参数共同定义了数据层的行为，确保数据被正确加载和预处理，以供深度学习模型使用。</p><h2 id="LMSCNet"><a href="#LMSCNet" class="headerlink" title="LMSCNet"></a>LMSCNet</h2><p>代码链接： <a href="https://github.com/astra-vision/LMSCNet">LMSCNet: Lightweight Multiscale 3D Semantic Completion</a>, 3DV 2020</p><p>一种从体素化稀疏三维激光雷达扫描中完成多尺度三维语义场景的新方法,使用具有全面多尺度跳跃连接的2D UNet主干来增强特征流，以及3D分割头</p><p><img src="/pic/LMSCNet.png"></p><p>首先看配置文件<a href="https://github.com/astra-vision/LMSCNet/blob/main/SSC_configs/examples/LMSCNet.yaml">LMSCNet.yaml</a>这段代码是一个用于训练深度学习模型的Python脚本，基于LMSCNet网络。以下是关于训练设置以及代码的关键部分的解释：</p><h3 id="训练设置"><a href="#训练设置" class="headerlink" title="训练设置"></a>训练设置</h3><ul><li><p><strong>数据加载器设置：</strong></p><ul><li><code>NUM_WORKERS: 4</code>：用于数据加载的工作线程数。</li></ul></li><li><p><strong>数据集设置：</strong></p><ul><li>数据增强：<ul><li><code>FLIPS: true</code>：表示是否进行数据增强，包括翻转等。</li></ul></li><li>模态设置：<ul><li><code>3D_LABEL: true</code>：表示是否使用3D标签数据。</li><li><code>3D_OCCLUDED: true</code>：表示是否使用3D遮挡数据。</li><li><code>3D_OCCUPANCY: true</code>：表示是否使用3D占用数据。</li></ul></li><li><code>ROOT_DIR: /datasets_local/datasets_lroldaoj/semantic_kitti_v1.0/</code>：数据集的根目录。</li><li><code>TYPE: SemanticKITTI</code>：数据集的类型，这里是SemanticKITTI。</li></ul></li><li><p><strong>模型设置：</strong></p><ul><li><code>TYPE: LMSCNet</code>：使用的模型类型为LMSCNet。</li></ul></li><li><p><strong>优化器设置：</strong></p><ul><li><code>BASE_LR: 0.001</code>：学习率的初始值。</li><li><code>BETA1: 0.9</code>：Adam优化器的beta1参数。</li><li><code>BETA2: 0.999</code>：Adam优化器的beta2参数。</li><li><code>MOMENTUM: NA</code>：动量参数（未设置）。</li><li><code>TYPE: Adam</code>：优化器的类型。</li><li><code>WEIGHT_DECAY: NA</code>：权重衰减参数（未设置）。</li></ul></li><li><p><strong>输出设置：</strong></p><ul><li><code>OUT_ROOT: ../SSC_out/</code>：输出结果的根目录。</li></ul></li><li><p><strong>调度器设置：</strong></p><ul><li><code>FREQUENCY: epoch</code>：学习率调度的频率为每个epoch。</li><li><code>LR_POWER: 0.98</code>：学习率调度的幂次方。</li><li><code>TYPE: power_iteration</code>：学习率调度类型。</li></ul></li><li><p><strong>状态设置：</strong></p><ul><li><code>RESUME: false</code>：是否从之前的检查点中恢复训练（未设置为恢复）。</li></ul></li><li><p><strong>训练设置：</strong></p><ul><li><code>BATCH_SIZE: 4</code>：每个批次的样本数量。</li><li><code>CHECKPOINT_PERIOD: 15</code>：保存检查点的周期。</li><li><code>EPOCHS: 80</code>：总共训练的轮数。</li><li><code>SUMMARY_PERIOD: 50</code>：汇总损失的周期。</li></ul></li><li><p><strong>验证设置：</strong></p><ul><li><code>BATCH_SIZE: 8</code>：验证时的批次大小。</li><li><code>SUMMARY_PERIOD: 20</code>：验证损失的周期。</li></ul></li></ul><h3 id="LMSCNet网络代码解释"><a href="#LMSCNet网络代码解释" class="headerlink" title="LMSCNet网络代码解释"></a>LMSCNet网络代码解释</h3><p><a href="https://github.com/astra-vision/LMSCNet/blob/main/LMSCNet/models/LMSCNet.py">LMSCNet.py</a></p><p>当分析这个神经网络模型时，我们可以将其分解为以下几个关键组件和部分。以下是对这些组件和部分的更详细解释：</p><h4 id="SegmentationHead（分割头部）"><a href="#SegmentationHead（分割头部）" class="headerlink" title="SegmentationHead（分割头部）"></a>SegmentationHead（分割头部）</h4><p><code>SegmentationHead</code> 类用于处理单一尺度的语义分割任务。它包括以下组件：</p><ul><li><p><strong>First Convolution (第一个卷积层)</strong>: <code>conv0</code> 定义了一个3D卷积层，用于将输入的特征从一个尺度（inplanes）转换到 <code>planes</code>。这是模型的初始特征处理步骤。</p></li><li><p><strong>ASPP Block (空间金字塔池化块)</strong>: <code>ASPP</code> 是”空间金字塔池化块”的缩写，它由多个卷积层和扩张卷积层（dilated convolution）组成。这些层用于捕捉不同感受野（receptive field）下的特征。这有助于模型更好地理解图像中的上下文信息。<code>conv1</code> 包括多个扩张卷积层，<code>bn1</code> 是相应的批归一化层，然后再通过 <code>conv2</code> 进行进一步处理。这些层通过 ReLU 激活函数进行激活。</p></li><li><p><strong>Convolution for Output (用于输出的卷积)</strong>: <code>conv_classes</code> 是用于生成最终语义分割预测的卷积层，其输出通道数等于目标类别数 <code>nbr_classes</code>。</p></li></ul><h4 id="LMSCNet（语义分割网络）"><a href="#LMSCNet（语义分割网络）" class="headerlink" title="LMSCNet（语义分割网络）"></a>LMSCNet（语义分割网络）</h4><p><code>LMSCNet</code> 类定义了整个语义分割网络，它的结构包括：</p><ul><li><p><strong>Encoder Blocks (编码块)</strong>: 这些块包括卷积层和激活函数，它们用于从输入数据提取特征。编码块分层堆叠，逐渐降低分辨率。其中 <code>Encoder_block1</code> 处理输入数据，然后 <code>Encoder_block2</code>、<code>Encoder_block3</code> 和 <code>Encoder_block4</code> 分别进行更多的特征提取。</p></li><li><p><strong>Output Scales (输出尺度)</strong>: 模型产生多个尺度的语义分割输出，包括1:8、1:4、1:2和1:1。每个输出尺度都有相应的卷积层和<code>SegmentationHead</code>。这些层用于生成不同分辨率下的语义分割预测。</p></li><li><p><strong>反卷积层 (Deconvolution Layers)</strong>: 这些层包括<code>deconv_1_8__1_2</code>、<code>deconv_1_8__1_1</code>、<code>deconv1_4</code>、<code>deconv1_2</code> 和 <code>deconv1_1</code>，用于上采样，将较低分辨率的输出转换为高分辨率。这些层用于与高分辨率的编码块特征进行连接。</p></li><li><p><strong>权重初始化和损失计算</strong>: 类中还包括了初始化权重的函数 <code>weights_initializer</code>，以及用于计算损失的函数 <code>compute_loss</code>。损失计算使用交叉熵损失，针对每个尺度的输出都会计算相应的损失。</p></li><li><p><strong>类别权重 (Class Weights)</strong>: 模型使用 <code>get_class_weights</code> 函数计算类别权重，以便处理不平衡的类别分布。</p></li><li><p><strong>其他辅助函数</strong>: 类还包括其他用于获取目标数据、设置训练尺度等的辅助函数。</p></li></ul><p>这个模型的核心思想是从低分辨率到高分辨率逐步提取特征，并生成多尺度的语义分割预测。模型的损失函数考虑了所有尺度的预测，以便综合多尺度信息来进行训练。这有助于提高语义分割任务的性能，特别是在处理不同尺度的对象时。模型的训练和验证过程通常需要提供训练数据、优化器和学习率调度器等组件。</p><h2 id="JS3CNet"><a href="#JS3CNet" class="headerlink" title="JS3CNet"></a>JS3CNet</h2><p>一种增强的联合单扫LiDAR点云语义分割方法，该方法利用学习的形状先验形式场景完成网络，即JS3C-Net</p><p>代码链接： <a href="https://github.com/yanx27/JS3C-Net">JS3CNet: Sparse Single Sweep LiDAR Point Cloud Segmentation via Learning Contextual Shape Priors from Scene Completion</a>, AAAI 2021</p><p><img src="/pic/JS3CNet.png" alt="在给定稀疏不完全单扫描点云的情况下，首先使用稀疏卷积U-Net对Fenc进行点特征编码。基于初始编码，MLP1用于生成形状嵌入（SE）FSE，该FSE与通过MLP2传递的初始编码一起流入MLP3，以生成用于点云语义分割的Fout。然后，来自SE的不完整细粒度点特征和来自语义场景完成（SSC）模块的完整体素特征流入点-体素交互（PVI）模块，以实现细化特征，最终在监督下输出完成体素。请注意，SSC和PVI模块可以在推理过程中丢弃。"></p><h3 id="代码分析"><a href="#代码分析" class="headerlink" title="代码分析"></a>代码分析</h3><p><a href="https://github.com/yanx27/JS3C-Net/blob/main/opt/JS3C_default_kitti.yaml">JS3C_default_kitti.yaml</a>这是一个JS3CNet网络的配置文件。配置文件用于定义训练和测试JS3CNet网络时的各种参数和设置。以下是关于配置文件的详细解释：</p><pre class="line-numbers language-yaml" data-language="yaml"><code class="language-yaml"><span class="token key atrule">GENERAL</span><span class="token punctuation">:</span>  <span class="token key atrule">task</span><span class="token punctuation">:</span> train  <span class="token key atrule">manual_seed</span><span class="token punctuation">:</span> <span class="token number">123</span>  <span class="token key atrule">dataset_dir</span><span class="token punctuation">:</span> /home/yxu/data/semantic_kitti/dataset/  <span class="token key atrule">debug</span><span class="token punctuation">:</span> <span class="token boolean important">False</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span></span></code></pre><ul><li><code>task</code>: 定义任务类型，这里设置为”train”表示进行训练。</li><li><code>manual_seed</code>: 随机数种子，用于确保实验的可复现性。</li><li><code>dataset_dir</code>: 数据集的目录路径。</li><li><code>debug</code>: 是否启用调试模式，设置为”False”表示不启用。</li></ul><pre class="line-numbers language-yaml" data-language="yaml"><code class="language-yaml"><span class="token key atrule">DATA</span><span class="token punctuation">:</span>  <span class="token key atrule">dataset</span><span class="token punctuation">:</span> SemanticKITTI  <span class="token key atrule">classes_seg</span><span class="token punctuation">:</span> <span class="token number">19</span>  <span class="token key atrule">classes_completion</span><span class="token punctuation">:</span> <span class="token number">20</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span></span></code></pre><ul><li><code>dataset</code>: 数据集的名称，这里使用SemanticKITTI数据集。</li><li><code>classes_seg</code>: 语义分割任务的类别数，这里设置为19。</li><li><code>classes_completion</code>: 完备性任务的类别数，这里设置为20。</li></ul><pre class="line-numbers language-yaml" data-language="yaml"><code class="language-yaml"><span class="token key atrule">Segmentation</span><span class="token punctuation">:</span>  <span class="token key atrule">model_name</span><span class="token punctuation">:</span> SubSparseConv  <span class="token key atrule">m</span><span class="token punctuation">:</span> <span class="token number">16</span>  <span class="token key atrule">block_residual</span><span class="token punctuation">:</span> <span class="token boolean important">False</span>  <span class="token key atrule">block_reps</span><span class="token punctuation">:</span> <span class="token number">1</span>  <span class="token key atrule">seg_groups</span><span class="token punctuation">:</span> <span class="token number">1</span>  <span class="token key atrule">use_coords</span><span class="token punctuation">:</span> <span class="token boolean important">False</span>  <span class="token key atrule">feature_dims</span><span class="token punctuation">:</span> <span class="token punctuation">[</span><span class="token number">16</span><span class="token punctuation">,</span><span class="token number">32</span><span class="token punctuation">,</span><span class="token number">48</span><span class="token punctuation">,</span><span class="token number">64</span><span class="token punctuation">,</span><span class="token number">80</span><span class="token punctuation">,</span><span class="token number">96</span><span class="token punctuation">,</span><span class="token number">112</span><span class="token punctuation">]</span>  <span class="token key atrule">input_channel</span><span class="token punctuation">:</span> <span class="token number">3</span>  <span class="token key atrule">scale</span><span class="token punctuation">:</span> <span class="token number">10</span>  <span class="token key atrule">full_scale</span><span class="token punctuation">:</span> <span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">2048</span><span class="token punctuation">]</span>  <span class="token key atrule">max_npoint</span><span class="token punctuation">:</span> <span class="token number">250000</span>  <span class="token key atrule">mode</span><span class="token punctuation">:</span> <span class="token number">4</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><ul><li><code>Segmentation</code>部分包含了与语义分割任务相关的参数设置。</li><li><code>model_name</code>: 语义分割模型的名称，这里使用SubSparseConv。</li><li><code>m</code>: SubSparseConv模型的参数，这里设置为16。</li><li><code>block_residual</code>: 是否使用块残差，设置为”False”表示不使用。</li><li><code>block_reps</code>: 块的重复次数，这里设置为1。</li><li><code>seg_groups</code>: 分割组的数量，这里设置为1。</li><li><code>use_coords</code>: 是否使用坐标信息，设置为”False”表示不使用。</li><li><code>feature_dims</code>: 特征维度的设置。</li><li><code>input_channel</code>: 输入通道的数量，这里设置为3。</li><li><code>scale</code>: 数据集的尺度。</li><li><code>full_scale</code>: 数据集的完全尺度范围。</li><li><code>max_npoint</code>: 最大点数，这里设置为250,000。</li><li><code>mode</code>: 模式设置，这里设置为4。</li></ul><pre class="line-numbers language-yaml" data-language="yaml"><code class="language-yaml"><span class="token key atrule">Completion</span><span class="token punctuation">:</span>  <span class="token key atrule">model_name</span><span class="token punctuation">:</span> SSCNet  <span class="token key atrule">m</span><span class="token punctuation">:</span> <span class="token number">32</span>  <span class="token key atrule">feeding</span><span class="token punctuation">:</span> both  <span class="token key atrule">no_fuse_feat</span><span class="token punctuation">:</span> <span class="token boolean important">False</span>  <span class="token key atrule">block_residual</span><span class="token punctuation">:</span> <span class="token boolean important">True</span>  <span class="token key atrule">block_reps</span><span class="token punctuation">:</span> <span class="token number">2</span>  <span class="token key atrule">use_coords</span><span class="token punctuation">:</span> <span class="token boolean important">False</span>  <span class="token key atrule">mode</span><span class="token punctuation">:</span> <span class="token number">0</span>  <span class="token key atrule">full_scale</span><span class="token punctuation">:</span> <span class="token punctuation">[</span><span class="token number">256</span><span class="token punctuation">,</span> <span class="token number">256</span><span class="token punctuation">,</span> <span class="token number">32</span><span class="token punctuation">]</span>  <span class="token key atrule">interaction</span><span class="token punctuation">:</span> <span class="token boolean important">True</span>  <span class="token key atrule">pooling_type</span><span class="token punctuation">:</span> mean  <span class="token key atrule">fuse_k</span><span class="token punctuation">:</span> <span class="token number">5</span>  <span class="token key atrule">point_cloud_range</span><span class="token punctuation">:</span> <span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">-25.6</span><span class="token punctuation">,</span> <span class="token number">-2</span><span class="token punctuation">,</span> <span class="token number">51.2</span><span class="token punctuation">,</span> <span class="token number">25.6</span><span class="token punctuation">,</span> <span class="token number">4.4</span><span class="token punctuation">]</span>  <span class="token key atrule">voxel_size</span><span class="token punctuation">:</span> <span class="token number">0.2</span>  <span class="token key atrule">search_k</span><span class="token punctuation">:</span> <span class="token number">8</span>  <span class="token key atrule">feat_relation</span><span class="token punctuation">:</span> <span class="token boolean important">False</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><ul><li><code>Completion</code>部分包含了与完备性任务相关的参数设置。</li><li><code>model_name</code>: 完备性模型的名称，这里使用SSCNet。</li><li><code>m</code>: SSCNet模型的参数，这里设置为32。</li><li><code>feeding</code>: 输入数据的类型，这里设置为”both”，表示同时输入特征和概率。</li><li><code>no_fuse_feat</code>: 是否不融合特征，设置为”False”表示融合特征。</li><li><code>block_residual</code>: 是否使用块残差，设置为”True”表示使用。</li><li><code>block_reps</code>: 块的重复次数，这里设置为2。</li><li><code>use_coords</code>: 是否使用坐标信息，设置为”False”表示不使用。</li><li><code>mode</code>: 模式设置，这里设置为0。</li><li><code>full_scale</code>: 完全尺度范围的设置。</li><li><code>interaction</code>: 是否启用交互，设置为”True”表示启用。</li><li><code>pooling_type</code>: 池化类型，这里设置为”mean”。</li><li><code>fuse_k</code>: 融合参数k的设置。</li><li><code>point_cloud_range</code>: 点云范围的设置。</li><li><code>voxel_size</code>: 体素大小的设置。</li><li><code>search_k</code>: 搜索参数k的设置。</li><li><code>feat_relation</code>: 特征关系的设置，这里设置为”False”。</li></ul><pre class="line-numbers language-yaml" data-language="yaml"><code class="language-yaml"><span class="token key atrule">TRAIN</span><span class="token punctuation">:</span>  <span class="token key atrule">epochs</span><span class="token punctuation">:</span> <span class="token number">100</span>  <span class="token key atrule">train_workers</span><span class="token punctuation">:</span> <span class="token number">10</span>  <span class="token key atrule">optim</span><span class="token punctuation">:</span> Adam  <span class="token key atrule">batch_size</span><span class="token punctuation">:</span> <span class="token number">2</span>  <span class="token key atrule">learning_rate</span><span class="token punctuation">:</span> <span class="token number">0.001</span>  <span class="token key atrule">lr_decay</span><span class="token punctuation">:</span> <span class="token number">0.7</span>  <span class="token key atrule">decay_step</span><span class="token punctuation">:</span> <span class="token number">10</span>  <span class="token key atrule">momentum</span><span class="token punctuation">:</span> <span class="token number">0.9</span>  <span class="token key atrule">weight_decay</span><span class="token punctuation">:</span> <span class="token number">0.0001</span>  <span class="token key atrule">save_freq</span><span class="token punctuation">:</span> <span class="token number">16</span>  <span class="token key atrule">uncertainty_loss</span><span class="token punctuation">:</span> <span class="token boolean important">True</span>  <span class="token key atrule">loss_weight</span><span class="token punctuation">:</span> <span class="token punctuation">[</span><span class="token number">1.0</span><span class="token punctuation">,</span> <span class="token number">0.8</span><span class="token punctuation">]</span>  <span class="token key atrule">pretrain_path</span><span class="token punctuation">:</span>  <span class="token key atrule">train_from</span><span class="token punctuation">:</span> <span class="token number">0</span>  <span class="token key atrule">seg_num_per_class</span><span class="token punctuation">:</span> <span class="token punctuation">[</span><span class="token number">55437630</span><span class="token punctuation">,</span> <span class="token number">320797</span><span class="token punctuation">,</span> <span class="token number">541736</span><span class="token punctuation">,</span> <span class="token number">2578735</span><span class="token punctuation">,</span> <span class="token number">3274484</span><span class="token punctuation">,</span> <span class="token number">552662</span><span class="token punctuation">,</span> <span class="token number">184064</span><span class="token punctuation">,</span> <span class="token number">78858</span><span class="token punctuation">,</span> <span class="token number">240942562</span><span class="token punctuation">,</span> <span class="token number">17294618</span><span class="token punctuation">,</span> <span class="token number">170599734</span><span class="token punctuation">,</span> <span class="token number">6369672</span><span class="token punctuation">,</span> <span class="token number">230413074</span><span class="token punctuation">,</span> <span class="token number">101130274</span><span class="token punctuation">,</span> <span class="token number">476491114</span><span class="token punctuation">,</span> <span class="token number">9833174</span><span class="token punctuation">,</span> <span class="token number">129609852</span><span class="token punctuation">,</span> <span class="token number">4506626</span><span class="token punctuation">,</span> <span class="token number">1168181</span><span class="token punctuation">]</span>  <span class="token key atrule">complt_num_per_class</span><span class="token punctuation">:</span> <span class="token punctuation">[</span><span class="token number">7632350044</span><span class="token punctuation">,</span> <span class="token number">15783539</span><span class="token punctuation">,</span>  <span class="token number">125136</span><span class="token punctuation">,</span> <span class="token number">118809</span><span class="token punctuation">,</span> <span class="token number">646799</span><span class="token punctuation">,</span> <span class="token number">821951</span><span class="token punctuation">,</span> <span class="token number">262978</span><span class="token punctuation">,</span> <span class="token number">283696</span><span class="token punctuation">,</span> <span class="token number">204750</span><span class="token punctuation">,</span> <span class="token number">61688703</span><span class="token punctuation">,</span> <span class="token number">4502961</span><span class="token punctuation">,</span> <span class="token number">44883650</span><span class="token punctuation">,</span> <span class="token number">2269923</span><span class="token punctuation">,</span> <span class="token number">56840218</span><span class="token punctuation">,</span> <span class="token number">15719652</span><span class="token punctuation">,</span> <span class="token number">158442623</span><span class="token punctuation">,</span> <span class="token number">2061623</span><span class="token punctuation">,</span> <span class="token number">36970522</span><span class="token punctuation">,</span> <span class="token number">1151988</span><span class="token punctuation">,</span> <span class="token number">334146</span><span class="token punctuation">]</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><ul><li><code>TRAIN</code>部分包含了训练相关的参数设置。</li><li><code>epochs</code>: 训练的轮数，这里设置为100。</li><li><code>train_workers</code>: 数据加载器的工作进程数，这里设置为10。</li><li><code>optim</code>: 优化器的选择，这里使用Adam。</li><li><code>batch_size</code>: 批量大小，这里设置为2。</li><li><code>learning_rate</code>: 学习率，这里设置为0.001。</li><li><code>lr_decay</code>: 学习率的衰减率，这里设置为0.7。</li><li><code>decay_step</code>: 学习率衰减的步数，这里设置为10。</li><li><code>momentum</code>: 优化器的动量，这里设置为0.9。</li><li><code>weight_decay</code>: 权重衰减，这里设置为0.0001。</li><li><code>save_freq</code>: 模型保存的频率，这里设置为16。</li><li><code>uncertainty_loss</code>: 是否启用不确定性损失，设置为”True”表示启用。</li><li><code>loss_weight</code>: 损失的权重，这里是一个列表，包括语义损失和完备性损失的权重。</li><li><code>pretrain_path</code>: 预训练模型的路径。</li><li><code>train_from</code>: 从哪一轮训练开始，这里设置为0。</li><li><code>seg_num_per_class</code>: 每个类别的语义分割样本数量。</li><li><code>complt_num_per_class</code>: 每个类别的完备性样本数量。</li></ul><p>这个配置文件包含了JS3CNet网络的各种参数和设置，用于训练和测试语义分割和完备性任务。</p><p><a href="https://github.com/yanx27/JS3C-Net/blob/main/train.py">J3SC_Net </a>是 JS3CNet 网络的核心部分，主要用于同时处理语义分割和点云完备性分割任务。它是一个继承自 PyTorch 的 <code>nn.Module</code> 的模型，下面将对其进行详细介绍：</p><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">class</span> <span class="token class-name">J3SC_Net</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> args<span class="token punctuation">)</span><span class="token punctuation">:</span>        <span class="token builtin">super</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>        self<span class="token punctuation">.</span>args <span class="token operator">=</span> args        self<span class="token punctuation">.</span>seg_head <span class="token operator">=</span> seg_model<span class="token punctuation">(</span>args<span class="token punctuation">)</span>        self<span class="token punctuation">.</span>complet_head <span class="token operator">=</span> complet_model<span class="token punctuation">(</span>args<span class="token punctuation">)</span>        self<span class="token punctuation">.</span>voxelpool <span class="token operator">=</span> model_utils<span class="token punctuation">.</span>VoxelPooling<span class="token punctuation">(</span>args<span class="token punctuation">)</span>        self<span class="token punctuation">.</span>seg_sigma <span class="token operator">=</span> nn<span class="token punctuation">.</span>Parameter<span class="token punctuation">(</span>torch<span class="token punctuation">.</span>Tensor<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">.</span>uniform_<span class="token punctuation">(</span><span class="token number">0.2</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">,</span> requires_grad<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>        self<span class="token punctuation">.</span>complet_sigma <span class="token operator">=</span> nn<span class="token punctuation">.</span>Parameter<span class="token punctuation">(</span>torch<span class="token punctuation">.</span>Tensor<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">.</span>uniform_<span class="token punctuation">(</span><span class="token number">0.2</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">,</span> requires_grad<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> x<span class="token punctuation">)</span><span class="token punctuation">:</span>        seg_inputs<span class="token punctuation">,</span> complet_inputs<span class="token punctuation">,</span> _ <span class="token operator">=</span> x        <span class="token comment"># 分割头部分</span>        seg_output<span class="token punctuation">,</span> feat <span class="token operator">=</span> self<span class="token punctuation">.</span>seg_head<span class="token punctuation">(</span>seg_inputs<span class="token punctuation">)</span>        <span class="token comment"># 完备性头部分</span>        coords <span class="token operator">=</span> complet_inputs<span class="token punctuation">[</span><span class="token string">'complet_coords'</span><span class="token punctuation">]</span>        coords <span class="token operator">=</span> coords<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span> <span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">]</span>        <span class="token keyword">if</span> args<span class="token punctuation">[</span><span class="token string">'DATA'</span><span class="token punctuation">]</span><span class="token punctuation">[</span><span class="token string">'dataset'</span><span class="token punctuation">]</span> <span class="token operator">==</span> <span class="token string">'SemanticKITTI'</span><span class="token punctuation">:</span>            coords<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">]</span> <span class="token operator">+=</span> <span class="token number">1</span>        <span class="token keyword">elif</span> args<span class="token punctuation">[</span><span class="token string">'DATA'</span><span class="token punctuation">]</span><span class="token punctuation">[</span><span class="token string">'dataset'</span><span class="token punctuation">]</span> <span class="token operator">==</span> <span class="token string">'SemanticPOSS'</span><span class="token punctuation">:</span>            coords<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">]</span><span class="token punctuation">[</span>coords<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">]</span> <span class="token operator">></span> <span class="token number">31</span><span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token number">31</span>        <span class="token keyword">if</span> args<span class="token punctuation">[</span><span class="token string">'Completion'</span><span class="token punctuation">]</span><span class="token punctuation">[</span><span class="token string">'feeding'</span><span class="token punctuation">]</span> <span class="token operator">==</span> <span class="token string">'both'</span><span class="token punctuation">:</span>            feeding <span class="token operator">=</span> torch<span class="token punctuation">.</span>cat<span class="token punctuation">(</span><span class="token punctuation">[</span>seg_output<span class="token punctuation">,</span> feat<span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span>        <span class="token keyword">elif</span> args<span class="token punctuation">[</span><span class="token string">'Completion'</span><span class="token punctuation">]</span><span class="token punctuation">[</span><span class="token string">'feeding'</span><span class="token punctuation">]</span> <span class="token operator">==</span> <span class="token string">'feat'</span><span class="token punctuation">:</span>            feeding <span class="token operator">=</span> feat        <span class="token keyword">else</span><span class="token punctuation">:</span>            feeding <span class="token operator">=</span> seg_output        <span class="token comment"># 使用Voxelpool模块进行特征池化操作</span>        features <span class="token operator">=</span> self<span class="token punctuation">.</span>voxelpool<span class="token punctuation">(</span>            invoxel_xyz<span class="token operator">=</span>complet_inputs<span class="token punctuation">[</span><span class="token string">'complet_invoxel_features'</span><span class="token punctuation">]</span><span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span> <span class="token punctuation">:</span><span class="token punctuation">,</span> <span class="token punctuation">:</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">,</span>            invoxel_map<span class="token operator">=</span>complet_inputs<span class="token punctuation">[</span><span class="token string">'complet_invoxel_features'</span><span class="token punctuation">]</span><span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span> <span class="token punctuation">:</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">.</span><span class="token builtin">long</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span>            src_feat<span class="token operator">=</span>feeding<span class="token punctuation">,</span>            voxel_center<span class="token operator">=</span>complet_inputs<span class="token punctuation">[</span><span class="token string">'voxel_centers'</span><span class="token punctuation">]</span>        <span class="token punctuation">)</span>        <span class="token keyword">if</span> self<span class="token punctuation">.</span>args<span class="token punctuation">[</span><span class="token string">'Completion'</span><span class="token punctuation">]</span><span class="token punctuation">[</span><span class="token string">'no_fuse_feat'</span><span class="token punctuation">]</span><span class="token punctuation">:</span>            features<span class="token punctuation">[</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token number">1</span>            features <span class="token operator">=</span> features<span class="token punctuation">.</span>detach<span class="token punctuation">(</span><span class="token punctuation">)</span>        <span class="token comment"># 创建SparseConvTensor，用于点云完备性分割</span>        batch_complet <span class="token operator">=</span> spconv<span class="token punctuation">.</span>SparseConvTensor<span class="token punctuation">(</span>            features<span class="token punctuation">.</span><span class="token builtin">float</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span> coords<span class="token punctuation">.</span><span class="token builtin">int</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span> args<span class="token punctuation">[</span><span class="token string">'Completion'</span><span class="token punctuation">]</span><span class="token punctuation">[</span><span class="token string">'full_scale'</span><span class="token punctuation">]</span><span class="token punctuation">,</span> args<span class="token punctuation">[</span><span class="token string">'TRAIN'</span><span class="token punctuation">]</span><span class="token punctuation">[</span><span class="token string">'batch_size'</span><span class="token punctuation">]</span>        <span class="token punctuation">)</span>        batch_complet <span class="token operator">=</span> dataset<span class="token punctuation">.</span>sparse_tensor_augmentation<span class="token punctuation">(</span>batch_complet<span class="token punctuation">,</span> complet_inputs<span class="token punctuation">[</span><span class="token string">'state'</span><span class="token punctuation">]</span><span class="token punctuation">)</span>        <span class="token comment"># 使用完备性头部分进行点云完备性分割</span>        complet_output <span class="token operator">=</span> self<span class="token punctuation">.</span>complet_head<span class="token punctuation">(</span>batch_complet<span class="token punctuation">)</span>        <span class="token keyword">return</span> seg_output<span class="token punctuation">,</span> complet_output<span class="token punctuation">,</span> <span class="token punctuation">[</span>self<span class="token punctuation">.</span>seg_sigma<span class="token punctuation">,</span> self<span class="token punctuation">.</span>complet_sigma<span class="token punctuation">]</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p><strong>主要组成部分</strong>：</p><ol><li><p><code>seg_head</code> 和 <code>complet_head</code>：这是分割头和完备性头的模型，它们通过 <code>args</code> 参数配置，用于执行分割任务和完备性任务。这些模型的初始化在 <code>__init__</code> 方法中进行。</p></li><li><p><code>voxelpool</code> 模块：用于对特征进行池化操作。根据输入参数，它将输入特征和相关的信息池化成新的特征。</p></li><li><p><code>seg_sigma</code> 和 <code>complet_sigma</code>：这是可学习的参数，分别表示语义分割和完备性分割任务的不确定性。模型在训练过程中会学习这些参数。</p></li></ol><p><strong>前向传播（<code>forward</code> 方法）</strong>：</p><p>在前向传播中，<code>J3SC_Net</code> 接受一个输入 <code>x</code>，其中 <code>x</code> 包含了分割和完备性任务的输入。首先，模型通过语义分割头 (<code>seg_head</code>) 处理分割任务的输入数据 <code>seg_inputs</code>，生成 <code>seg_output</code> 和 <code>feat</code>。</p><p>然后，对完备性任务的输入数据进行处理。首先调整坐标 <code>coords</code>，然后根据 <code>args</code> 的配置选择适当的输入数据 <code>feeding</code>。接下来，使用 <code>voxelpool</code> 模块对特征进行池化操作，生成 <code>features</code>。</p><p>最后，将生成的 <code>features</code> 传递给完备性头部分 (<code>complet_head</code>) 进行点云完备性分割。模型的输出包括语义分割结果 <code>seg_output</code> 和点云完备性分割结果 <code>complet_output</code>，以及模型学习的不确定性参数 <code>seg_sigma</code> 和 <code>complet_sigma</code>。</p><p><code>J3SC_Net</code> 模型将同时执行语义分割和点云完备性分割任务，使其成为 JS3CNet 网络的核心组件。这种多任务学习可以在处理点云数据时提高效率和性能。</p><p><code>SSCNet</code> 是一个用于点云完备性分割的神经网络模型，它是 JS3CNet 中完备性分割部分的模型。下面将详细介绍 <code>SSCNet</code> 模型的主要组成部分。</p><h4 id="SSCNet-Decoder"><a href="#SSCNet-Decoder" class="headerlink" title="SSCNet_Decoder"></a>SSCNet_Decoder</h4><p><code>SSCNet_Decoder</code> 是 <code>SSCNet</code> 的解码器部分。它主要负责将输入特征映射到点云的完备性分割结果。该解码器采用了类似 U-Net 架构的设计，分为不同的块（Block），其中包括卷积、批归一化和 ReLU 激活函数层。以下是 <code>SSCNet_Decoder</code> 中各个块的主要部分：</p><ul><li><p>Block 1：包括两个卷积层，每个卷积层后接批归一化和 ReLU 激活函数。该块的输出与一个残差块相加，并经过最大池化。</p></li><li><p>Block 2：与 Block 1 类似，包括两个卷积层，批归一化和 ReLU 激活函数。输出与残差块相加。</p></li><li><p>Block 3：包括两个卷积层，批归一化和 ReLU 激活函数。没有残差连接。</p></li><li><p>Block 4：包括两个卷积层，批归一化和 ReLU 激活函数。没有残差连接。</p></li><li><p>Block 5：包括两个卷积层，批归一化和 ReLU 激活函数。没有残差连接。</p></li><li><p>Prediction：该块用于生成最终的点云完备性分割结果。它包括两个卷积层，最终输出预测结果。</p></li></ul><p><code>SSCNet_Decoder</code> 的各个块逐渐提取并综合特征，最终生成点云完备性分割的预测结果。</p><h4 id="SSCNet"><a href="#SSCNet" class="headerlink" title="SSCNet"></a>SSCNet</h4><p><code>SSCNet</code> 是完备性分割网络的主要模型。它包含以下组件：</p><ul><li><p><code>Decoder</code>：前面介绍的 <code>SSCNet_Decoder</code>，用于点云完备性分割任务。它将输入的点云特征进行解码操作，最终生成点云完备性分割的预测结果。</p></li><li><p><code>upsample</code>：这是上采样模块，用于上采样点云完备性分割的结果，以便与语义分割结果相融合。</p></li><li><p><code>interaction_module</code>（可选）：如果配置中启用了交互模块（<code>args[&#39;Completion&#39;][&#39;interaction&#39;]</code> 为 <code>True</code>），则此组件用于执行点云之间的交互操作。这可以有助于改进点云完备性分割性能。</p></li></ul><p><code>SSCNet</code> 的前向传播过程首先将输入特征通过 <code>Decoder</code> 进行解码，然后根据配置选择是否执行交互操作，最后经过上采样模块生成最终的点云完备性分割结果。</p><p>这些组件一起构成了 <code>SSCNet</code> 模型，用于处理点云数据的完备性分割任务。</p><p><code>complet_head</code> 是用于点云语义完成任务的头部模块。它定义了一个 Unet 模型，用于处理 3D 点云数据。</p><h4 id="类-Unet"><a href="#类-Unet" class="headerlink" title="类 Unet"></a>类 <code>Unet</code></h4><p>这个类包含了 Unet 模型的构建和前向传播方法。以下是类 <code>Unet</code> 的详细分析：</p><ol><li><p><strong>构造方法 <code>__init__</code></strong>:</p><ul><li>这个方法接受一个配置参数 <code>config</code>，用于指定模型的各种参数和配置信息。</li><li><code>m</code> 是配置中指定的通道数。</li><li><code>input_dim</code> 是输入数据的维度，如果配置中指定了使用坐标信息 (<code>use_coords</code> 为 <code>True</code>)，则为 4，否则为 1。</li><li><code>sparseModel</code> 是一个 SparseConvNet 序列模型，它包含了 Unet 架构，包括编码器、解码器和跳跃连接部分。</li></ul></li><li><p><strong>前向传播方法 <code>forward</code></strong>:</p><ul><li>这个方法接受输入 <code>x</code>，其中 <code>x</code> 包含 <code>seg_coords</code> 和 <code>seg_features</code>。</li><li><code>x</code> 被传递给 <code>sparseModel</code>，这一步会进行 Unet 架构的前向传播。</li><li>如果在配置中启用了 <code>interaction</code>，则模型还会执行特征嵌入过程，以处理点云的交互信息。</li><li>最后，模型执行线性变换，将特征映射到类别预测空间，并返回预测结果以及特征信息。</li></ul></li></ol><h4 id="函数-Merge-tbl"><a href="#函数-Merge-tbl" class="headerlink" title="函数 Merge(tbl)"></a>函数 <code>Merge(tbl)</code></h4><p>这个函数是用于处理训练数据的工具函数，它接受一个包含多个样本数据的列表 <code>tbl</code>。</p><ol><li>函数通过迭代遍历每个样本数据，并从每个样本中提取出所需的数据，分别处理分割和完成任务的部分。</li><li>分割部分的数据包括坐标信息、标签和特征。</li><li>完成部分的数据包括点云坐标、输入数据、体素中心坐标、有效性信息、标签、统计信息以及点云体素特征。</li><li>这些数据被整理成适合输入到模型的格式，并以字典形式返回，包括 <code>seg_inputs</code> 和 <code>complet_inputs</code>。</li><li>还返回了一个包含样本文件名的列表，用于后续的分析和处理。</li></ol><p>总之，<code>complet_head</code> 中的代码定义了一个用于点云语义完成任务的 Unet 模型，同时提供了一个数据处理函数 <code>Merge(tbl)</code>，用于将原始数据整理成适合输入模型的格式。</p><p><code>model_utils.py</code> 包含了用于模型训练和实用工具的函数和类。下面是该文件中主要部分的详细分析：</p><ol><li><p><code>checkpoint_restore(model, exp_name, use_cuda=True, train_from=0)</code>:</p><ul><li>此函数用于从检查点文件中恢复模型的参数。</li><li><code>model</code> 是模型对象，<code>exp_name</code> 是实验名称，<code>use_cuda</code> 是是否使用 CUDA 运行模型，<code>train_from</code> 是指定的训练起始时期。</li><li>函数会查找以 “.pth” 结尾的模型检查点文件，找到最新的模型检查点并加载到模型中。</li><li>如果指定了 <code>train_from</code> 大于0，会从该时期开始训练。</li><li>函数返回模型训练的当前时期。</li></ul></li><li><p><code>VoxelPooling</code>:</p><ul><li>这是一个用于点云特征池化的自定义模块。</li><li>通过此模块，可以将点云体素汇聚为单一特征。</li><li>模块会对点云体素进行池化操作，并考虑了点云体素之间的关系。</li></ul></li><li><p><code>Loss</code>:</p><ul><li>这是一个自定义的损失函数模块，用于计算训练中的损失。</li><li>它计算分割损失和完成损失，支持带权重的损失计算。</li><li>如果 <code>uncertainty_loss</code> 被启用，还会计算损失的不确定性。</li></ul></li><li><p><code>interaction_module</code>:</p><ul><li>这是一个模块，用于模型中的点云交互。</li><li>它允许点云之间的特征交互，可以选择使用特征关系或直接的插值方法。</li><li>此模块通过模型的前向传播执行点云特征的交互操作。</li></ul></li><li><p><code>ResidualBlock</code>, <code>VGGBlock</code>, <code>UBlock</code>:</p><ul><li>这些模块用于构建模型的特定类型的块，如残差块、VGG 块和 U 块。</li><li>这些块用于构建点云处理模型的不同组件。</li></ul></li><li><p>其他工具函数:</p><ul><li>文件中还包括了其他用于文件读写、坐标转换、特征提取和其他辅助功能的函数。</li></ul></li></ol><p>总之，<code>model_utils.py</code> 包含了模型训练中的损失函数、模型恢复函数、自定义模块和工具函数，这些组成部分用于构建和训练点云处理模型。</p><h2 id="MonoScene"><a href="#MonoScene" class="headerlink" title="MonoScene"></a>MonoScene</h2><p>一种能在室内与室外场景均可使用的单目 SSC 方案：</p><ul><li>提出一种将 2D 特征投影到 3D 的方法： FLoSP </li><li>提出一种  3D Context Relation Prior （3D CRP） 提升网络的上下文意识</li><li>新的SSC损失函数，用于优化场景类亲和力（affinity）和局部锥体（frustum）比例</li></ul><p>代码链接： <a href="https://github.com/astra-vision/MonoScene">MonoScene: Monocular 3D Semantic Scene Completion</a>, CVPR 2022</p><p><img src="/pic/MonoScence.png"></p><p>Monoscene  网络流程</p><ul><li>输入 2d 图片，经过 2d unet 提取多层次的特征</li><li>Features Line of Sight Projection module (FLoSP) 用于将 2d 特征提升到 3d 位置上，增强信息流并实现2D-3D分离</li><li>3D Context Relation Prior （3D CRP） 用于增强长距离的上下文信息提取</li><li>loss 优化：Scene-Class Affinity Loss：提升类内和类间的场景方面度量；Frustum Proportion Loss：对齐局部截头体中的类分布，提供超越场景遮挡的监督信息</li></ul><p>网络结构</p><ul><li>2D unet：EfficientNetB7 用于提取图像特征</li><li>3D UNets：2层 encoder-decoder 结构，用于提取 3d 特征</li><li>completion head：3D ASPP 结构和 softmax 层，用于处理 3D UNet 输出得到3d场景 completion 结果</li></ul><h3 id="代码介绍"><a href="#代码介绍" class="headerlink" title="代码介绍"></a>代码介绍</h3><p><a href="https://github.com/astra-vision/MonoScene/blob/master/monoscene/models/monoscene.py">monoscene.py</a>使用PyTorch Lightning构建的一个名为<code>MonoScene</code>的模型类，用于单目深度估计和场景分割的任务。以下是该类的主要组件和功能的详细解释：</p><ol><li><p><strong>初始化方法</strong> (<code>__init__</code>)：这是类的构造方法，在创建<code>MonoScene</code>对象时被调用。它接受多个参数，包括类别数 (<code>n_classes</code>)、类别名称 (<code>class_names</code>)、特征数量 (<code>feature</code>)、类别权重 (<code>class_weights</code>) 等，用于初始化模型的各种参数和组件。其中的许多参数用于配置损失函数和训练过程中的各种选项。这里还创建了一个UNet 2D模型和多个FLoSP (Feature Learning from Single Point) 模型，用于3D场景估计。</p></li><li><p><strong>前向传播方法</strong> (<code>forward</code>)：<code>forward</code> 方法定义了如何计算模型的前向传播。它接受一个输入批次 (<code>batch</code>)，包括图像 (<code>img</code>)，并根据输入数据的尺寸进行前向传播计算。首先，通过UNet 2D模型 (<code>net_rgb</code>) 处理输入图像，然后根据尺度变化对图像进行3D投影，最终由UNet 3D模型 (<code>net_3d_decoder</code>) 进行3D场景估计。前向传播的结果保存在<code>out</code>变量中。</p></li><li><p><strong><code>step</code> 方法</strong>：<code>step</code> 方法执行了训练、验证和测试过程中的通用逻辑。它接受一个数据批次 (<code>batch</code>)、一个步骤类型 (<code>step_type</code>) 和一个度量指标 (<code>metric</code>)。在该方法中，对输入数据进行前向传播，计算并更新损失，同时也更新度量指标。这部分逻辑用于在训练、验证和测试时共享相同的操作。</p></li><li><p><strong>训练步骤</strong> (<code>training_step</code>)：<code>training_step</code> 方法是PyTorch Lightning中定义训练步骤的函数。它调用了<code>step</code> 方法，以及指定了步骤类型 (“train”) 和训练度量指标 (<code>self.train_metrics</code>)。</p></li><li><p><strong>验证步骤</strong> (<code>validation_step</code>) 和 <strong>验证结束方法</strong> (<code>validation_epoch_end</code>)：这两个方法用于在验证集上进行评估。<code>validation_step</code> 方法执行一个验证步骤，计算损失和更新验证度量指标 (<code>self.val_metrics</code>)。<code>validation_epoch_end</code> 方法在每个验证周期结束后，从度量指标中获取评估结果并进行记录。</p></li><li><p><strong>测试步骤</strong> (<code>test_step</code>) 和 <strong>测试结束方法</strong> (<code>test_epoch_end</code>)：这两个方法用于在测试集上进行评估，与验证过程类似。</p></li><li><p><strong>配置优化器</strong> (<code>configure_optimizers</code>)：在此方法中配置了模型的优化器和学习率调度器。根据数据集的不同（NYU或KITTI），选择了不同的优化器和学习率调度策略。</p></li><li><p><strong>指标记录和日志输出</strong>：在不同的步骤中，通过<code>self.log</code> 方法记录和输出训练、验证和测试的损失、IoU、精确度等度量指标。这些度量指标在每个epoch结束时会被重置，以便记录下一个epoch的结果。</p></li></ol><p>这段代码使用了PyTorch Lightning来规范化训练和评估流程，提供了清晰的组织结构，以及易于扩展和修改的接口，使其更容易适应不同的任务和数据集。根据您的需求，可以调整和扩展这个类来满足您的具体任务。</p><p>让我们逐行解释<code>MonoScene</code>类中的代码：</p><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">import</span> pytorch_lightning <span class="token keyword">as</span> pl<span class="token keyword">import</span> torch<span class="token keyword">import</span> torch<span class="token punctuation">.</span>nn <span class="token keyword">as</span> nn<span class="token keyword">from</span> monoscene<span class="token punctuation">.</span>models<span class="token punctuation">.</span>unet3d_nyu <span class="token keyword">import</span> UNet3D <span class="token keyword">as</span> UNet3DNYU<span class="token keyword">from</span> monoscene<span class="token punctuation">.</span>models<span class="token punctuation">.</span>unet3d_kitti <span class="token keyword">import</span> UNet3D <span class="token keyword">as</span> UNet3DKitti<span class="token keyword">from</span> monoscene<span class="token punctuation">.</span>loss<span class="token punctuation">.</span>sscMetrics <span class="token keyword">import</span> SSCMetrics<span class="token keyword">from</span> monoscene<span class="token punctuation">.</span>loss<span class="token punctuation">.</span>ssc_loss <span class="token keyword">import</span> sem_scal_loss<span class="token punctuation">,</span> CE_ssc_loss<span class="token punctuation">,</span> KL_sep<span class="token punctuation">,</span> geo_scal_loss<span class="token keyword">from</span> monoscene<span class="token punctuation">.</span>models<span class="token punctuation">.</span>flosp <span class="token keyword">import</span> FLoSP<span class="token keyword">from</span> monoscene<span class="token punctuation">.</span>loss<span class="token punctuation">.</span>CRP_loss <span class="token keyword">import</span> compute_super_CP_multilabel_loss<span class="token keyword">import</span> numpy <span class="token keyword">as</span> np<span class="token keyword">import</span> torch<span class="token punctuation">.</span>nn<span class="token punctuation">.</span>functional <span class="token keyword">as</span> F<span class="token keyword">from</span> monoscene<span class="token punctuation">.</span>models<span class="token punctuation">.</span>unet2d <span class="token keyword">import</span> UNet2D<span class="token keyword">from</span> torch<span class="token punctuation">.</span>optim<span class="token punctuation">.</span>lr_scheduler <span class="token keyword">import</span> MultiStepLR<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>这些行导入所需的Python库和模块，包括PyTorch、PyTorch Lightning、模型类、损失函数和其他依赖项。</p><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">class</span> <span class="token class-name">MonoScene</span><span class="token punctuation">(</span>pl<span class="token punctuation">.</span>LightningModule<span class="token punctuation">)</span><span class="token punctuation">:</span><span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>这一行定义了一个名为<code>MonoScene</code>的PyTorch Lightning模型类，它继承自<code>pl.LightningModule</code>，这是PyTorch Lightning中定义自定义模型的基类。</p><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>    self<span class="token punctuation">,</span>    n_classes<span class="token punctuation">,</span>    class_names<span class="token punctuation">,</span>    feature<span class="token punctuation">,</span>    class_weights<span class="token punctuation">,</span>    project_scale<span class="token punctuation">,</span>    full_scene_size<span class="token punctuation">,</span>    dataset<span class="token punctuation">,</span>    n_relations<span class="token operator">=</span><span class="token number">4</span><span class="token punctuation">,</span>    context_prior<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span>    fp_loss<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span>    project_res<span class="token operator">=</span><span class="token punctuation">[</span><span class="token punctuation">]</span><span class="token punctuation">,</span>    frustum_size<span class="token operator">=</span><span class="token number">4</span><span class="token punctuation">,</span>    relation_loss<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">,</span>    CE_ssc_loss<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span>    geo_scal_loss<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span>    sem_scal_loss<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span>    lr<span class="token operator">=</span><span class="token number">1e-4</span><span class="token punctuation">,</span>    weight_decay<span class="token operator">=</span><span class="token number">1e-4</span><span class="token punctuation">,</span><span class="token punctuation">)</span><span class="token punctuation">:</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>这是<code>MonoScene</code>类的构造方法 (<code>__init__</code>)。它接受许多参数，这些参数用于配置模型的不同方面，如类别数、类别名称、特征数量、损失权重、训练选项等。这些参数被用于初始化模型的各个组件和超参数。</p><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token builtin">super</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>__init<span class="token punctuation">(</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>这一行调用了基类的构造方法，即<code>pl.LightningModule</code>的构造方法。</p><pre class="line-numbers language-python" data-language="python"><code class="language-python">self<span class="token punctuation">.</span>project_res <span class="token operator">=</span> project_resself<span class="token punctuation">.</span>fp_loss <span class="token operator">=</span> fp_lossself<span class="token punctuation">.</span>dataset <span class="token operator">=</span> datasetself<span class="token punctuation">.</span>context_prior <span class="token operator">=</span> context_priorself<span class="token punctuation">.</span>frustum_size <span class="token operator">=</span> frustum_sizeself<span class="token punctuation">.</span>class_names <span class="token operator">=</span> class_namesself<span class="token punctuation">.</span>relation_loss <span class="token operator">=</span> relation_lossself<span class="token punctuation">.</span>CE_ssc_loss <span class="token operator">=</span> CE_ssc_lossself<span class="token punctuation">.</span>sem_scal_loss <span class="token operator">=</span> sem_scal_lossself<span class="token punctuation">.</span>geo_scal_loss <span class="token operator">=</span> geo_scal_lossself<span class="token punctuation">.</span>project_scale <span class="token operator">=</span> project_scaleself<span class="token punctuation">.</span>class_weights <span class="token operator">=</span> class_weightsself<span class="token punctuation">.</span>lr <span class="token operator">=</span> lrself<span class="token punctuation">.</span>weight_decay <span class="token operator">=</span> weight_decay<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>这些行将构造方法中传入的参数赋值给对象的属性，以便它们可以在整个类中使用。</p><pre class="line-numbers language-python" data-language="python"><code class="language-python">self<span class="token punctuation">.</span>projects <span class="token operator">=</span> <span class="token punctuation">&#123;</span><span class="token punctuation">&#125;</span>self<span class="token punctuation">.</span>scale_2ds <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">4</span><span class="token punctuation">,</span> <span class="token number">8</span><span class="token punctuation">]</span>  <span class="token comment"># 2D scales</span><span class="token keyword">for</span> scale_2d <span class="token keyword">in</span> self<span class="token punctuation">.</span>scale_2ds<span class="token punctuation">:</span>    self<span class="token punctuation">.</span>projects<span class="token punctuation">[</span><span class="token builtin">str</span><span class="token punctuation">(</span>scale_2d<span class="token punctuation">)</span><span class="token punctuation">]</span> <span class="token operator">=</span> FLoSP<span class="token punctuation">(</span>        full_scene_size<span class="token punctuation">,</span> project_scale<span class="token operator">=</span>self<span class="token punctuation">.</span>project_scale<span class="token punctuation">,</span> dataset<span class="token operator">=</span>self<span class="token punctuation">.</span>dataset    <span class="token punctuation">)</span>self<span class="token punctuation">.</span>projects <span class="token operator">=</span> nn<span class="token punctuation">.</span>ModuleDict<span class="token punctuation">(</span>self<span class="token punctuation">.</span>projects<span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>这一部分初始化了模型的<code>projects</code>属性，其中包含多个FLoSP模型，用于将2D特征投影到3D场景。不同的投影比例 (<code>scale_2d</code>) 在这里被迭代，为每个比例创建一个FLoSP模型，并将它们存储在<code>projects</code>字典中。</p><pre class="line-numbers language-python" data-language="python"><code class="language-python">self<span class="token punctuation">.</span>n_classes <span class="token operator">=</span> n_classes<span class="token keyword">if</span> self<span class="token punctuation">.</span>dataset <span class="token operator">==</span> <span class="token string">"NYU"</span><span class="token punctuation">:</span>    self<span class="token punctuation">.</span>net_3d_decoder <span class="token operator">=</span> UNet3DNYU<span class="token punctuation">(</span>        self<span class="token punctuation">.</span>n_classes<span class="token punctuation">,</span>        nn<span class="token punctuation">.</span>BatchNorm3d<span class="token punctuation">,</span>        n_relations<span class="token operator">=</span>n_relations<span class="token punctuation">,</span>        feature<span class="token operator">=</span>feature<span class="token punctuation">,</span>        full_scene_size<span class="token operator">=</span>full_scene_size<span class="token punctuation">,</span>        context_prior<span class="token operator">=</span>context_prior<span class="token punctuation">,</span>    <span class="token punctuation">)</span><span class="token keyword">elif</span> self<span class="token punctuation">.</span>dataset <span class="token operator">==</span> <span class="token string">"kitti"</span><span class="token punctuation">:</span>    self<span class="token punctuation">.</span>net_3d_decoder <span class="token operator">=</span> UNet3DKitti<span class="token punctuation">(</span>        self<span class="token punctuation">.</span>n_classes<span class="token punctuation">,</span>        nn<span class="token punctuation">.</span>BatchNorm3d<span class="token punctuation">,</span>        project_scale<span class="token operator">=</span>project_scale<span class="token punctuation">,</span>        feature<span class="token operator">=</span>feature<span class="token punctuation">,</span>        full_scene_size<span class="token operator">=</span>full_scene_size<span class="token punctuation">,</span>        context_prior<span class="token operator">=</span>context_prior<span class="token punctuation">,</span>    <span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>这一部分初始化了模型的3D解码器 (<code>net_3d_decoder</code>)，根据数据集的类型 (“NYU” 或 “kitti”) 选择不同的UNet 3D模型。该解码器将用于对3D场景进行估计。</p><pre class="line-numbers language-python" data-language="python"><code class="language-python">self<span class="token punctuation">.</span>net_rgb <span class="token operator">=</span> UNet2D<span class="token punctuation">.</span>build<span class="token punctuation">(</span>out_feature<span class="token operator">=</span>feature<span class="token punctuation">,</span> use_decoder<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>这一行创建了模型的2D UNet模型 (<code>net_rgb</code>)，该模型用于处理输入图像，其中的<code>feature</code>参数指定了模型的特征数量。</p><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token comment"># log hyperparameters</span>self<span class="token punctuation">.</span>save_hyperparameters<span class="token punctuation">(</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><p>这一行记录了模型的超参数，以便后续可以查看它们。</p><pre class="line-numbers language-python" data-language="python"><code class="language-python">self<span class="token punctuation">.</span>train_metrics <span class="token operator">=</span> SSCMetrics<span class="token punctuation">(</span>self<span class="token punctuation">.</span>n_classes<span class="token punctuation">)</span>self<span class="token punctuation">.</span>val_metrics <span class="token operator">=</span> SSCMetrics<span class="token punctuation">(</span>self<span class="token punctuation">.</span>n_classes<span class="token punctuation">)</span>self<span class="token punctuation">.</span>test_metrics <span class="token operator">=</span> SSCMetrics<span class="token punctuation">(</span>self<span class="token punctuation">.</span>n_classes<span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre><p>这些行初始化了训练、验证和测试度量指标 (<code>train_metrics</code>, <code>val_metrics</code>, <code>test_metrics</code>)，用于跟踪模型性能。</p><p>这只是构造方法的设置部分。整个<code>MonoScene</code>类的构造方法用于初始化模型的各个组件和超参数。在下面的部分中，将解释类中的其他方法。</p><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> batch<span class="token punctuation">)</span><span class="token punctuation">:</span>    img <span class="token operator">=</span> batch<span class="token punctuation">[</span><span class="token string">"img"</span><span class="token punctuation">]</span>    bs <span class="token operator">=</span> <span class="token builtin">len</span><span class="token punctuation">(</span>img<span class="token punctuation">)</span>    out <span class="token operator">=</span> <span class="token punctuation">&#123;</span><span class="token punctuation">&#125;</span>    x_rgb <span class="token operator">=</span> self<span class="token punctuation">.</span>net_rgb<span class="token punctuation">(</span>img<span class="token punctuation">)</span>    x3ds <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>    <span class="token keyword">for</span> i <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span>bs<span class="token punctuation">)</span><span class="token punctuation">:</span>        x3d <span class="token operator">=</span> <span class="token boolean">None</span>        <span class="token keyword">for</span> scale_2d <span class="token keyword">in</span> self<span class="token punctuation">.</span>project_res<span class="token punctuation">:</span>            <span class="token comment"># project features at each 2D scale to target 3D scale</span>            scale_2d <span class="token operator">=</span> <span class="token builtin">int</span><span class="token punctuation">(</span>scale_2d<span class="token punctuation">)</span>            projected_pix <span class="token operator">=</span> batch<span class="token punctuation">[</span><span class="token string">"projected_pix_&#123;&#125;"</span><span class="token punctuation">.</span><span class="token builtin">format</span><span class="token punctuation">(</span>self<span class="token punctuation">.</span>project_scale<span class="token punctuation">)</span><span class="token punctuation">]</span><span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token punctuation">.</span>cuda<span class="token punctuation">(</span><span class="token punctuation">)</span>            fov_mask <span class="token operator">=</span> batch<span class="token punctuation">[</span><span class="token string">"fov_mask_&#123;&#125;"</span><span class="token punctuation">.</span><span class="token builtin">format</span><span class="token punctuation">(</span>self<span class="token punctuation">.</span>project_scale<span class="token punctuation">)</span><span class="token punctuation">]</span><span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token punctuation">.</span>cuda<span class="token punctuation">(</span><span class="token punctuation">)</span>            <span class="token keyword">if</span> x3d <span class="token keyword">is</span> <span class="token boolean">None</span><span class="token punctuation">:</span>                x3d <span class="token operator">=</span> self<span class="token punctuation">.</span>projects<span class="token punctuation">[</span><span class="token builtin">str</span><span class="token punctuation">(</span>scale_2d<span class="token punctuation">)</span><span class="token punctuation">]</span><span class="token punctuation">(</span>                    x_rgb<span class="token punctuation">[</span><span class="token string">"1_"</span> <span class="token operator">+</span> <span class="token builtin">str</span><span class="token punctuation">(</span>scale_2d<span class="token punctuation">)</span><span class="token punctuation">]</span><span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token punctuation">,</span>                    projected_pix <span class="token operator">//</span> scale_2d<span class="token punctuation">,</span>                    fov_mask<span class="token punctuation">,</span>                <span class="token punctuation">)</span>            <span class="token keyword">else</span><span class="token punctuation">:</span>                x3d <span class="token operator">+=</span> self<span class="token punctuation">.</span>projects<span class="token punctuation">[</span><span class="token builtin">str</span><span class="token punctuation">(</span>scale_2d<span class="token punctuation">)</span><span class="token punctuation">]</span><span class="token punctuation">(</span>                    x_rgb<span class="token punctuation">[</span><span class="token string">"1_"</span> <span class="token operator">+</span> <span class="token builtin">str</span><span class="token punctuation">(</span>scale_2d<span class="token punctuation">)</span><span class="token punctuation">]</span><span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token punctuation">,</span>                    projected_pix <span class="token operator">//</span> scale_2d<span class="token punctuation">,</span>                    fov_mask<span class="token punctuation">,</span>                <span class="token punctuation">)</span>        x3ds<span class="token punctuation">.</span>append<span class="token punctuation">(</span>x3d<span class="token punctuation">)</span>    input_dict <span class="token operator">=</span> <span class="token punctuation">&#123;</span><span class="token string">"x3d"</span><span class="token punctuation">:</span> torch<span class="token punctuation">.</span>stack<span class="token punctuation">(</span>x3ds<span class="token punctuation">)</span><span class="token punctuation">&#125;</span>    out <span class="token operator">=</span> self<span class="token punctuation">.</span>net_3d_decoder<span class="token punctuation">(</span>input_dict<span class="token punctuation">)</span>    <span class="token keyword">return</span> out<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>这是模型的前向传播函数 <code>forward</code>。在这个函数中，输入图像 <code>img</code> 被传递给 2D UNet 模型 <code>net_rgb</code> 以提取特征。然后，特征将根据指定的投影比例 (<code>self.project_res</code>) 投影到3D场景中。对于每个输入样本，它会循环遍历不同的投影比例，并调用FLoSP模型 (<code>self.projects</code>) 来执行特征投影。最终，投影的特征将被传递给3D解码器 <code>net_3d_decoder</code> 进行3D场景估计。</p><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">def</span> <span class="token function">step</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> batch<span class="token punctuation">,</span> step_type<span class="token punctuation">,</span> metric<span class="token punctuation">)</span><span class="token punctuation">:</span>    bs <span class="token operator">=</span> <span class="token builtin">len</span><span class="token punctuation">(</span>batch<span class="token punctuation">[</span><span class="token string">"img"</span><span class="token punctuation">]</span><span class="token punctuation">)</span>    loss <span class="token operator">=</span> <span class="token number">0</span>    out_dict <span class="token operator">=</span> self<span class="token punctuation">(</span>batch<span class="token punctuation">)</span>    ssc_pred <span class="token operator">=</span> out_dict<span class="token punctuation">[</span><span class="token string">"ssc_logit"</span><span class="token punctuation">]</span>    target <span class="token operator">=</span> batch<span class="token punctuation">[</span><span class="token string">"target"</span><span class="token punctuation">]</span>    <span class="token keyword">if</span> self<span class="token punctuation">.</span>context_prior<span class="token punctuation">:</span>        P_logits <span class="token operator">=</span> out_dict<span class="token punctuation">[</span><span class="token string">"P_logits"</span><span class="token punctuation">]</span>        CP_mega_matrices <span class="token operator">=</span> batch<span class="token punctuation">[</span><span class="token string">"CP_mega_matrices"</span><span class="token punctuation">]</span>        <span class="token keyword">if</span> self<span class="token punctuation">.</span>relation_loss<span class="token punctuation">:</span>            loss_rel_ce <span class="token operator">=</span> compute_super_CP_multilabel_loss<span class="token punctuation">(</span>                P_logits<span class="token punctuation">,</span> CP_mega_matrices            <span class="token punctuation">)</span>            loss <span class="token operator">+=</span> loss_rel_ce            self<span class="token punctuation">.</span>log<span class="token punctuation">(</span>                step_type <span class="token operator">+</span> <span class="token string">"/loss_relation_ce_super"</span><span class="token punctuation">,</span>                loss_rel_ce<span class="token punctuation">.</span>detach<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span>                on_epoch<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span>                sync_dist<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span>            <span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>这个方法 <code>step</code> 用于执行训练、验证和测试步骤的公共逻辑。它接受批量数据 <code>batch</code>、步骤类型 <code>step_type</code>（如 “train”、”val”、”test”）和度量对象 <code>metric</code> 作为参数。在这个方法中，模型的前向传播被调用，并计算损失 <code>loss</code>。具体的损失计算依赖于模型的配置和目标。在这里，根据模型的配置，可以计算包括关系损失 (<code>loss_rel_ce</code>) 的不同损失项。如果启用了关系损失 (<code>self.relation_loss</code>)，则将计算关系损失并将其添加到总损失中。</p><pre class="line-numbers language-python" data-language="python"><code class="language-python">class_weight <span class="token operator">=</span> self<span class="token punctuation">.</span>class_weights<span class="token punctuation">.</span>type_as<span class="token punctuation">(</span>batch<span class="token punctuation">[</span><span class="token string">"img"</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token keyword">if</span> self<span class="token punctuation">.</span>CE_ssc_loss<span class="token punctuation">:</span>    loss_ssc <span class="token operator">=</span> CE_ssc_loss<span class="token punctuation">(</span>ssc_pred<span class="token punctuation">,</span> target<span class="token punctuation">,</span> class_weight<span class="token punctuation">)</span>    loss <span class="token operator">+=</span> loss_ssc    self<span class="token punctuation">.</span>log<span class="token punctuation">(</span>        step_type <span class="token operator">+</span> <span class="token string">"/loss_ssc"</span><span class="token punctuation">,</span>        loss_ssc<span class="token punctuation">.</span>detach<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span>        on_epoch<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span>        sync_dist<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span>    <span class="token punctuation">)</span><span class="token keyword">if</span> self<span class="token punctuation">.</span>sem_scal_loss<span class="token punctuation">:</span>    loss_sem_scal <span class="token operator">=</span> sem_scal_loss<span class="token punctuation">(</span>ssc_pred<span class="token punctuation">,</span> target<span class="token punctuation">)</span>    loss <span class="token operator">+=</span> loss_sem_scal    self<span class="token punctuation">.</span>log<span class="token punctuation">(</span>        step_type <span class="token operator">+</span> <span class="token string">"/loss_sem_scal"</span><span class="token punctuation">,</span>        loss_sem_scal<span class="token punctuation">.</span>detach<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span>        on_epoch<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span>        sync_dist<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span>    <span class="token punctuation">)</span><span class="token keyword">if</span> self<span class="token punctuation">.</span>geo_scal_loss<span class="token punctuation">:</span>    loss_geo_scal <span class="token operator">=</span> geo_scal_loss<span class="token punctuation">(</span>ssc_pred<span class="token punctuation">,</span> target<span class="token punctuation">)</span>    loss <span class="token operator">+=</span> loss_geo_scal    self<span class="token punctuation">.</span>log<span class="token punctuation">(</span>        step_type <span class="token operator">+</span> <span class="token string">"/loss_geo_scal"</span><span class="token punctuation">,</span>        loss_geo_scal<span class="token punctuation">.</span>detach<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span>        on_epoch<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span>        sync_dist<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span>    <span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>在这一部分中，根据模型的配置，计算了交叉熵损失 (<code>loss_ssc</code>)、语义分割损失 (<code>loss_sem_scal</code>) 和几何尺度损失 (<code>loss_geo_scal</code>)。这些损失用于对模型进行监督训练，以便它能够学习适应输入数据并进行3D场景估计。</p><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">if</span> self<span class="token punctuation">.</span>fp_loss <span class="token keyword">and</span> step_type <span class="token operator">!=</span> <span class="token string">"test"</span><span class="token punctuation">:</span>    frustums_masks <span class="token operator">=</span> torch<span class="token punctuation">.</span>stack<span class="token punctuation">(</span>batch<span class="token punctuation">[</span><span class="token string">"frustums_masks"</span><span class="token punctuation">]</span><span class="token punctuation">)</span>    frustums_class_dists <span class="token operator">=</span> torch<span class="token punctuation">.</span>stack<span class="token punctuation">(</span>        batch<span class="token punctuation">[</span><span class="token string">"frustums_class_dists"</span><span class="token punctuation">]</span>    <span class="token punctuation">)</span><span class="token punctuation">.</span><span class="token builtin">float</span><span class="token punctuation">(</span><span class="token punctuation">)</span>  <span class="token comment"># (bs, n_frustums, n_classes)</span>    n_frustums <span class="token operator">=</span> frustums_class_dists<span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span>    pred_prob <span class="token operator">=</span> F<span class="token punctuation">.</span>softmax<span class="token punctuation">(</span>ssc_pred<span class="token punctuation">,</span> dim<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span>    batch_cnt <span class="token operator">=</span> frustums_class_dists<span class="token punctuation">.</span><span class="token builtin">sum</span><span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">)</span>  <span class="token comment"># (n_frustums, n_classes)</span>    frustum_loss <span class="token operator">=</span> <span class="token number">0</span>    frustum_nonempty <span class="token operator">=</span> <span class="token number">0</span>    <span class="token keyword">for</span> frus <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span>n_frustums<span class="token punctuation">)</span><span class="token punctuation">:</span>        frustum_mask <span class="token operator">=</span> frustums_masks<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span> frus<span class="token punctuation">,</span> <span class="token punctuation">:</span><span class="token punctuation">,</span> <span class="token punctuation">:</span><span class="token punctuation">,</span> <span class="token punctuation">:</span><span class="token punctuation">]</span><span class="token punctuation">.</span>unsqueeze<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">.</span><span class="token builtin">float</span><span class="token punctuation">(</span><span class="token punctuation">)</span>        prob <span class="token operator">=</span> frustum_mask <span class="token operator">*</span> pred_prob  <span class="token comment"># bs, n_classes, H, W, D</span>        prob <span class="token operator">=</span> prob<span class="token punctuation">.</span>reshape<span class="token punctuation">(</span>bs<span class="token punctuation">,</span> self<span class="token punctuation">.</span>n_classes<span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">.</span>permute<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">)</span>        prob <span class="token operator">=</span> prob<span class="token punctuation">.</span>reshape<span class="token punctuation">(</span>self<span class="token punctuation">.</span>n_classes<span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span>        cum_prob <span class="token operator">=</span> prob<span class="token punctuation">.</span><span class="token builtin">sum</span><span class="token punctuation">(</span>dim<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span>  <span class="token comment"># n_classes</span>        total_cnt <span class="token operator">=</span> torch<span class="token punctuation">.</span><span class="token builtin">sum</span><span class="token punctuation">(</span>batch_cnt<span class="token punctuation">[</span>frus<span class="token punctuation">]</span><span class="token punctuation">)</span>        total_prob <span class="token operator">=</span> prob<span class="token punctuation">.</span><span class="token builtin">sum</span><span class="token punctuation">(</span><span class="token punctuation">)</span>        <span class="token keyword">if</span> total_prob <span class="token operator">></span> <span class="token number">0</span> <span class="token keyword">and</span> total_cnt <span class="token operator">></span> <span class="token number">0</span><span class="token punctuation">:</span>            frustum_target_proportion <span class="token operator">=</span> batch_cnt<span class="token punctuation">[</span>frus<span class="token punctuation">]</span> <span class="token operator">/</span> total_cnt            cum_prob <span class="token operator">=</span> cum_prob <span class="token operator">/</span> total_prob  <span class="token comment"># n_classes</span>            frustum_loss_i <span class="token operator">=</span> KL_sep<span class="token punctuation">(</span>cum_prob<span class="token punctuation">,</span> frustum_target_proportion<span class="token punctuation">)</span>            frustum_loss <span class="token operator">+=</span> frustum_loss_i            frustum_nonempty <span class="token operator">+=</span> <span class="token number">1</span>    frustum_loss <span class="token operator">=</span> frustum_loss <span class="token operator">/</span> frustum_nonempty    loss <span class="token operator">+=</span> frustum_loss    self<span class="token punctuation">.</span>log<span class="token punctuation">(</span>        step_type <span class="token operator">+</span> <span class="token string">"/loss_frustums"</span><span class="token punctuation">,</span>        frustum_loss<span class="token punctuation">.</span>detach<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span>        on_epoch<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span>        sync_dist<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span>    <span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>在这部分代码中，计算</p><p>了与模型输出的SSC预测和输入数据中的frustum masks 相关的损失项。如果 <code>self.fp_loss</code> 被设置为 <code>True</code> 并且步骤类型不是 “test”，则将计算frustum损失。这个损失考虑了模型对视场的理解，以及模型的预测与输入数据的关系。</p><p>最后，损失项被添加到总损失 <code>loss</code> 中，并使用 <code>self.log</code> 方法记录损失，以便在训练期间进行监控。</p><pre class="line-numbers language-python" data-language="python"><code class="language-python">y_true <span class="token operator">=</span> target<span class="token punctuation">.</span>cpu<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>numpy<span class="token punctuation">(</span><span class="token punctuation">)</span>y_pred <span class="token operator">=</span> ssc_pred<span class="token punctuation">.</span>detach<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>cpu<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>numpy<span class="token punctuation">(</span><span class="token punctuation">)</span>y_pred <span class="token operator">=</span> np<span class="token punctuation">.</span>argmax<span class="token punctuation">(</span>y_pred<span class="token punctuation">,</span> axis<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span>metric<span class="token punctuation">.</span>add_batch<span class="token punctuation">(</span>y_pred<span class="token punctuation">,</span> y_true<span class="token punctuation">)</span>self<span class="token punctuation">.</span>log<span class="token punctuation">(</span>step_type <span class="token operator">+</span> <span class="token string">"/loss"</span><span class="token punctuation">,</span> loss<span class="token punctuation">.</span>detach<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span> on_epoch<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span> sync_dist<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>在这里，真实标签和模型预测的标签用于计算模型性能指标，并将它们传递给 <code>metric</code> 对象。度量对象用于跟踪模型性能，并使用 <code>self.log</code> 方法记录损失。</p><p>这部分代码覆盖了<code>step</code> 方法中的主要逻辑，它是用于训练、验证和测试的公共代码。</p><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">def</span> <span class="token function">training_step</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> batch<span class="token punctuation">,</span> batch_idx<span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token keyword">return</span> self<span class="token punctuation">.</span>step<span class="token punctuation">(</span>batch<span class="token punctuation">,</span> <span class="token string">"train"</span><span class="token punctuation">,</span> self<span class="token punctuation">.</span>train_metrics<span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><p>这个方法是用于训练的步骤。它调用了 <code>step</code> 方法，并传递了相应的参数。在训练期间，它还返回损失以供 PyTorch Lightning 进行后续的处理。</p><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">def</span> <span class="token function">validation_step</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> batch<span class="token punctuation">,</span> batch_idx<span class="token punctuation">)</span><span class="token punctuation">:</span>    self<span class="token punctuation">.</span>step<span class="token punctuation">(</span>batch<span class="token punctuation">,</span> <span class="token string">"val"</span><span class="token punctuation">,</span> self<span class="token punctuation">.</span>val_metrics<span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><p>这个方法是验证步骤。它也调用了 <code>step</code> 方法，但不返回损失，而是将性能指标记录到验证度量对象 <code>val_metrics</code> 中。</p><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">def</span> <span class="token function">validation_epoch_end</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> outputs<span class="token punctuation">)</span><span class="token punctuation">:</span>    metric_list <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">(</span><span class="token string">"train"</span><span class="token punctuation">,</span> self<span class="token punctuation">.</span>train_metrics<span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token punctuation">(</span><span class="token string">"val"</span><span class="token punctuation">,</span> self<span class="token punctuation">.</span>val_metrics<span class="token punctuation">)</span><span class="token punctuation">]</span>    <span class="token keyword">for</span> prefix<span class="token punctuation">,</span> metric <span class="token keyword">in</span> metric_list<span class="token punctuation">:</span>        stats <span class="token operator">=</span> metric<span class="token punctuation">.</span>get_stats<span class="token punctuation">(</span><span class="token punctuation">)</span>        <span class="token keyword">for</span> i<span class="token punctuation">,</span> class_name <span class="token keyword">in</span> <span class="token builtin">enumerate</span><span class="token punctuation">(</span>self<span class="token punctuation">.</span>class_names<span class="token punctuation">)</span><span class="token punctuation">:</span>            self<span class="token punctuation">.</span>log<span class="token punctuation">(</span>                <span class="token string">"&#123;&#125;_SemIoU/&#123;&#125;"</span><span class="token punctuation">.</span><span class="token builtin">format</span><span class="token punctuation">(</span>prefix<span class="token punctuation">,</span> class_name<span class="token punctuation">)</span><span class="token punctuation">,</span>                stats<span class="token punctuation">[</span><span class="token string">"iou_ssc"</span><span class="token punctuation">]</span><span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token punctuation">,</span>                sync_dist<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span>            <span class="token punctuation">)</span>        self<span class="token punctuation">.</span>log<span class="token punctuation">(</span><span class="token string">"&#123;&#125;/mIoU"</span><span class="token punctuation">.</span><span class="token builtin">format</span><span class="token punctuation">(</span>prefix<span class="token punctuation">)</span><span class="token punctuation">,</span> stats<span class="token punctuation">[</span><span class="token string">"iou_ssc_mean"</span><span class="token punctuation">]</span><span class="token punctuation">,</span> sync_dist<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>        self<span class="token punctuation">.</span>log<span class="token punctuation">(</span><span class="token string">"&#123;&#125;/IoU"</span><span class="token punctuation">.</span><span class="token builtin">format</span><span class="token punctuation">(</span>prefix<span class="token punctuation">)</span><span class="token punctuation">,</span> stats<span class="token punctuation">[</span><span class="token string">"iou"</span><span class="token punctuation">]</span><span class="token punctuation">,</span> sync_dist<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>        self<span class="token punctuation">.</span>log<span class="token punctuation">(</span><span class="token string">"&#123;&#125;/Precision"</span><span class="token punctuation">.</span><span class="token builtin">format</span><span class="token punctuation">(</span>prefix<span class="token punctuation">)</span><span class="token punctuation">,</span> stats<span class="token punctuation">[</span><span class="token string">"recall"</span><span class="token punctuation">]</span><span class="token punctuation">,</span> sync_dist<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>        self<span class="token punctuation">.</span>log<span class="token punctuation">(</span><span class="token string">"&#123;&#125;/Recall"</span><span class="token punctuation">.</span><span class="token builtin">format</span><span class="token punctuation">(</span>prefix<span class="token punctuation">)</span><span class="token punctuation">,</span> stats<span class="token punctuation">[</span><span class="token string">"recall"</span><span class="token punctuation">]</span><span class="token punctuation">,</span> sync_dist<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>        metric<span class="token punctuation">.</span>reset<span class="token punctuation">(</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>这个方法在验证周期结束时被调用，用于总结验证阶段的性能。它计算各个类别的语义分割IoU、平均IoU、IoU和精确度，并使用 <code>self.log</code> 方法记录这些度量值。</p><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">def</span> <span class="token function">test_step</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> batch<span class="token punctuation">,</span> batch_idx<span class="token punctuation">)</span><span class="token punctuation">:</span>    self<span class="token punctuation">.</span>step<span class="token punctuation">(</span>batch<span class="token punctuation">,</span> <span class="token string">"test"</span><span class="token punctuation">,</span> self<span class="token punctuation">.</span>test_metrics<span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><p>这个方法是测试步骤，类似于验证步骤，它调用 <code>step</code> 方法并将性能指标记录到测试度量对象 <code>test_metrics</code> 中。</p><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">def</span> <span class="token function">test_epoch_end</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> outputs<span class="token punctuation">)</span><span class="token punctuation">:</span>    classes <span class="token operator">=</span> self<span class="token punctuation">.</span>class_names    metric_list <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">(</span><span class="token string">"test"</span><span class="token punctuation">,</span> self<span class="token punctuation">.</span>test_metrics<span class="token punctuation">)</span><span class="token punctuation">]</span>    <span class="token keyword">for</span> prefix<span class="token punctuation">,</span> metric <span class="token keyword">in</span> metric_list<span class="token punctuation">:</span>        <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">"&#123;&#125;======"</span><span class="token punctuation">.</span><span class="token builtin">format</span><span class="token punctuation">(</span>prefix<span class="token punctuation">)</span><span class="token punctuation">)</span>        stats <span class="token operator">=</span> metric<span class="token punctuation">.</span>get_stats<span class="token punctuation">(</span><span class="token punctuation">)</span>        <span class="token keyword">print</span><span class="token punctuation">(</span>            <span class="token string">"Precision=&#123;:.4f&#125;, Recall=&#123;:.4f&#125;, IoU=&#123;:.4f&#125;"</span><span class="token punctuation">.</span><span class="token builtin">format</span><span class="token punctuation">(</span>                stats<span class="token punctuation">[</span><span class="token string">"precision"</span><span class="token punctuation">]</span> <span class="token operator">*</span> <span class="token number">100</span><span class="token punctuation">,</span> stats<span class="token punctuation">[</span><span class="token string">"recall"</span><span class="token punctuation">]</span> <span class="token operator">*</span> <span class="token number">100</span><span class="token punctuation">,</span> stats<span class="token punctuation">[</span><span class="token string">"iou"</span><span class="token punctuation">]</span> <span class="token operator">*</span> <span class="token number">100</span>            <span class="token punctuation">)</span>        <span class="token punctuation">)</span>        <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">"class IoU: &#123;&#125;, "</span><span class="token punctuation">.</span><span class="token builtin">format</span><span class="token punctuation">(</span>classes<span class="token punctuation">)</span><span class="token punctuation">)</span>        <span class="token keyword">print</span><span class="token punctuation">(</span>            <span class="token string">" "</span><span class="token punctuation">.</span>join<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token string">"&#123;:.4f&#125;, "</span><span class="token punctuation">]</span> <span class="token operator">*</span> <span class="token builtin">len</span><span class="token punctuation">(</span>classes<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">.</span><span class="token builtin">format</span><span class="token punctuation">(</span>                <span class="token operator">*</span><span class="token punctuation">(</span>stats<span class="token punctuation">[</span><span class="token string">"iou_ssc"</span><span class="token punctuation">]</span> <span class="token operator">*</span> <span class="token number">100</span><span class="token punctuation">)</span><span class="token punctuation">.</span>tolist<span class="token punctuation">(</span><span class="token punctuation">)</span>            <span class="token punctuation">)</span>        <span class="token punctuation">)</span>        <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">"mIoU=&#123;:.4f&#125;"</span><span class="token punctuation">.</span><span class="token builtin">format</span><span class="token punctuation">(</span>stats<span class="token punctuation">[</span><span class="token string">"iou_ssc_mean"</span><span class="token punctuation">]</span> <span class="token operator">*</span> <span class="token number">100</span><span class="token punctuation">)</span><span class="token punctuation">)</span>        metric<span class="token punctuation">.</span>reset<span class="token punctuation">(</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>这个方法在测试周期结束时被调用，用于总结测试阶段的性能。它打印了精确度、召回率、IoU 和类别IoU的信息，并将这些信息显示在控制台上。</p><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">def</span> <span class="token function">configure_optimizers</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token keyword">if</span> self<span class="token punctuation">.</span>dataset <span class="token operator">==</span> <span class="token string">"NYU"</span><span class="token punctuation">:</span>        optimizer <span class="token operator">=</span> torch<span class="token punctuation">.</span>optim<span class="token punctuation">.</span>AdamW<span class="token punctuation">(</span>            self<span class="token punctuation">.</span>parameters<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span> lr<span class="token operator">=</span>self<span class="token punctuation">.</span>lr<span class="token punctuation">,</span> weight_decay<span class="token operator">=</span>self<span class="token punctuation">.</span>weight_decay        <span class="token punctuation">)</span>        scheduler <span class="token operator">=</span> MultiStepLR<span class="token punctuation">(</span>optimizer<span class="token punctuation">,</span> milestones<span class="token operator">=</span><span class="token punctuation">[</span><span class="token number">20</span><span class="token punctuation">]</span><span class="token punctuation">,</span> gamma<span class="token operator">=</span><span class="token number">0.1</span><span class="token punctuation">)</span>        <span class="token keyword">return</span> <span class="token punctuation">[</span>optimizer<span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token punctuation">[</span>scheduler<span class="token punctuation">]</span>    <span class="token keyword">elif</span> self<span class="token punctuation">.</span>dataset <span class="token operator">==</span> <span class="token string">"kitti"</span><span class="token punctuation">:</span>        optimizer <span class="token operator">=</span> torch<span class="token punctuation">.</span>optim<span class="token punctuation">.</span>AdamW<span class="token punctuation">(</span>            self<span class="token punctuation">.</span>parameters<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span> lr<span class="token operator">=</span>self<span class="token punctuation">.</span>lr<span class="token punctuation">,</span> weight_decay<span class="token operator">=</span>self<span class="token punctuation">.</span>weight_decay        <span class="token punctuation">)</span>        scheduler <span class="token operator">=</span> MultiStepLR<span class="token punctuation">(</span>optimizer<span class="token punctuation">,</span> milestones<span class="token operator">=</span><span class="token punctuation">[</span><span class="token number">20</span><span class="token punctuation">]</span><span class="token punctuation">,</span> gamma<span class="token operator">=</span><span class="token number">0.1</span><span class="token punctuation">)</span>        <span class="token keyword">return</span> <span class="token punctuation">[</span>optimizer<span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token punctuation">[</span>scheduler<span class="token punctuation">]</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>这个方法用于配置优化器和学习率调度器。根据数据集类型 (<code>self.dataset</code>)，它初始化一个AdamW优化器并将其与一个学习率调度器一起返回。调度器将在训练期间按照指定的里程碑来调整学习率。<br>这就是<code>MonoScene</code>类中的所有方法和逻辑的解释。该类代表了一个用于场景语义分割的PyTorch Lightning模型，它包括了许多用于定义前向传播、计算损失和跟踪性能的方法。</p><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">import</span> torch<span class="token keyword">import</span> torch<span class="token punctuation">.</span>nn <span class="token keyword">as</span> nn<span class="token keyword">class</span> <span class="token class-name">FLoSP</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> scene_size<span class="token punctuation">,</span> dataset<span class="token punctuation">,</span> project_scale<span class="token punctuation">)</span><span class="token punctuation">:</span>        <span class="token builtin">super</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>        self<span class="token punctuation">.</span>scene_size <span class="token operator">=</span> scene_size        self<span class="token punctuation">.</span>dataset <span class="token operator">=</span> dataset        self<span class="token punctuation">.</span>project_scale <span class="token operator">=</span> project_scale    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> x2d<span class="token punctuation">,</span> projected_pix<span class="token punctuation">,</span> fov_mask<span class="token punctuation">)</span><span class="token punctuation">:</span>        c<span class="token punctuation">,</span> h<span class="token punctuation">,</span> w <span class="token operator">=</span> x2d<span class="token punctuation">.</span>shape        src <span class="token operator">=</span> x2d<span class="token punctuation">.</span>view<span class="token punctuation">(</span>c<span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span>        zeros_vec <span class="token operator">=</span> torch<span class="token punctuation">.</span>zeros<span class="token punctuation">(</span>c<span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">.</span>type_as<span class="token punctuation">(</span>src<span class="token punctuation">)</span>        src <span class="token operator">=</span> torch<span class="token punctuation">.</span>cat<span class="token punctuation">(</span><span class="token punctuation">[</span>src<span class="token punctuation">,</span> zeros_vec<span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span>        pix_x<span class="token punctuation">,</span> pix_y <span class="token operator">=</span> projected_pix<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">,</span> projected_pix<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">]</span>        img_indices <span class="token operator">=</span> pix_y <span class="token operator">*</span> w <span class="token operator">+</span> pix_x        img_indices<span class="token punctuation">[</span><span class="token operator">~</span>fov_mask<span class="token punctuation">]</span> <span class="token operator">=</span> h <span class="token operator">*</span> w        img_indices <span class="token operator">=</span> img_indices<span class="token punctuation">.</span>expand<span class="token punctuation">(</span>c<span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">.</span><span class="token builtin">long</span><span class="token punctuation">(</span><span class="token punctuation">)</span>  <span class="token comment"># c, HWD</span>        src_feature <span class="token operator">=</span> torch<span class="token punctuation">.</span>gather<span class="token punctuation">(</span>src<span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> img_indices<span class="token punctuation">)</span>        <span class="token keyword">if</span> self<span class="token punctuation">.</span>dataset <span class="token operator">==</span> <span class="token string">"NYU"</span><span class="token punctuation">:</span>            x3d <span class="token operator">=</span> src_feature<span class="token punctuation">.</span>reshape<span class="token punctuation">(</span>                c<span class="token punctuation">,</span>                self<span class="token punctuation">.</span>scene_size<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span> <span class="token operator">//</span> self<span class="token punctuation">.</span>project_scale<span class="token punctuation">,</span>                self<span class="token punctuation">.</span>scene_size<span class="token punctuation">[</span><span class="token number">2</span><span class="token punctuation">]</span> <span class="token operator">//</span> self<span class="token punctuation">.</span>project_scale<span class="token punctuation">,</span>                self<span class="token punctuation">.</span>scene_size<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span> <span class="token operator">//</span> self<span class="token punctuation">.</span>project_scale<span class="token punctuation">,</span>            <span class="token punctuation">)</span>            x3d <span class="token operator">=</span> x3d<span class="token punctuation">.</span>permute<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">)</span>        <span class="token keyword">elif</span> self<span class="token punctuation">.</span>dataset <span class="token operator">==</span> <span class="token string">"kitti"</span><span class="token punctuation">:</span>            x3d <span class="token operator">=</span> src_feature<span class="token punctuation">.</span>reshape<span class="token punctuation">(</span>                c<span class="token punctuation">,</span>                self<span class="token punctuation">.</span>scene_size<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span> <span class="token operator">//</span> self<span class="token punctuation">.</span>project_scale<span class="token punctuation">,</span>                self<span class="token punctuation">.</span>scene_size<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span> <span class="token operator">//</span> self<span class="token punctuation">.</span>project_scale<span class="token punctuation">,</span>                self<span class="token punctuation">.</span>scene_size<span class="token punctuation">[</span><span class="token number">2</span><span class="token punctuation">]</span> <span class="token operator">//</span> self<span class="token punctuation">.</span>project_scale<span class="token punctuation">,</span>            <span class="token punctuation">)</span>        <span class="token keyword">return</span> x3d<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>这段代码定义了一个名为 <code>FLoSP</code> 的 PyTorch 模块，它用于执行特征投影操作。FLoSP 的作用是将一个 2D 图像中的特征投影到 3D 空间中。让我们逐行解释这个模块的功能和实现：</p><ol><li><p><code>class FLoSP(nn.Module):</code>：定义了一个名为 <code>FLoSP</code> 的 PyTorch 模块，它继承自 <code>nn.Module</code>。</p></li><li><p><code>def __init__(self, scene_size, dataset, project_scale):</code>：初始化方法，接受三个参数：</p><ul><li><code>scene_size</code>：表示 3D 场景的大小（宽度、高度、深度）。</li><li><code>dataset</code>：表示数据集的名称，可以是 “NYU” 或 “kitti”。</li><li><code>project_scale</code>：表示投影的尺度，通常是 2 的幂次方，用于将 2D 特征图中的像素投影到 3D 空间。</li></ul></li><li><p><code>self.scene_size = scene_size</code>：将输入的 <code>scene_size</code> 存储为对象属性，以便在模块的其他方法中使用。</p></li><li><p><code>self.dataset = dataset</code>：将输入的 <code>dataset</code> 存储为对象属性，以便在模块的其他方法中使用。</p></li><li><p><code>self.project_scale = project_scale</code>：将输入的 <code>project_scale</code> 存储为对象属性，以便在模块的其他方法中使用。</p></li><li><p><code>def forward(self, x2d, projected_pix, fov_mask):</code>：前向传播方法，用于执行特征投影操作。接受三个输入参数：</p><ul><li><code>x2d</code>：2D 特征图，是一个张量，其形状为 <code>(c, h, w)</code>，其中 <code>c</code> 表示通道数，<code>h</code> 和 <code>w</code> 表示高度和宽度。</li><li><code>projected_pix</code>：包含 2D 像素坐标的张量，其形状为 <code>(bs, 2)</code>，其中 <code>bs</code> 表示批处理大小。</li><li><code>fov_mask</code>：表示感兴趣区域的遮罩，是一个布尔值张量，形状与 <code>projected_pix</code> 相同。</li></ul></li><li><p><code>c, h, w = x2d.shape</code>：获取输入 2D 特征图 <code>x2d</code> 的通道数、高度和宽度。</p></li><li><p><code>src = x2d.view(c, -1)</code>：将 2D 特征图 <code>x2d</code> 重新形状为 <code>(c, h * w)</code>，即将每个像素的特征连接到一个向量中。</p></li><li><p><code>zeros_vec = torch.zeros(c, 1).type_as(src)</code>：创建一个与 <code>src</code> 相同数据类型的零向量，用于在特征向量后添加一个零值，以匹配 3D 坐标。</p></li><li><p><code>src = torch.cat([src, zeros_vec], 1)</code>：将零向量添加到特征向量的末尾，以将特征向量的维度从 <code>(c, h * w)</code> 扩展到 <code>(c, h * w + 1)</code>。</p></li><li><p><code>pix_x, pix_y = projected_pix[:, 0], projected_pix[:, 1]</code>：从 <code>projected_pix</code> 中提取 2D 像素坐标的 x 和 y 值。</p></li><li><p><code>img_indices = pix_y * w + pix_x</code>：计算像素坐标对应的在扁平化 2D 特征图中的索引。</p></li><li><p><code>img_indices[~fov_mask] = h * w</code>：将不在感兴趣区域内的像素坐标对应的索引设置为 2D 特征图的最大索引值（类似于超出图像范围的像素索引）。</p></li><li><p><code>img_indices = img_indices.expand(c, -1).long()</code>：将计算得到的索引扩展为 <code>(c, h * w)</code> 的长整型张量。</p></li><li><p><code>src_feature = torch.gather(src, 1, img_indices)</code>：使用计算得到的索引从 <code>src</code> 中聚合出感兴趣的像素特征。</p></li><li><p><code>if self.dataset == &quot;NYU&quot;:</code>：如果数据集是 “NYU”，则执行以下操作。否则，执行 “kitti” 分支。</p><p>a. <code>x3d = src_feature.reshape(c, self.scene_size[0] // self.project_scale, self.scene_size[2] // self.project_scale, self.scene_size[1] // self.project_scale)</code>：将特征投影到 3D 空间，根据输入的 <code>scene_size</code> 和 <code>project_scale</code> 进行重新形状。这一步是根据 NYU 数据集的特定情况进行的形状转换。</p><p>b. <code>x3d = x3d.permute(0, 1, 3, 2)</code>：对投影得到的 3D 特征进行维度的置换。</p></li><li><p><code>elif self.dataset == &quot;kitti&quot;:</code>：如果数据集是 “kitti”，则执行以下操作。</p><p>a. <code>x3d = src_feature.reshape(c, self.scene_size[0] // self.project_scale, self.scene_size[1] // self.project_scale, self.scene_size[2] // self.project_scale)</code>：将特征投影到 3D 空间，根据输入的 <code>scene_size</code> 和 <code>project_scale</code> 进行重新形状。这一步是根据 kitti 数据集的特定情况进行的形状转换。</p></li><li><p><code>return x3d</code>：返回投影到 3D 空间的特征张量 <code>x3d</code>。</p></li></ol><p>这个模块的主要功能是将 2D 特征图中的像素特征投影到 3D 空间中，以便后续在 3D 空间中进行深度学习任务。具体的投影方法和维度变换取决于数据集的类型（”NYU” 或 “kitti”）和投影尺度（<code>project_scale</code>）。这个模块通常用于处理语义分割和深度估计等任务。</p><p>unet3d_kitti.py是一个3D U-Net模型的定义，用于语义分割任务。以下是代码的逐行解释：</p><ol><li><p><code>UNet3D</code> 类继承自 <code>nn.Module</code>，用于定义3D U-Net模型。</p></li><li><p><code>__init__</code> 函数用于初始化模型，它接受一些参数，如类别数 <code>class_num</code>、标准化层 <code>norm_layer</code>、全尺寸 <code>full_scene_size</code>、特征数 <code>feature</code>、项目尺度 <code>project_scale</code>、上下文先验 <code>context_prior</code> 和 Batch Normalization 层的动量 <code>bn_momentum</code>。</p></li><li><p><code>size_l1</code>、<code>size_l2</code> 和 <code>size_l3</code> 分别表示3D U-Net的3个不同尺寸层。</p></li><li><p><code>dilations</code> 是用于处理图像的卷积核的膨胀率。</p></li><li><p><code>self.process_l1</code> 和 <code>self.process_l2</code> 定义了两个处理层，用于处理输入3D数据，包括一系列的卷积、标准化和下采样操作。</p></li><li><p><code>self.up_13_l2</code>、<code>self.up_12_l1</code> 和 <code>self.up_l1_lfull</code> 定义了上采样层，将3D数据上采样到不同尺寸的层。</p></li><li><p><code>self.ssc_head</code> 定义了用于预测语义分割的头部，包括一系列卷积和输出层。</p></li><li><p><code>self.context_prior</code> 用于确定是否使用上下文先验。如果 <code>context_prior</code> 为真，将定义 <code>self.CP_mega_voxels</code>，它是上下文先验处理的一部分。</p></li><li><p><code>forward</code> 函数接受输入数据的字典 <code>input_dict</code>，包括 <code>x3d</code>，表示3D数据。</p></li><li><p>通过一系列处理步骤，如 <code>self.process_l1</code> 和 <code>self.process_l2</code>，输入数据被处理和下采样到不同尺寸层。</p></li><li><p>如果存在上下文先验，将使用 <code>self.CP_mega_voxels</code> 处理数据。</p></li><li><p>使用上采样层 <code>self.up_13_l2</code> 和 <code>self.up_12_l1</code> 将数据上采样到不同尺寸的层。</p></li><li><p>最终，通过 <code>self.ssc_head</code> 进行语义分割预测，并将结果存储在 <code>res</code> 字典中。</p></li></ol><p>这个模型的核心部分是3D U-Net结构，它包含编码器和解码器部分，用于从3D输入数据中提取特征并生成语义分割结果。根据输入数据的尺寸和具体任务，该模型可以适应不同的上下文先验处理。</p><h2 id="StereoScene"><a href="#StereoScene" class="headerlink" title="StereoScene"></a>StereoScene</h2><p>StereoScene用于3D语义场景补全（SSC），它充分利用了轻量级相机输入而无需使用任何外部3D传感器。研究者的关键想法是利用立体匹配来解决几何模糊性问题。为了提高其在不匹配区域的鲁棒性，论文引入了鸟瞰图（BEV）表示法，以激发具有丰富上下文信息的未知区域预测能力。在立体匹配和BEV表示法的基础上，论文精心设计了一个相互交互聚合（MIA）模块，以充分发挥两者的作用，促进它们互补聚合</p><p>代码链接：<a href="https://github.com/Arlo0o/StereoScene">StereoScene: BEV-Assisted Stereo Matching Empowers 3D Semantic Scene Completion</a>, arXiv 2023.</p><p><img src="/pic/StereoScene.png"></p><p>整体的StereoScene框架如上图所示。论文遵循使用连续的2D和3 UNetsf作为backbones。给定输入的双目图像，我们使用2D UNet来提取多尺度特征。BEV潜在体积和立体几何体积分别由BEV构造器和立体构造器构造。为了充分利用这两个卷的互补潜力，提出了一个相互交互聚合模块来相互引导和聚合它们。</p><h3 id="代码解释"><a href="#代码解释" class="headerlink" title="代码解释"></a>代码解释</h3><p><a href="https://github.com/Arlo0o/StereoScene/blob/main/projects/configs/occupancy/semantickitti/stereoscene.py">stereoscene.py</a>代码是一个配置文件，它定义了一个基于 MMDetection3D 的三维计算机视觉模型的配置。以下是代码中各部分的解释：</p><ol><li><p><code>_base_</code>：这是导入配置的基础文件的地方，<code>custom_nus-3d.py</code> 和 <code>default_runtime.py</code> 包含了一些共享的配置选项。这些基础配置文件通常包括数据集设置、运行时设置等。</p></li><li><p><code>plugin</code> 和 <code>plugin_dir</code>：这两个参数用于启用和指定 MMDetection3D 插件。插件是额外的功能模块，可以扩展 MMDetection3D 的功能。</p></li><li><p><code>img_norm_cfg</code>：这是图像归一化配置，指定了均值和标准差，以便对图像进行归一化。这通常用于图像预处理。</p></li><li><p><code>class_names</code>：定义了数据集中的类别名称。在这个例子中，有 20 个类别，包括车辆、行人、道路、建筑物等。</p></li><li><p><code>point_cloud_range</code>、<code>occ_size</code> 和 <code>lss_downsample</code>：这些参数定义了点云数据的范围、体素的大小和降采样率。它们是用于处理点云数据的重要参数。</p></li><li><p><code>model</code>：这是定义模型的部分。它指定了模型的各个组件，包括图像骨干网络、头部网络、点云处理头部等。这里使用的是 <code>BEVDepthOccupancy</code> 模型，该模型在三维计算机视觉中执行深度估计和语义分割任务。</p></li><li><p><code>dataset_type</code> 和 <code>data_root</code>：这些参数定义了数据集的类型和数据集的根目录。在这个例子中，使用的是 <code>CustomSemanticKITTILssDataset</code> 数据集，数据位于 <code>./data/occupancy/semanticKITTI/RGB/</code>。</p></li><li><p><code>train_pipeline</code> 和 <code>test_pipeline</code>：这些参数定义了数据预处理和增强的步骤。它们包括图像加载、语义分割标签加载、深度图生成等。</p></li><li><p><code>input_modality</code>：定义了输入的模态，包括激光雷达、相机、雷达、地图等。在这个例子中，仅使用相机数据。</p></li><li><p><code>test_config</code> 和 <code>data</code>：这些参数定义了训练和测试数据加载的配置，包括数据预处理、数据集类型、类别、点云范围等。</p></li><li><p><code>optimizer</code> 和 <code>optimizer_config</code>：这些参数定义了优化器的类型、学习率和权重衰减等配置。</p></li><li><p><code>lr_config</code>：定义了学习率调度策略，这里使用的是阶段性学习率下降。</p></li><li><p><code>checkpoint_config</code>：用于定义模型检查点的保存和保留策略。</p></li><li><p><code>runner</code>：指定了训练的运行方式，这里使用的是按照 epoch 训练。</p></li><li><p><code>evaluation</code>：定义了模型评估的配置，包括评估间隔和保存最佳模型的规则。</p></li></ol><p>这个配置文件的目的是定义了一个用于深度估计和语义分割的三维计算机视觉模型，并指定了数据加载和训练配置。模型的结构和数据集的具体细节在其他文件中定义。这个配置文件会被传递给 MMDetection3D 的训练和测试脚本，以实际执行训练和测试任务。</p><p>这个配置文件中定义了一个名为<code>model</code>的模型，其结构和配置包括以下几个主要组件：</p><ol><li><p><code>type</code>: 这里指定了模型的类型为<code>BEVDepthOccupancy</code>，这是一个自定义的三维计算机视觉模型，用于深度估计和语义分割任务。</p></li><li><p><code>img_backbone</code>: 定义了图像骨干网络的配置，该网络用于提取图像特征。在这个配置中，使用了<code>CustomEfficientNet</code>骨干网络，具体配置包括网络的类型、预训练模型的路径等。</p></li><li><p><code>img_neck</code>: 这部分定义了图像特征的“颈部”或附加处理，用于进一步处理骨干网络提取的特征。这里使用了<code>SECONDFPN</code>结构，对特征进行了上采样和融合。</p></li><li><p><code>img_view_transformer</code>: 这是一个自定义的视图变换器，用于将图像特征映射到点云视图，包括视角的转换和体素化。</p></li><li><p><code>img_bev_encoder_backbone</code> 和 <code>img_bev_encoder_neck</code>: 定义了用于处理点云的背景信息的网络。<code>img_bev_encoder_backbone</code>用于提取点云的背景特征，<code>img_bev_encoder_neck</code>用于进一步处理这些特征。</p></li><li><p><code>pts_bbox_head</code>: 这是点云处理头部的配置，用于执行深度估计和语义分割任务。它包括输出通道数、语义分割类别数、点云范围等配置。</p></li><li><p><code>train_cfg</code> 和 <code>test_cfg</code>: 分别定义了模型的训练和测试配置，这些配置包括损失函数、评价指标等。</p></li></ol><p>这个<code>model</code>定义了整个三维计算机视觉模型的结构，包括图像骨干网络、点云处理头部等组件。模型将通过这些组件来提取和处理图像和点云数据，以执行深度估计和语义分割任务。配置文件中还包括数据加载和训练策略等，用于训练和测试这个模型。</p><p>这个<code>model</code>配置是在使用MMDetection框架时的一种标准格式。MMDetection使用Python的字典格式来组织模型的配置。下面是对这个<code>model</code>配置的格式解释：</p><ol><li><p><code>type</code>: 这个字段指定了要使用的模型的类型。在这个配置中，<code>type</code>的值是<code>BEVDepthOccupancy</code>，表示要使用名为<code>BEVDepthOccupancy</code>的自定义模型。</p></li><li><p><code>img_backbone</code>, <code>img_neck</code>, <code>img_view_transformer</code>, <code>img_bev_encoder_backbone</code>, <code>img_bev_encoder_neck</code>, <code>pts_bbox_head</code>: 这些字段用于配置模型中不同组件的设置。每个组件的配置包含了该组件的类型、参数、超参数等。</p></li><li><p><code>train_cfg</code> 和 <code>test_cfg</code>: 这些字段定义了模型的训练和测试配置，包括损失函数、评价指标等。这些配置用于指导训练和测试过程。</p></li></ol><p>整个<code>model</code>配置是一个嵌套的字典，用于描述模型的结构和超参数设置。在MMDetection框架中，模型的配置采用了类似于YAML格式的字典结构，以便灵活配置不同的模型和任务。这种配置方式使得用户可以根据自己的需求轻松地定制和修改模型的设置。</p><p><a href="https://github.com/Arlo0o/StereoScene/blob/main/projects/mmdet3d_plugin/occupancy/detectors/bevdepth_occupancy.py">bevdepth_occupancy.py</a>在上述代码中，<code>model</code> 的配置与类 <code>BEVDepthOccupancy</code> 相关，特别是与 <code>BEVDepthOccupancy</code> 类中的不同函数和组件有关。</p><p><code>BEVDepthOccupancy</code> 类是一个自定义的模型类，它继承自 <code>BEVDepth</code> 类。以下是与 <code>model</code> 配置中的组件相关的一些重要功能和配置项：</p><ol><li><p><code>image_encoder</code> 和 <code>bev_encoder</code> 函数：这些函数是模型的核心组件，分别用于处理图像信息和点云信息。<code>image_encoder</code> 用于处理输入的图像信息，而 <code>bev_encoder</code> 用于处理点云信息。这些函数将输入数据转换为特征表示。</p></li><li><p><code>forward_pts_train</code> 函数：这个函数在模型的训练过程中使用，计算了与点云相关的训练损失。这包括点云的目标检测和语义分割任务。</p></li><li><p><code>extract_img_feat</code> 函数：这个函数用于从输入的图像中提取特征。它将输入的图像分别送入 <code>image_encoder</code> 和 <code>img_view_transformer</code> 中，然后将它们的输出特征合并。</p></li><li><p><code>simple_test</code> 函数：这个函数用于在测试阶段生成模型的输出。它调用了 <code>extract_feat</code> 函数来提取特征，然后将这些特征传递给点云检测头部 <code>pts_bbox_head</code>，最后生成了测试结果。</p></li><li><p>其他与损失函数、数据预处理、评估等相关的函数：代码中还包括了与训练和测试相关的其他函数，用于处理损失函数的计算、数据的预处理以及评估模型性能等任务。</p></li></ol><p>总的来说，<code>model</code> 配置中的组件是 <code>BEVDepthOccupancy</code> 类中的一部分，用于定义模型的结构和训练&#x2F;测试过程中的操作。这些组件协同工作以实现点云目标检测和语义分割的任务。不同的组件负责处理不同类型的输入数据（图像、点云等）并生成相应的特征表示，最终用于模型的训练和测试。这种模块化的设计使得模型可以方便地扩展和配置，以适应不同的应用场景。</p><p><a href="https://github.com/Arlo0o/StereoScene/blob/main/projects/mmdet3d_plugin/occupancy/dense_heads/occhead.py">occhead.py</a>这是一个名为<code>OccHead</code>的模型头，主要用于3D语义分割任务。以下是对该头部的关键要点：</p><ol><li><p><strong>输入和输出</strong>:</p><ul><li>输入：<code>OccHead</code>头部的输入包括体素特征（<code>voxel_feats</code>）以及点云信息（<code>points</code>）。</li><li>输出：它的输出通常有两部分，一部分是用于体素级别的语义分割（<code>output_voxels</code>），另一部分是用于点云级别的语义分割（<code>output_points</code>）。</li></ul></li><li><p><strong>监督方式</strong>:</p><ul><li>该头部支持两种不同级别的监督方式：体素级别和点云级别。</li><li>体素级别监督是指根据体素特征生成语义分割结果，用于离散的体素数据。</li><li>点云级别监督是指根据点云信息生成语义分割结果，用于稠密点云数据。</li></ul></li><li><p><strong>损失函数</strong>:</p><ul><li>对于体素级别监督，支持交叉熵（<code>voxel_ce</code>）、Lovasz-Softmax（<code>voxel_lovasz</code>）、Voxel Semantic Scaling（<code>voxel_sem_scal</code>）、Voxel Geometric Scaling（<code>voxel_geo_scal</code>）、IoU损失（<code>voxel_dice</code>）等损失函数。</li><li>对于点云级别监督，支持交叉熵（<code>point_ce</code>）和Lovasz-Softmax（<code>point_lovasz</code>）损失函数。</li></ul></li><li><p><strong>学习策略</strong>:</p><ul><li>对于体素级别监督，通过卷积操作生成语义分割结果。</li><li>对于点云级别监督，采样相关的体素特征和图像特征，然后使用多层感知机（Mlp）处理这些特征生成点云级别的语义分割结果。</li></ul></li><li><p><strong>语义Kitti</strong>:</p><ul><li>该模型头部支持语义Kitti数据集，并提供了相关的损失函数和度量指标。</li></ul></li><li><p><strong>损失权重</strong>:</p><ul><li>您可以设置不同损失函数的权重，以控制它们对最终损失的贡献。</li></ul></li><li><p><strong>可选功能</strong>:</p><ul><li>该模型头部支持不同的可选功能，如软权重（<code>soft_weights</code>）和图像特征采样（<code>sampling_img_feats</code>）。</li></ul></li><li><p><strong>其他</strong>:</p><ul><li>还包括一些图像处理操作，如上采样和网格采样。</li></ul></li></ol><p>总之，<code>OccHead</code>是一个用于语义分割任务的多功能头部，支持体素级别和点云级别的监督，提供了多种损失函数和度量指标以用于不同的数据集和任务。这些选择和配置可以根据具体的任务需求进行调整和优化。</p>]]></content>
      
      
      <categories>
          
          <category> DL </category>
          
      </categories>
      
      
        <tags>
            
            <tag> ssc </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>语义场景补全（SSC）</title>
      <link href="/2023/10/29/ssc/"/>
      <url>/2023/10/29/ssc/</url>
      
        <content type="html"><![CDATA[<h1 id="语义场景补全（SSC）相关文章与数据集总结"><a href="#语义场景补全（SSC）相关文章与数据集总结" class="headerlink" title="语义场景补全（SSC）相关文章与数据集总结"></a>语义场景补全（SSC）相关文章与数据集总结</h1><p><em><strong>首先附上链接</strong></em></p><h2 id="SSC"><a href="#SSC" class="headerlink" title="SSC"></a>SSC</h2><ul><li><a href="https://github.com/Jiawei-Yao0812/NDCScene">NDC-Scene: Boost Monocular 3D Semantic Scene Completion in Normalized Device Coordinates Space</a>, ICCV 2023.</li><li><a href="https://arxiv.org/abs/2307.05873">OG: Equip vision occupancy with instance segmentation and visual grounding</a>, arXiv 2023.</li><li><a href="https://github.com/NVlabs/FB-BEV">FB-OCC: 3D Occupancy Prediction based on Forward-Backward View Transformation</a>, CVPRW 2023.</li><li><a href="https://github.com/hustvl/Symphonies">Symphonize 3D Semantic Scene Completion with Contextual Instance Queries</a>, arXiv 2023.</li><li><a href="https://arxiv.org/pdf/2305.16133.pdf">OVO: Open-Vocabulary Occupancy</a>, arXiv 2023.</li><li><a href="https://github.com/opendrivelab/occnet">OccNet: Scene as Occupancy</a>, ICCV 2023.</li><li><a href="https://astra-vision.github.io/SceneRF/">SceneRF: Self-Supervised Monocular 3D Scene Reconstruction with Radiance Fields</a>, ICCV 2023.</li><li><a href="https://fwmb.github.io/bts/">Behind the Scenes: Density Fields for Single View Reconstruction</a>, CVPR 2023.</li><li><a href="https://github.com/shurans/sscnet">Semantic Scene Completion from a Single Depth Image</a>, CVPR 2017</li><li><a href="https://github.com/astra-vision/LMSCNet">LMSCNet: Lightweight Multiscale 3D Semantic Completion</a>, 3DV 2020</li><li><a href="https://github.com/astra-vision/MonoScene">MonoScene: Monocular 3D Semantic Scene Completion</a>, CVPR 2022</li><li><a href="https://github.com/Arlo0o/StereoScene">StereoScene: BEV-Assisted Stereo Matching Empowers 3D Semantic Scene Completion</a>, arXiv 2023.</li><li><a href="https://github.com/megvii-research/OccDepth">OccDepth: A Depth-aware Method for 3D Semantic Occupancy Network</a>, arXiv 2023.</li><li><a href="https://github.com/NVlabs/VoxFormer">VoxFormer: a Cutting-edge Baseline for 3D Semantic Occupancy Prediction</a>, CVPR 2023</li><li><a href="https://github.com/wzzheng/TPVFormer">TPVFormer: An academic alternative to Tesla’s Occupancy Network</a>, CVPR2023</li><li><a href="https://github.com/zhangyp15/OccFormer">OccFormer: Dual-path Transformer for Vision-based 3D Semantic Occupancy Prediction</a>, ICCV 2023</li><li><a href="https://github.com/weiyithu/SurroundOcc">SurroundOcc: Multi-Camera 3D Occupancy Prediction for Autonomous Driving</a>, ICCV 2023</li><li><a href="https://ahayler.github.io/publications/s4c/">S4C: Self-Supervised Semantic Scene Completion with Neural Fields</a>, arXiv 2023</li><li><a href="https://arxiv.org/abs/2306.10013">PanoOcc: Unified Occupancy Representation for Camera-based 3D Panoptic Segmentation</a>, arXiv 2023.</li><li><a href="https://github.com/wzzheng/PointOcc">PointOcc: Cylindrical Tri-Perspective View for Point-based 3D Semantic Occupancy Prediction</a>, arXiv 2023.</li><li><a href="https://arxiv.org/abs/2309.09502">RenderOcc: Vision-Centric 3D Occupancy Prediction with 2D Rendering Supervision</a>, arXiv 2023.</li></ul><h2 id="相关-Dataset-x2F-Benchmark"><a href="#相关-Dataset-x2F-Benchmark" class="headerlink" title="相关 Dataset&#x2F;Benchmark"></a>相关 Dataset&#x2F;Benchmark</h2><ul><li><a href="https://arxiv.org/abs/2309.12708">PointSSC: A Cooperative Vehicle-Infrastructure Point Cloud Benchmark for Semantic Scene Completion</a>, arXiv 2023.</li><li><a href="https://github.com/Tsinghua-MARS-Lab/Occ3D">Occ3D: A Large-Scale 3D Occupancy Prediction Benchmark for Autonomous Driving</a>, arXiv 2023</li><li><a href="https://github.com/JeffWang987/OpenOccupancy">OpenOccupancy: A Large Scale Benchmark for Surrounding Semantic Occupancy Perception</a>, ICCV 2023</li><li><a href="https://github.com/ai4ce/Occ4cast/">Occ4cast: LiDAR-based 4D Occupancy Completion and Forecasting</a>, arXiv 2023.</li><li><a href="https://github.com/opendrivelab/occnet">OccNet: Scene as Occupancy</a>, ICCV 2023.</li><li><a href="https://github.com/ai4ce/SSCBench">SSCBench: A Large-Scale 3D Semantic Scene Completion Benchmark for Autonomous Driving</a>, arXiv 2023.</li></ul><p><em><strong>我们从SSCBench数据集开始介绍起</strong></em></p><h2 id="SSCBench-A-Large-Scale-3D-Semantic-Scene-Completion-Benchmark-for-Autonomous-Driving"><a href="#SSCBench-A-Large-Scale-3D-Semantic-Scene-Completion-Benchmark-for-Autonomous-Driving" class="headerlink" title="SSCBench: A Large-Scale 3D Semantic Scene Completion Benchmark for Autonomous Driving"></a>SSCBench: A Large-Scale 3D Semantic Scene Completion Benchmark for Autonomous Driving</h2><p>SSCBench提供的数据集格式与SemanticKITTI兼容，这是一个综合了 KITTI-360 、nuScenes和Waymo 中的场景的全面基准，总体而言，我们的SSCBench由三个子集组成，包括38562帧用于训练，15798帧用于验证，12553帧用于测试，总计66913帧（～67K），大大超过了上述SemanticKITTI的规模～7.7倍。SSCBench 便于在各种实际场景中轻松探索基于摄像头和LiDAR的SSC。</p><h3 id="相关工作"><a href="#相关工作" class="headerlink" title="相关工作"></a>相关工作</h3><ul><li><strong>单眼感知和3D语义场景完成</strong>。单眼相机的简单性、效率、可负担性和可访问性使单眼感知成为视觉和机器人界关注的焦点。SSCNet（2017）引入了单目3D语义场景完成（SSC）的概念，该概念旨在从单个深度图像重建和完成3D体积内的语义和几何结构。然而，由于缺乏室外数据集，他们只考虑有界的室内场景。Semantickitti（2019）构建了第一个基于KITTI的户外数据集，用于街景中的3D语义场景完成。现有的方法通常依赖于3D输入，如激光雷达点云（Lmscnet，2020；S3CNet，2021a；JS3C-Net，2021），而最近的基于单目视觉的解决方案也出现了（Monoscene，2022；Voxformer，2023），双目视觉的解决方案也出现了（OccDepth，2021）。然而，户外SSC的发展受到数据集缺乏的阻碍，SemanticKITTI（Behley et al.，2019）是唯一支持街景SSC的数据集。构建多样化的数据集对于释放SSC在自主系统中的全部潜力至关重要。</li><li><strong>街景中的点云分割</strong>。3D激光雷达分割旨在为点云分配逐点语义标签，在这一领域，源于PointNet++（2017）的基于点的方法在小型合成点云上表现良好。基于体素的方法通过最初通过笛卡尔坐标将3D空间划分为体素来处理点云。注意，3D激光雷达分割旨在基于原始激光雷达扫描对场景进行分类和理解，而3D语义场景完成包括在相机或激光雷达的输入下完成遮挡区域。</li><li><strong>自动驾驶数据集和基准</strong>。自动驾驶研究在高质量数据集上蓬勃发展，这些数据集是训练和评估感知、预测和规划算法的生命线。2012年，开创性的KITTI数据集引发了自动驾驶研究的一场革命，开启了包括物体检测、跟踪、映射和光学&#x2F;深度估计在内的多项任务。从那时起，研究界接受了这一挑战，产生了丰富的数据集。这些数据集通过应对多模式融合、多任务学习、恶劣天气、协同驾驶、重复驾驶，以及密集交通场景等。有几个有影响力且广泛使用的驾驶数据集，如KITTI-360（2022）、nuScenes（2020）和Waymo（2017）。它们提供了激光雷达和相机记录以及点云语义和边界注释，因此，我们可以通过聚合多个语义点云并利用3D框来处理动态对象，为SSC创建准确的地面实况标签。</li></ul><h3 id="介绍SSCNet"><a href="#介绍SSCNet" class="headerlink" title="介绍SSCNet"></a>介绍SSCNet</h3><p>本文的重点是语义场景补全，这是一个任务，从单视图深度地图观察生成一个完整的三维体素表示的体积占用和语义标签的场景。之前的工作分别考虑了场景补全和深度地图的语义标记。然而，我们注意到这两个问题是紧密联系在一起的。为了利用这两个任务的耦合特性，我们引入了语义场景完成网络(SSCNet)，<strong>这是一个端到端3D卷积网络，以单个深度图像作为输入，并同时输出相机视图截锥中所有体素的占用率和语义标签</strong>。我们的网络使用了一个基于扩张的3D上下文模块，以有效地扩展接受域，使3D上下文学习成为可能。为了训练我们的网络，我们构建了SUNCG——一个人工创建的大规模合成3D场景数据集，包含密集的体积标注。我们的实验表明，联合模型在语义场景完成任务方面优于单独处理每个任务的方法和替代方法。数据集、代码和经过训练的模型将在接受后在线提供。</p><p><img src="/pic/SSCNet.png"></p><p>语义场景补全网络（SSCNet），SSCNet [36] 是第一个将语义分割和场景完成与 3D CNN 端到端结合的工作。这是一种端到端的3D卷积网络，它以单个深度图像为输入，同时输出相机视图截头体中所有<em>体素的占用和语义标签</em>。网络使用基于扩张的3D上下文模块来有效地扩展感受野并实现3D上下文学习。</p><h5 id="数据编码（第3-1节）、网络架构（第3-2节）和训练数据生成（第4节）。"><a href="#数据编码（第3-1节）、网络架构（第3-2节）和训练数据生成（第4节）。" class="headerlink" title="数据编码（第3.1节）、网络架构（第3.2节）和训练数据生成（第4节）。"></a>数据编码（第3.1节）、网络架构（第3.2节）和训练数据生成（第4节）。</h5><ul><li><strong>数据编码</strong>。采用截断有符号距离函数（TSDF）对三维空间进行编码，其中每个体素都存储到其最近表面的距离值d，该值的符号表示体素是在自由空间中还是在遮挡空间中。对标准TSDF进行了以下修改：（1）消除视图相关性：选择计算到整个观测表面上任何地方最近点的距离。（2）消除空位中的强梯度,使用flipped TSDF(翻转的TSDF)</li><li><strong>网络架构</strong>。网络以高分辨率三维体积为输入，首先使用几个三维卷积层来学习局部几何表示。我们使用带有步长和池化层的卷积层，将分辨率降低到原始输入的四分之一。然后，我们使用基于膨胀的3D上下文模块来捕获更高级别的对象间上下文信息。之后，来自不同尺度的网络响应被连接并馈送到另外两个卷积层中，以聚合来自多个尺度的信息。最后，使用体素方向的softmax层来预测最终的体素标签。</li><li><strong>训练数据生成</strong>。SUNCG：一个大型合成场景数据集。SUNCG数据集包含45622个不同的场景，这些场景具有通过Planner5D平台手动创建的逼真的房间和家具布局[25]。有49884个有效楼层，包含404058个房间和5697217个对象实例，这些实例来自2644个覆盖84个类别的唯一对象网格。为了生成模拟典型图像捕获过程的合成深度图，我们使用一组简单的启发式方法来拾取相机视点。给定一个3D场景，我们从地板上间隔1米且不被物体占据的位置的统一网格开始。然后，我们根据NYU Depth v2数据集的分布选择相机姿势。1然后，我们使用Kinect的内部特性和分辨率渲染深度图。之后，我们使用一组简单的启发式方法来排除不好的观点。SUNCG数据集中的3D场景由有限数量的对象实例组成，我们通过首先对库中的每个单独对象进行体素化，然后根据每个场景配置和视点变换标签来加快体素化过程</li></ul><p><img src="/pic/SSCNet2.png"></p><p>上图显示<strong>表面(a)的不同编码</strong>。投影TSDF (b)是根据相机计算的，因此是视景相关的。准确的TSDF (c)具有较少的视图依赖性，但在沿着遮挡边界的空空间中显示出强烈的梯度(用灰色圈出)。相反，翻转TSDF (d)在近地表有最强的梯度。</p><p><img src="/pic/SSCNet2.png"></p><p>上图显示<strong>合成训练数据</strong>。我们收集了一个大规模的合成三维场景数据集来训练我们的网络。对于每个3D场景，我们选择一组摄像机位置，并生成成对的渲染深度图像和体积地面真实作为训练示例。</p><h3 id="介绍LMSCNet"><a href="#介绍LMSCNet" class="headerlink" title="介绍LMSCNet"></a>介绍LMSCNet</h3><p>一种从体素化稀疏三维激光雷达扫描中完成多尺度三维语义场景的新方法。与文献相反，我们使用具有全面多尺度跳跃连接的2D UNet主干来增强特征流，以及3D分割头。在SemanticKITTI基准测试中，与所有其他已发布的方法相比，我们的方法在语义完成方面表现相当，在占用完成方面表现更好，同时明显更轻、更快。</p><h4 id="方法"><a href="#方法" class="headerlink" title="方法"></a>方法</h4><p>我们解决密集 3D 语义完成的问题，其中的任务是为每个单独的体素分配语义标签。给定稀疏 3D 体素网格，目标是预测 3D 语义场景表示，其中每个体素被分配一个语义标签 C &#x3D; [c0, c1, … 。 。 , cN]，其中 N 是语义类别的数量，c0 代表自由体素。我们的架构称为 LMSCNet，如下图所示，使用轻量级 UNet 风格架构来预测多个尺度的 3D 语义完成，允许快速粗略推理，有利于移动机器人应用。我们主要沿高度轴使用 2D 卷积，而不是贪婪的 3D 卷积；类似于鸟瞰图。下面我们详细介绍我们的定制轻量级 2D&#x2F;3D 架构、多尺度重建和整体训练流程</p><p><img src="/pic/LMSCNet.png" alt="&quot;LMSCNet：轻量级多尺度语义完成网络。我们的管道使用具有 2D 主干卷积（蓝色）和 3D 分割头（灰色）的 UNet 架构来执行不同尺度的 3D 语义分割和完成，同时保持低复杂性。卷积参数显示为：（滤波器数量、内核大小和步幅）。请注意，我们有意降低 2D 特征维度并使用 Atrous 3D 卷积（来自 [26] 的 ASPP 块）来保持较低的推理复杂度。&quot;"></p><h3 id="介绍S3CNet"><a href="#介绍S3CNet" class="headerlink" title="介绍S3CNet"></a>介绍S3CNet</h3><p>主要贡献如下：（a）一种基于稀疏张量的神经网络架构，该架构能够有效地从稀疏的三维点云数据中学习特征，并联合解决耦合的场景完成和语义分割问题；（b） 一种新颖的几何感知三维张量分割损失；（c） 一种多视图融合和语义后处理策略，解决了远距离或遮挡区域和小尺寸对象的挑战。给定单个稀疏点云帧，我们的模型预测了一个密集的3D占用长方体，其中为每个体素单元分配了语义标签（如图1所示），生成了原始输入中不包含的3D环境的丰富信息，如激光雷达扫描之间的间隙、遮挡区域和未来场景。</p><p>利用逐点法向量作为几何特征编码来指导我们的模型根据对象的局部曲面凸性来填充间隙。我们还利用从球面范围图像计算的基于LiDAR的翻转截断有符号距离函数（fTSDF[5]）作为空间编码，来区分场景的自由空间、占用空间和遮挡空间。至于未来的场景，由于这些区域远离车辆，主要是道路或其他形式的地形，我们提出了稀疏语义场景完成网络的2D变体，以支持通过与鸟瞰图（BEV）语义图预测的多视图融合来构建3D场景。为了解决稀疏性，我们利用Minkowski引擎[6]，一个用于稀疏张量的自动微分库来构建我们的2D和3D语义场景完成网络。我们还采用了组合的几何启发语义分割损失来提高语义标签预测的准确性。</p><p><img src="/pic/S3CNet.png"><br>整个系统管道如图所示。从一次激光雷达扫描中，我们构建了两个稀疏张量，将场景封装为内存高效的2D和3D表示。每个张量通过其对应的语义场景完成网络，2D S3CNet或3D S3CNet，以在相应维度上语义地完成场景。我们提出了一种动态体素融合方法，用预测的2D语义BEV图进一步加密重建场景（详细讨论见第3.3节）。这抵消了对3D网络的显著内存需求——3D空间中的指数稀疏性增长使在一定范围内完成类变得困难。使用稀疏张量空间传播网络[30]，我们细化融合2D-3D预测的噪声区域中的语义标签。</p><h3 id="介绍JS3C-Net"><a href="#介绍JS3C-Net" class="headerlink" title="介绍JS3C-Net"></a>介绍JS3C-Net</h3><p>提出了一种增强的联合单扫LiDAR点云语义分割方法，该方法利用学习的形状先验形式场景完成网络，即JS3C-Net。具体来说，通过在LiDAR序列中合并数十个连续帧，在没有额外标注的情况下，实现了一个大的完整点云作为语义场景完成(SSC)任务的地面真相。利用这些标注优化的SSC可以捕获引人注意的形状先验，使不完整的输入完整到带有语义标签的可接受形状(Song et al. 2017)。因此，完全端到端的训练策略使得完成的形状先验在本质上有利于语义分割(SS)。进一步提出了一个设计良好的点体素交互(PVI)模块，用于SS和SSC任务之间的隐式相互知识融合。具体来说，通过PVI模块，利用逐点分割和逐体补全来维护粗全局结构和细粒度局部几何。更重要的是，我们设计的SSC和PVI模块是一次性的。为了实现这一点，JS3C-Net以级联的方式将SS和SSC结合在一起，这意味着它不会影响SS的信息流，同时在推理阶段丢弃SSC和PVI模块。因此，它可以避免生成完整的高分辨率密集体而带来额外的计算负担。</p><p><img src="/pic/JS3CNet.png"></p><p>JS3C Net的总体管道。在给定稀疏不完全单扫描点云的情况下，首先使用稀疏卷积U-Net对Fenc进行点特征编码。基于初始编码，MLP1用于生成形状嵌入（SE）FSE，该FSE与通过MLP2传递的初始编码一起流入MLP3，以生成用于点云语义分割的Fout。然后，来自SE的不完整细粒度点特征和来自语义场景完成（SSC）模块的完整体素特征流入点-体素交互（PVI）模块，以实现细化特征，最终在监督下输出完成体素。请注意，SSC和PVI模块可以在推理过程中丢弃。</p><p><img src="/pic/JS3CNet2.png"></p><p>上图 (a)部分显示了SSC模块的内部结构，该模块以分割网络中的语义概率为输入，通过多个卷积块和密集的上样本生成完整体积。(b)部分演示了PVI模块的一个二维实例，该实例利用数字“5”的粗全局结构的中心点，从原始点云中查询k个最近邻，然后应用基于图的聚合，通过细粒度的局部几何实现完整的“5”。</p><h3 id="MonoScene介绍"><a href="#MonoScene介绍" class="headerlink" title="MonoScene介绍"></a>MonoScene介绍</h3><p>一种能在室内与室外场景均可使用的单目 SSC 方案：</p><ul><li>提出一种将 2D 特征投影到 3D 的方法： FLoSP </li><li>提出一种  3D Context Relation Prior （3D CRP） 提升网络的上下文意识</li><li>新的SSC损失函数，用于优化场景类亲和力（affinity）和局部锥体（frustum）比例</li></ul><p><img src="/pic/MonoScence.png"></p><p>Monoscene  网络流程</p><ul><li>输入 2d 图片，经过 2d unet 提取多层次的特征</li><li>Features Line of Sight Projection module (FLoSP) 用于将 2d 特征提升到 3d 位置上，增强信息流并实现2D-3D分离</li><li>3D Context Relation Prior （3D CRP） 用于增强长距离的上下文信息提取</li><li>loss 优化：Scene-Class Affinity Loss：提升类内和类间的场景方面度量；Frustum Proportion Loss：对齐局部截头体中的类分布，提供超越场景遮挡的监督信息</li></ul><p>网络结构</p><ul><li>2D unet：EfficientNetB7 用于提取图像特征</li><li>3D UNets：2层 encoder-decoder 结构，用于提取 3d 特征</li><li>completion head：3D ASPP 结构和 softmax 层，用于处理 3D UNet 输出得到3d场景 completion 结果</li></ul><p><strong>Features Line of Sight Projection (FLoSP)</strong><br>从3维网络处理后者将为来自2维特征的集合提供线索。整个过程如下图所示，实际上假设已知相机内参，将3维体素中心投影到2维，从2维的解码器特征图采样得到对应特征，在所有尺度集合上进行重复，最终的3维特征图可用如下表示：</p><p><img src="/pic/MonoScence2.png"></p><h3 id="StereoScene介绍"><a href="#StereoScene介绍" class="headerlink" title="StereoScene介绍"></a>StereoScene介绍</h3><p><strong>3D语义场景补全（SSC）是一个需要从不完整观测中推断出密集3D场景的不适定问题。</strong>以往的方法要么明确地融合3D几何信息，要么依赖于从单目RGB图像中学习到的3D先验知识。然而，像LiDAR这样的3D传感器昂贵且具有侵入性，而单目相机由于固有的尺度模糊性而难以建模精确的几何信息。在这项工作中，研究者提出了StereoScene用于3D语义场景补全（SSC），它充分利用了轻量级相机输入而无需使用任何外部3D传感器。研究者的关键想法是利用<strong>立体匹配来解决几何模糊性问题</strong>。为了提高其在不匹配区域的鲁棒性，论文引入了鸟瞰图（BEV）表示法，以激发具有丰富上下文信息的未知区域预测能力。在立体匹配和BEV表示法的基础上，论文精心设计了一个<strong>相互交互聚合（MIA）</strong>模块，以充分发挥两者的作用，促进它们互补聚合。</p><p><img src="/pic/StereoScene.png"></p><p>整体的StereoScene框架如上图所示。论文遵循使用连续的2D和3 UNetsf作为backbones。给定输入的双目图像，我们使用2D UNet来提取多尺度特征。BEV潜在体积和立体几何体积分别由BEV构造器和立体构造器构造。为了充分利用这两个卷的互补潜力，提出了一个相互交互聚合模块来相互引导和聚合它们。</p><h3 id="介绍OccDepth"><a href="#介绍OccDepth" class="headerlink" title="介绍OccDepth"></a>介绍OccDepth</h3><p>第一个只输入视觉的立体3D SSC方法。为了有效地利用深度信息，提出了一种立体软特征分配（Stereo-SFA）模块，通过隐式学习立体图像之间的相关性来更好地融合3D深度感知特征。占用感知深度（OAD）模块将知识提取与预先训练的深度模型明确地用于生成感知深度的3D特征。此外，为了更有效地验证立体输入场景，我们提出了基于TartanAir的SemanticTartanAir数据集。这是一个虚拟场景，可以使用真实的地面实况来更好地评估该方法的有效性。</p><p><img src="/pic/OccDepth.png"><br>3D SSC是通过桥接立体SFA模块来从立体图像中推断出来的，该模块用于将特征提升到3D空间，OAD模块用于增强深度预测，3D U-Net用于提取几何结构和语义。立体深度网络仅在训练中用于提供深度监督。</p><h3 id="介绍VoxFormer"><a href="#介绍VoxFormer" class="headerlink" title="介绍VoxFormer"></a>介绍VoxFormer</h3><p>一种基于 Transformer 的语义场景完成框架，可以<strong>仅从 2D 图像输出完整的 3D 体积语义</strong>。我们的框架采用两阶段设计，从深度估计中一组稀疏的可见和占用体素查询开始，然后是从稀疏体素生成密集 3D 体素的致密化阶段。这种设计的一个关键思想是，2D 图像上的视觉特征仅对应于可见的场景结构，而不对应于被遮挡或空白的空间。因此，从可见结构的特征化和预测开始更为可靠。一旦我们获得了稀疏查询集，我们就应用屏蔽自动编码器设计，通过自注意力将信息传播到所有体素。</p><p>VoxFormer由类不可知的查询建议（阶段-1）和类特定的语义分割（阶段-2）组成，其中阶段-1提出了一组稀疏的占用体素，阶段-2完成了从阶段-1给出的建议开始的场景表示。具体而言，阶段1具有轻量级的基于2D CNN的查询建议网络，该网络使用图像深度来重建场景几何结构。然后，它从整个视场上预定义的可学习体素查询中提出了一组稀疏的体素。第2阶段基于一种新颖的稀疏到密集类MAE架构，如图（a）所示。它首先通过允许所提出的体素关注图像观察来加强其特征化。接下来，未提出的体素将与可学习的掩码标记相关联，并且将通过自注意来处理全套体素，以完成每体素语义分割的场景表示。</p><p><img src="/pic/VoxFormer2.png"></p><p>VoxFormer的总体框架如下图所示，给定RGB图像，通过ResNet50[61]提取2D特征，并通过现成的深度预测器估计深度。校正后的估计深度启用了类不可知查询建议阶段：将选择位于占用位置的查询，以与图像特征进行可变形的交叉关注。之后，将添加掩码令牌，用于通过可变形的自我注意来完成体素特征。细化的体素特征将被上采样并投影到输出空间，用于每个体素的语义分割。请注意，我们的框架支持单个或多个图像的输入。<br><img src="/pic/VoxFormer.png"></p><h3 id="介绍OccFormer"><a href="#介绍OccFormer" class="headerlink" title="介绍OccFormer"></a>介绍OccFormer</h3><p>提出了一种双路变压器网络OccFormer，该网络可以有效地处理三维体积进行语义占用预测。OccFormer实现了摄像机生成的3D体素特性的长程、动态和高效编码。它是通过将繁重的三维处理分解为局部和全局的变压器路径沿水平面得到的。在占位解码器中，我们将传统的Mask2Former用于3D语义占位，提出了保留池化和类引导采样，显著缓解了稀疏性和类的不平衡。</p><p><img src="/pic/OccFormer.png"></p><p>上图显示的为所提出的用于基于相机的3D语义占用预测的OccFormer的框架。该流水线由用于提取多尺度2D特征的图像编码器、用于将2D特征提升到3D体积的图像到3D变换以及用于获得3D语义特征并预测3D语义占用的基于变换器的编码器-解码器组成。</p><h4 id="对于编码器部分："><a href="#对于编码器部分：" class="headerlink" title="对于编码器部分："></a>对于编码器部分：</h4><p>我们提出了 dual-path transformer 模块，以释放 self-attention 的能力，同时限制了二次复杂性 (quadratic complexity)。</p><p>具体来说，</p><ul><li>local path 沿每个二维BEV切片运行，并使用共享窗口注意力 ( shared windowed attention) 来捕捉细粒度的细节。</li><li>global path 对 collapsed 的BEV特征进行处理，以获得场景级的理解。</li><li>最后，双路径的输出被自适应地融合以生成输出的三维特征体 3D feature volume 。<br>双路径设计明显打破了三维特征体的挑战性处理，我们证明了它比传统的三维卷积有明显优势。</li></ul><h4 id="对于解码器部分："><a href="#对于解码器部分：" class="headerlink" title="对于解码器部分："></a>对于解码器部分：</h4><p>我们是第一个将最先进的 Mask2Former[9] 方法用于三维语义占用预测的。</p><p>我们进一步提出使用最大池化而不是默认的双线性来计算 attention的 masked regions，这可以更好地保留小类。此外，我们还提出了 class-guided 的采样方法，以捕获前景区域，从而进行更有效的优化。</p><h3 id="介绍S4CNet"><a href="#介绍S4CNet" class="headerlink" title="介绍S4CNet"></a>介绍S4CNet</h3><p><img src="/pic/S4C.png"></p><p>S4C是第一种完全自我监督的语义场景完成（SSC）方法，该方法仅基于图像数据（和相机姿势）进行训练。我们的方法从单个图像中预测体积场景表示，即使在遮挡区域中也能捕获几何和语义信息。与以前的SSC方法相比，我们不需要任何3D地面实况信息，允许使用仅图像的数据集进行训练。尽管缺乏基本事实数据，但我们的方法与性能差异很小的监督方法相比具有竞争力。</p><p><img src="/pic/S4C2.png"></p><p>上图显示网络概述a） 根据输入图像II，编码器-解码器网络预测描述图像的截头体中的语义场的像素对齐特征图F。像素ui的特征fui对从光学中心通过像素投射的光线上的语义和占用分布进行编码。b） 语义场允许通过体积渲染来渲染新颖的视图及其相应的语义分割。将3D点xi投影到输入图像中，并因此将F投影到采样fui中。结合xi的位置编码，两个MLP分别对点σi和语义标签li的密度进行解码。用于新颖视图合成的颜色ci是通过颜色采样从其他图像中获得的。c） 为了获得最佳效果，我们要求训练视图覆盖尽可能多的场景表面。因此，我们从随机的未来时间步长中采样侧视图，这些时间步长观察在输入帧中被遮挡的场景区域。</p><h4 id="语义多视图一致性训练"><a href="#语义多视图一致性训练" class="headerlink" title="语义多视图一致性训练"></a>语义多视图一致性训练</h4><p>SSC的所有现有方法都依赖于3D地面实况数据进行训练。这些数据通常是从带注释的激光雷达扫描中获得的，这是非常困难和昂贵的。与3D数据相比，具有语义标签的图像是大量可用的。我们建议利用这些可用的2D数据来训练用于3D语义场景完成的神经网络。为了使我们的方法尽可能通用，我们使用从预先训练的语义分割网络生成的伪语义标签。这使我们能够以完全自我监督的方式，仅从姿势图像中训练我们的架构，而不需要2D或3D地面实况数据。</p><p><strong>训练数据和目标</strong>：该方法的目标是进行语义场景完成，包括对3D场景的重建和分配语义标签。与传统方法不同，传统方法依赖于昂贵的从LiDAR扫描中获得的3D真实数据，这种方法旨在利用具有语义标签的2D图像数据的丰富性。作者建议使用从预训练的语义分割网络生成的伪语义标签来训练SSC的神经网络。这种方法允许在不需要2D或3D真实数据的情况下进行全面的自我监督训练。</p><p><strong>数据来源</strong>：在自动驾驶场景中，汽车上装有多个摄像头，包括前置摄像头和侧置摄像头。该方法在多个时间步的视频序列中使用这些摄像头拍摄的多个构图图像进行训练。</p><p><strong>训练目标</strong>：从这些帧中随机选择一部分作为新视图合成的重建目标。神经网络经过训练，可以根据其他帧的语义场和颜色样本来重建颜色信息和语义标签。</p><p><strong>训练信号</strong>：训练信号是通过测量伪语义真值语义掩模和重建图像之间的差异生成的。这种差异用作训练期间的损失函数。</p><p><strong>战略视图选择</strong>：为了确保有效的训练，选择训练视图是经过策略性考虑的。选择了带有与输入图像的随机偏移的侧置摄像视图。这些视图对于捕捉输入图像中被遮挡的区域提供了重要线索，为场景完成提供了重要线索。</p><p><strong>重建块</strong>：只从不同帧中随机选择的块进行颜色（ˆPi,k）和语义标签（ˆSi）的重建。这种方法旨在进行高效的训练，同时确保神经网络学习了一般场景几何和特定对象的语义。</p><p><strong>损失函数</strong>：进行SSC的训练涉及使用语义和光度重建损失的组合。光度损失有助于捕捉一般场景几何，而语义损失对于区分对象和学习粗略几何非常重要，并且引导模型学习物体周围更清晰的边缘。</p><p><img src="/pic/S4C3.png"></p><p>上图显示的是在SSCBench-KITTI-360预测体素网格。对占用图的定性评价表明，该方法能够准确地重建和标注场景。特别是与MonoScene等其他基于图像的方法相比，S4C能够恢复图像1中右侧车道等细节。S4C产生的体素占用显示出比基于激光雷达的训练更少的洞，后者再现了在地面上发现的洞。</p><h3 id="与Occ3D，OpenOccupancy以及Occ4cast对比。"><a href="#与Occ3D，OpenOccupancy以及Occ4cast对比。" class="headerlink" title="与Occ3D，OpenOccupancy以及Occ4cast对比。"></a>与Occ3D，OpenOccupancy以及Occ4cast对比。</h3><p> 我们将SSCBench与同时进行的相关工作Occ3D进行了比较（Tian et al.，2023）。差异在于：</p><ul><li>（a）设置：Occ3D使用周围视图图像作为输入，并且只考虑相机可见的3D体素的重建。SSCBench考虑了一个更具挑战性但更有意义的设置（也是一个公认的设置）：如何仅使用单眼视觉输入在可见和遮挡区域重建和完成3D语义。这项任务需要对时间信息和三维几何关系进行推理，以摆脱有限的视野；</li><li>（b） 规模：SSCBench提供了比Occ3D更多的数据集，由于单目驾驶记录丰富，计划增加更多数据集；</li><li>（c） 可访问性：我们继承了先驱KITTI广泛使用的设置，从而使SSCBench更容易被社区访问；</li><li>（d） 全面性：我们将SSC方法与单目、三目和点云输入进行比较，并为跨领域泛化测试提供统一的标签。另一个相关的基准，OpenOccupancy（Wang et al.，2023），也表现出类似的差异，特别是它只使用了nuScenes数据集（Caesar et al.，2020），这导致了多样性的限制。</li></ul><p>Occ4cast提出了一个新的LiDAR感知任务：将占用补全和预测Occupancy Completion and Forecasting（OCF）统一到一个框架中<br>主要贡献总结如下：</p><ul><li>我们提出了OCF任务，该任务要求从稀疏的3D输入中获得时空密集的4D感知。</li><li>我们利用公共自动驾驶数据生成了一个名为OCFBench的大规模数据集。</li><li>我们提出了基线方法来处理OCF任务，并在我们的数据集上提供了详细的基准。</li></ul><h3 id="数据集介绍："><a href="#数据集介绍：" class="headerlink" title="数据集介绍："></a>数据集介绍：</h3><ul><li>SemanticKITTI：大规模的 LiDAR 点云标注数据集 SemanticKITTI，标注 28 类语义，共 22 个 sequences，43000 scans，不仅支持3D语义分割，而且是第一个户外SSC基准。一个明显的局限性是它在生成真值时遗漏了动态物体，导致了标签的不准确，产生痕迹。其次，它受到规模有限和缺乏多样化地理覆盖范围的限制，数据收集仅限于一个城市。</li><li>SSCBench-KITTI-360。KITTI-360（Liao et al.，2022）在著名的KITTI数据集的基础上，引入了一个丰富的数据收集框架，具有不同的传感器模态和全景视点（一个透视立体相机和一对鱼眼相机），并提供了全面的注释，包括一致的2D和3D语义实例标签以及3D边界基元。密集和连贯的标签不仅支持分割和检测等既定任务，还支持语义SLAM（Bowman et al.，2017）和新视图合成（Zhang et al.，2023a）等新应用。虽然KITTI-360包括基于点云的语义场景完成，但SSC的流行方法仍然以体素化表示为中心（Roldao等人，2022），这在机器人技术中表现出更广泛的适用性。我们利用开源训练和验证集，我们构建了由9个长序列组成的SSCBench-KITTI-360。为了减少冗余，我们按照SemanticKITTI SSC基准，每5帧采样一次。训练集包括来自场景00、02-05、07和10的8487帧，而验证集包括来自情景06的1812帧。测试集包括来自场景09的2566帧。数据集总共包含12865（~13K）帧，比SemanticKITTI的规模高出约1.5倍。</li><li>SSCBench-nuScenes。与KITTI的前置摄像头不同，nuScenes（Caesar et al.，2020）捕捉到了自我车辆周围的360度全景。它提供了各种各样的多模式传感数据，包括在波士顿和新加坡收集的相机图像、激光雷达点云和雷达数据。nuScenes为复杂的城市驾驶场景提供了细致的注释，包括不同的天气条件、施工区域和不同的照明。全景nuScenes（Fong et al.，2022）用语义和实例标签扩展了原始nuScene数据集。凭借全面的指标和评估协议，nuScenes在自动驾驶研究中得到了广泛应用（Gu et al.，2023；胡等人，2023，李等人，2021；Huang等人，2023.）。nuScenes数据集由1K 20秒的场景组成，其中仅为训练和验证集提供标签，共850个场景。从可用的850个场景中，我们分配了500个场景用于训练，200个场景用于验证，150个场景用于测试。这种分布导致20064帧用于训练，8050帧用于验证，5949帧用于测试，总计34078帧（～34K）。这个规模大约是SemanticKITTI的四倍。由于“nuScenes”仅为频率为2Hz的关键帧提供注释，因此在SSCBench“nuScene”中没有下采样。</li><li>SSC Bench Waymo。Waymo数据集（Sun et al.，2020）收集自美国各地，提供了大规模的多模式传感器记录。Waymo提供了5台相机，其组合水平视场为～230度，略小于nuScenes。数据是在多个城市的不同条件下采集的，包括旧金山、凤凰城和山景城，确保了每个城市的广泛地理覆盖。它包括1000个用于训练和验证的场景，以及150个用于测试的场景，每个场景的时间跨度为20秒。评论为了构建SSCBench Waymo，我们利用开源训练和验证场景，并将它们重新分配到500、298和202个场景中，分别用于训练、验证和测试。为了减少基准测试的冗余和训练时间，我们将原始数据的样本减少了10倍。这种下采样产生了10011帧的训练集、5936帧的验证集和4038帧的测试集，总计19985帧（～20K）。</li></ul><h3 id="构建Pipeline"><a href="#构建Pipeline" class="headerlink" title="构建Pipeline"></a>构建Pipeline</h3><ul><li><strong>先决条件</strong>。为了建立SSCBench，基于激光雷达或基于相机的SSC需要具有多模式记录的驾驶数据集。数据集应包括顺序收集的3D激光雷达点云，具有用于完成几何图形的精确传感器姿态，用于理解语义场景的逐点语义注释，以及用于处理动态实例的3D边界注释。</li><li><strong>点云聚合</strong>。为了生成完整的表示，我们的方法包括在车辆前方的定义区域内叠加一组广泛的激光扫描。在像nuScenes和Waymo这样的短序列中，我们利用未来的扫描和相应区域的测量来创建密集的语义点云。在像KITTI-360这样具有多个循环闭包的长序列中，除了时间邻域之外，我们还合并了所有空间相邻点云。先进的SLAM系统（Bailey&amp;Durrant-White，2006）提供了精确的传感器姿态，极大地促进了静态环境中点云的聚集。对于动态对象，我们通过同步来避免时空管道。我们使用实例标签将动态对象转换为当前帧内的空间对齐。</li><li><strong>聚合点云的Voxeization</strong>。体素化是将连续的3D空间离散为由称为体素的体积元素组成的规则网格结构，使非结构化数据能够转换为可由卷积神经网络（CNNs）或视觉变换器（ViTs）有效处理的结构化格式。Voxelization引入了空间分辨率和内存消耗之间的权衡，并为3D感知提供了灵活和可扩展的表示（Maturana&amp;Scherer，2015；周和图泽尔，2018；李等人，2023）。为了便于集成，SSCBench遵循SemanticKITTI的设置，体积向前延伸51.2米，每侧延伸25.6米，高度为6.4米。体素分辨率为0.2m，产生256×256×32的体素体积。每个体素的标签由其内标记点的多数投票决定，而如果不存在点，则相应地标记空体素。</li><li><strong>排除未知Voxels</strong>。如果没有无处不在的场景感知，捕捉完整的3D户外动态场景几乎是不可能的。虽然可以利用空间或先验知识推理，但我们的意图是通过最小化这些步骤产生的误差来确保基本事实的保真度。因此，在训练和评估过程中，我们只考虑来自所有视点的可见和探测体素。具体来说，我们首先从不同的角度使用光线跟踪来识别和去除物体内或墙后的遮挡体素。此外，在具有稀疏感测的数据集中，其中许多体素仍然没有被标记，我们在训练和评估期间去除这些未知体素，以增强基本事实标签的可靠性</li></ul><p>讨论与分析</p><ul><li>点云密度的影响。我们的实验阐明了激光雷达输入密度对模型性能的影响。在SSCBench nuScenes数据集中，其特征是相对稀疏的激光雷达输入（32个通道），基于相机的方法在几何度量上优于基于激光雷达的方法。然而，在SSCBench Waymo数据集中，得益于密集的激光雷达输入（64个通道，5个激光雷达），基于激光雷达的方法大大优于基于相机的方法。基于激光雷达的方法对输入的敏感性变得明显，在密集输入中观察到优势，而在稀疏输入中则观察到显著的性能下降。这突出了未来研究开发强大的基于激光雷达的方法的必要性，该方法可以在利用效益的同时减轻退化。</li><li>单眼与三眼。表3显示了具有单目和三目输入的TPVFormer的性能。而三眼设置提供了更宽的视野，有助于提高IoU的整体性能（36.78→ 39.06）和mIoU（10.91→ 13.70），仅使用一台相机就能获得优异的结果仍然是一个引人注目的学术挑战。开发能够将模型的性能与全景图相匹配的单目方法仍然具有重要的研究价值，因为它们具有内存高效、计算高效和易于部署的特点。</li><li>与SemanticKITTI的比较。当将我们在SSCBench上的实验结果与SemanticKITTI的实验结果进行比较时，我们观察到了显著的差异（Behley et al.，2019）（有关更多细节，我们请读者参阅VoxFormer（Li et al.，2023））。虽然VoxFormer在IoU和mIoU等指标上在SemanticKITTI上表现出色，但它面临着SSCBench数据集多样性的挑战。这一挑战主要源于其深度估计模块无法超越SemanticKITTI进行推广。此外，LMSCNet在SemanticKITTI上通常表现出优于SSCNet的几何性能，而在SSCBench上则表现出相反的趋势。这些差异强调了两个要点。首先，他们强调了SSCBench的重要性，它为全面评估提供了多样化和苛刻的现实世界场景。其次，他们强调了在各种环境中保持高性能的稳健方法的必要性。</li></ul><h3 id="结论"><a href="#结论" class="headerlink" title="结论"></a>结论</h3><ul><li>限制和未来工作。SSCBench仅包含符合SSC问题惯例的3D数据。这限制了对具有时间维度的4D方法的评估。未来的工作将旨在扩大SSCBench，以包括时间信息。</li><li>总结。在本文中，我们介绍了SSCBench，这是一个由不同街景组成的大型基准，旨在促进稳健和可推广的语义场景完成模型的开发。通过细致的策划和全面的基准测试，我们发现了现有方法的瓶颈，并为未来的研究方向提供了宝贵的见解。我们的目标是让SSCBench刺激3D语义场景完成的进步，最终增强下一代自主系统的感知能力。</li></ul>]]></content>
      
      
      <categories>
          
          <category> DL </category>
          
      </categories>
      
      
        <tags>
            
            <tag> ssc </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>ubuntu学习记录</title>
      <link href="/2023/05/15/zhiling/"/>
      <url>/2023/05/15/zhiling/</url>
      
        <content type="html"><![CDATA[<p>&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;节点&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;<br><a href="https://subscribe.pronetworklink.com/api/v1/client/e5cea80c08c2367003269501e9cbea19">https://subscribe.pronetworklink.com/api/v1/client/e5cea80c08c2367003269501e9cbea19</a></p><p>&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;orbslam&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;</p><h1 id="运行摄像头，发布话题"><a href="#运行摄像头，发布话题" class="headerlink" title="运行摄像头，发布话题"></a>运行摄像头，发布话题</h1><p>roslaunch usb_cam usb_cam-test.launch</p><h1 id="新终端：开启ORBSLAM2"><a href="#新终端：开启ORBSLAM2" class="headerlink" title="新终端：开启ORBSLAM2"></a>新终端：开启ORBSLAM2</h1><p>rosrun ORB_SLAM2 Mono .&#x2F;Vocabulary&#x2F;ORBvoc.txt .&#x2F;Examples&#x2F;ROS&#x2F;ORB_SLAM2&#x2F;Asus.yaml</p><p>pcl_viewer的使用:pcl_viewer a.pcd</p><p>&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;vscode使用&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;</p><p>vscode查看函数列表Ctrl + Shift + O</p><p>markdown ctrl-shift-vOpen preview(打开新窗口预览该文件)</p><p>ctrl+c+c </p><p>pkg-config –modversion opencv</p><p>g++ -std&#x3D;c++11 test.cpp <code>pkg-config --libs --cflags opencv</code> -o result</p><p>sudo gedit ~&#x2F;.bashrc</p><p>source ~&#x2F;.bashrc</p><p>echo $ROS_PACKAGE_PATH</p><p>问题：liunx系统打开vscode之后代码和终端字体有的单词间距很奇怪，修改字体大小和间距后无效，修改了字体完成。</p><p>操作：打开设置，输入Editor:Font Family，</p><p>修改终端字体：在Terminal › Integrated: Font Family下输入monospace，</p><p>修改编辑器字体：在Editor:Font Family输入monospac</p><p>&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;服务器&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;</p><p>ssh -p 1938 <a href="mailto:&#x63;&#104;&#x65;&#x6e;&#x68;&#97;&#105;&#121;&#97;&#110;&#x67;&#64;&#49;&#48;&#x2e;&#x36;&#x39;&#46;&#x34;&#55;&#46;&#56;&#50;">&#x63;&#104;&#x65;&#x6e;&#x68;&#97;&#105;&#121;&#97;&#110;&#x67;&#64;&#49;&#48;&#x2e;&#x36;&#x39;&#46;&#x34;&#55;&#46;&#56;&#50;</a></p><p>nvidia-smi</p><p>gpustat -i 1</p><p>-i <a href="http://pypi.douban.com/simple/">http://pypi.douban.com/simple/</a> –trusted-host pypi.douban.com</p><p>vim ~&#x2F;.bashrc</p><p>source ~&#x2F;.bashrc</p><p>&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;cuda&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;</p><p>export PATH&#x3D;”$PATH:&#x2F;usr&#x2F;local&#x2F;cuda&#x2F;bin”<br>export LD_LIBRARY_PATH&#x3D;”$LD_LIBRARY_PATH:&#x2F;usr&#x2F;local&#x2F;cuda&#x2F;lib64&#x2F;“<br>export LIBRARY_PATH&#x3D;”$LIBRARY_PATH:&#x2F;usr&#x2F;local&#x2F;cuda&#x2F;lib64”<br>export CUDA_HOME&#x3D;$CUDA_HOME:”&#x2F;usr&#x2F;local&#x2F;cuda”</p><p>chmod 777 file</p><p>&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;docker&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;</p><p>查看容器运行现状:docker ps。<br>停止容器:docker stop 容器id，<br>查看容器标准输出：docker logs<br>获取镜像：docker pull 命令来载入 ubuntu 镜像<br>启动容器：docker run -it ubuntu &#x2F;bin&#x2F;bash<br>查看容器：docker ps -a<br>退出容器：exit或着CTRL+D<br>启动已停止的容器：docker start 容器id<br>后台运行：-d指定容器运行模式，如：docker run -itd ubuntu-test ubuntu &#x2F;bin&#x2F;bash<br>注：-d参数默认不会进入容器<br>进入容器：docker attach容器id（从容器最初会导致容器停止）以及docker exec容器id(推荐使用，从容器退出不会导致容器停止)<br>导出容器：docker export 如：docker export 容器id  &gt;  ubuntu.tar<br>导入容器：docker import  如：cat docker&#x2F;ubuntu.tar | docker import - test&#x2F;ubuntu:v1<br>将快照文件 ubuntu.tar 导入到镜像 test&#x2F;ubuntu:v1:<br>删除容器：docker rm -f 容器id<br>删除所有终止状态的容器：docker container prune</p><p>sudo docker ps -a</p><p>sudo docker container ls -a</p><p>docker stop 容器id</p><p>sudo docker start 容器id          # 启动容器</p><p>sudo docker attach 容器id  进入容器正在执行的终端</p><p>sudo docker stats</p><p>sudo docker images</p><p>&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;conda源&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;</p><p>channels:</p><ul><li>defaults<br>show_channel_urls: true<br>channel_alias: <a href="https://mirrors.bfsu.edu.cn/anaconda">https://mirrors.bfsu.edu.cn/anaconda</a><br>default_channels:</li><li><a href="https://mirrors.bfsu.edu.cn/anaconda/pkgs/main">https://mirrors.bfsu.edu.cn/anaconda/pkgs/main</a></li><li><a href="https://mirrors.bfsu.edu.cn/anaconda/pkgs/free">https://mirrors.bfsu.edu.cn/anaconda/pkgs/free</a></li><li><a href="https://mirrors.bfsu.edu.cn/anaconda/pkgs/r">https://mirrors.bfsu.edu.cn/anaconda/pkgs/r</a></li><li><a href="https://mirrors.bfsu.edu.cn/anaconda/pkgs/pro">https://mirrors.bfsu.edu.cn/anaconda/pkgs/pro</a></li><li><a href="https://mirrors.bfsu.edu.cn/anaconda/pkgs/msys2">https://mirrors.bfsu.edu.cn/anaconda/pkgs/msys2</a><br>custom_channels:<br>  conda-forge: <a href="https://mirrors.bfsu.edu.cn/anaconda/cloud">https://mirrors.bfsu.edu.cn/anaconda/cloud</a><br>  msys2: <a href="https://mirrors.bfsu.edu.cn/anaconda/cloud">https://mirrors.bfsu.edu.cn/anaconda/cloud</a><br>  bioconda: <a href="https://mirrors.bfsu.edu.cn/anaconda/cloud">https://mirrors.bfsu.edu.cn/anaconda/cloud</a><br>  menpo: <a href="https://mirrors.bfsu.edu.cn/anaconda/cloud">https://mirrors.bfsu.edu.cn/anaconda/cloud</a><br>  pytorch: <a href="https://mirrors.bfsu.edu.cn/anaconda/cloud">https://mirrors.bfsu.edu.cn/anaconda/cloud</a><br>  simpleitk: <a href="https://mirrors.bfsu.edu.cn/anaconda/cloud">https://mirrors.bfsu.edu.cn/anaconda/cloud</a></li></ul><p>channels:</p><ul><li>defaults<br>show_channel_urls: true<br>channel_alias: <a href="https://mirrors.tuna.tsinghua.edu.cn/anaconda">https://mirrors.tuna.tsinghua.edu.cn/anaconda</a><br>default_channels:</li><li><a href="https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/main">https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/main</a></li><li><a href="https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/free">https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/free</a></li><li><a href="https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/r">https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/r</a></li><li><a href="https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/pro">https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/pro</a></li><li><a href="https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/msys2">https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/msys2</a><br>custom_channels:<br>  conda-forge: <a href="https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud">https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud</a><br>  msys2: <a href="https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud">https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud</a><br>  bioconda: <a href="https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud">https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud</a><br>  menpo: <a href="https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud">https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud</a><br>  pytorch: <a href="https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud">https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud</a><br>  simpleitk: <a href="https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud">https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud</a></li></ul><p>&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;opencv安装&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;</p><p>cmake -D CMAKE_BUILD_TYPE&#x3D;RELEASE <br>      -D CMAKE_INSTALL_PREFIX&#x3D;&#x2F;usr&#x2F;local&#x2F;opencv_2.4.11 <br>      -D WITH_CUDA&#x3D;OFF <br>      -D WITH_OPENGL&#x3D;OFF <br>      -D WITH_OPENCL&#x3D;OFF <br>      -D BUILD_JPEG&#x3D;OFF <br>      -D BUILD_PNG&#x3D;OFF <br>      -D BUILD_JASPER&#x3D;OFF <br>      -DBUILD_OPENEXR&#x3D;OFF <br>      -D BUILD_TIFF&#x3D;OFF <br>      -D BUILD_ZLIB&#x3D;OFF <br>      -D WITH_FFMPEG&#x3D;OFF <br>    ..</p><p>cmake -D CMAKE_BUILD_TYPE&#x3D;RELEASE -D CMAKE_INSTALL_PREFIX&#x3D;&#x2F;usr&#x2F;local&#x2F;opencv2411 -D CUDA_GENERATION&#x3D;Kepler -D WITH_TBB&#x3D;ON -D BUILD_NEW_PYTHON_SUPPORT&#x3D;ON -D WITH_V4L&#x3D;ON -D INSTALL_C_EXAMPLES&#x3D;ON -D INSTALL_PYTHON_EXAMPLES&#x3D;ON -D BUILD_EXAMPLES&#x3D;ON -D WITH_QT&#x3D;OFF -D WITH_OPENGL&#x3D;ON -D BUILD_TIFF&#x3D;ON ..&#x2F;local&#x2F;opencv2411 ..</p><p>sudo cmake -D CMAKE_BUILD_TYPE&#x3D;RELEASE \ -D CMAKE_INSTALL_PREFIX&#x3D;&#x2F;usr&#x2F;local \ -D OPENCV_EXTRA_MODULES_PATH&#x3D;~&#x2F;opencv_contrib-3.4.3&#x2F;modules -D INSTALL_PYTHON_EXAMPLES&#x3D;ON \ -D INSTALL_C_EXAMPLES&#x3D;ON -D OPENCV_ENABLE_NONFREE:BOOL&#x3D;ON -D BUILD_opencv_world:BOOL&#x3D;ON -D BUILD_EXAMPLES&#x3D;ON .. </p><p>—————————-ROS—————————————</p><p>rostopic list</p><p>rostopic type &#x2F;tianbot_mini&#x2F;odom    输出：类型</p><p>rosmsg show 类型    输出：类型包含的结构</p><p>rosrun  turtlesim turtlesim_node 运行节点</p><p>rosrun turtlesim turtle_teleop_key 键盘控制运动</p><p>查看当前所有的topic：rostopic list<br>查看某个topic的输出：rostopic echo [topic_name]<br>查看某个topic的发布频率：rostopic hz [topic_name]<br>查看某个topic的数据格式：rostopic echo [topic_name]&#x2F;encoding</p><p>&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;slam-yolov5&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;</p><p>.&#x2F;Examples&#x2F;RGB-D&#x2F;rgbd_tum  Vocabulary&#x2F;ORBvoc.txt Examples&#x2F;RGB-D&#x2F;TUM3.yaml &#x2F;home&#x2F;chy&#x2F;dataset&#x2F;rgbd_dataset_freiburg3_walking_halfsphere &#x2F;home&#x2F;chy&#x2F;dataset&#x2F;rgbd_dataset_freiburg3_walking_halfsphere&#x2F;associations.txt   </p><p>&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;segment-anything&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;</p><p>python scripts&#x2F;amg.py –input &#x2F;home&#x2F;chy&#x2F;code_chy&#x2F;segment-anything&#x2F;99.jpg –output &#x2F;home&#x2F;chy&#x2F;code_chy&#x2F;segment-anything&#x2F;outs –model-type vit_b –checkpoint &#x2F;home&#x2F;chy&#x2F;code_chy&#x2F;segment-anything&#x2F;sam_vit_b_01ec64.pth</p><p>&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;解压&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;<br>tgz格式解压到当前文件夹：<br>tar -zxvf xxx.tar.gz<br>zip格式解压到当前文件夹：<br>unzip xxx.zip<br>tar.xz格式解压<br>首先：xz -d xxx.tar.xz 解压得到tar文件 ;<br>其次：tar -xvf xxx.tar得到完整解压文件。</p><p>&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;git&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;</p><p>在git的repo中，可能会有子项目的代码，也就是”git中的git”</p><p> –recursive是递归的意思，不仅会git clone当前项目中的代码，也会clone项目中子项目的代码。</p><p><em><strong>未完待续</strong></em></p>]]></content>
      
      
      <categories>
          
          <category> linux </category>
          
      </categories>
      
      
        <tags>
            
            <tag> linux </tag>
            
            <tag> ubuntu </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>docker学习记录</title>
      <link href="/2023/05/07/docker1/"/>
      <url>/2023/05/07/docker1/</url>
      
        <content type="html"><![CDATA[<h1 id="docker-的基本使用"><a href="#docker-的基本使用" class="headerlink" title="docker 的基本使用"></a>docker 的基本使用</h1><h2 id="docker镜像命令和容器命令"><a href="#docker镜像命令和容器命令" class="headerlink" title="docker镜像命令和容器命令"></a>docker镜像命令和容器命令</h2><p><img src="/pic/e59c326db4bece61c8e5916822302408.png" alt="uTools_1683184235571.png"></p><p><img src="/pic/1bed533c0452fc72ba67462e37e5d850.png" alt="uTools_1683184257569.png"></p><br/><h2 id="x3D-x3D-docker-作业练习-x3D-x3D"><a href="#x3D-x3D-docker-作业练习-x3D-x3D" class="headerlink" title="&#x3D;&#x3D;docker 作业练习&#x3D;&#x3D;"></a><em><strong>&#x3D;&#x3D;docker 作业练习&#x3D;&#x3D;</strong></em></h2><blockquote><p><strong>&#x3D;&#x3D;Docker 安装 Nginx&#x3D;&#x3D;</strong></p></blockquote><pre class="line-numbers language-powershell" data-language="powershell"><code class="language-powershell"><span class="token comment"># -d后台运行</span><span class="token comment">#--name   给容器起名字</span><span class="token comment"># -p 宿主机端口：容器内部端口</span>chy@ocean:~$ docker run <span class="token operator">-</span>d <span class="token operator">--</span>name nginx01 <span class="token operator">-</span>p 8000:80 nginxc83728c3f1d2d20cba7571a6fd08c506cb5bc14b1fbb9385b8fca0e687615e4dchy@ocean:~$ docker <span class="token function">ps</span>CONTAINER ID   IMAGE     COMMAND                   CREATED         STATUS         PORTS                                   NAMESc83728c3f1d2   nginx     <span class="token string">"/docker-entrypoint.…"</span>   6 seconds ago   Up 5 seconds   0<span class="token punctuation">.</span>0<span class="token punctuation">.</span>0<span class="token punctuation">.</span>0:8000->80/tcp<span class="token punctuation">,</span> :::8000->80/tcp   nginx01chy@ocean:~$ curl localhost:8000&lt;<span class="token operator">!</span>DOCTYPE html>&lt;html>&lt;head>&lt;title>Welcome to nginx!&lt;<span class="token operator">/</span>title>&lt;style>html <span class="token punctuation">&#123;</span> color-scheme: light dark<span class="token punctuation">;</span> <span class="token punctuation">&#125;</span>body <span class="token punctuation">&#123;</span> width: 35em<span class="token punctuation">;</span> margin: 0 auto<span class="token punctuation">;</span>font-family: Tahoma<span class="token punctuation">,</span> Verdana<span class="token punctuation">,</span> Arial<span class="token punctuation">,</span> sans-serif<span class="token punctuation">;</span> <span class="token punctuation">&#125;</span>&lt;<span class="token operator">/</span>style>&lt;<span class="token operator">/</span>head>&lt;body>&lt;h1>Welcome to nginx!&lt;<span class="token operator">/</span>h1><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p><img src="/pic/27c501e8902ecff60cf6e1b31b56641e.png" alt="截图"></p><blockquote><p><strong>&#x3D;&#x3D;作业：docker 来装一个tomcat&#x3D;&#x3D;</strong></p></blockquote><pre class="line-numbers language-powershell" data-language="powershell"><code class="language-powershell">docker run <span class="token operator">-</span>it <span class="token operator">--</span><span class="token function">rm</span> tomcat:9<span class="token punctuation">.</span>0<span class="token comment"># --rm 一般用来测试，用完就删 </span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre><br/><blockquote><p>作业三：部署es+kibana</p></blockquote><pre class="line-numbers language-powershell" data-language="powershell"><code class="language-powershell"><span class="token comment"># es暴露端口多</span><span class="token comment">#es 十分的耗内存</span><span class="token comment">#es的数据一般挂在到安全目录！挂载</span><span class="token comment"># --net somenetwork  网络配置</span>docker stats查看docker所用内存<span class="token operator">-</span>e环境配置修改docker run <span class="token operator">-</span>d <span class="token operator">--</span>name elasticsearch <span class="token operator">-</span>p 9200:9200 <span class="token operator">-</span>p 9300:9300 <span class="token operator">-</span>e <span class="token string">"discovery.type=single-node"</span> elasticsearch:7<span class="token punctuation">.</span>6<span class="token punctuation">.</span>2chy@ocean:~$ curl localhost:9200<span class="token punctuation">&#123;</span>  <span class="token string">"name"</span> : <span class="token string">"c41cc3475bc3"</span><span class="token punctuation">,</span>  <span class="token string">"cluster_name"</span> : <span class="token string">"docker-cluster"</span><span class="token punctuation">,</span>  <span class="token string">"cluster_uuid"</span> : <span class="token string">"bhypk5Q-RvC4x_FOMoFF-g"</span><span class="token punctuation">,</span>  <span class="token string">"version"</span> : <span class="token punctuation">&#123;</span>    <span class="token string">"number"</span> : <span class="token string">"7.6.2"</span><span class="token punctuation">,</span>    <span class="token string">"build_flavor"</span> : <span class="token string">"default"</span><span class="token punctuation">,</span>    <span class="token string">"build_type"</span> : <span class="token string">"docker"</span><span class="token punctuation">,</span>    <span class="token string">"build_hash"</span> : <span class="token string">"ef48eb35cf30adf4db14086e8aabd07ef6fb113f"</span><span class="token punctuation">,</span>    <span class="token string">"build_date"</span> : <span class="token string">"2020-03-26T06:34:37.794943Z"</span><span class="token punctuation">,</span>    <span class="token string">"build_snapshot"</span> : false<span class="token punctuation">,</span>    <span class="token string">"lucene_version"</span> : <span class="token string">"8.4.0"</span><span class="token punctuation">,</span>    <span class="token string">"minimum_wire_compatibility_version"</span> : <span class="token string">"6.8.0"</span><span class="token punctuation">,</span>    <span class="token string">"minimum_index_compatibility_version"</span> : <span class="token string">"6.0.0-beta1"</span>  <span class="token punctuation">&#125;</span><span class="token punctuation">,</span>  <span class="token string">"tagline"</span> : <span class="token string">"You Know, for Search"</span><span class="token punctuation">&#125;</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p><strong>&#x3D;&#x3D;可视化&#x3D;&#x3D;</strong></p><ul><li>portainer(先用这个)</li></ul><pre class="line-numbers language-powershell" data-language="powershell"><code class="language-powershell">docker run <span class="token operator">-</span>d <span class="token operator">-</span>p 8080:9000\<span class="token operator">--</span>restart=always <span class="token operator">-</span>v <span class="token operator">/</span><span class="token keyword">var</span><span class="token operator">/</span>run/docker<span class="token punctuation">.</span>sock:<span class="token operator">/</span><span class="token keyword">var</span><span class="token operator">/</span>run/docker<span class="token punctuation">.</span>sock <span class="token operator">--</span>privileged=true portainer/portainer<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre><br/><p><strong>&#x3D;&#x3D;什么是portainer&#x3D;&#x3D;</strong></p><p>Docker 图像化界面管理工具，提供一个后台面板供我们操作</p><p>访问测试：<a href="http://192.168.1.40:8088/">http://192.168.1.40:8088</a></p><br/><h2 id="docker-镜像"><a href="#docker-镜像" class="headerlink" title="docker  镜像"></a>docker  镜像</h2><p><img src="/pic/c2cccd377124402c404b71f338287944.png" alt="截图"></p><p><strong>&#x3D;&#x3D;commit镜像&#x3D;&#x3D;</strong></p><br/><pre class="line-numbers language-powershell" data-language="powershell"><code class="language-powershell">docker commit 提交容器成为一个新的副本docker commit <span class="token operator">-</span>m=提交的描述信息  <span class="token operator">-</span>a=“作者 ” 容器 id 目标镜像名:<span class="token namespace">[TAG]</span>docker commit <span class="token operator">-</span>a=<span class="token string">"chy"</span> <span class="token operator">-</span>m=<span class="token string">"add webapps application"</span> 8f9660706542 tomcat_chy:1<span class="token punctuation">.</span>0<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span></span></code></pre><h2 id="x3D-x3D-容器数据卷-x3D-x3D"><a href="#x3D-x3D-容器数据卷-x3D-x3D" class="headerlink" title="&#x3D;&#x3D;容器数据卷&#x3D;&#x3D;"></a><strong>&#x3D;&#x3D;容器数据卷&#x3D;&#x3D;</strong></h2><h3 id="什么是容器数据卷"><a href="#什么是容器数据卷" class="headerlink" title="什么是容器数据卷"></a>什么是容器数据卷</h3><p><img src="/pic/2eb6ed69ff73865ee2e352c59cfa3263.png" alt="截图"></p><p><strong>总结一句话:容器的持久化和同步操作！容器间也可以进行数据共享的！</strong></p><br/><h3 id="使用数据卷"><a href="#使用数据卷" class="headerlink" title="使用数据卷"></a>使用数据卷</h3><br/><blockquote><p>方式一：直接使用命令来挂载 -v</p><p>docker run -it -v  主机目录：容器内目录    类似于-p</p><p> docker run -it -v &#x2F;home&#x2F;chy&#x2F;docker_ubuntu18_home:&#x2F;home ubuntu:18.04 &#x2F;bin&#x2F;bash</p></blockquote><pre class="line-numbers language-c_cpp" data-language="c_cpp"><code class="language-c_cpp">&quot;Mounts&quot;: [            &#123;                &quot;Type&quot;: &quot;bind&quot;,                &quot;Source&quot;: &quot;&#x2F;home&#x2F;chy&#x2F;docker_ubuntu18_home&quot;,                &quot;Destination&quot;: &quot;&#x2F;home&quot;,                &quot;Mode&quot;: &quot;&quot;,                &quot;RW&quot;: true,                &quot;Propagation&quot;: &quot;rprivate&quot;            &#125;        ],<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><br/><pre class="line-numbers language-powershell" data-language="powershell"><code class="language-powershell">chy@ocean:~<span class="token operator">/</span>docker_ubuntu18_home$ vim ceshi<span class="token punctuation">.</span>py <span class="token punctuation">[</span>2<span class="token punctuation">]</span><span class="token operator">+</span>  已停止               vim ceshi<span class="token punctuation">.</span>pychy@ocean:~<span class="token operator">/</span>docker_ubuntu18_home$ docker <span class="token function">ps</span>CONTAINER ID   IMAGE                 COMMAND        CREATED       STATUS       PORTS                                       NAMESd11b0f59e8b1   portainer/portainer   <span class="token string">"/portainer"</span>   5 hours ago   Up 5 hours   0<span class="token punctuation">.</span>0<span class="token punctuation">.</span>0<span class="token punctuation">.</span>0:8088->9000/tcp<span class="token punctuation">,</span> :::8088->9000/tcp   elated_davincichy@ocean:~<span class="token operator">/</span>docker_ubuntu18_home$ docker <span class="token function">ps</span> <span class="token operator">-</span>aCONTAINER ID   IMAGE                 COMMAND                   CREATED          STATUS                      PORTS                                       NAMESd591bed7171e   ubuntu:18<span class="token punctuation">.</span>04          <span class="token string">"/bin/bash"</span>               22 minutes ago   Exited <span class="token punctuation">(</span>0<span class="token punctuation">)</span> 21 minutes ago                                               wizardly_spenced11b0f59e8b1   portainer/portainer   <span class="token string">"/portainer"</span>              5 hours ago      Up 5 hours                  0<span class="token punctuation">.</span>0<span class="token punctuation">.</span>0<span class="token punctuation">.</span>0:8088->9000/tcp<span class="token punctuation">,</span> :::8088->9000/tcp   elated_davincic83728c3f1d2   nginx                 <span class="token string">"/docker-entrypoint.…"</span>   6 hours ago      Exited <span class="token punctuation">(</span>0<span class="token punctuation">)</span> 6 hours ago                                                  nginx01ff3cf19a0b79   ubuntu:18<span class="token punctuation">.</span>04          <span class="token string">"/bin/bash"</span>               7 hours ago      Exited <span class="token punctuation">(</span>127<span class="token punctuation">)</span> 7 hours ago                                                lucid_chatterjeee19f4782e1c5   ubuntu:18<span class="token punctuation">.</span>04          <span class="token string">"bash"</span>                    7 hours ago      Exited <span class="token punctuation">(</span>130<span class="token punctuation">)</span> 7 hours ago                                                pedantic_paninic29921e3edcf   composetest_web       <span class="token string">"flask run"</span>               2 weeks ago      Exited <span class="token punctuation">(</span>137<span class="token punctuation">)</span> 2 weeks ago                                                composetest-web-10a446f730c1f   redis:alpine          <span class="token string">"docker-entrypoint.s…"</span>   2 weeks ago      Exited <span class="token punctuation">(</span>0<span class="token punctuation">)</span> 6 days ago                                                   composetest-redis-1chy@ocean:~<span class="token operator">/</span>docker_ubuntu18_home$ docker <span class="token function">start</span> d591bed7171ed591bed7171echy@ocean:~<span class="token operator">/</span>docker_ubuntu18_home$ docker <span class="token function">ps</span> CONTAINER ID   IMAGE                 COMMAND        CREATED          STATUS         PORTS                                       NAMESd591bed7171e   ubuntu:18<span class="token punctuation">.</span>04          <span class="token string">"/bin/bash"</span>    22 minutes ago   Up 3 seconds                                               wizardly_spenced11b0f59e8b1   portainer/portainer   <span class="token string">"/portainer"</span>   5 hours ago      Up 5 hours     0<span class="token punctuation">.</span>0<span class="token punctuation">.</span>0<span class="token punctuation">.</span>0:8088->9000/tcp<span class="token punctuation">,</span> :::8088->9000/tcp   elated_davincichy@ocean:~<span class="token operator">/</span>docker_ubuntu18_home$ docker attach d591bed7171eroot@d591bed7171e:<span class="token operator">/</span><span class="token comment"># ls</span>bin  boot  dev  etc  home  lib  lib64  media  mnt  opt  proc  root  run  sbin  srv  sys  tmp  usr  <span class="token keyword">var</span>root@d591bed7171e:<span class="token operator">/</span><span class="token comment"># cd home/</span>root@d591bed7171e:<span class="token operator">/</span>home<span class="token comment"># ls</span>ceshi<span class="token punctuation">.</span>pyroot@d591bed7171e:<span class="token operator">/</span>home<span class="token comment"># cat ceshi.py </span>hello<span class="token punctuation">,</span> linux updata<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h3 id="好处：我们以后修改只需要在本地修改即可，容器内会自动同步"><a href="#好处：我们以后修改只需要在本地修改即可，容器内会自动同步" class="headerlink" title="好处：我们以后修改只需要在本地修改即可，容器内会自动同步"></a>好处：我们以后修改只需要在本地修改即可，容器内会自动同步</h3><br/><h3 id="实战：安装MySQL"><a href="#实战：安装MySQL" class="headerlink" title="实战：安装MySQL"></a>实战：安装MySQL</h3><p>思考：MySQL的数据持久化的问题</p><pre class="line-numbers language-powershell" data-language="powershell"><code class="language-powershell"><span class="token comment"># 官方的运行，有配置密码</span>docker run <span class="token operator">--</span>name some-mysql <span class="token operator">-</span>e MYSQL_ROOT_PASSWORD=my-secret-pw <span class="token operator">-</span>d mysql:tag<span class="token operator">-</span>e 环境配置<span class="token comment">#本主机上测试</span>docker run <span class="token operator">-</span>d <span class="token operator">-</span>p 3310:3306 <span class="token operator">-</span>v <span class="token operator">/</span>home/chy/docker_volume/mysql/conf:<span class="token operator">/</span>etc/mysql/conf<span class="token punctuation">.</span>d <span class="token operator">-</span>v <span class="token operator">/</span>home/chy/docker_volume/mysql/<span class="token keyword">data</span>:<span class="token operator">/</span><span class="token keyword">var</span><span class="token operator">/</span>lib/mysql <span class="token operator">-</span>e MYSQL_ROOT_PASSWOR=chy <span class="token operator">--</span>name mysql_chy mysql:5<span class="token punctuation">.</span>7<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><br/><h3 id="具名和匿名挂载"><a href="#具名和匿名挂载" class="headerlink" title="具名和匿名挂载"></a>具名和匿名挂载</h3><pre class="line-numbers language-powershell" data-language="powershell"><code class="language-powershell"><span class="token comment"># 匿名挂载</span><span class="token operator">-</span>v 容器内路径docker run <span class="token operator">-</span>d <span class="token operator">-</span>P <span class="token operator">--</span>name nginx <span class="token operator">-</span>v <span class="token operator">/</span>etc/nginx nginx<span class="token comment"># 查看所有volume 的情况</span>chy@ocean:~$ docker volume <span class="token function">ls</span>DRIVER    VOLUME NAMElocal     2198da5e7d86ba5a3de24d1aa71ab791bcb22e94e9f84e3d2f1fef73dc54d6b7local     a361a22ba8ef6ffffd78f233128054bd668b3e5315adffab8cfa79b5921ea720local     ba8715be0d242c0a4d004e38640b10c5dd5833f021920f8bd6fc0e830e012ebb<span class="token comment"># 这里的乱码号就是匿名容器名</span>chy@ocean:~$ docker run <span class="token operator">-</span>d <span class="token operator">-</span>P <span class="token operator">--</span>name nginx01 <span class="token operator">-</span>v juming-nginx:<span class="token operator">/</span>etc/nginx nginxa221eae136c83f19fa5f87944efa8eb1e9ea7841b8b982c0b2a943626be6842dchy@ocean:~$ docker volume <span class="token function">ls</span>DRIVER    VOLUME NAMElocal     2198da5e7d86ba5a3de24d1aa71ab791bcb22e94e9f84e3d2f1fef73dc54d6b7local     a361a22ba8ef6ffffd78f233128054bd668b3e5315adffab8cfa79b5921ea720local     ba8715be0d242c0a4d004e38640b10c5dd5833f021920f8bd6fc0e830e012ebblocal     juming-nginx<span class="token comment">#这里的juming-nginx就是具名挂载</span><span class="token comment"># 通过 -v 卷名：容器内路径</span><span class="token comment"># 查看一下这个卷</span>chy@ocean:~$ docker volume inspect juming-nginx<span class="token punctuation">[</span>    <span class="token punctuation">&#123;</span>        <span class="token string">"CreatedAt"</span>: <span class="token string">"2023-05-05T10:03:11+08:00"</span><span class="token punctuation">,</span>        <span class="token string">"Driver"</span>: <span class="token string">"local"</span><span class="token punctuation">,</span>        <span class="token string">"Labels"</span>: null<span class="token punctuation">,</span>        <span class="token string">"Mountpoint"</span>: <span class="token string">"/var/lib/docker/volumes/juming-nginx/_data"</span><span class="token punctuation">,</span>        <span class="token string">"Name"</span>: <span class="token string">"juming-nginx"</span><span class="token punctuation">,</span>        <span class="token string">"Options"</span>: null<span class="token punctuation">,</span>        <span class="token string">"Scope"</span>: <span class="token string">"local"</span>    <span class="token punctuation">&#125;</span><span class="token punctuation">]</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>挂载位置在&#x2F;var&#x2F;lib&#x2F;docker&#x2F;volumes&#x2F;XXXX&#x2F;_data内</p><br/><pre class="line-numbers language-powershell" data-language="powershell"><code class="language-powershell"><span class="token comment">#如何确定是具名挂载还是匿名挂载，还是指定路径挂载！</span><span class="token operator">-</span>V容器内路径  <span class="token comment">#匿名挂载</span><span class="token operator">-</span>V卷名：容器内路径  <span class="token comment">#具名挂载</span><span class="token operator">-</span>V/宿主机路径：：容器内路径   <span class="token comment">#指定路径挂载！</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span></span></code></pre><p>&#x3D;&#x3D;扩展&#x3D;&#x3D;</p><pre class="line-numbers language-powershell" data-language="powershell"><code class="language-powershell"><span class="token comment">#通过-V容器内路径：rorw改变读写权限</span>ro    readonly   <span class="token comment">#只读</span>rw   readwrite  <span class="token comment">#可读可写</span><span class="token comment">#一旦这个了设置了容器权限，容器对我们挂载出来的内容就有限定了！</span>docker run <span class="token operator">-</span>d <span class="token operator">-</span>P-<span class="token operator">-</span>name nginx02 <span class="token operator">-</span>v juming-nginx:<span class="token operator">/</span>etc/nginx:ro nginxdocker run <span class="token operator">-</span>d <span class="token operator">-</span>P <span class="token operator">--</span>name nginx02 <span class="token operator">-</span>v juming-nginx:<span class="token operator">/</span>etc/nginx:rw nginx<span class="token comment">#ro只要看到ro就说明这个路径只能通过宿主机来操作，容器内部是无法操作</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><br/><h2 id="x3D-x3D-Docker-File-x3D-x3D"><a href="#x3D-x3D-Docker-File-x3D-x3D" class="headerlink" title="&#x3D;&#x3D;Docker File&#x3D;&#x3D;"></a><strong>&#x3D;&#x3D;Docker File&#x3D;&#x3D;</strong></h2><h3 id="dockerfile介绍"><a href="#dockerfile介绍" class="headerlink" title="dockerfile介绍"></a>dockerfile介绍</h3><br/><p>dockerfile是用来构建dokcer镜像的文件！命令参数脚本I</p><h3 id="构建步骤："><a href="#构建步骤：" class="headerlink" title="构建步骤："></a>构建步骤：</h3><p>1、编写一个dockerfile文件<br>2、docker build构建成为一个镜像<br>3、docker run运行镜像<br>4、docker push发布镜像(DockerHub、阿里云镜像仓库！)</p><p><img src="/pic/4a105decfbf4a49ac1c660091b87418e.png" alt="截图"></p><p><img src="/pic/f49f57bbfe2c62d2a5c54b898d94ce24.png" alt="截图"></p><br/><p><strong>dockerfile是面向开发的，我们以后要发布项目，做镜像，就需要编写dockerfile文件，这个文件十分简单！</strong></p><p>&#x3D;&#x3D;Docker镜像逐渐成为企业交付的标准，必须要掌握！&#x3D;&#x3D;</p><br/><p><strong>步骤：开发，部署，运维。。。缺一不可</strong></p><ul><li>DockerFile:构建文件，定义了一切的步骤，源代码</li><li>Dockerlmages:通过DockerFile构建生成的镜像，最终发布和运行的产品！</li><li>Docker容器：容器就是镜像运行起来提供服务器</li></ul><br/><p><img src="/pic/83ae3813ab76ef7f521bbbb61aa89f30.png" alt="截图"></p><p><img src="/pic/fe5789bef1d222aaed7c55d0d3a99709.png" alt="截图"></p><pre class="line-numbers language-powershell" data-language="powershell"><code class="language-powershell"><span class="token keyword">FROM</span>    <span class="token comment">#基础镜镜像，一切从这里开始构建</span>MAINTAINER    <span class="token comment">#镜像是谁写的，姓名+邮箱</span>RUN   <span class="token comment">#镜像构建的时候需要运行的命令</span>ADD   <span class="token comment">#步骤：tomcat镜像，这个tomcat压缩包！添加内容</span>WORKDIR       <span class="token comment">#镜像的工作目录</span>VOLUME      <span class="token comment">#挂载的目录</span>EXPOST    <span class="token comment">#保留端口配置</span>ENTRYPOINT  <span class="token comment">#指定这个容器启动的时候要运行的命令，可以追加命令</span>ONBUILD   <span class="token comment">#当构建一个被继承DockerFile这个时候就会运行ONBUILD的指令。触发指令。</span><span class="token function">COPY</span>    <span class="token comment">#类似ADD，将我们文件拷贝到镜像中</span>ENV   <span class="token comment">#构建的时候设置环境变量！</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><br/><br/><br/><br/><br/><br/><br/><br/><br/><p>Dockerfile就是用来构建docker镜像的构建文件！命令脚本！先体验一下！<br>通过这个脚本可以生成镜像，镜像是一层一层的，脚本一个个的命令，每个命令都是一层！</p><pre class="line-numbers language-powershell" data-language="powershell"><code class="language-powershell"><span class="token comment">#创建一个dockerfi1e文件，名字可以随机建议Dockerfi1e</span><span class="token comment">#文件中的内容指令（大写）参数</span><span class="token keyword">FROM</span> ubuntu:18<span class="token punctuation">.</span>04VOLUME <span class="token punctuation">[</span><span class="token string">"volume01"</span><span class="token punctuation">,</span><span class="token string">"volume02"</span><span class="token punctuation">]</span><span class="token comment">#匿名挂载，未来使用特别多</span>CMD <span class="token function">echo</span> <span class="token string">"-------end-------"</span>CMD <span class="token operator">/</span>bin/bash<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p><img src="/pic/9082fb7a18429cafcfa617e90b675cd1.png" alt="截图"></p><br/><pre class="line-numbers language-powershell" data-language="powershell"><code class="language-powershell"><span class="token string">"Mounts"</span>: <span class="token punctuation">[</span>    <span class="token punctuation">&#123;</span>        <span class="token string">"Type"</span>: <span class="token string">"volume"</span><span class="token punctuation">,</span>        <span class="token string">"Name"</span>: <span class="token string">"22c524e180d59166fb95b5012971cfe1e5566bca1ceb5ecabd26fbe20d6371c0"</span><span class="token punctuation">,</span>        <span class="token string">"Source"</span>: <span class="token string">"/var/lib/docker/volumes/22c524e180d59166fb95b5012971cfe1e5566bca1ceb5ecabd26fbe20d6371c0/_data"</span><span class="token punctuation">,</span>        <span class="token string">"Destination"</span>: <span class="token string">"volume01"</span><span class="token punctuation">,</span>        <span class="token string">"Driver"</span>: <span class="token string">"local"</span><span class="token punctuation">,</span>        <span class="token string">"Mode"</span>: <span class="token string">""</span><span class="token punctuation">,</span>        <span class="token string">"RW"</span>: true<span class="token punctuation">,</span>        <span class="token string">"Propagation"</span>: <span class="token string">""</span>    <span class="token punctuation">&#125;</span><span class="token punctuation">,</span>    <span class="token punctuation">&#123;</span>        <span class="token string">"Type"</span>: <span class="token string">"volume"</span><span class="token punctuation">,</span>        <span class="token string">"Name"</span>: <span class="token string">"20a0e49e357f110e9452991d152ec1a8ac34d48a8c77b0fed84a1d8232f6282b"</span><span class="token punctuation">,</span>        <span class="token string">"Source"</span>: <span class="token string">"/var/lib/docker/volumes/20a0e49e357f110e9452991d152ec1a8ac34d48a8c77b0fed84a1d8232f6282b/_data"</span><span class="token punctuation">,</span>        <span class="token string">"Destination"</span>: <span class="token string">"volume02"</span><span class="token punctuation">,</span>        <span class="token string">"Driver"</span>: <span class="token string">"local"</span><span class="token punctuation">,</span>        <span class="token string">"Mode"</span>: <span class="token string">""</span><span class="token punctuation">,</span>        <span class="token string">"RW"</span>: true<span class="token punctuation">,</span>        <span class="token string">"Propagation"</span>: <span class="token string">""</span>    <span class="token punctuation">&#125;</span><span class="token punctuation">]</span><span class="token punctuation">,</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>测试一下刚才的文件是否同步出去了！</p><br/><p>这种方式我们未来使用的十分多，因为我们通常会构建自己的镜像！</p><br/><p>假设构建镜像时候没有挂载卷，要手动镜像挂载V卷名：容器内路径！</p><br/><h3 id="数据卷挂载"><a href="#数据卷挂载" class="headerlink" title="数据卷挂载"></a>数据卷挂载</h3><p>多个容器实现数据同步</p><p><img src="/pic/eb4af3f4a09cdc70786469482529743b.png" alt="截图"></p><pre class="line-numbers language-powershell" data-language="powershell"><code class="language-powershell">chy@ocean:~<span class="token operator">/</span>docker_volume$ docker run <span class="token operator">-</span>it <span class="token operator">--</span>name chy_ubuntu02 <span class="token operator">--</span>volumes-<span class="token keyword">from</span> chy_ubuntu01 chy_ubuntu:1<span class="token punctuation">.</span>0  <span class="token operator">/</span>bin/bashroot@bd9d9eddee71:<span class="token operator">/</span><span class="token comment"># ls</span>bin  boot  dev  etc  home  lib  lib64  media  mnt  opt  proc  root  run  sbin  srv  sys  tmp  usr  <span class="token keyword">var</span>  volume01  volume02root@bd9d9eddee71:<span class="token operator">/</span><span class="token comment"># cd volume01</span>root@bd9d9eddee71:<span class="token operator">/</span>volume01<span class="token comment"># ls</span>docker01<span class="token punctuation">.</span>txtroot@bd9d9eddee71:<span class="token operator">/</span>volume01<span class="token comment"># touch docker02.txt</span>root@bd9d9eddee71:<span class="token operator">/</span>volume01<span class="token comment"># ls</span>docker01<span class="token punctuation">.</span>txt  docker02<span class="token punctuation">.</span>txtroot@bd9d9eddee71:<span class="token operator">/</span>volume01<span class="token comment"># ls</span>docker01<span class="token punctuation">.</span>txt  docker02<span class="token punctuation">.</span>txt  docker03<span class="token punctuation">.</span>txtroot@bd9d9eddee71:<span class="token operator">/</span>volume01<span class="token comment"># </span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><pre class="line-numbers language-powershell" data-language="powershell"><code class="language-powershell">chy@ocean:~$ docker  run <span class="token operator">-</span>it <span class="token operator">--</span>name chy_ubuntu03 <span class="token operator">--</span>volumes-<span class="token keyword">from</span> chy_ubuntu02 chy_ubuntu:1<span class="token punctuation">.</span>0 <span class="token operator">/</span>bin/bashroot@0872aa038cfb:<span class="token operator">/</span><span class="token comment"># ls</span>bin   dev  home  lib64  mnt  proc  run   srv  tmp  <span class="token keyword">var</span>       volume02boot  etc  lib   media  opt  root  sbin  sys  usr  volume01root@0872aa038cfb:<span class="token operator">/</span><span class="token comment"># cd volume01</span>root@0872aa038cfb:<span class="token operator">/</span>volume01<span class="token comment"># ls</span>docker01<span class="token punctuation">.</span>txt  docker02<span class="token punctuation">.</span>txtroot@0872aa038cfb:<span class="token operator">/</span>volume01<span class="token comment"># touch docker03.txt</span>root@0872aa038cfb:<span class="token operator">/</span>volume01<span class="token comment"># ls</span>docker01<span class="token punctuation">.</span>txt  docker02<span class="token punctuation">.</span>txt  docker03<span class="token punctuation">.</span>txtroot@0872aa038cfb:<span class="token operator">/</span>volume01<span class="token comment"># </span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>&#x3D;&#x3D;主要使用的是 –volumes-from 实现容器间的数据共享 &#x3D;&#x3D;</p><p><strong>结论：</strong></p><p>容器之间配置信息的传递，数据卷容器的生命周期一直持续到没有容器使用为止。<br>但是一旦你持久化到了本地，这个时候，本地的数据是不会删除的！</p><h3 id="docker-compose常用命令"><a href="#docker-compose常用命令" class="headerlink" title="docker-compose常用命令"></a>docker-compose常用命令</h3><br/><pre class="line-numbers language-powershell" data-language="powershell"><code class="language-powershell">docker-compose up <span class="token operator">-</span>d nginx                     构建建启动nignx容器docker-compose exec nginx bash            登录到nginx容器中docker-compose down                              删除所有nginx容器<span class="token punctuation">,</span>镜像docker-compose <span class="token function">ps</span>                                   显示所有容器docker-compose restart nginx                   重新启动nginx容器docker-compose run <span class="token operator">--</span>no-deps <span class="token operator">--</span><span class="token function">rm</span> php-fpm php <span class="token operator">-</span>v  在php-fpm中不启动关联容器，并容器执行php <span class="token operator">-</span>v 执行完成后删除容器docker-compose build nginx                     构建镜像 。        docker-compose build <span class="token operator">--</span>no-cache nginx   不带缓存的构建。docker-compose logs  nginx                     查看nginx的日志 docker-compose logs <span class="token operator">-</span>f nginx                   查看nginx的实时日志docker-compose config  <span class="token operator">-</span>q                        验证（docker-compose<span class="token punctuation">.</span>yml）文件配置，当配置正确时，不输出任何内容，当文件配置错误，输出错误信息。 docker-compose events <span class="token operator">--</span>json nginx       以json的形式输出nginx的docker日志docker-compose pause nginx                 暂停nignx容器docker-compose unpause nginx             恢复ningx容器docker-compose <span class="token function">rm</span> nginx                       删除容器（删除前必须关闭容器）docker-compose stop nginx                    停止nignx容器docker-compose <span class="token function">start</span> nginx                    启动nignx容器<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h2 id="Docker-小结"><a href="#Docker-小结" class="headerlink" title="Docker 小结"></a>Docker 小结</h2><p><img src="/pic/82819ea9bbc4474aba79ad5353b2f288.png" alt="截图"></p><br/><h2 id="x3D-x3D-Docker-网络（未来安排，集成部署）-x3D-x3D"><a href="#x3D-x3D-Docker-网络（未来安排，集成部署）-x3D-x3D" class="headerlink" title="&#x3D;&#x3D;Docker 网络（未来安排，集成部署）&#x3D;&#x3D;"></a><strong>&#x3D;&#x3D;Docker 网络（未来安排，集成部署）&#x3D;&#x3D;</strong></h2><h3 id="理解Docker0"><a href="#理解Docker0" class="headerlink" title="理解Docker0"></a>理解Docker0</h3><p>清空所有环境</p><blockquote><p> 测试</p></blockquote><pre class="line-numbers language-powershell" data-language="powershell"><code class="language-powershell">1: lo: &lt;LOOPBACK<span class="token punctuation">,</span>UP<span class="token punctuation">,</span>LOWER_UP> mtu 65536 qdisc noqueue state UNKNOWN <span class="token function">group</span> default qlen 1000    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00    inet 127<span class="token punctuation">.</span>0<span class="token punctuation">.</span>0<span class="token punctuation">.</span>1/8 scope host lo<span class="token comment">#本机回环地址</span>       valid_lft forever preferred_lft forever    inet6 ::1/128 scope host        valid_lft forever preferred_lft forever2: enp5s0: &lt;BROADCAST<span class="token punctuation">,</span>MULTICAST<span class="token punctuation">,</span>UP<span class="token punctuation">,</span>LOWER_UP> mtu 1500 qdisc fq_codel state UP <span class="token function">group</span> default qlen 1000    link/ether c8:7f:54:57:48:62 brd ff:ff:ff:ff:ff:ff    inet 192<span class="token punctuation">.</span>168<span class="token punctuation">.</span>1<span class="token punctuation">.</span>40/24 brd 192<span class="token punctuation">.</span>168<span class="token punctuation">.</span>1<span class="token punctuation">.</span>255 scope global dynamic noprefixroute enp5s0       valid_lft 5419sec preferred_lft 5419sec<span class="token comment">#阿里云内网地址</span>    inet6 fe80::f8aa:d5c6:44bb:4a07/64 scope link noprefixroute        valid_lft forever preferred_lft forever       4: docker0: &lt;BROADCAST<span class="token punctuation">,</span>MULTICAST<span class="token punctuation">,</span>UP<span class="token punctuation">,</span>LOWER_UP> mtu 1500 qdisc noqueue state UP <span class="token function">group</span> default     link/ether 02:42:87:11:cb:17 brd ff:ff:ff:ff:ff:ff    inet 172<span class="token punctuation">.</span>17<span class="token punctuation">.</span>0<span class="token punctuation">.</span>1/16 brd 172<span class="token punctuation">.</span>17<span class="token punctuation">.</span>255<span class="token punctuation">.</span>255 scope global docker0       valid_lft forever preferred_lft forever<span class="token comment">#docker地址</span>    inet6 fe80::42:87ff:fe11:cb17/64 scope link        valid_lft forever preferred_lft forever<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>三个网络</p><br/><pre class="line-numbers language-powershell" data-language="powershell"><code class="language-powershell">root@e75dea24bd59:~<span class="token operator">/</span>catkin_ws<span class="token comment"># ifconfig</span>eth0: flags=4163&lt;UP<span class="token punctuation">,</span>BROADCAST<span class="token punctuation">,</span>RUNNING<span class="token punctuation">,</span>MULTICAST>  mtu 1500        inet 172<span class="token punctuation">.</span>17<span class="token punctuation">.</span>0<span class="token punctuation">.</span>4  netmask 255<span class="token punctuation">.</span>255<span class="token punctuation">.</span>0<span class="token punctuation">.</span>0  broadcast 172<span class="token punctuation">.</span>17<span class="token punctuation">.</span>255<span class="token punctuation">.</span>255        ether 02:42:<span class="token function">ac</span>:11:00:04  txqueuelen 0  <span class="token punctuation">(</span>Ethernet<span class="token punctuation">)</span>        RX packets 35  bytes 4999 <span class="token punctuation">(</span>4<span class="token punctuation">.</span>9 KB<span class="token punctuation">)</span>        RX errors 0  dropped 0  overruns 0  frame 0        TX packets 0  bytes 0 <span class="token punctuation">(</span>0<span class="token punctuation">.</span>0 B<span class="token punctuation">)</span>        TX errors 0  dropped 0 overruns 0  carrier 0  collisions 0lo: flags=73&lt;UP<span class="token punctuation">,</span>LOOPBACK<span class="token punctuation">,</span>RUNNING>  mtu 65536        inet 127<span class="token punctuation">.</span>0<span class="token punctuation">.</span>0<span class="token punctuation">.</span>1  netmask 255<span class="token punctuation">.</span>0<span class="token punctuation">.</span>0<span class="token punctuation">.</span>0        loop  txqueuelen 1000  <span class="token punctuation">(</span>Local Loopback<span class="token punctuation">)</span>        RX packets 0  bytes 0 <span class="token punctuation">(</span>0<span class="token punctuation">.</span>0 B<span class="token punctuation">)</span>        RX errors 0  dropped 0  overruns 0  frame 0        TX packets 0  bytes 0 <span class="token punctuation">(</span>0<span class="token punctuation">.</span>0 B<span class="token punctuation">)</span>        TX errors 0  dropped 0 overruns 0  carrier 0  collisions 0root@e75dea24bd59:~<span class="token operator">/</span>catkin_ws<span class="token comment"># read escape sequence</span>chy@ocean:~$ docker <span class="token function">ps</span> CONTAINER ID   IMAGE                 COMMAND                   CREATED        STATUS          PORTS                                       NAMESe75dea24bd59   4483fce64730          <span class="token string">"/entrypoint-melodic…"</span>   46 hours ago   Up 2 minutes                                                elastic_brownd11b0f59e8b1   portainer/portainer   <span class="token string">"/portainer"</span>              2 days ago     Up 11 minutes   0<span class="token punctuation">.</span>0<span class="token punctuation">.</span>0<span class="token punctuation">.</span>0:8088->9000/tcp<span class="token punctuation">,</span> :::8088->9000/tcp   elated_davincichy@ocean:~$ ping 172<span class="token punctuation">.</span>17<span class="token punctuation">.</span>0<span class="token punctuation">.</span>4PING 172<span class="token punctuation">.</span>17<span class="token punctuation">.</span>0<span class="token punctuation">.</span>4 <span class="token punctuation">(</span>172<span class="token punctuation">.</span>17<span class="token punctuation">.</span>0<span class="token punctuation">.</span>4<span class="token punctuation">)</span> 56<span class="token punctuation">(</span>84<span class="token punctuation">)</span> bytes of <span class="token keyword">data</span><span class="token punctuation">.</span>64 bytes <span class="token keyword">from</span> 172<span class="token punctuation">.</span>17<span class="token punctuation">.</span>0<span class="token punctuation">.</span>4: icmp_seq=1 ttl=64 time=0<span class="token punctuation">.</span>120 ms64 bytes <span class="token keyword">from</span> 172<span class="token punctuation">.</span>17<span class="token punctuation">.</span>0<span class="token punctuation">.</span>4: icmp_seq=2 ttl=64 time=0<span class="token punctuation">.</span>078 ms64 bytes <span class="token keyword">from</span> 172<span class="token punctuation">.</span>17<span class="token punctuation">.</span>0<span class="token punctuation">.</span>4: icmp_seq=3 ttl=64 time=0<span class="token punctuation">.</span>086 ms<span class="token comment">#Linux可以ping通docker容器</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><blockquote><p>原理</p></blockquote><br/><p>1、我们每启动一个docker容器，docker就会给docker容器分配一个ip,我们只要安装了docker,就会有一个网卡docker0<br>桥接模式，使用的技术是evth-pair技术！</p><br/><pre class="line-numbers language-powershell" data-language="powershell"><code class="language-powershell"><span class="token comment">#我们发现这个容器带来网卡，都是一对对的</span><span class="token comment"># evth-pair就是一对的虚拟设备接口，他们都是成对出现的，一段连着协议，一段彼此相连</span><span class="token comment">#正因为有这个特性，evth-pair充当一个桥梁，连接各种虚拟网络设备的</span><span class="token comment">#Openstac,Docker容器之间的连接，OVS的连接，都是使用evth-pair技术</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span></span></code></pre><p><img src="/pic/9bffa4485e6f6dbfc14beeb7a8fd9dd1.png" alt="截图"></p><br/><p>结论：tomcat01和tomcat(02是公用的一个路由器，docker0.<br>所有的容器不指定网络的情况下，都是docker0路由的，docker:会给我们的容器分配一个默认的可用IP<br>255.255.0.1&#x2F;16域局域网<br>0000000.000000.000000.000000<br>255.255.255.255</p><blockquote><p>结论</p></blockquote><br/><p>Docker 使用的是Linux的桥接，宿主机中是IGDocker容器的网桥 docker0</p><br/><br/><p><img src="/pic/09cd8423a8f4a1175f54bfa430c0ac5a.png" alt="截图"></p><p>Docker 中所哟的网络接口都是虚拟的，虚拟的转发效率高（内网传递文件）</p><br/><p>只要容器删除，对应网桥一对也没了</p><br/><h3 id="自定义网络"><a href="#自定义网络" class="headerlink" title="自定义网络"></a>自定义网络</h3><br/><blockquote><p> 查看所有的docker网络</p></blockquote><pre class="line-numbers language-powershell" data-language="powershell"><code class="language-powershell">chy@ocean:~$ docker network <span class="token function">ls</span>NETWORK ID     NAME                  DRIVER    SCOPE0e0cf6ea89e8   bridge                bridge    local2a0cbafc7650   composetest_default   bridge    local9a4f385414eb   host                  host      local7772e34bde04   none                  null      local<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h3 id="网络模式"><a href="#网络模式" class="headerlink" title="网络模式"></a>网络模式</h3><p>bridge:桥接docker(默认，自己创建也是使用桥接模式)</p><p>none:不配置网络</p><p>host:和宿主机共享网络</p><p>container:容器网络连通！（用的少！局限很大）</p><br/><pre class="line-numbers language-powershell" data-language="powershell"><code class="language-powershell">Usage:  docker network COMMANDManage networksCommands:  connect     Connect a container to a network  create      Create a network  disconnect  Disconnect a container <span class="token keyword">from</span> a network  inspect     Display detailed information on one or more networks  <span class="token function">ls</span>          List networks  prune       Remove all unused networks  <span class="token function">rm</span>          Remove one or more networksRun <span class="token string">'docker network COMMAND --help'</span> <span class="token keyword">for</span> more information on a command<span class="token punctuation">.</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><pre class="line-numbers language-powershell" data-language="powershell"><code class="language-powershell"><span class="token comment">#docker0特点：默认，域名不能访问，--1ink可以打通连接！</span><span class="token comment">#我们可以自定义一个网络！</span><span class="token comment">#--driver bridge</span><span class="token comment">#--subnet192.168.0.0/16</span><span class="token comment">#--gateway192.168.0.1</span><span class="token namespace">[root@kuangshen /]</span>docker network create <span class="token operator">--</span>driver bridge <span class="token operator">--</span>subnet 192<span class="token punctuation">.</span>168<span class="token punctuation">.</span>0<span class="token punctuation">.</span>0/16 <span class="token operator">--</span>gateway 192<span class="token punctuation">.</span>168<span class="token punctuation">.</span>0<span class="token punctuation">.</span>1myneteb21272b3a35 ceaba11b4aa5bbff131c3fb09c4790f0852ed4540707438db052<span class="token namespace">[root@kuangshen /]</span>docker network 1s<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>]]></content>
      
      
      <categories>
          
          <category> docker </category>
          
      </categories>
      
      
        <tags>
            
            <tag> docker </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>c++核心编程</title>
      <link href="/2023/04/11/c-base2/"/>
      <url>/2023/04/11/c-base2/</url>
      
        <content type="html"><![CDATA[<h1 id="内存分区模型"><a href="#内存分区模型" class="headerlink" title="内存分区模型"></a>内存分区模型</h1><p>c++程序执行时，将内存大方向分为4个区域：</p><ul><li>代码区：存放函数体的<strong>二进制代码</strong>，由操作系统进行管理的</li><li>全局区：存放全局变量和静态变量以及常量</li><li>栈区：由编译器自动分配释放，存放函数的参数值，局部变量等</li><li>堆区：由程序员分配和释放，若程序员不释放，程序结束时由操作系统回收</li></ul><p>内存四区的意义：</p><p><em><strong>不同区域存放的数据，赋予不同声明周期给我们更大的灵活编程</strong></em></p><h2 id="程序运行前"><a href="#程序运行前" class="headerlink" title="程序运行前"></a>程序运行前</h2><p>在程序编译后，生成了exe可执行程序，<strong>未执行该程序前</strong>分为两个区域</p><p><strong>代码区：</strong></p><ul><li>存放CPU执行的机器指令</li><li>代码区是共享的，共享的目的是对于频繁被执行的程序，只需要在内存中有一份代码即可</li><li>代码区是只读的，使其只读的原因是防止程序意外的修改了它的指令</li></ul><p><strong>全局区：</strong></p><ul><li><strong>全局变量</strong>和<strong>静态变量</strong>存放在此</li><li>全局区还包含**常量区，字符串常量和其他常量(const修饰的全局变量)**也存在于此</li><li>该区域的数据在程序执行结束后由<strong>操作系统</strong>释放</li></ul><blockquote><p>注意：局部变量和const修饰的局部变量不在全局区里</p></blockquote><p>总结：</p><ul><li>c++中程序运行前分为全局区和代码区</li><li>代码区特点是共享和只读</li><li>全局区中存放全局变量、静态变量、常量 </li><li>常量区存放const修饰的全局常量和字符串常量</li></ul><p>示例</p><pre class="line-numbers language-c++" data-language="c++"><code class="language-c++">#include&lt;iostream&gt;using namespace std;&#x2F;&#x2F;全局变量int g_a&#x3D;10;int g_b&#x3D;20;const int c_g_a&#x3D;10;const int c_g_b&#x3D;20;int main()&#123;    &#x2F;&#x2F;全局区    &#x2F;&#x2F;全局变量、静态变量，常量    &#x2F;&#x2F;创建普通局部变量    int a&#x3D;10;    int b &#x3D;20;    cout&lt;&lt;&quot;局部变量a的地址:&quot;&lt;&lt;(long long)&amp;a&lt;&lt;endl;    cout&lt;&lt;&quot;局部变量b的地址:&quot;&lt;&lt;(long long)&amp;b&lt;&lt;endl;&#x2F;&#x2F;     局部变量a的地址:140723942533960&#x2F;&#x2F; 局部变量b的地址:140723942533964    cout&lt;&lt;&quot;全局变量g_a的地址:&quot;&lt;&lt;(long long)&amp;g_a&lt;&lt;endl;    cout&lt;&lt;&quot;全局变量g_b的地址:&quot;&lt;&lt;(long long)&amp;g_b&lt;&lt;endl;&#x2F;&#x2F;     全局变量g_a的地址:94603780857872&#x2F;&#x2F; 全局变量g_b的地址:94603780857876    &#x2F;&#x2F;静态变量    static int s_a&#x3D;10;    static int s_b&#x3D;20;    cout&lt;&lt;&quot;静态变量s_a的地址:&quot;&lt;&lt;(long long)&amp;s_a&lt;&lt;endl;    cout&lt;&lt;&quot;静态变量s_b的地址:&quot;&lt;&lt;(long long)&amp;s_b&lt;&lt;endl;&#x2F;&#x2F;     静态变量s_a的地址:94603780857880&#x2F;&#x2F; 静态变量s_b的地址:94603780857884    &#x2F;&#x2F;常量    &#x2F;&#x2F;字符串常量    cout&lt;&lt;&quot;字符串常量的地址：&quot;&lt;&lt;(long long)&amp;&quot;hello world&quot;&lt;&lt;endl;&#x2F;&#x2F; 字符串常量的地址：94603778756100    &#x2F;&#x2F;const修饰的变量：const修饰的全局变量以及const修饰的局部变量    cout&lt;&lt;&quot;全局变量c_g_a的地址:&quot;&lt;&lt;(long long)&amp;c_g_a&lt;&lt;endl;    cout&lt;&lt;&quot;全局变量c_g_b的地址:&quot;&lt;&lt;(long long)&amp;c_g_b&lt;&lt;endl;&#x2F;&#x2F;     全局变量c_g_a的地址:94603778755912&#x2F;&#x2F; 全局变量c_g_b的地址:94603778755916    const int c_l_a&#x3D;10;    const int c_l_b&#x3D;20;    cout&lt;&lt;&quot;局部变量c_l_a的地址:&quot;&lt;&lt;(long long)&amp;c_l_a&lt;&lt;endl;    cout&lt;&lt;&quot;局部变量c_l_b的地址:&quot;&lt;&lt;(long long)&amp;c_l_a&lt;&lt;endl;&#x2F;&#x2F;     局部变量c_l_a的地址:140723942533968&#x2F;&#x2F; 局部变量c_l_b的地址:140723942533968    return 0;&#125;<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h2 id="程序运行后"><a href="#程序运行后" class="headerlink" title="程序运行后"></a>程序运行后</h2><p><strong>栈区：</strong></p><ul><li>由编译器自动释放，存放函数的参数值，局部变量等</li></ul><blockquote><p>注意事项：不要返回局部变量的地址，栈区开辟的数据由编译器自动释放</p></blockquote><p>示例</p><pre class="line-numbers language-c++" data-language="c++"><code class="language-c++">#include&lt;iostream&gt;using namespace std;&#x2F;&#x2F;栈区数据的注意事项 ----不要返回局部变量的地址&#x2F;&#x2F;栈区的数据由编译器管理开辟与释放int* func(int b)&#123;&#x2F;&#x2F;形参数据也会放在栈区    b&#x3D;100;    int a &#x3D;10;&#x2F;&#x2F;局部变量存放栈区，栈区数据在函数执行完后自动释放    return &amp;a;&#125;int main()&#123;    int *p&#x3D;func(1);    cout&lt;&lt;*p&lt;&lt;endl;    system(&quot;read -p &#39;Press Enter to continue...&#39; var&quot;);    return 0;&#125;<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p><strong>堆区：</strong></p><p>由程序员分配释放，若程序员不释放，程序结束时由系统回收</p><p>在c++中主要利用new在堆区开辟内存</p><p>示例</p><pre class="line-numbers language-c++" data-language="c++"><code class="language-c++">#include&lt;iostream&gt;using namespace std;int *func()&#123;    &#x2F;&#x2F;利用new关键字可以将数据开辟到堆区    &#x2F;&#x2F;指针本质也是局部变量，放在栈上。指针保存的数据放在堆区    int *p&#x3D;new int(10);    return p;&#125;int main()&#123;    &#x2F;&#x2F;在堆区开辟数据    int *p&#x3D;func();    cout&lt;&lt;*p&lt;&lt;endl;    system(&quot;read -p &#39;Press Enter to continue...&#39; var&quot;);    return 0;&#125;<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h2 id="new操作符"><a href="#new操作符" class="headerlink" title="new操作符"></a>new操作符</h2><p>c++中利用new操作符在堆区开辟数据</p><p>堆区开辟的数据由程序员手动开辟与释放，释放利用操作符delect</p><p>语法：new 数据类型</p><p>利用new创建的数据，会返回该数据对应的类型的指针</p><h1 id="引用"><a href="#引用" class="headerlink" title="引用"></a>引用</h1><h2 id="引用的基本能使用"><a href="#引用的基本能使用" class="headerlink" title="引用的基本能使用"></a>引用的基本能使用</h2><p>作用：给变量起别名</p><p>语法：数据类型 &amp;别名&#x3D;原名</p><p>示例：</p><pre class="line-numbers language-c++" data-language="c++"><code class="language-c++">#include&lt;iostream&gt;using namespace std;int main()&#123;    int a&#x3D;10;    int &amp;b&#x3D;a;    b&#x3D;20;    cout&lt;&lt;a&lt;&lt;endl;    &#x2F;&#x2F;20<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p><a href="https://blog.csdn.net/qq_58325487/article/details/124691945">c++引用入门</a></p><h2 id="引用的注意事项"><a href="#引用的注意事项" class="headerlink" title="引用的注意事项"></a>引用的注意事项</h2><ul><li>引用必须初始化</li><li>引用初始化后，不可以改变</li></ul><p>示例：</p><pre class="line-numbers language-c++" data-language="c++"><code class="language-c++">#include&lt;iostream&gt;using namespace std;int main()&#123;    int a&#x3D;10;    &#x2F;&#x2F;int&amp; b; &#x2F;&#x2F;错误，必须要初始化    int &amp;b&#x3D;a;    int c&#x3D;20;    b&#x3D;c;&#x2F;&#x2F;赋值操作不是更改引用    cout&lt;&lt;a&lt;&lt;endl;    &#x2F;&#x2F;20&#125;<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h2 id="y引用做函数参数"><a href="#y引用做函数参数" class="headerlink" title="y引用做函数参数"></a>y引用做函数参数</h2><p>作用：函数传参时，可以利用引用的技术让形参修饰实参</p><p>有点：可以简化指针修改实参</p><p>示例：</p><pre class="line-numbers language-c++" data-language="c++"><code class="language-c++"><span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre>]]></content>
      
      
      <categories>
          
          <category> c++ </category>
          
      </categories>
      
      
        <tags>
            
            <tag> c++ </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>linux环境下c++实现通讯录系统</title>
      <link href="/2023/04/11/c-base/"/>
      <url>/2023/04/11/c-base/</url>
      
        <content type="html"><![CDATA[<h1 id="通讯录系统"><a href="#通讯录系统" class="headerlink" title="通讯录系统"></a>通讯录系统</h1><p>该系统具有下面7种操作：</p><hr><p>***** 1、添加联系人*****</p><p>***** 2、显示联系人*****</p><p>***** 3、删除联系人*****</p><p>***** 4、查找联系人*****</p><p>***** 5、修改联系人*****</p><p>***** 6、清空联系人*****</p><p>***** 0、退出通讯录 ****</p><hr><p>源代码如下：</p><pre class="line-numbers language-c++" data-language="c++"><code class="language-c++">&#x2F;*封装函数显示界面，如void showMenu()*&#x2F;#include&lt;iostream&gt;#include &lt;unistd.h&gt;#include &lt;string&gt;using namespace std;#define MAX 1000&#x2F;&#x2F;联系人结构体struct Person&#123;    string m_Name;    int m_Sex;    int m_Age;    string m_Phone;    string m_Addr;&#125;;&#x2F;&#x2F;通讯录结构体struct Addressbooks&#123;    Person personArray [MAX];    int m_Size;&#x2F;&#x2F;通讯录中人员个数&#125;;&#x2F;&#x2F;1.添加联系人void addPerson(Addressbooks *abs)&#123;        if(abs-&gt;m_Size&#x3D;&#x3D;MAX)&#123;            cout&lt;&lt;&quot;通讯录已满，无法添加&quot;&lt;&lt; endl;            return;        &#125;else&#123;            string name;            cout&lt;&lt;&quot;请输入姓名：&quot;&lt;&lt;endl;            cin&gt;&gt;name;            abs-&gt;personArray[abs-&gt;m_Size].m_Name&#x3D;name;            cout&lt;&lt;&quot;1---男&quot;&lt;&lt;endl;            cout&lt;&lt;&quot;2---女&quot;&lt;&lt;endl;            int sex&#x3D;0;            while(true)&#123;                cin&gt;&gt;sex;                    if(sex&#x3D;&#x3D;1||sex&#x3D;&#x3D;2)&#123;                        abs-&gt;personArray[abs-&gt;m_Size].m_Sex&#x3D;sex;                        break;         &#125;            cout&lt;&lt;&quot;输入有误，请重新输入&quot;&lt;&lt;endl;        &#125;        cout&lt;&lt;&quot;请输入年龄&quot;&lt;&lt;endl;        int age &#x3D;0;        cin&gt;&gt;age;        abs-&gt;personArray[abs-&gt;m_Size].m_Age&#x3D;age;        cout&lt;&lt;&quot;请输入电话号码&quot;&lt;&lt;endl;        string iphone;        cin&gt;&gt;iphone;        abs-&gt;personArray[abs-&gt;m_Size].m_Phone&#x3D;iphone;        cout&lt;&lt;&quot;请输入家庭住址&quot;&lt;&lt;endl;        string address;        cin&gt;&gt;address;        abs-&gt;personArray[abs-&gt;m_Size].m_Addr&#x3D;address;        abs-&gt;m_Size++;        cout&lt;&lt;&quot;添加成功&quot;&lt;&lt;endl;        &#125;                        &#x2F;&#x2F;linux按任意键继续命令        system(&quot;read -p &#39;Press Enter to continue...&#39; var&quot;);        system(&quot;clear&quot;);&#125;&#x2F;&#x2F;2.示所有的联系人void showPerson(Addressbooks *abs)&#123;    if(abs-&gt;m_Size&#x3D;&#x3D;0)&#123;        cout&lt;&lt;&quot;当前记录为空&quot;&lt;&lt;endl;    &#125;else&#123;        for(int i&#x3D;0;i&lt;abs-&gt;m_Size;i++)&#123;            cout&lt;&lt;&quot;姓名：&quot;&lt;&lt;abs-&gt;personArray[i].m_Name&lt;&lt;&quot;\t&quot;;            cout&lt;&lt;&quot;性别：&quot;&lt;&lt;(abs-&gt;personArray[i].m_Sex &#x3D;&#x3D;1 ?&quot;男&quot;:&quot;女&quot;)&lt;&lt;&quot;\t&quot;;            cout&lt;&lt;&quot;年龄：&quot;&lt;&lt;abs-&gt;personArray[i].m_Age&lt;&lt;&quot;\t&quot;;            cout&lt;&lt;&quot;电话：&quot;&lt;&lt;abs-&gt;personArray[i].m_Phone&lt;&lt;&quot;\t&quot;;            cout&lt;&lt;&quot;地址：&quot;&lt;&lt;abs-&gt;personArray[i].m_Addr&lt;&lt;endl;        &#125;    &#125;                &#x2F;&#x2F;linux按任意键继续命令            system(&quot;read -p &#39;Press Enter to continue...&#39; var&quot;);            system(&quot;clear&quot;);&#x2F;&#x2F;清屏&#125;    &#x2F;&#x2F;检测联系人是否存在，如果存在返回数组的具体位置，不存在返回-1int isExist(Addressbooks *abs,string name)&#123;        for(int i&#x3D;0;i&lt;abs-&gt;m_Size;i++)&#123;            if(abs-&gt;personArray[i].m_Name&#x3D;&#x3D;name)&#123;                    return i;            &#125;        &#125;        return -1;&#125;&#x2F;&#x2F;3.删除指定联系人void deletePerson(Addressbooks *abs)&#123;    cout&lt;&lt;&quot;请输入您要删除的联系人&quot;&lt;&lt;endl;    string name;    cin&gt;&gt;name;    int ret&#x3D;isExist(abs,name);    if(ret&#x3D;&#x3D;-1)&#123;        cout&lt;&lt;&quot;查无此人&quot;&lt;&lt;endl;    &#125;else&#123;            for(int i&#x3D;ret;i&lt;abs-&gt;m_Size;i++)&#123;                &#x2F;&#x2F;数据迁移                abs-&gt;personArray[i]&#x3D;abs-&gt;personArray[i+1];            &#125;            abs-&gt;m_Size--;            cout&lt;&lt;&quot;删除成功&quot;&lt;&lt;endl;    &#125;                &#x2F;&#x2F;linux按任意键继续命令            system(&quot;read -p &#39;Press Enter to continue...&#39; var&quot;);            system(&quot;clear&quot;);&#x2F;&#x2F;清屏&#125;&#x2F;&#x2F;4.查找联系人信息void findPerson(Addressbooks *abs)&#123;    cout&lt;&lt;&quot;请输入您要查找的联系人&quot;&lt;&lt;endl;    string name;    cin&gt;&gt;name;    int ret &#x3D;isExist(abs,name);    if(ret!&#x3D;-1)&#123;            cout&lt;&lt;&quot;姓名：&quot;&lt;&lt;abs-&gt;personArray[ret].m_Name&lt;&lt;&quot;\t&quot;;            cout&lt;&lt;&quot;性别：&quot;&lt;&lt;(abs-&gt;personArray[ret].m_Sex &#x3D;&#x3D;1 ?&quot;男&quot;:&quot;女&quot;)&lt;&lt;&quot;\t&quot;;            cout&lt;&lt;&quot;年龄：&quot;&lt;&lt;abs-&gt;personArray[ret].m_Age&lt;&lt;&quot;\t&quot;;            cout&lt;&lt;&quot;电话：&quot;&lt;&lt;abs-&gt;personArray[ret].m_Phone&lt;&lt;&quot;\t&quot;;            cout&lt;&lt;&quot;地址：&quot;&lt;&lt;abs-&gt;personArray[ret].m_Addr&lt;&lt;endl;    &#125;else&#123;        cout&lt;&lt;&quot;查无此人&quot;&lt;&lt;endl;    &#125;                &#x2F;&#x2F;linux按任意键继续命令            system(&quot;read -p &#39;Press Enter to continue...&#39; var&quot;);            system(&quot;clear&quot;);&#x2F;&#x2F;清屏&#125;&#x2F;&#x2F;5.修改联系人void modifyPerson(Addressbooks* abs)&#123;    cout&lt;&lt;&quot;请输入你要修改的联系人&quot;&lt;&lt;endl;    string name;    cin&gt;&gt;name;    int ret &#x3D; isExist(abs,name);    if(ret!&#x3D;-1)&#123;        string name;        cout&lt;&lt;&quot;请输入姓名：&quot;&lt;&lt;endl;        cin&gt;&gt;name;        abs-&gt;personArray[ret].m_Name&#x3D;name;        cout&lt;&lt;&quot;请输入年龄：&quot;&lt;&lt;endl;        cout&lt;&lt;&quot;1---男&quot;&lt;&lt;endl;        cout&lt;&lt;&quot;2---女&quot;&lt;&lt;endl;        int sex&#x3D;0;        while(true)&#123;            cin&gt;&gt;sex;            if(sex&#x3D;&#x3D;1||sex&#x3D;&#x3D;2)&#123;                abs-&gt;personArray[ret].m_Sex&#x3D;sex;                break;            &#125;            cout&lt;&lt;&quot;输入有误，请重新输入&quot;&lt;&lt;endl;        &#125;        cout&lt;&lt;&quot;请输入年龄：&quot;&lt;&lt;endl;        int age&#x3D;0;        cin&gt;&gt;age;        abs-&gt;personArray[ret].m_Age&#x3D;age;        cout&lt;&lt;&quot;请输入联系电话：&quot;&lt;&lt;endl;        string iphone;        cin&gt;&gt;iphone;        abs-&gt;personArray[ret].m_Phone&#x3D;iphone;        cout&lt;&lt;&quot;请输入家庭住址：&quot;&lt;&lt;endl;        string address;        cin&gt;&gt;address;        abs-&gt;personArray[ret].m_Addr&#x3D;address;        cout&lt;&lt;&quot;修改成功&quot;&lt;&lt;endl;    &#125;else&#123;        cout&lt;&lt;&quot;查无此人&quot;&lt;&lt;endl;    &#125;                    &#x2F;&#x2F;linux按任意键继续命令            system(&quot;read -p &#39;Press Enter to continue...&#39; var&quot;);            system(&quot;clear&quot;);&#x2F;&#x2F;清屏&#125;&#x2F;&#x2F;6.清空联系人void cleanPerson(Addressbooks *abs)&#123;    abs-&gt;m_Size&#x3D;0;    cout&lt;&lt;&quot;通讯录已清空&quot;&lt;&lt;endl;    system(&quot;read -p &#39;Press Enter to continue...&#39; var&quot;);    system(&quot;clear&quot;);&#x2F;&#x2F;清屏&#125;void showMenu()&#123;    cout&lt;&lt;&quot;*************************&quot;&lt;&lt;endl;    cout&lt;&lt;&quot;***** 1、添加联系人*****&quot; &lt;&lt;endl;    cout&lt;&lt;&quot;***** 2、显示联系人*****&quot;&lt;&lt;endl;    cout&lt;&lt;&quot;***** 3、删除联系人*****&quot;&lt;&lt;endl;    cout&lt;&lt;&quot;***** 4、查找联系人*****&quot;&lt;&lt;endl;    cout&lt;&lt;&quot;***** 5、修改联系人*****&quot;&lt;&lt;endl;    cout&lt;&lt;&quot;***** 6、清空联系人*****&quot;&lt;&lt;endl;    cout&lt;&lt;&quot;***** 0、退出通讯录 ****&quot;&lt;&lt;endl;    cout&lt;&lt;&quot;*************************&quot;&lt;&lt;endl;&#125;int main()&#123;    Addressbooks abs;    abs.m_Size&#x3D;0;    int select &#x3D;0;    &#x2F;&#x2F;菜单调用    while(true)&#123;        showMenu();        cin &gt;&gt;select;        switch (select)&#123;            case 1:     &#x2F;&#x2F;添加联系人            addPerson(&amp;abs);                break;            case 2:     &#x2F;&#x2F;显示联系人                showPerson(&amp;abs);                break;            case 3: &#x2F;&#x2F;删除联系人                deletePerson(&amp;abs);            &#x2F;&#x2F; &#123;            &#x2F;&#x2F;     cout&lt;&lt;&quot; 请输入要删除联系人的姓名&quot;&lt;&lt;endl;            &#x2F;&#x2F;     string name;            &#x2F;&#x2F;     cin&gt;&gt;name;            &#x2F;&#x2F;     cout&lt;&lt;(isExist(&amp;abs,name)&#x3D;&#x3D;-1?&quot;查无此人&quot;:&quot;查有此人&quot;)&lt;&lt;endl;            &#x2F;&#x2F; &#125;                break;            case 4:&#x2F;&#x2F;查找联系人                findPerson(&amp;abs);                break;            case 5:&#x2F;&#x2F;修改联系人                modifyPerson(&amp;abs);                break;            case 6: &#x2F;&#x2F;清空联系人                cleanPerson(&amp;abs);                break;            case 0:                cout&lt;&lt;&quot;欢迎下次使用&quot;&lt;&lt;endl;                &#x2F;&#x2F;linux按任意键继续命令                system(&quot;read -p &#39;Press Enter to continue...&#39; var&quot;);                return 0;                break;             default:                break;        &#125;    &#125;&#x2F;&#x2F;linux按任意键继续命令    system(&quot;read -p &#39;Press Enter to continue...&#39; var&quot;);    return 0;&#125;<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>]]></content>
      
      
      <categories>
          
          <category> c++ </category>
          
      </categories>
      
      
        <tags>
            
            <tag> c++ </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>slam14讲源代码的记录（后九讲）</title>
      <link href="/2023/04/08/slam14-3/"/>
      <url>/2023/04/08/slam14-3/</url>
      
        <content type="html"><![CDATA[<h2 id="第六讲"><a href="#第六讲" class="headerlink" title="第六讲"></a>第六讲</h2><p>本讲只要讲解最小二乘法的含义以及处理方式，如高斯牛顿（GN）、列文伯格-马夸尔特法(L-M)等下降法策略</p><h3 id="在ch6-x2F-gaussNewton-cpp中，手写GN法，最小二乘估计曲线参数"><a href="#在ch6-x2F-gaussNewton-cpp中，手写GN法，最小二乘估计曲线参数" class="headerlink" title="在ch6&#x2F;gaussNewton.cpp中，手写GN法，最小二乘估计曲线参数"></a>在ch6&#x2F;gaussNewton.cpp中，手写GN法，最小二乘估计曲线参数</h3><pre class="line-numbers language-c++" data-language="c++"><code class="language-c++">&#x2F;&#x2F;求解误差项    for (int i &#x3D; 0; i &lt; N; i++) &#123;  double xi &#x3D; x_data[i], yi &#x3D; y_data[i];  &#x2F;&#x2F; 第i个数据点  double error &#x3D; yi - exp(ae * xi * xi + be * xi + ce);  Vector3d J; &#x2F;&#x2F; 雅可比矩阵  J[0] &#x3D; -xi * xi * exp(ae * xi * xi + be * xi + ce);  &#x2F;&#x2F; de&#x2F;da  J[1] &#x3D; -xi * exp(ae * xi * xi + be * xi + ce);  &#x2F;&#x2F; de&#x2F;db  J[2] &#x3D; -exp(ae * xi * xi + be * xi + ce);  &#x2F;&#x2F; de&#x2F;dc  H +&#x3D; inv_sigma * inv_sigma * J * J.transpose();  b +&#x3D; -inv_sigma * inv_sigma * error * J;  cost +&#x3D; error * error;&#125;&#x2F;&#x2F; 求解线性方程 Hx&#x3D;b&#x2F;&#x2F;对于正定矩阵，可以使用cholesky分解来解方程Vector3d dx &#x3D; H.ldlt().solve(b);&#x2F;&#x2F;也可以使用QR分解Vector3d dx &#x3D; H.colPivHouseholderQr().solve(b);<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h3 id="ch6-x2F-ceresCurveFitting-cpp"><a href="#ch6-x2F-ceresCurveFitting-cpp" class="headerlink" title="ch6&#x2F;ceresCurveFitting.cpp"></a>ch6&#x2F;ceresCurveFitting.cpp</h3><p>使用ceres拟合曲线</p><p><img src="/pic/%E9%80%89%E5%8C%BA_131.png" alt="Ceres简介"></p><pre class="line-numbers language-c++" data-language="c++"><code class="language-c++">&#x2F;&#x2F; 代价函数的计算模型,结构体struct CURVE_FITTING_COST &#123;  CURVE_FITTING_COST(double x, double y) : _x(x), _y(y) &#123;&#125;&#x2F;&#x2F;构造函数初始化方式，给_x赋值x,_y赋值y  &#x2F;&#x2F; 残差的计算  template&lt;typename T&gt;&#x2F;&#x2F;函数模板  bool operator()(          &#x2F;&#x2F;括号运算符重载    const T *const abc, &#x2F;&#x2F; 模型参数，有3维    T *residual) const &#123;    residual[0] &#x3D; T(_y) - ceres::exp(abc[0] * T(_x) * T(_x) + abc[1] * T(_x) + abc[2]); &#x2F;&#x2F; y-exp(ax^2+bx+c)    return true;  &#125;  const double _x, _y;    &#x2F;&#x2F; x,y数据&#125;;  &#x2F;&#x2F; 构建最小二乘问题  ceres::Problem problem;  for (int i &#x3D; 0; i &lt; N; i++) &#123;    problem.AddResidualBlock(     &#x2F;&#x2F; 向问题中添加误差项      &#x2F;&#x2F; 使用自动求导，模板参数：误差类型，输出维度，输入维度，维数要与前面struct中一致      new ceres::AutoDiffCostFunction&lt;CURVE_FITTING_COST, 1, 3&gt;(        new CURVE_FITTING_COST(x_data[i], y_data[i])      ),      nullptr,            &#x2F;&#x2F; 核函数，这里不使用，为空      abc                 &#x2F;&#x2F; 待估计参数    );  &#125;  &#x2F;&#x2F; 配置求解器  ceres::Solver::Options options;     &#x2F;&#x2F; 这里有很多配置项可以填  options.linear_solver_type &#x3D; ceres::DENSE_NORMAL_CHOLESKY;  &#x2F;&#x2F; 增量方程如何求解  options.minimizer_progress_to_stdout &#x3D; true;   &#x2F;&#x2F; 输出到cout  ceres::Solver::Summary summary;                &#x2F;&#x2F; 优化信息  chrono::steady_clock::time_point t1 &#x3D; chrono::steady_clock::now();  ceres::Solve(options, &amp;problem, &amp;summary);  &#x2F;&#x2F; 开始优化  chrono::steady_clock::time_point t2 &#x3D; chrono::steady_clock::now();  chrono::duration&lt;double&gt; time_used &#x3D; chrono::duration_cast&lt;chrono::duration&lt;double&gt;&gt;(t2 - t1);  cout &lt;&lt; &quot;solve time cost &#x3D; &quot; &lt;&lt; time_used.count() &lt;&lt; &quot; seconds. &quot; &lt;&lt; endl;<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p><a href="http://www.ceres-solver.org/tutorial.html">ceres官方教程</a></p><h3 id="ch6-x2F-g2oCurveFitting-cpp"><a href="#ch6-x2F-g2oCurveFitting-cpp" class="headerlink" title="ch6&#x2F;g2oCurveFitting.cpp"></a>ch6&#x2F;g2oCurveFitting.cpp</h3><p>待更新</p><h2 id="第七讲"><a href="#第七讲" class="headerlink" title="第七讲"></a>第七讲</h2><p>视觉里程计1,vo,特征提取与匹配，对极几何，PnP，ICP,三角化,BA,SVD,直接法，光流法，光度误差</p><h3 id="ch7-x2F-orb-cv-cpp"><a href="#ch7-x2F-orb-cv-cpp" class="headerlink" title="ch7&#x2F;orb_cv.cpp"></a>ch7&#x2F;orb_cv.cpp</h3><p><a href="https://zhuanlan.zhihu.com/p/345482379">ORB特征匹配、手写ORB特征代码详解</a></p><pre class="line-numbers language-c++" data-language="c++"><code class="language-c++">#include &lt;iostream&gt;#include &lt;opencv2&#x2F;core&#x2F;core.hpp&gt;#include &lt;opencv2&#x2F;features2d&#x2F;features2d.hpp&gt;#include &lt;opencv2&#x2F;highgui&#x2F;highgui.hpp&gt;#include &lt;chrono&gt;using namespace std;using namespace cv;int main(int argc, char **argv) &#123;  if (argc !&#x3D; 3) &#123;    cout &lt;&lt; &quot;usage: feature_extraction img1 img2&quot; &lt;&lt; endl;    return 1;  &#125;  &#x2F;&#x2F;-- 读取图像  Mat img_1 &#x3D; imread(argv[1], CV_LOAD_IMAGE_COLOR);  Mat img_2 &#x3D; imread(argv[2], CV_LOAD_IMAGE_COLOR);  assert(img_1.data !&#x3D; nullptr &amp;&amp; img_2.data !&#x3D; nullptr);  &#x2F;&#x2F;-- 初始化  std::vector&lt;KeyPoint&gt; keypoints_1, keypoints_2;  Mat descriptors_1, descriptors_2;  Ptr&lt;FeatureDetector&gt; detector &#x3D; ORB::create();        &#x2F;&#x2F;ORB  Ptr&lt;DescriptorExtractor&gt; descriptor &#x3D; ORB::create();  Ptr&lt;DescriptorMatcher&gt; matcher &#x3D; DescriptorMatcher::create(&quot;BruteForce-Hamming&quot;);  &#x2F;&#x2F;-- 第一步:检测 Oriented FAST 角点位置  chrono::steady_clock::time_point t1 &#x3D; chrono::steady_clock::now();  detector-&gt;detect(img_1, keypoints_1);  detector-&gt;detect(img_2, keypoints_2);  &#x2F;&#x2F;-- 第二步:根据角点位置计算 BRIEF 描述子  descriptor-&gt;compute(img_1, keypoints_1, descriptors_1);  descriptor-&gt;compute(img_2, keypoints_2, descriptors_2);  chrono::steady_clock::time_point t2 &#x3D; chrono::steady_clock::now();  chrono::duration&lt;double&gt; time_used &#x3D; chrono::duration_cast&lt;chrono::duration&lt;double&gt;&gt;(t2 - t1);  cout &lt;&lt; &quot;extract ORB cost &#x3D; &quot; &lt;&lt; time_used.count() &lt;&lt; &quot; seconds. &quot; &lt;&lt; endl;  Mat outimg1,outimg2;  drawKeypoints(img_1, keypoints_1, outimg1, Scalar::all(-1), DrawMatchesFlags::DEFAULT);  drawKeypoints(img_2, keypoints_2, outimg2, Scalar::all(-1), DrawMatchesFlags::DEFAULT);  imshow(&quot;pic1_ORB features&quot;, outimg1);  imshow(&quot;pic2_ORB features&quot;, outimg2);  &#x2F;&#x2F;-- 第三步:对两幅图像中的BRIEF描述子进行匹配，使用 Hamming 距离  vector&lt;DMatch&gt; matches;  t1 &#x3D; chrono::steady_clock::now();  matcher-&gt;match(descriptors_1, descriptors_2, matches);  t2 &#x3D; chrono::steady_clock::now();  time_used &#x3D; chrono::duration_cast&lt;chrono::duration&lt;double&gt;&gt;(t2 - t1);  cout &lt;&lt; &quot;match ORB cost &#x3D; &quot; &lt;&lt; time_used.count() &lt;&lt; &quot; seconds. &quot; &lt;&lt; endl;  &#x2F;&#x2F;-- 第四步:匹配点对筛选  &#x2F;&#x2F; 计算最小距离和最大距离  auto min_max &#x3D; minmax_element(matches.begin(), matches.end(),                                [](const DMatch &amp;m1, const DMatch &amp;m2) &#123; return m1.distance &lt; m2.distance; &#125;);  double min_dist &#x3D; min_max.first-&gt;distance;  double max_dist &#x3D; min_max.second-&gt;distance;  printf(&quot;-- Max dist : %f \n&quot;, max_dist);  printf(&quot;-- Min dist : %f \n&quot;, min_dist);  &#x2F;&#x2F;当描述子之间的距离大于两倍的最小距离时,即认为匹配有误.但有时候最小距离会非常小,设置一个经验值30作为下限.  std::vector&lt;DMatch&gt; good_matches;  for (int i &#x3D; 0; i &lt; descriptors_1.rows; i++) &#123;    if (matches[i].distance &lt;&#x3D; max(2 * min_dist, 30.0)) &#123;      good_matches.push_back(matches[i]);    &#125;  &#125;  &#x2F;&#x2F;-- 第五步:绘制匹配结果  Mat img_match;  Mat img_goodmatch;  drawMatches(img_1, keypoints_1, img_2, keypoints_2, matches, img_match);  drawMatches(img_1, keypoints_1, img_2, keypoints_2, good_matches, img_goodmatch);  imshow(&quot;all matches&quot;, img_match);  imshow(&quot;good matches&quot;, img_goodmatch);  waitKey(0);  return 0;&#125;<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p><a href="https://vimsky.com/examples/usage/cpp-algorithm-minmax_element-function-01.html">C++ Algorithm minmax_element()用法及代码示例</a></p><p><img src="/pic/%E9%80%89%E5%8C%BA_133.png" alt="两图特征点"></p><p><img src="/pic/%E9%80%89%E5%8C%BA_134.png" alt="匹配情况"></p><h3 id="ch7-x2F-orb-self-cpp"><a href="#ch7-x2F-orb-self-cpp" class="headerlink" title="ch7&#x2F;orb_self.cpp"></a>ch7&#x2F;orb_self.cpp</h3><p>手写ORB特征</p><p><strong>(1)FAST角点检测</strong></p><p>opencv库函数：利用ORB特征检测器detector里的detect函数。</p><p>手写：利用改进的FAST算法，增加了中心像素和围绕该像素的圆的像素之间的强度差阈值以及非最大值抑制。</p><p>其中的FAST（）函数结构如下：</p><pre class="line-numbers language-C++" data-language="C++"><code class="language-C++">CV_EXPORTS void FAST( InputArray image, CV_OUT std::vector&lt;KeyPoint&gt;&amp; keypoints,                      int threshold, bool nonmaxSuppression&#x3D;true );&#x2F;*image： 检测的灰度图像keypoints: 在图像上检测到的关键点threshold: 中心像素和围绕该像素的圆的像素之间的强度差阈值nonmaxSuppression: 参数非最大值抑制,默认为真，对检测到的角点应用非最大值抑制。*&#x2F;<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p><strong>（2）描述子计算</strong></p><p>opencv库函数：构建ORB特征描述器descriptor里的compute函数</p><p>手写：自定义函数ComputeORB()来计算描述子，在这个函数里面首先会排除一些靠近边缘的特征点。排除的坏点的判断方式如下：</p><ul><li><p>当kp.pt.x &lt; half_boundary时，边长为boundary的图像块将在-x轴上超出图像。</p></li><li><p>当kp.pt.y &lt; half_boundary时，边长为boundary的图像块将在-y轴上超出图像。</p></li><li><p>当kp.pt.x &gt;&#x3D; img.cols - half_boundary（kp.pt.x&gt;&#x3D; 640-16）时，边长为boundary(32)的图像块将在+x轴上超出图像。</p></li><li><p>当kp.pt.y &gt;&#x3D; img.rows - half_boundary（kp.pt.y&gt;&#x3D; 480-16）时，边长为boundary(32)的图像块将在+y轴上超出图像。</p></li></ul><p>同时，在手写代码中，按照灰度质心法定义了图像块的矩和质心，最后求出了特征点的角度。</p><p>在求描述子时，事先准备了256*4个数据集，这些数据集表示以关键点为中心，[-13,12]的范围内,随机选点对p,q。选取两个点p,q，这两个点的坐标从数据集选取，然后乘上之前求的角度再加上关键点，以此找到关键点附近的两个随机像素，然后比较像素值。最终形成描述子。</p><p><strong>（3）BRIEF描述子匹配函数</strong></p><p>opencv下：利用自带的match函数，比较两副图像的描述子的汉明距离，并从小到大排序在matches容器中，然后在容器中挑选好的描述子，这些描述子满足描述子之间的距离小于两倍的最小距离和经验阈值的最小值，因为最小距离可能是0；</p><p>手写：描述子是采用256位二进制描述，对应到8个32位的unsigned int 数据，并利用SSE指令集计算每个unsigned int变量中1的个数，从而计算汉明距离。手写的暴力匹配代码中输入三个参数，分别是第一副和第二副图像的描述子，和存放输出匹配对的容器；这里的暴力匹配的思路为：取第一副图片中的一个描述子，分别计算与第二副图片每个描述子的汉明距离，然后选取最近的距离以及所对应的匹配对，然后多次选取图片1中的描述子重复上述操作，分别找到最短距离和相应的匹配对。最后再将比较得到的最小距离与设定的经验阈值作比较，如果小于经验阈值则保留并输出该匹配对。</p><p>具体代码如下：</p><pre class="line-numbers language-C++" data-language="C++"><code class="language-C++"> &#x2F;&#x2F; compute the descriptor&#x2F;&#x2F;(1)计算角点的方向；(2)计算描述子。void ComputeORB(const cv::Mat &amp;img, vector&lt;cv::KeyPoint&gt; &amp;keypoints, vector&lt;DescType&gt; &amp;descriptors) &#123;  const int half_patch_size &#x3D; 8; &#x2F;&#x2F;计算特征点方向时，选取的图像块，16*16  const int half_boundary &#x3D; 16;&#x2F;&#x2F;计算描述子时在32*32的图像块中选点  int bad_points &#x3D; 0; &#x2F;&#x2F;计算描述子时，在32*32的区域块选择两个点比较，所选择的点超出图像范围的。出现这种情况下的FAST角点的数目。    &#x2F;&#x2F;遍历所有FAST角点  for (auto &amp;kp: keypoints)   &#123;    &#x2F;&#x2F;超出图像边界的角点的描述子设为空    if (kp.pt.x &lt; half_boundary || kp.pt.y &lt; half_boundary ||        kp.pt.x &gt;&#x3D; img.cols - half_boundary || kp.pt.y &gt;&#x3D; img.rows - half_boundary) &#123;      &#x2F;&#x2F; outside      bad_points++; &#x2F;&#x2F;bad_points的描述子设为空      descriptors.push_back(&#123;&#125;);      continue;    &#125;    &#x2F;&#x2F;计算16*16图像块的灰度质心    &#x2F;&#x2F;可参照下面的图片帮助理解    float m01 &#x3D; 0, m10 &#x3D; 0;&#x2F;&#x2F;图像块的矩 视觉slam十四讲中p157    for (int dx &#x3D; -half_patch_size; dx &lt; half_patch_size; ++dx)     &#123;      for (int dy &#x3D; -half_patch_size; dy &lt; half_patch_size; ++dy)       &#123;        uchar pixel &#x3D; img.at&lt;uchar&gt;(kp.pt.y + dy, kp.pt.x + dx);        m10 +&#x3D; dx * pixel; &#x2F;&#x2F;pixel表示灰度值 视觉slam十四讲中p157最上面的公式        m01 +&#x3D; dy * pixel;&#x2F;&#x2F;pixel表示灰度值 视觉slam十四讲中p157最上面的公式      &#125;    &#125;     &#x2F;&#x2F; angle should be arc tan(m01&#x2F;m10);参照下面第三章图片帮助理解    float m_sqrt &#x3D; sqrt(m01 * m01 + m10 * m10) + 1e-18; &#x2F;&#x2F; avoid divide by zero  1e-18避免了m_sqrt的值为0（图像块全黑）将m01 * m01 + m10 * m10进行开根    float sin_theta &#x3D; m01 &#x2F; m_sqrt;&#x2F;&#x2F;sin_theta &#x3D; m01 &#x2F; 根号下（m01 * m01 + m10 * m10））    float cos_theta &#x3D; m10 &#x2F; m_sqrt;&#x2F;&#x2F;cos_theta &#x3D; m10 &#x2F; 根号下（m01 * m01 + m10 * m10））    &#x2F;&#x2F;因为tan_theta &#x3D; m01&#x2F;m10 即为tan_theta &#x3D; sin_theta &#x2F; cos_theta &#x3D; [m01 &#x2F; 根号下（m01 * m01 + m10 * m10] &#x2F; [m10 &#x2F; 根号下（m01 * m01 + m10 * m10]    &#x2F;&#x2F;目的是求出特征点的方向  视觉slam十四讲中p157第三个公式    &#x2F;&#x2F; compute the angle of this point    DescType desc(8, 0); &#x2F;&#x2F;8个元素，它们的值初始化为0        for (int i &#x3D; 0; i &lt; 8; i++) &#123;      uint32_t d &#x3D; 0;      for (int k &#x3D; 0; k &lt; 32; k++)       &#123;        int idx_pq &#x3D; i * 32 + k;&#x2F;&#x2F;idx_pq表示二进制描述子中的第几位        cv::Point2f p(ORB_pattern[idx_pq * 4], ORB_pattern[idx_pq * 4 + 1]);        cv::Point2f q(ORB_pattern[idx_pq * 4 + 2], ORB_pattern[idx_pq * 4 + 3]);         &#x2F;&#x2F; rotate with theta        &#x2F;&#x2F;p,q绕原点旋转theta得到pp,qq        cv::Point2f pp &#x3D; cv::Point2f(cos_theta * p.x - sin_theta * p.y, sin_theta * p.x + cos_theta * p.y)                         + kp.pt;        cv::Point2f qq &#x3D; cv::Point2f(cos_theta * q.x - sin_theta * q.y, sin_theta * q.x + cos_theta * q.y)                         + kp.pt;        if (img.at&lt;uchar&gt;(pp.y, pp.x) &lt; img.at&lt;uchar&gt;(qq.y, qq.x)) &#123;          d |&#x3D; 1 &lt;&lt; k;        &#125;      &#125;      desc[i] &#x3D; d;    &#125;    descriptors.push_back(desc);&#x2F;&#x2F;desc表示该Oriented_FAST角点的描述子  &#125;   cout &lt;&lt; &quot;bad&#x2F;total: &quot; &lt;&lt; bad_points &lt;&lt; &quot;&#x2F;&quot; &lt;&lt; keypoints.size() &lt;&lt; endl;&#125; &#x2F;&#x2F; brute-force matchingvoid BfMatch(const vector&lt;DescType&gt; &amp;desc1, const vector&lt;DescType&gt; &amp;desc2, vector&lt;cv::DMatch&gt; &amp;matches) &#123;  const int d_max &#x3D; 40;&#x2F;&#x2F;描述子之间的距离小于这个值，才被认为是正确匹配   for (size_t i1 &#x3D; 0; i1 &lt; desc1.size(); ++i1)  &#x2F;&#x2F;size_t相当于int，便于代码移植  &#123;    if (desc1[i1].empty()) continue;    cv::DMatch m&#123;i1, 0, 256&#125;; &#x2F;&#x2F;定义了一个匹配对m    for (size_t i2 &#x3D; 0; i2 &lt; desc2.size(); ++i2)&#x2F;&#x2F;计算描述子desc1[i1]和描述子desc2[i2]的距离，即不同位数的数目     &#123;      if (desc2[i2].empty()) continue;      int distance &#x3D; 0;      for (int k &#x3D; 0; k &lt; 8; k++) &#123;        distance +&#x3D; _mm_popcnt_u32(desc1[i1][k] ^ desc2[i2][k]);      &#125;      if (distance &lt; d_max &amp;&amp; distance &lt; m.distance) &#123;        m.distance &#x3D; distance;        m.trainIdx &#x3D; i2;      &#125;    &#125;    if (m.distance &lt; d_max) &#123;      matches.push_back(m);    &#125;  &#125;<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><blockquote><p>参考<a href="https://blog.csdn.net/weixin_53660567/article/details/121095677">视觉SLAM十四讲CH7代码解析及课后习题详解</a></p></blockquote><h3 id="ch7-x2F-pose-estimation-2d2d-cpp"><a href="#ch7-x2F-pose-estimation-2d2d-cpp" class="headerlink" title="ch7&#x2F;pose_estimation_2d2d.cpp"></a>ch7&#x2F;pose_estimation_2d2d.cpp</h3><p>本程序演示了如何使用2D-2D的特征匹配估计相机运动,对极约束求解相机运动</p><pre class="line-numbers language-c++" data-language="c++"><code class="language-c++">&#x2F;&#x2F;&#x2F;&#x2F;#include &lt;iostream&gt;#include &lt;opencv2&#x2F;core&#x2F;core.hpp&gt;#include &lt;opencv2&#x2F;features2d&#x2F;features2d.hpp&gt;#include &lt;opencv2&#x2F;highgui&#x2F;highgui.hpp&gt;#include &lt;opencv2&#x2F;calib3d&#x2F;calib3d.hpp&gt;using namespace std;using namespace cv;&#x2F;&#x2F;声明特征匹配函数void find_feature_matches(const Mat &amp;img1,const Mat &amp;img2,                          std::vector&lt;KeyPoint&gt; &amp;keypoints_1,                          std::vector&lt;KeyPoint&gt; &amp;keypoints_2,                          std::vector&lt;DMatch&gt; &amp;matches);&#x2F;&#x2F;声明位姿估计函数void pose_estimated_2d2d( std::vector&lt;KeyPoint&gt; keypoints_1,                          std::vector&lt;KeyPoint&gt; keypoints_2,                          std::vector&lt;DMatch&gt; matches,                          Mat &amp;R, Mat &amp;t);&#x2F;&#x2F;声明 像素坐标转归一化坐标 p99页 公式5.5 变换一下就好啦！&#x2F;&#x2F;这里的函数我改了一下，书上的只是得到二维点，主程序中，还把二维点转换为三维点，我觉得多此一举，一个函数实现不就好了么&#x2F;&#x2F;下面是我实现的坐标变换函数 返回的是Mat类型，其实照片也是矩阵Mat pixel2cam(const Point2d &amp;p,const Mat &amp;K);&#x2F;&#x2F;p是像素坐标，K是相机内参矩阵int main(int argc,char** argv)&#123;    &#x2F;&#x2F;运行可执行文件+图片1的路径+图片2的路径    if(argc!&#x3D;3)    &#123;        cout&lt;&lt;&quot;usage: .&#x2F;pose_estimated_2d2d img1 img2&quot;&lt;&lt;endl;        return 1;    &#125;    &#x2F;*argv[1]&#x3D;&quot;&#x2F;home&#x2F;nnz&#x2F;data&#x2F;slam_pratice&#x2F;pratice_pose_estimated&#x2F;1.png&quot;;    argv[2]&#x3D;&quot;&#x2F;home&#x2F;nnz&#x2F;data&#x2F;slam_pratice&#x2F;pratice_pose_estimated&#x2F;2.png&quot;;*&#x2F;    &#x2F;&#x2F;读取图片    Mat img1&#x3D;imread(argv[1],CV_LOAD_IMAGE_COLOR);&#x2F;&#x2F;彩色图    Mat img2&#x3D;imread(argv[2],CV_LOAD_IMAGE_COLOR);    &#x2F;&#x2F;定义要用到的变量啊，比如 keypoints_1,keypoints_2,matches    vector&lt;KeyPoint&gt; keypoints_1, keypoints_2;    vector&lt;DMatch&gt; matches;    &#x2F;&#x2F;计算两个keypoints的matches    find_feature_matches(img1,img2,keypoints_1,keypoints_2,matches);    &#x2F;&#x2F;这样就得到matche啦！    cout &lt;&lt; &quot;一共找到了&quot; &lt;&lt; matches.size() &lt;&lt; &quot;组匹配点&quot; &lt;&lt; endl;    &#x2F;&#x2F;重点来了    &#x2F;&#x2F;估计两组图片之间的运动    Mat R,t;&#x2F;&#x2F;定义一下旋转和平移    pose_estimated_2d2d(keypoints_1,keypoints_2,matches,R,t);    &#x2F;&#x2F;这样就算出R ,t啦    &#x2F;&#x2F;当然我们还需要验证一下对极约束 准不准了    &#x2F;&#x2F;验证 E&#x3D;t^R*scale;    &#x2F;&#x2F;t_x是t的反对称矩阵    Mat t_x &#x3D;            (Mat_&lt;double&gt;(3, 3) &lt;&lt; 0, -t.at&lt;double&gt;(2, 0), t.at&lt;double&gt;(1, 0),                    t.at&lt;double&gt;(2, 0), 0, -t.at&lt;double&gt;(0, 0),                    -t.at&lt;double&gt;(1, 0), t.at&lt;double&gt;(0, 0), 0);    cout &lt;&lt; &quot;t^R&#x3D;&quot; &lt;&lt; endl &lt;&lt; t_x * R &lt;&lt; endl;    &#x2F;&#x2F;验证对极约束 对应P167 页公式7.10 x2TEx1&#x3D;0  E&#x3D;t^*R    &#x2F;&#x2F;定义内参矩阵    Mat K &#x3D; (Mat_&lt;double&gt;(3, 3) &lt;&lt; 520.9, 0, 325.1,                                                 0, 521.0,249.7,                                                 0, 0, 1);    for(DMatch m: matches)    &#123;        Mat x1&#x3D;pixel2cam(keypoints_1[m.queryIdx].pt,K);        Mat x2&#x3D;pixel2cam(keypoints_2[m.queryIdx].pt,K);        Mat d&#x3D;x2.t()*t_x*R*x1;&#x2F;&#x2F;若d很趋近于零，则说明没啥问题        cout &lt;&lt; &quot;epipolar constraint &#x3D; &quot; &lt;&lt; d &lt;&lt; endl;    &#125;    return 0;&#125;&#x2F;&#x2F;最复杂的地方来咯&#x2F;&#x2F;函数的实现&#x2F;&#x2F;匹配函数void find_feature_matches(const Mat &amp;img1,const Mat &amp;img2,                          std::vector&lt;KeyPoint&gt; &amp;keypoints_1,                          std::vector&lt;KeyPoint&gt; &amp;keypoints_2,                          std::vector&lt;DMatch&gt; &amp;matches)&#123;    &#x2F;&#x2F;先初始化，创建咱们要用到的对象    &#x2F;&#x2F;定义两个关键点对应的描述子，同时创建检测keypoints的检测器    Mat descriptors_1, descriptors_2;    vector&lt;DMatch&gt; match;&#x2F;&#x2F;暂时存放匹配点,因为后面还要进行筛选    Ptr&lt;FeatureDetector&gt; detector&#x3D;ORB::create();&#x2F;&#x2F;keypoints检测器    Ptr&lt;DescriptorExtractor&gt; descriptor &#x3D;ORB::create();&#x2F;&#x2F;描述子提取器    Ptr&lt;DescriptorMatcher&gt; matcher &#x3D;DescriptorMatcher::create(            &quot;BruteForce-Hamming&quot;);&#x2F;&#x2F;描述子匹配器（方法：暴力匹配）    &#x2F;&#x2F;step1:找到角点    detector-&gt;detect(img1,keypoints_1);&#x2F;&#x2F;得到图1的关键点（keypoints_1）    detector-&gt;detect(img2,keypoints_2);&#x2F;&#x2F;得到图2的关键点（keypoints_2）    &#x2F;&#x2F;step2:计算关键点所对应的描述子    descriptor-&gt;compute(img1,keypoints_1,descriptors_1);&#x2F;&#x2F;得到descriptors_1    descriptor-&gt;compute(img2,keypoints_2,descriptors_2);&#x2F;&#x2F;得到descriptors_2    &#x2F;&#x2F;step3:进行暴力匹配    matcher-&gt;match(descriptors_1,descriptors_2,match);    &#x2F;&#x2F;step4:对match进行筛选，得到好的匹配点，把好的匹配点放在matches中    &#x2F;&#x2F;先定义两个变量，一个是最大距离，一个是最小距离    double min_dist&#x3D;1000, max_dist&#x3D;0;    &#x2F;&#x2F;找出所有匹配之间的最小距离和最大距离, 即是最相似的和最不相似的两组点之间的距离    for(int i&#x3D;0;i&lt;descriptors_1.rows;i++) &#x2F;&#x2F;描述子本质是由 0,1 组成的向量    &#123;        double dist &#x3D;match[i].distance;        &#x2F;&#x2F;还记得orb_cv中如何找最大距离和最远距离的吗，那里面的程序是用下面的函数实现的，下面的函数得到的是pair first 里面是最小距离，second里面是最大距离        &#x2F;&#x2F; minmax_element(matches.begin(),matched.end,[](const DMatch &amp;m1,const DMatch &amp;m2)&#123;return m1.distance&lt;m2.distance;&#125;);        &#x2F;&#x2F;本程序用下面的if语句得到距离        if (dist &lt; min_dist) min_dist &#x3D; dist;        if (dist &gt; max_dist) max_dist &#x3D; dist;    &#125;    printf(&quot;-- Max dist : %f \n&quot;, max_dist);    printf(&quot;-- Min dist : %f \n&quot;, min_dist);    &#x2F;&#x2F;当描述子之间的距离大于两倍的最小距离时,即认为匹配有误.    &#x2F;&#x2F; 但有时候最小距离会非常小,设置一个经验值30作为下限.    for(int i&#x3D;0;i&lt;descriptors_1.rows;i++)    &#123;        if(match[i].distance&lt;&#x3D;max(2*min_dist,30.0))        &#123;            matches.push_back(match[i]);        &#125;    &#125;&#125;&#x2F;&#x2F;像素到归一化坐标Mat pixel2cam(const Point2d &amp;p,const Mat &amp;K)&#x2F;&#x2F;p是像素坐标，K是相机内参矩阵&#123;    Mat t;    t&#x3D;(Mat_&lt;double&gt;(3,1)&lt;&lt;(p.x - K.at&lt;double&gt;(0, 2)) &#x2F; K.at&lt;double&gt;(0, 0),            (p.y - K.at&lt;double&gt;(1, 2)) &#x2F; K.at&lt;double&gt;(1, 1),1);    return t;&#125;&#x2F;&#x2F;实现位姿估计函数void pose_estimated_2d2d( std::vector&lt;KeyPoint&gt; keypoints_1,                          std::vector&lt;KeyPoint&gt; keypoints_2,                          std::vector&lt;DMatch&gt; matches,                          Mat &amp;R, Mat &amp;t)&#123;    &#x2F;&#x2F; 相机内参来源于 TUM Freiburg2    Mat K &#x3D; (Mat_&lt;double&gt;(3, 3) &lt;&lt; 520.9, 0, 325.1,                                                 0, 521.0, 249.7,                                                 0, 0, 1);    &#x2F;&#x2F;咱们要把关键点的像素点拿出来 ,定义两个容器接受两个图关键点的像素位置    vector&lt;Point2d&gt; points1;    vector&lt;Point2d&gt; points2;    for(int i&#x3D;0;i&lt;(int)matches.size();i++)    &#123;        &#x2F;&#x2F;queryIdx是图1中匹配的关键点的对应编号        &#x2F;&#x2F;trainIdx是图2中匹配的关键点的对应编号        &#x2F;&#x2F;pt可以把关键点的像素位置取出来        points1.push_back(keypoints_1[matches[i].queryIdx].pt);        points2.push_back(keypoints_2[matches[i].trainIdx].pt);    &#125;    &#x2F;&#x2F;-- 计算基础矩阵    Mat fundamental_matrix;    fundamental_matrix &#x3D; findFundamentalMat(points1, points2, CV_FM_8POINT);    cout &lt;&lt; &quot;fundamental_matrix is &quot; &lt;&lt; endl &lt;&lt; fundamental_matrix &lt;&lt; endl;    &#x2F;&#x2F;计算本质矩阵 E    &#x2F;&#x2F;把cx ,cy放进一个向量里面 &#x3D;相机的光心    Point2d principal_point(325.1, 249.7);    double focal_length&#x3D;521;&#x2F;&#x2F;相机的焦距    &#x2F;&#x2F;之所以取上面的principal_point、focal_length是因为计算本质矩阵的函数要用    &#x2F;&#x2F;得到本质矩阵essential_matrix    Mat essential_matrix&#x3D;findEssentialMat(points1,points2,focal_length,principal_point);    cout&lt;&lt;&quot; essential_matrix &#x3D;\n&quot;&lt;&lt; essential_matrix &lt;&lt;endl;    &#x2F;&#x2F;-- 计算单应矩阵 homography_matrix    &#x2F;&#x2F;-- 但是本例中场景不是平面，单应矩阵意义不大    Mat homography_matrix;    homography_matrix &#x3D; findHomography(points1, points2, RANSAC, 3);    cout &lt;&lt; &quot;homography_matrix is &quot; &lt;&lt; endl &lt;&lt; homography_matrix &lt;&lt; endl;    &#x2F;&#x2F;通过本质矩阵恢复咱们的 R  t    recoverPose(essential_matrix,points1,points2,R,t,focal_length,principal_point);    &#x2F;&#x2F;输出咱们的 R t    cout&lt;&lt;&quot; 得到图1到图2 的位姿变换:\n &quot;&lt;&lt;endl;    cout&lt;&lt;&quot;R&#x3D; \n&quot;&lt;&lt; R &lt;&lt;endl;    cout&lt;&lt;&quot;t&#x3D; \n&quot;&lt;&lt; t &lt;&lt;endl;&#125;<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p><a href="https://blog.csdn.net/joun772/article/details/109466153?utm_medium=distribute.pc_relevant.none-task-blog-2~default~baidujs_baidulandingword~default-12-109466153-blog-116243451.235%5Ev28%5Epc_relevant_recovery_v2&spm=1001.2101.3001.4242.7&utm_relevant_index=15">SLAM十四讲-ch7(2)-位姿估计(包含2d-2d、3d-2d、3d-3d、以及三角化实现代码的注释)</a></p><h3 id="ch7-x2F-triangulation-cpp"><a href="#ch7-x2F-triangulation-cpp" class="headerlink" title="ch7&#x2F;triangulation.cpp"></a>ch7&#x2F;triangulation.cpp</h3><p>在上面的2d2d位姿估计的基础上，利用三角化来获得特征匹配点的深度信息(通过画图，验证三维点与特征点的重投影关系)</p><pre class="line-numbers language-c++" data-language="c++"><code class="language-c++">&#x2F;&#x2F;&#x2F;&#x2F; Created by wenbo on 2020&#x2F;11&#x2F;3.&#x2F;&#x2F;&#x2F;&#x2F;该程序在pose_estimated_2d2d的基础上加上三角化，以求得匹配的特征点在世界下的三维点&#x2F;&#x2F;&#x2F;&#x2F;#include &lt;iostream&gt;#include &lt;opencv2&#x2F;opencv.hpp&gt;&#x2F;&#x2F; #include &quot;extra.h&quot; &#x2F;&#x2F; used in opencv2using namespace std;using namespace cv;&#x2F;&#x2F;声明特征匹配函数void find_feature_matches(const Mat &amp;img1,const Mat &amp;img2,                          std::vector&lt;KeyPoint&gt; &amp;keypoints_1,                          std::vector&lt;KeyPoint&gt; &amp;keypoints_2,                          std::vector&lt;DMatch&gt; &amp;matches);&#x2F;&#x2F;声明位姿估计函数void pose_estimated_2d2d( std::vector&lt;KeyPoint&gt; &amp;keypoints_1,                          std::vector&lt;KeyPoint&gt; &amp;keypoints_2,                          std::vector&lt;DMatch&gt; &amp;matches,                          Mat &amp;R, Mat &amp;t);&#x2F;&#x2F;声明 像素坐标转归一化坐标 p99页 公式5.5 变换一下就好啦！&#x2F;&#x2F;这里的函数我改了一下，书上的只是得到二维点，主程序中，还把二维点转换为三维点，我觉得多此一举，一个函数实现不就好了么&#x2F;&#x2F;下面是我实现的坐标变换函数 返回的是Mat类型，其实照片也是矩阵Point2f pixel2cam(const Point2d &amp;p, const Mat &amp;K);&#x2F;&#x2F;p是像素坐标，K是相机内参矩阵&#x2F;&#x2F;声明三角化函数void triangulation(        const vector&lt;KeyPoint&gt; &amp;keypoint_1,        const vector&lt;KeyPoint&gt; &amp;keypoint_2,        const std::vector&lt;DMatch&gt; &amp;matches,        const Mat &amp;R, const Mat &amp;t,        vector&lt;Point3d&gt; &amp;points);&#x2F;&#x2F;&#x2F; 作图用inline cv::Scalar get_color(float depth) &#123;    float up_th &#x3D; 50, low_th &#x3D; 10, th_range &#x3D; up_th - low_th;    if (depth &gt; up_th) depth &#x3D; up_th;    if (depth &lt; low_th) depth &#x3D; low_th;    return cv::Scalar(255 * depth &#x2F; th_range, 0, 255 * (1 - depth &#x2F; th_range));&#125;int main(int argc,char** argv)&#123;    &#x2F;&#x2F;运行可执行文件+图片1的路径+图片2的路径    if(argc!&#x3D;3)    &#123;        cout&lt;&lt;&quot;usage: .&#x2F;triangulation img1 img2&quot;&lt;&lt;endl;        return 1;    &#125;    &#x2F;&#x2F;读取图片    Mat img1&#x3D;imread(argv[1],CV_LOAD_IMAGE_COLOR);&#x2F;&#x2F;彩色图    Mat img2&#x3D;imread(argv[2],CV_LOAD_IMAGE_COLOR);    &#x2F;&#x2F;定义要用到的变量啊，比如 keypoints_1,keypoints_2,matches    vector&lt;KeyPoint&gt; keypoints_1, keypoints_2;    vector&lt;DMatch&gt; matches;    &#x2F;&#x2F;计算两个keypoints的matches    find_feature_matches(img1,img2,keypoints_1,keypoints_2,matches);    &#x2F;&#x2F;这样就得到matches啦！    cout &lt;&lt; &quot;一共找到了&quot; &lt;&lt; matches.size() &lt;&lt; &quot;组匹配点&quot; &lt;&lt; endl;    &#x2F;&#x2F;重点来了    &#x2F;&#x2F;估计两组图片之间的运动    Mat R,t;&#x2F;&#x2F;定义一下旋转和平移    pose_estimated_2d2d(keypoints_1,keypoints_2,matches,R,t);    &#x2F;&#x2F;这样就算出R ,t啦    &#x2F;&#x2F;三角化    &#x2F;&#x2F;定义一个容器 points 用来存放特征匹配点在世界坐标系下的3d点    vector&lt;Point3d&gt; points;    triangulation(keypoints_1,keypoints_2,matches,R,t,points);    &#x2F;&#x2F;得到三维点    &#x2F;&#x2F;验证三维点与特征点的重投影关系    Mat K &#x3D; (Mat_&lt;double&gt;(3, 3) &lt;&lt; 520.9, 0, 325.1,                                                 0, 521.0, 249.7,                                                 0, 0, 1);    Mat img_plot1&#x3D;img1.clone();    Mat img_plot2&#x3D;img2.clone();    &#x2F;&#x2F;利用循环找到图1和图2特征点在图上的位置，并圈出来    for(int i&#x3D;0;i&lt;matches.size();i++)    &#123;        &#x2F;&#x2F;先画图1中特征点        &#x2F;&#x2F;在这里，为什么从一个世界坐标系下的3d点，就可以得到，图1相机坐标下的深度点呢？        &#x2F;&#x2F;我觉得是因为 图1的位姿: R是单位矩阵，t为0（在三角化函数中有写到） 所以可以把图1的相机坐标看成是世界坐标        float  depth1&#x3D;points[i].z;&#x2F;&#x2F;取出图1各个特征点的深度信息        cout&lt;&lt;&quot;depth: &quot;&lt;&lt;depth1&lt;&lt;endl;        Point2d pt1_cam&#x3D;pixel2cam(keypoints_1[matches[i].queryIdx].pt,K);        cv::circle(img_plot1, keypoints_1[matches[i].queryIdx].pt, 2, get_color(depth1), 2);        &#x2F;&#x2F;画图2        &#x2F;&#x2F;得到图2坐标系下的3d点，得到图2的深度信息        Mat pt2_trans&#x3D;R*(Mat_&lt;double&gt;(3, 1) &lt;&lt;points[i].x,points[i].y,points[i].z)+t;        float depth2 &#x3D; pt2_trans.at&lt;double&gt;(2, 0);        cv::circle(img_plot2, keypoints_2[matches[i].trainIdx].pt, 2, get_color(depth2), 2);    &#125;&#x2F;&#x2F;画图    cv::imshow(&quot;img 1&quot;, img_plot1);    cv::imshow(&quot;img 2&quot;, img_plot2);    cv::waitKey();    return 0;&#125;void triangulation(        const vector&lt;KeyPoint&gt; &amp;keypoint_1,        const vector&lt;KeyPoint&gt; &amp;keypoint_2,        const std::vector&lt;DMatch&gt; &amp;matches,        const Mat &amp;R, const Mat &amp;t,        vector&lt;Point3d&gt; &amp;points)&#123;    &#x2F;&#x2F;定义图1在世界坐标系下的位姿    Mat T1 &#x3D; (Mat_&lt;float&gt;(3, 4) &lt;&lt;                                1, 0, 0, 0,                                0, 1, 0, 0,                                0, 0, 1, 0);    &#x2F;&#x2F;定义图2在世界坐标系下的位姿    Mat T2 &#x3D; (Mat_&lt;float&gt;(3, 4) &lt;&lt;                                R.at&lt;double&gt;(0, 0), R.at&lt;double&gt;(0, 1), R.at&lt;double&gt;(0, 2), t.at&lt;double&gt;(0, 0),            R.at&lt;double&gt;(1, 0), R.at&lt;double&gt;(1, 1), R.at&lt;double&gt;(1, 2), t.at&lt;double&gt;(1, 0),            R.at&lt;double&gt;(2, 0), R.at&lt;double&gt;(2, 1), R.at&lt;double&gt;(2, 2), t.at&lt;double&gt;(2, 0)    );    Mat K &#x3D; (Mat_&lt;double&gt;(3, 3) &lt;&lt; 520.9, 0, 325.1, 0, 521.0, 249.7, 0, 0, 1);    &#x2F;&#x2F;容器 pts_1、pts_2分别存放图1和图2中特征点对应的自己相机归一化坐标中的 x与 y    vector&lt;Point2f&gt; pts_1,pts_2;    for(DMatch m:matches)&#x2F;&#x2F;这样的遍历写起来比较快    &#123;        &#x2F;&#x2F;将像素坐标变为相机下的归一化坐标        pts_1.push_back(pixel2cam(keypoint_1[m.queryIdx].pt,K));        pts_2.push_back(pixel2cam(keypoint_2[m.trainIdx].pt,K));    &#125;    Mat pts_4d;    cv::triangulatePoints(T1,T2,pts_1,pts_2,pts_4d);    &#x2F;*    传入两个图像对应相机的变化矩阵，各自相机坐标系下归一化相机坐标，    输出的3D坐标是齐次坐标，共四个维度，因此需要将前三个维度除以第四个维度以得到非齐次坐标xyz    *&#x2F;    &#x2F;&#x2F;转换为非齐次坐标    for(int i&#x3D;0;i&lt;pts_4d.cols;i++)&#x2F;&#x2F;遍历所有的点，列数表述点的数量    &#123;        &#x2F;&#x2F;定义x来接收每一个三维点        Mat x&#x3D;pts_4d.col(i); &#x2F;&#x2F;x为4x1维度        x&#x2F;&#x3D;x.at&lt;float&gt;(3,0);&#x2F;&#x2F;归一化        Point3d p(x.at&lt;float&gt;(0, 0),                  x.at&lt;float&gt;(1, 0),                  x.at&lt;float&gt;(2, 0));        points.push_back(p);&#x2F;&#x2F;将图1测得的目标相对相机实际位置（Xc,Yc,Zc）存入points    &#125;&#125;<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p><img src="/pic/%E9%80%89%E5%8C%BA_135.png"></p><p><a href="https://www.codenong.com/cs105088833/">对极约束和三角测量</a></p><h3 id="ch7-x2F-pose-estimation-3d2d-cpp"><a href="#ch7-x2F-pose-estimation-3d2d-cpp" class="headerlink" title="ch7&#x2F;pose_estimation_3d2d.cpp"></a>ch7&#x2F;pose_estimation_3d2d.cpp</h3><p>本程序使用Opencv的EPnP求解pnp问题，并手写了一个高斯牛顿法的PnP,然后调用g2o来求解</p><pre class="line-numbers language-c++" data-language="c++"><code class="language-c++">&#x2F;&#x2F;&#x2F;&#x2F; Created by nnz on 2020&#x2F;11&#x2F;5.&#x2F;&#x2F;&#x2F;&#x2F;&#x2F;&#x2F; Created by nnz on 2020&#x2F;11&#x2F;4.&#x2F;&#x2F;#include &lt;iostream&gt;#include &lt;opencv2&#x2F;core&#x2F;core.hpp&gt;#include &lt;opencv2&#x2F;features2d&#x2F;features2d.hpp&gt;#include &lt;opencv2&#x2F;highgui&#x2F;highgui.hpp&gt;#include &lt;opencv2&#x2F;calib3d&#x2F;calib3d.hpp&gt;#include &lt;Eigen&#x2F;Core&gt;#include &lt;g2o&#x2F;core&#x2F;base_vertex.h&gt;#include &lt;g2o&#x2F;core&#x2F;base_unary_edge.h&gt;#include &lt;g2o&#x2F;core&#x2F;sparse_optimizer.h&gt;#include &lt;g2o&#x2F;core&#x2F;block_solver.h&gt;#include &lt;g2o&#x2F;core&#x2F;solver.h&gt;#include &lt;g2o&#x2F;core&#x2F;optimization_algorithm_gauss_newton.h&gt;#include &lt;g2o&#x2F;solvers&#x2F;dense&#x2F;linear_solver_dense.h&gt;#include &lt;sophus&#x2F;se3.hpp&gt;#include &lt;chrono&gt;&#x2F;&#x2F;该程序用了三种方法实现位姿估计&#x2F;&#x2F;第一种，调用cv的函数pnp求解 R ,t&#x2F;&#x2F;第二种，手写高斯牛顿进行位姿优化&#x2F;&#x2F;第三种，利用g2o进行位姿优化using namespace std;using namespace cv;typedef vector&lt;Eigen::Vector2d, Eigen::aligned_allocator&lt;Eigen::Vector2d&gt;&gt; VecVector2d;&#x2F;&#x2F;VecVector2d可以定义存放二维向量的容器typedef vector&lt;Eigen::Vector3d, Eigen::aligned_allocator&lt;Eigen::Vector3d&gt;&gt; VecVector3d;&#x2F;&#x2F;VecVector3d可以定义存放三维向量的容器void find_feature_matches(        const Mat &amp;img_1, const Mat &amp;img_2,        std::vector&lt;KeyPoint&gt; &amp;keypoints_1,        std::vector&lt;KeyPoint&gt; &amp;keypoints_2,        std::vector&lt;DMatch&gt; &amp;matches);&#x2F;&#x2F; 像素坐标转相机归一化坐标Point2d pixel2cam(const Point2d &amp;p, const Mat &amp;K);&#x2F;&#x2F; BA by gauss-newton 手写高斯牛顿进行位姿优化void bundleAdjustmentGaussNewton(        const VecVector3d &amp;points_3d,        const VecVector2d &amp;points_2d,        const Mat &amp;K,        Sophus::SE3d &amp;pose);&#x2F;&#x2F;利用g2o优化posevoid bundleAdjustmentG2O(        const VecVector3d &amp;points_3d,        const VecVector2d &amp;points_2d,        const Mat &amp;K,        Sophus::SE3d &amp;pose);int main(int argc ,char** argv)&#123;    &#x2F;&#x2F;读取图片    if (argc !&#x3D; 5) &#123;        cout &lt;&lt; &quot;usage: pose_estimation_3d2d img1 img2 depth1 depth2&quot; &lt;&lt; endl;        return 1;    &#125;    &#x2F;&#x2F;读取图片    Mat img_1&#x3D;imread(argv[1],CV_LOAD_IMAGE_COLOR);&#x2F;&#x2F;读取彩色图    Mat img_2&#x3D;imread(argv[2],CV_LOAD_IMAGE_COLOR);    assert(img_1.data &amp;&amp; img_2.data &amp;&amp; &quot;Can Not load images!&quot;);&#x2F;&#x2F;若读取的图片没有内容，就终止程序    vector&lt;KeyPoint&gt; keypoints_1, keypoints_2;    vector&lt;DMatch&gt; matches;    find_feature_matches(img_1,img_2,keypoints_1,keypoints_2,matches);&#x2F;&#x2F;得到两个图片的特征匹配点    cout &lt;&lt; &quot;一共找到了&quot; &lt;&lt; matches.size() &lt;&lt; &quot;组匹配点&quot; &lt;&lt; endl;    &#x2F;&#x2F;建立3d点，把深度图信息读进来，构造三维点    Mat d1 &#x3D; imread(argv[3], CV_LOAD_IMAGE_UNCHANGED);       &#x2F;&#x2F; 深度图为16位无符号数，单通道图像    Mat K &#x3D; (Mat_&lt;double&gt;(3, 3) &lt;&lt; 520.9, 0, 325.1, 0, 521.0, 249.7, 0, 0, 1);    vector&lt;Point3d&gt; pts_3d;&#x2F;&#x2F;创建容器pts_3d存放3d点（图1对应的特征点的相机坐标下的3d点）    vector&lt;Point2d&gt; pts_2d;&#x2F;&#x2F;创建容器pts_2d存放图2的特征点    for(DMatch m:matches)    &#123;        &#x2F;&#x2F;把对应的图1的特征点的深度信息拿出来        ushort d &#x3D; d1.ptr&lt;unsigned short&gt;(int(keypoints_1[m.queryIdx].pt.y))[int(keypoints_1[m.queryIdx].pt.x)];        if(d&#x3D;&#x3D;0) &#x2F;&#x2F;深度有问题            continue;        float dd&#x3D;d&#x2F;5000.0;&#x2F;&#x2F;用dd存放换算过尺度的深度信息        Point2d p1&#x3D;pixel2cam(keypoints_1[m.queryIdx].pt,K);&#x2F;&#x2F;p1里面放的是图1特征点在相机坐标下的归一化坐标（只包含 x,y）        pts_3d.push_back(Point3d(p1.x*dd,p1.y*dd,dd));&#x2F;&#x2F;得到图1特征点在相机坐标下的3d坐标        pts_2d.push_back(keypoints_2[m.trainIdx].pt);&#x2F;&#x2F;得到图2特张点的像素坐标    &#125;    cout&lt;&lt;&quot;3d-2d pairs:&quot;&lt;&lt; pts_3d.size() &lt;&lt;endl;&#x2F;&#x2F;3d-2d配对个数得用pts_3d的size    cout&lt;&lt;&quot;使用cv求解 位姿&quot;&lt;&lt;endl;    cout&lt;&lt;&quot;***********************************opencv***********************************&quot;&lt;&lt;endl;    chrono::steady_clock::time_point t1 &#x3D; chrono::steady_clock::now();    Mat r, t;    &#x2F;&#x2F;Mat()这个参数指的是畸变系数向量？    solvePnP(pts_3d, pts_2d, K, Mat(), r, t, false); &#x2F;&#x2F; 调用OpenCV 的 PnP 求解，可选择EPNP，DLS等方法    Mat R;    cv::Rodrigues(r,R);&#x2F;&#x2F;r是旋转向量，利用cv的Rodrigues()函数将旋转向量转换为旋转矩阵    chrono::steady_clock::time_point t2 &#x3D; chrono::steady_clock::now();    chrono::duration&lt;double&gt; time_used &#x3D; chrono::duration_cast&lt;chrono::duration&lt;double&gt;&gt;(t2 - t1);    cout &lt;&lt; &quot;solve pnp in opencv cost time: &quot; &lt;&lt; time_used.count() &lt;&lt; &quot; seconds.&quot; &lt;&lt; endl;    cout &lt;&lt; &quot;R&#x3D;&quot; &lt;&lt; endl &lt;&lt; R &lt;&lt; endl;    cout &lt;&lt; &quot;t&#x3D;&quot; &lt;&lt; endl &lt;&lt; t &lt;&lt; endl;    cout&lt;&lt;&quot;***********************************opencv***********************************&quot;&lt;&lt;endl;    cout&lt;&lt;&quot;手写高斯牛顿优化位姿&quot;&lt;&lt;endl;    cout&lt;&lt;&quot;***********************************手写高斯牛顿***********************************&quot;&lt;&lt;endl;    VecVector3d pts_3d_eigen;&#x2F;&#x2F;存放3d点（图1对应的特征点的相机坐标下的3d点）    VecVector2d pts_2d_eigen;&#x2F;&#x2F;存放图2的特征点    for(size_t i&#x3D;0;i&lt;pts_3d.size();i++)&#x2F;&#x2F;size_t    &#123;        pts_3d_eigen.push_back(Eigen::Vector3d(pts_3d[i].x,pts_3d[i].y,pts_3d[i].z));        pts_2d_eigen.push_back(Eigen::Vector2d(pts_2d[i].x,pts_2d[i].y));    &#125;    Sophus::SE3d pose_gn;&#x2F;&#x2F;位姿（李群）    t1 &#x3D; chrono::steady_clock::now();    bundleAdjustmentGaussNewton(pts_3d_eigen, pts_2d_eigen, K, pose_gn);    t2 &#x3D; chrono::steady_clock::now();    time_used &#x3D; chrono::duration_cast&lt;chrono::duration&lt;double&gt;&gt;(t2 - t1);    cout &lt;&lt; &quot;solve pnp by gauss newton cost time: &quot; &lt;&lt; time_used.count() &lt;&lt; &quot; seconds.&quot; &lt;&lt; endl;    cout&lt;&lt;&quot;R &#x3D; \n&quot;&lt;&lt;pose_gn.rotationMatrix()&lt;&lt;endl;    cout&lt;&lt;&quot;t &#x3D; &quot;&lt;&lt;pose_gn.translation().transpose()&lt;&lt;endl;    cout&lt;&lt;&quot;***********************************手写高斯牛顿***********************************&quot;&lt;&lt;endl;    cout&lt;&lt;&quot;g2o优化位姿&quot;&lt;&lt;endl;    cout &lt;&lt; &quot;***********************************g2o***********************************&quot; &lt;&lt; endl;    Sophus::SE3d pose_g2o;    t1 &#x3D; chrono::steady_clock::now();    bundleAdjustmentG2O(pts_3d_eigen, pts_2d_eigen, K, pose_g2o);    t2 &#x3D; chrono::steady_clock::now();    time_used &#x3D; chrono::duration_cast&lt;chrono::duration&lt;double&gt;&gt;(t2 - t1);    cout &lt;&lt; &quot;solve pnp by g2o cost time: &quot; &lt;&lt; time_used.count() &lt;&lt; &quot; seconds.&quot; &lt;&lt; endl;    cout &lt;&lt; &quot;***********************************g2o***********************************&quot; &lt;&lt; endl;    return 0;&#125;&#x2F;&#x2F;实现特征匹配void find_feature_matches(const Mat &amp;img_1, const Mat &amp;img_2,                          std::vector&lt;KeyPoint&gt; &amp;keypoints_1,                          std::vector&lt;KeyPoint&gt; &amp;keypoints_2,                          std::vector&lt;DMatch&gt; &amp;matches) &#123;    &#x2F;&#x2F;-- 初始化    Mat descriptors_1, descriptors_2;    &#x2F;&#x2F; used in OpenCV3    Ptr&lt;FeatureDetector&gt; detector &#x3D; ORB::create();    Ptr&lt;DescriptorExtractor&gt; descriptor &#x3D; ORB::create();    &#x2F;&#x2F; use this if you are in OpenCV2    &#x2F;&#x2F; Ptr&lt;FeatureDetector&gt; detector &#x3D; FeatureDetector::create ( &quot;ORB&quot; );    &#x2F;&#x2F; Ptr&lt;DescriptorExtractor&gt; descriptor &#x3D; DescriptorExtractor::create ( &quot;ORB&quot; );    Ptr&lt;DescriptorMatcher&gt; matcher &#x3D; DescriptorMatcher::create(&quot;BruteForce-Hamming&quot;);    &#x2F;&#x2F;-- 第一步:检测 Oriented FAST 角点位置    detector-&gt;detect(img_1, keypoints_1);    detector-&gt;detect(img_2, keypoints_2);    &#x2F;&#x2F;-- 第二步:根据角点位置计算 BRIEF 描述子    descriptor-&gt;compute(img_1, keypoints_1, descriptors_1);    descriptor-&gt;compute(img_2, keypoints_2, descriptors_2);    &#x2F;&#x2F;-- 第三步:对两幅图像中的BRIEF描述子进行匹配，使用 Hamming 距离    vector&lt;DMatch&gt; match;    &#x2F;&#x2F; BFMatcher matcher ( NORM_HAMMING );    matcher-&gt;match(descriptors_1, descriptors_2, match);    &#x2F;&#x2F;-- 第四步:匹配点对筛选    double min_dist &#x3D; 10000, max_dist &#x3D; 0;    &#x2F;&#x2F;找出所有匹配之间的最小距离和最大距离, 即是最相似的和最不相似的两组点之间的距离    for (int i &#x3D; 0; i &lt; descriptors_1.rows; i++) &#123;        double dist &#x3D; match[i].distance;        if (dist &lt; min_dist) min_dist &#x3D; dist;        if (dist &gt; max_dist) max_dist &#x3D; dist;    &#125;    printf(&quot;-- Max dist : %f \n&quot;, max_dist);    printf(&quot;-- Min dist : %f \n&quot;, min_dist);    &#x2F;&#x2F;当描述子之间的距离大于两倍的最小距离时,即认为匹配有误.但有时候最小距离会非常小,设置一个经验值30作为下限.    for (int i &#x3D; 0; i &lt; descriptors_1.rows; i++) &#123;        if (match[i].distance &lt;&#x3D; max(2 * min_dist, 30.0)) &#123;            matches.push_back(match[i]);        &#125;    &#125;&#125;&#x2F;&#x2F;实现像素坐标到相机坐标的转换（求出来的只是包含相机坐标下的x,y的二维点）Point2d pixel2cam(const Point2d &amp;p, const Mat &amp;K) &#123;    return Point2d            (                    (p.x - K.at&lt;double&gt;(0, 2)) &#x2F; K.at&lt;double&gt;(0, 0),                    (p.y - K.at&lt;double&gt;(1, 2)) &#x2F; K.at&lt;double&gt;(1, 1)            );&#125;&#x2F;&#x2F;手写高斯牛顿void bundleAdjustmentGaussNewton(        const VecVector3d &amp;points_3d,        const VecVector2d &amp;points_2d,        const Mat &amp;K,        Sophus::SE3d &amp;pose)&#123;    typedef Eigen::Matrix&lt;double,6,1&gt; Vector6d;    const int iters&#x3D;10;&#x2F;&#x2F;迭代次数    double cost&#x3D;0,lastcost&#x3D;0;&#x2F;&#x2F;代价函数（目标函数）    &#x2F;&#x2F;拿出内参    double fx &#x3D; K.at&lt;double&gt;(0, 0);    double fy &#x3D; K.at&lt;double&gt;(1, 1);    double cx &#x3D; K.at&lt;double&gt;(0, 2);    double cy &#x3D; K.at&lt;double&gt;(1, 2);    &#x2F;&#x2F;进入迭代    for (int iter &#x3D; 0; iter &lt;iters ; iter++)    &#123;        Eigen::Matrix&lt;double,6,6&gt; H &#x3D; Eigen::Matrix&lt;double,6,6&gt;::Zero();&#x2F;&#x2F;初始化H矩阵        Vector6d b &#x3D; Vector6d::Zero();&#x2F;&#x2F;对b矩阵初始化        cost &#x3D; 0;        &#x2F;&#x2F; 遍历所有的特征点  计算cost        for(int i&#x3D;0;i&lt;points_3d.size();i++)        &#123;            Eigen::Vector3d pc&#x3D;pose*points_3d[i];&#x2F;&#x2F;利用待优化的pose得到图2的相机坐标下的3d点            double inv_z&#x3D;1.0&#x2F;pc[2];&#x2F;&#x2F;得到图2的相机坐标下的3d点的z的倒数，也就是1&#x2F;z            double inv_z2 &#x3D; inv_z * inv_z;&#x2F;&#x2F;(1&#x2F;z)^2            &#x2F;&#x2F;定义投影            Eigen::Vector2d proj(fx * pc[0] &#x2F; pc[2] + cx, fy * pc[1] &#x2F; pc[2] + cy);            &#x2F;&#x2F;定义误差            Eigen::Vector2d e&#x3D;points_2d[i]-proj;            cost +&#x3D; e.squaredNorm();&#x2F;&#x2F;cost&#x3D;e*e            &#x2F;&#x2F;定义雅克比矩阵J            Eigen::Matrix&lt;double, 2, 6&gt; J;            J &lt;&lt; -fx * inv_z,                    0,                    fx * pc[0] * inv_z2,                    fx * pc[0] * pc[1] * inv_z2,                    -fx - fx * pc[0] * pc[0] * inv_z2,                    fx * pc[1] * inv_z,                    0,                    -fy * inv_z,                    fy * pc[1] * inv_z2,                    fy + fy * pc[1] * pc[1] * inv_z2,                    -fy * pc[0] * pc[1] * inv_z2,                    -fy * pc[0] * inv_z;            H +&#x3D; J.transpose() * J;            b +&#x3D; -J.transpose() * e;        &#125;        &#x2F;&#x2F;出了这个内循环，表述结束一次迭代的计算，接下来，要求pose了        Vector6d dx;&#x2F;&#x2F;P129页 公式6.33 计算增量方程 Hdx&#x3D;b        dx &#x3D; H.ldlt().solve(b);&#x2F;&#x2F;算出增量dx        &#x2F;&#x2F;判断dx这个数是否有效        if (isnan(dx[0]))        &#123;            cout &lt;&lt; &quot;result is nan!&quot; &lt;&lt; endl;            break;        &#125;        &#x2F;&#x2F;如果我们进行了迭代，且最后的cost&gt;&#x3D;lastcost的话，那就表明满足要求了，可以停止迭代了        if (iter &gt; 0 &amp;&amp; cost &gt;&#x3D; lastcost)        &#123;            &#x2F;&#x2F; cost increase, update is not good            cout &lt;&lt; &quot;cost: &quot; &lt;&lt; cost &lt;&lt; &quot;, last cost: &quot; &lt;&lt; lastcost &lt;&lt; endl;            break;        &#125;        &#x2F;&#x2F;优化pose 也就是用dx更新pose        pose&#x3D;Sophus::SE3d::exp(dx) * pose;&#x2F;&#x2F;dx是李代数，要转换为李群        lastcost&#x3D;cost;        cout &lt;&lt; &quot;iteration &quot; &lt;&lt; iter &lt;&lt; &quot; cost&#x3D;&quot;&lt;&lt; std::setprecision(12) &lt;&lt; cost &lt;&lt; endl;        &#x2F;&#x2F;std::setprecision(12)浮点数控制位数为12位        &#x2F;&#x2F;如果误差特别小了，也结束迭代        if (dx.norm() &lt; 1e-6)        &#123;            &#x2F;&#x2F; converge            break;        &#125;    &#125;    cout&lt;&lt;&quot;pose by g-n \n&quot;&lt;&lt;pose.matrix()&lt;&lt;endl;&#125;&#x2F;&#x2F;对于用g2o来进行优化的话，首先要定义顶点和边的模板&#x2F;&#x2F;顶点，也就是咱们要优化的pose 用李代数表示它 6维class Vertexpose: public g2o::BaseVertex&lt;6,Sophus::SE3d&gt;L&#123;public:    EIGEN_MAKE_ALIGNED_OPERATOR_NEW;&#x2F;&#x2F;必须写，我也不知道为什么    &#x2F;&#x2F;重载setToOriginImpl函数 这个应该就是把刚开的待优化的pose放进去    virtual void setToOriginImpl() override    &#123;        _estimate &#x3D; Sophus::SE3d();    &#125;    &#x2F;&#x2F;重载oplusImpl函数，用来更新pose（待优化的系数）    virtual void oplusImpl(const double *update) override    &#123;        Eigen::Matrix&lt;double,6,1&gt; update_eigen;&#x2F;&#x2F;更新的量，就是增量呗，dx        update_eigen &lt;&lt; update[0], update[1], update[2], update[3], update[4], update[5];        _estimate&#x3D;Sophus::SE3d::exp(update_eigen)* _estimate;&#x2F;&#x2F;更新pose 李代数要转换为李群，这样才可以左乘    &#125;    &#x2F;&#x2F;存盘 读盘 ：留空    virtual bool read(istream &amp;in) override &#123;&#125;    virtual bool write(ostream &amp;out) const override &#123;&#125;&#125;;&#x2F;&#x2F;定义边模板 边也就是误差，二维 并且把顶点也放进去class EdgeProjection : public g2o::BaseUnaryEdge&lt;2,Eigen::Vector2d,Vertexpose&gt;&#123;public:    EIGEN_MAKE_ALIGNED_OPERATOR_NEW;&#x2F;&#x2F;必须写，我也不知道为什么    &#x2F;&#x2F;有参构造，初始化 图1中的3d点 以及相机内参K    EdgeProjection(const Eigen::Vector3d &amp;pos, const Eigen::Matrix3d &amp;K) : _pos3d(pos),_K(K) &#123;&#125;    &#x2F;&#x2F;计算误差    virtual void computeError() override    &#123;        const Vertexpose *v&#x3D;static_cast&lt;const Vertexpose *&gt;(_vertices[0]);        Sophus::SE3d T&#x3D;v-&gt;estimate();        Eigen::Vector3d pos_pixel &#x3D; _K * (T * _pos3d);&#x2F;&#x2F;T * _pos3d是图2的相机坐标下的3d点        pos_pixel &#x2F;&#x3D; pos_pixel[2];&#x2F;&#x2F;得到了像素坐标的齐次形式        _error &#x3D; _measurement - pos_pixel.head&lt;2&gt;();    &#125;    &#x2F;&#x2F;计算雅克比矩阵    virtual void linearizeOplus() override    &#123;        const Vertexpose *v &#x3D; static_cast&lt;Vertexpose *&gt; (_vertices[0]);        Sophus::SE3d T &#x3D; v-&gt;estimate();        Eigen::Vector3d pos_cam&#x3D;T*_pos3d;&#x2F;&#x2F;图2的相机坐标下的3d点        double fx &#x3D; _K(0, 0);        double fy &#x3D; _K(1, 1);        double cx &#x3D; _K(0, 2);        double cy &#x3D; _K(1, 2);        double X &#x3D; pos_cam[0];        double Y &#x3D; pos_cam[1];        double Z &#x3D; pos_cam[2];        double Z2 &#x3D; Z * Z;        &#x2F;&#x2F;雅克比矩阵见 书 p187 公式7.46        _jacobianOplusXi                &lt;&lt; -fx &#x2F; Z, 0, fx * X &#x2F; Z2, fx * X * Y &#x2F; Z2, -fx - fx * X * X &#x2F; Z2, fx * Y &#x2F; Z,                0, -fy &#x2F; Z, fy * Y &#x2F; (Z * Z), fy + fy * Y * Y &#x2F; Z2, -fy * X * Y &#x2F; Z2, -fy * X &#x2F; Z;    &#125;    &#x2F;&#x2F;存盘 读盘 ：留空    virtual bool read(istream &amp;in) override &#123;&#125;    virtual bool write(ostream &amp;out) const override &#123;&#125;private:    Eigen::Vector3d _pos3d;    Eigen::Matrix3d _K;&#125;;&#x2F;&#x2F;利用g2o优化posevoid bundleAdjustmentG2O(        const VecVector3d &amp;points_3d,        const VecVector2d &amp;points_2d,        const Mat &amp;K,        Sophus::SE3d &amp;pose)&#123;    &#x2F;&#x2F; 构建图优化，先设定g2o    typedef g2o::BlockSolver&lt;g2o::BlockSolverTraits&lt;6, 3&gt;&gt; BlockSolverType;  &#x2F;&#x2F;  优化系数pose is 6, 数据点 landmark is 3    typedef g2o::LinearSolverDense&lt;BlockSolverType::PoseMatrixType&gt; LinearSolverType; &#x2F;&#x2F; 线性求解器类型    &#x2F;&#x2F; 梯度下降方法，可以从GN, LM, DogLeg 中选    auto solver &#x3D; new g2o::OptimizationAlgorithmGaussNewton(            g2o::make_unique&lt;BlockSolverType&gt;                    (g2o::make_unique&lt;LinearSolverType&gt;()));&#x2F;&#x2F;把设定的类型都放进求解器    g2o::SparseOptimizer optimizer;     &#x2F;&#x2F; 图模型    optimizer.setAlgorithm(solver);   &#x2F;&#x2F; 设置求解器 算法g-n    optimizer.setVerbose(true);       &#x2F;&#x2F; 打开调试输出    &#x2F;&#x2F;加入顶点    Vertexpose *v&#x3D;new Vertexpose();    v-&gt;setEstimate(Sophus::SE3d());    v-&gt;setId(0);    optimizer.addVertex(v);    &#x2F;&#x2F;K    &#x2F;&#x2F; K    Eigen::Matrix3d K_eigen;    K_eigen &lt;&lt;            K.at&lt;double&gt;(0, 0), K.at&lt;double&gt;(0, 1), K.at&lt;double&gt;(0, 2),            K.at&lt;double&gt;(1, 0), K.at&lt;double&gt;(1, 1), K.at&lt;double&gt;(1, 2),            K.at&lt;double&gt;(2, 0), K.at&lt;double&gt;(2, 1), K.at&lt;double&gt;(2, 2);    &#x2F;&#x2F;加入边    int index&#x3D;1;    for(size_t i&#x3D;0;i&lt;points_2d.size();++i)    &#123;        &#x2F;&#x2F;遍历 把3d点和像素点拿出来        auto p2d &#x3D; points_2d[i];        auto p3d &#x3D; points_3d[i];        EdgeProjection *edge &#x3D; new EdgeProjection(p3d, K_eigen);&#x2F;&#x2F;有参构造        edge-&gt;setId(index);        edge-&gt;setVertex(0,v);        edge-&gt;setMeasurement(p2d);&#x2F;&#x2F;设置观测值，其实就是图2 里的匹配特征点的像素位置        edge-&gt;setInformation(Eigen::Matrix2d::Identity());&#x2F;&#x2F;信息矩阵是二维方阵，因为误差是二维        optimizer.addEdge(edge);&#x2F;&#x2F;加入边        index++;&#x2F;&#x2F;边的编号++    &#125;    chrono::steady_clock::time_point t1 &#x3D; chrono::steady_clock::now();    optimizer.setVerbose(true);    optimizer.initializeOptimization();&#x2F;&#x2F;开始  初始化    optimizer.optimize(10);&#x2F;&#x2F;迭代次数    chrono::steady_clock::time_point t2 &#x3D; chrono::steady_clock::now();    chrono::duration&lt;double&gt; time_used &#x3D; chrono::duration_cast&lt;chrono::duration&lt;double&gt;&gt;(t2 - t1);    cout &lt;&lt; &quot;optimization costs time: &quot; &lt;&lt; time_used.count() &lt;&lt; &quot; seconds.&quot; &lt;&lt; endl;    cout &lt;&lt; &quot;pose estimated by g2o &#x3D;\n&quot; &lt;&lt; v-&gt;estimate().matrix() &lt;&lt; endl;    pose &#x3D; v-&gt;estimate();&#125;<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p><a href="https://blog.csdn.net/joun772/article/details/109466153?utm_medium=distribute.pc_relevant.none-task-blog-2~default~baidujs_baidulandingword~default-12-109466153-blog-116243451.235%5Ev28%5Epc_relevant_recovery_v2&spm=1001.2101.3001.4242.7&utm_relevant_index=15">SLAM十四讲-ch7(2)-位姿估计(包含2d-2d、3d-2d、3d-3d、以及三角化实现代码的注释)</a></p><h3 id="ch7-x2F-pose-estimation-3d3d-cpp"><a href="#ch7-x2F-pose-estimation-3d3d-cpp" class="headerlink" title="ch7&#x2F;pose_estimation_3d3d.cpp"></a>ch7&#x2F;pose_estimation_3d3d.cpp</h3><pre class="line-numbers language-c++" data-language="c++"><code class="language-c++">&#x2F;&#x2F;&#x2F;&#x2F; Created by nnz on 2020&#x2F;11&#x2F;5.&#x2F;&#x2F;#include &lt;iostream&gt;#include &lt;opencv2&#x2F;core&#x2F;core.hpp&gt;#include &lt;opencv2&#x2F;features2d&#x2F;features2d.hpp&gt;#include &lt;opencv2&#x2F;highgui&#x2F;highgui.hpp&gt;#include &lt;opencv2&#x2F;calib3d&#x2F;calib3d.hpp&gt;#include &lt;Eigen&#x2F;Core&gt;#include &lt;g2o&#x2F;core&#x2F;base_vertex.h&gt;#include &lt;g2o&#x2F;core&#x2F;base_unary_edge.h&gt;#include &lt;g2o&#x2F;core&#x2F;sparse_optimizer.h&gt;#include &lt;g2o&#x2F;core&#x2F;block_solver.h&gt;#include &lt;g2o&#x2F;core&#x2F;solver.h&gt;#include &lt;g2o&#x2F;core&#x2F;optimization_algorithm_gauss_newton.h&gt;#include &lt;g2o&#x2F;solvers&#x2F;dense&#x2F;linear_solver_dense.h&gt;#include &lt;sophus&#x2F;se3.hpp&gt;#include &lt;chrono&gt;using namespace std;using namespace cv;void find_feature_matches(        const Mat &amp;img_1, const Mat &amp;img_2,        std::vector&lt;KeyPoint&gt; &amp;keypoints_1,        std::vector&lt;KeyPoint&gt; &amp;keypoints_2,        std::vector&lt;DMatch&gt; &amp;matches);&#x2F;&#x2F; 像素坐标转相机归一化坐标Point2d pixel2cam(const Point2d &amp;p, const Mat &amp;K);void pose_estimation_3d3d(        const vector&lt;Point3f&gt; &amp;pts1,        const vector&lt;Point3f&gt; &amp;pts2,        Mat &amp;R, Mat &amp;t);&#x2F;&#x2F;void bundleAdjustment(        const vector&lt;Point3f&gt; &amp;pts1,        const vector&lt;Point3f&gt; &amp;pts2,        Mat &amp;R, Mat &amp;t);int main(int argc,char** argv)&#123;    if(argc!&#x3D;5)    &#123;        cout&lt;&lt;&quot; usage: pose_estimation_3d3d img1 img2 depth1 depth2 &quot;&lt;&lt;endl;        return 1;    &#125;    &#x2F;&#x2F;读取图片    Mat img_1&#x3D;imread(argv[1],CV_LOAD_IMAGE_COLOR);&#x2F;&#x2F;读取彩色图    Mat img_2&#x3D;imread(argv[2],CV_LOAD_IMAGE_COLOR);    vector&lt;KeyPoint&gt; keypoints_1, keypoints_2;&#x2F;&#x2F;容器keypoints_1, keypoints_2分别存放图1和图2的特征点    vector&lt;DMatch&gt; matches;    find_feature_matches(img_1, img_2, keypoints_1, keypoints_2, matches);&#x2F;&#x2F;得到图1与图2的特征匹配点    cout &lt;&lt; &quot;一共找到了&quot; &lt;&lt; matches.size() &lt;&lt; &quot;组匹配点&quot; &lt;&lt; endl;    &#x2F;&#x2F;接下来的是建立3d点 利用深度图可以获取深度信息    &#x2F;&#x2F;depth1是图1对应的深度图 depth2是图2对应的深度图    Mat depth1 &#x3D; imread(argv[3], CV_LOAD_IMAGE_UNCHANGED);       &#x2F;&#x2F; 深度图为16位无符号数，单通道图像    Mat depth2 &#x3D; imread(argv[4], CV_LOAD_IMAGE_UNCHANGED);    &#x2F;&#x2F;内参矩阵    Mat K &#x3D;(Mat_&lt;double&gt;(3, 3) &lt;&lt; 520.9, 0, 325.1, 0, 521.0, 249.7, 0, 0, 1);    vector&lt;Point3f&gt; pts1, pts2;    for(DMatch m:matches)    &#123;        &#x2F;&#x2F;先把两图特征匹配点对应的深度拿出来        ushort d1&#x3D;depth1.ptr&lt;unsigned short &gt;(int(keypoints_1[m.queryIdx].pt.y))[int(keypoints_1[m.queryIdx].pt.x)];        ushort d2&#x3D;depth2.ptr&lt;unsigned short &gt;(int(keypoints_2[m.trainIdx].pt.y))[int(keypoints_2[m.trainIdx].pt.x)];        if(d1&#x3D;&#x3D;0 || d2&#x3D;&#x3D;0)&#x2F;&#x2F;深度无效            continue;        Point2d p1&#x3D;pixel2cam(keypoints_1[m.queryIdx].pt,K);&#x2F;&#x2F;得到图1的特征匹配点在其相机坐标下的x ,y        Point2d p2&#x3D;pixel2cam(keypoints_2[m.trainIdx].pt,K);&#x2F;&#x2F;得到图2的特征匹配点在其相机坐标下的x ,y        &#x2F;&#x2F;对深度进行尺度变化得到真正的深度        float dd1 &#x3D; float(d1) &#x2F; 5000.0;        float dd2 &#x3D; float(d2) &#x2F; 5000.0;        &#x2F;&#x2F;容器 pts_1与pts_2分别存放 图1中的特征匹配点其相机坐标下的3d点 和 图2中的特征匹配点其相机坐标下的3d点        pts1.push_back(Point3f(p1.x * dd1, p1.y * dd1, dd1));        pts2.push_back(Point3f(p2.x * dd2, p2.y * dd2, dd2));    &#125;    &#x2F;&#x2F;这样就可以得到 3d-3d的匹配点    cout &lt;&lt; &quot;3d-3d pairs: &quot; &lt;&lt; pts1.size() &lt;&lt; endl;    Mat R, t;    cout&lt;&lt;&quot; ************************************ICP 3d-3d by SVD**************************************** &quot;&lt;&lt;endl;    pose_estimation_3d3d(pts1, pts2, R, t);    cout &lt;&lt; &quot;ICP via SVD results: &quot; &lt;&lt; endl;    cout &lt;&lt; &quot;R &#x3D; &quot; &lt;&lt; R &lt;&lt; endl;    cout &lt;&lt; &quot;t &#x3D; &quot; &lt;&lt; t &lt;&lt; endl;    cout &lt;&lt; &quot;R^T &#x3D; &quot; &lt;&lt; R.t() &lt;&lt; endl;    cout &lt;&lt; &quot;t^T &#x3D; &quot; &lt;&lt; -R.t() * t &lt;&lt; endl;    cout&lt;&lt;&quot; ************************************ICP 3d-3d by SVD**************************************** &quot;&lt;&lt;endl;    cout&lt;&lt;endl;    cout&lt;&lt;&quot; ************************************ICP 3d-3d by g2o**************************************** &quot;&lt;&lt;endl;    bundleAdjustment(pts1, pts2, R, t);    cout&lt;&lt;&quot;R&#x3D; \n&quot;&lt;&lt;R&lt;&lt;endl;    cout&lt;&lt;&quot;t &#x3D; &quot;&lt;&lt; t.t() &lt;&lt;endl;    cout&lt;&lt;&quot;验证 p2 &#x3D; R*P1 +t &quot;&lt;&lt;endl;    for (int i &#x3D; 0; i &lt; 5; i++) &#123;        cout &lt;&lt; &quot;p1 &#x3D; &quot; &lt;&lt; pts1[i] &lt;&lt; endl;        cout &lt;&lt; &quot;p2 &#x3D; &quot; &lt;&lt; pts2[i] &lt;&lt; endl;        cout &lt;&lt; &quot;(R*p1+t) &#x3D; &quot; &lt;&lt;             R * (Mat_&lt;double&gt;(3, 1) &lt;&lt; pts1[i].x, pts1[i].y, pts1[i].z) + t             &lt;&lt; endl;        cout &lt;&lt; endl;    &#125;    cout&lt;&lt;&quot; ************************************ICP 3d-3d by g2o**************************************** &quot;&lt;&lt;endl;    return 0;&#125;void find_feature_matches(const Mat &amp;img_1, const Mat &amp;img_2,                          std::vector&lt;KeyPoint&gt; &amp;keypoints_1,                          std::vector&lt;KeyPoint&gt; &amp;keypoints_2,                          std::vector&lt;DMatch&gt; &amp;matches) &#123;    &#x2F;&#x2F;-- 初始化    Mat descriptors_1, descriptors_2;    &#x2F;&#x2F; used in OpenCV3    Ptr&lt;FeatureDetector&gt; detector &#x3D; ORB::create();    Ptr&lt;DescriptorExtractor&gt; descriptor &#x3D; ORB::create();    &#x2F;&#x2F; use this if you are in OpenCV2    &#x2F;&#x2F; Ptr&lt;FeatureDetector&gt; detector &#x3D; FeatureDetector::create ( &quot;ORB&quot; );    &#x2F;&#x2F; Ptr&lt;DescriptorExtractor&gt; descriptor &#x3D; DescriptorExtractor::create ( &quot;ORB&quot; );    Ptr&lt;DescriptorMatcher&gt; matcher &#x3D; DescriptorMatcher::create(&quot;BruteForce-Hamming&quot;);    &#x2F;&#x2F;-- 第一步:检测 Oriented FAST 角点位置    detector-&gt;detect(img_1, keypoints_1);    detector-&gt;detect(img_2, keypoints_2);    &#x2F;&#x2F;-- 第二步:根据角点位置计算 BRIEF 描述子    descriptor-&gt;compute(img_1, keypoints_1, descriptors_1);    descriptor-&gt;compute(img_2, keypoints_2, descriptors_2);    &#x2F;&#x2F;-- 第三步:对两幅图像中的BRIEF描述子进行匹配，使用 Hamming 距离    vector&lt;DMatch&gt; match;    &#x2F;&#x2F; BFMatcher matcher ( NORM_HAMMING );    matcher-&gt;match(descriptors_1, descriptors_2, match);    &#x2F;&#x2F;-- 第四步:匹配点对筛选    double min_dist &#x3D; 10000, max_dist &#x3D; 0;    &#x2F;&#x2F;找出所有匹配之间的最小距离和最大距离, 即是最相似的和最不相似的两组点之间的距离    for (int i &#x3D; 0; i &lt; descriptors_1.rows; i++) &#123;        double dist &#x3D; match[i].distance;        if (dist &lt; min_dist) min_dist &#x3D; dist;        if (dist &gt; max_dist) max_dist &#x3D; dist;    &#125;    printf(&quot;-- Max dist : %f \n&quot;, max_dist);    printf(&quot;-- Min dist : %f \n&quot;, min_dist);    &#x2F;&#x2F;当描述子之间的距离大于两倍的最小距离时,即认为匹配有误.但有时候最小距离会非常小,设置一个经验值30作为下限.    for (int i &#x3D; 0; i &lt; descriptors_1.rows; i++) &#123;        if (match[i].distance &lt;&#x3D; max(2 * min_dist, 30.0)) &#123;            matches.push_back(match[i]);        &#125;    &#125;&#125;&#x2F;&#x2F;实现像素坐标到相机坐标的转换（求出来的只是包含相机坐标下的x,y的二维点）Point2d pixel2cam(const Point2d &amp;p, const Mat &amp;K) &#123;    return Point2d            (                    (p.x - K.at&lt;double&gt;(0, 2)) &#x2F; K.at&lt;double&gt;(0, 0),                    (p.y - K.at&lt;double&gt;(1, 2)) &#x2F; K.at&lt;double&gt;(1, 1)            );&#125;&#x2F;&#x2F;参考书上的p197页void pose_estimation_3d3d(        const vector&lt;Point3f&gt; &amp;pts1,        const vector&lt;Point3f&gt; &amp;pts2,        Mat &amp;R, Mat &amp;t)&#123;    int N&#x3D;pts1.size();&#x2F;&#x2F;匹配的3d点个数    Point3f p1,p2;&#x2F;&#x2F;质心    for(int i&#x3D;0;i&lt;N;i++)    &#123;        p1+&#x3D;pts1[i];        p2+&#x3D;pts2[i];    &#125;    p1 &#x3D; Point3f(Vec3f(p1)&#x2F;N);&#x2F;&#x2F;得到质心    p2 &#x3D; Point3f(Vec3f(p2) &#x2F; N);    vector&lt;Point3f&gt; q1(N),q2(N);    for(int i&#x3D;0;i&lt;N;i++)    &#123;        &#x2F;&#x2F;去质心        q1[i]&#x3D;pts1[i]-p1;        q2[i]&#x3D;pts2[i]-p2;    &#125;    &#x2F;&#x2F;计算 W+&#x3D;q1*q2^T(求和)    Eigen::Matrix3d W&#x3D;Eigen::Matrix3d::Zero();&#x2F;&#x2F;初始化    for(int i&#x3D;0;i&lt;N;i++)    &#123;        W+&#x3D; Eigen::Vector3d (q1[i].x,q1[i].y,q1[i].z)*(Eigen::Vector3d (q2[i].x,q2[i].y,q2[i].z).transpose());    &#125;    cout&lt;&lt;&quot;W &#x3D; &quot;&lt;&lt;endl&lt;&lt;W&lt;&lt;endl;    &#x2F;&#x2F;利用svd分解 W&#x3D;U*sigema*V    Eigen::JacobiSVD&lt;Eigen::Matrix3d&gt; svd(W,Eigen::ComputeFullU | Eigen::ComputeFullV);    Eigen::Matrix3d U&#x3D;svd.matrixU();&#x2F;&#x2F;得到U矩阵    Eigen::Matrix3d V&#x3D;svd.matrixV();&#x2F;&#x2F;得到V矩阵    cout &lt;&lt; &quot;U&#x3D;&quot; &lt;&lt; U &lt;&lt; endl;    cout &lt;&lt; &quot;V&#x3D;&quot; &lt;&lt; V &lt;&lt; endl;    Eigen::Matrix3d R_&#x3D;U*(V.transpose());    if (R_.determinant() &lt; 0)&#x2F;&#x2F;若旋转矩阵R_的行列式&lt;0 则取负号    &#123;        R_ &#x3D; -R_;    &#125;    Eigen::Vector3d t_&#x3D;Eigen::Vector3d (p1.x,p1.y,p1.z)-R_*Eigen::Vector3d (p2.x,p2.y,p2.z);&#x2F;&#x2F;得到平移向量    &#x2F;&#x2F;把 Eigen形式的 r 和 t_ 转换为CV 中的Mat格式    R &#x3D; (Mat_&lt;double&gt;(3, 3) &lt;&lt;                            R_(0, 0), R_(0, 1), R_(0, 2),            R_(1, 0), R_(1, 1), R_(1, 2),            R_(2, 0), R_(2, 1), R_(2, 2)    );    t &#x3D; (Mat_&lt;double&gt;(3, 1) &lt;&lt; t_(0, 0), t_(1, 0), t_(2, 0));&#125;&#x2F;&#x2F;对于用g2o来进行优化的话，首先要定义顶点和边的模板&#x2F;&#x2F;顶点，也就是咱们要优化的pose 用李代数表示它 6维class Vertexpose: public g2o::BaseVertex&lt;6,Sophus::SE3d&gt;&#123;public:    EIGEN_MAKE_ALIGNED_OPERATOR_NEW;&#x2F;&#x2F;必须写，我也不知道为什么    &#x2F;&#x2F;重载setToOriginImpl函数 这个应该就是把刚开的待优化的pose放进去    virtual void setToOriginImpl() override    &#123;        _estimate &#x3D; Sophus::SE3d();    &#125;    &#x2F;&#x2F;重载oplusImpl函数，用来更新pose（待优化的系数）    virtual void oplusImpl(const double *update) override    &#123;        Eigen::Matrix&lt;double,6,1&gt; update_eigen;&#x2F;&#x2F;更新的量，就是增量呗，dx        update_eigen &lt;&lt; update[0], update[1], update[2], update[3], update[4], update[5];        _estimate&#x3D;Sophus::SE3d::exp(update_eigen)* _estimate;&#x2F;&#x2F;更新pose 李代数要转换为李群，这样才可以左乘    &#125;    &#x2F;&#x2F;存盘 读盘 ：留空    virtual bool read(istream &amp;in) override &#123;&#125;    virtual bool write(ostream &amp;out) const override &#123;&#125;&#125;;&#x2F;&#x2F;定义边class EdgeProjectXYZRGBD: public g2o::BaseUnaryEdge&lt;3,Eigen::Vector3d,Vertexpose&gt;&#123;public:    EdgeProjectXYZRGBD(const Eigen::Vector3d &amp;point) : _point(point) &#123;&#125;&#x2F;&#x2F;赋值这个是图1坐标下的3d点    &#x2F;&#x2F;计算误差    virtual void computeError() override    &#123;        const Vertexpose *v&#x3D;static_cast&lt;const Vertexpose *&gt;(_vertices[0]);&#x2F;&#x2F;顶点v        _error &#x3D; _measurement - v-&gt;estimate() * _point;    &#125;    &#x2F;&#x2F;计算雅克比    virtual void linearizeOplus() override    &#123;        const Vertexpose *v&#x3D;static_cast&lt;const Vertexpose *&gt;(_vertices[0]);&#x2F;&#x2F;顶点v        Sophus::SE3d T&#x3D;v-&gt;estimate();&#x2F;&#x2F;把顶点的待优化系数拿出来        Eigen::Vector3d xyz_trans&#x3D;T*_point;&#x2F;&#x2F;变换到图2下的坐标点        &#x2F;&#x2F;下面的雅克比没看懂        _jacobianOplusXi.block&lt;3, 3&gt;(0, 0) &#x3D; -Eigen::Matrix3d::Identity();        _jacobianOplusXi.block&lt;3, 3&gt;(0, 3) &#x3D; Sophus::SO3d::hat(xyz_trans);    &#125;    bool read(istream &amp;in) &#123;&#125;    bool write(ostream &amp;out) const &#123;&#125;protected:    Eigen::Vector3d _point;&#125;;&#x2F;&#x2F;利用g2ovoid bundleAdjustment(const vector&lt;Point3f&gt; &amp;pts1,                      const vector&lt;Point3f&gt; &amp;pts2,                      Mat &amp;R, Mat &amp;t)&#123;&#x2F;&#x2F; 构建图优化，先设定g2o    typedef g2o::BlockSolver&lt;g2o::BlockSolverTraits&lt;6, 3&gt;&gt; BlockSolverType;  &#x2F;&#x2F;  优化系数pose is 6, 数据点 landmark is 3    typedef g2o::LinearSolverDense&lt;BlockSolverType::PoseMatrixType&gt; LinearSolverType; &#x2F;&#x2F; 线性求解器类型    &#x2F;&#x2F; 梯度下降方法，可以从GN, LM, DogLeg 中选    auto solver &#x3D; new g2o::OptimizationAlgorithmGaussNewton(            g2o::make_unique&lt;BlockSolverType&gt;                    (g2o::make_unique&lt;LinearSolverType&gt;()));&#x2F;&#x2F;把设定的类型都放进求解器    g2o::SparseOptimizer optimizer;     &#x2F;&#x2F; 图模型    optimizer.setAlgorithm(solver);   &#x2F;&#x2F; 设置求解器 算法g-n    optimizer.setVerbose(true);       &#x2F;&#x2F; 打开调试输出    &#x2F;&#x2F;加入顶点    Vertexpose *v&#x3D;new Vertexpose();    v-&gt;setEstimate(Sophus::SE3d());    v-&gt;setId(0);    optimizer.addVertex(v);    &#x2F;&#x2F;加入边    int index&#x3D;1;    for(size_t i&#x3D;0;i&lt;pts1.size();i++)    &#123;        EdgeProjectXYZRGBD *edge &#x3D; new EdgeProjectXYZRGBD(Eigen::Vector3d(pts1[i].x,pts1[i].y,pts1[i].z));        edge-&gt;setId(index);&#x2F;&#x2F;边的编号        edge-&gt;setVertex(0,v);&#x2F;&#x2F;设置顶点  顶点编号        edge-&gt;setMeasurement(Eigen::Vector3d(pts2[i].x,pts2[i].y,pts2[i].z));        edge-&gt;setInformation(Eigen::Matrix3d::Identity());&#x2F;&#x2F;set信息矩阵为单位矩阵        optimizer.addEdge(edge);&#x2F;&#x2F;加入边        index++;    &#125;    chrono::steady_clock::time_point t1 &#x3D; chrono::steady_clock::now();    optimizer.initializeOptimization();&#x2F;&#x2F;开始    optimizer.optimize(10);&#x2F;&#x2F;迭代次数    chrono::steady_clock::time_point t2 &#x3D; chrono::steady_clock::now();    chrono::duration&lt;double&gt; time_used &#x3D; chrono::duration_cast&lt;chrono::duration&lt;double&gt;&gt;(t2 - t1);    cout &lt;&lt; &quot;optimization costs time: &quot; &lt;&lt; time_used.count() &lt;&lt; &quot; seconds.&quot; &lt;&lt; endl;    cout &lt;&lt; endl &lt;&lt; &quot;after optimization:&quot; &lt;&lt; endl;    cout &lt;&lt; &quot;T&#x3D;\n&quot; &lt;&lt; v-&gt;estimate().matrix() &lt;&lt; endl;    &#x2F;&#x2F; 把位姿转换为Mat类型    Eigen::Matrix3d R_ &#x3D; v-&gt;estimate().rotationMatrix();    Eigen::Vector3d t_ &#x3D; v-&gt;estimate().translation();    R &#x3D; (Mat_&lt;double&gt;(3, 3) &lt;&lt;                            R_(0, 0), R_(0, 1), R_(0, 2),            R_(1, 0), R_(1, 1), R_(1, 2),            R_(2, 0), R_(2, 1), R_(2, 2)    );    t &#x3D; (Mat_&lt;double&gt;(3, 1) &lt;&lt; t_(0, 0), t_(1, 0), t_(2, 0));&#125;<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p><a href="https://blog.csdn.net/joun772/article/details/109466153?utm_medium=distribute.pc_relevant.none-task-blog-2~default~baidujs_baidulandingword~default-12-109466153-blog-116243451.235%5Ev28%5Epc_relevant_recovery_v2&spm=1001.2101.3001.4242.7&utm_relevant_index=15">SLAM十四讲-ch7(2)-位姿估计(包含2d-2d、3d-2d、3d-3d、以及三角化实现代码的注释)</a></p><h2 id="第八讲"><a href="#第八讲" class="headerlink" title="第八讲"></a>第八讲</h2><p>直接法是vo的另一个主要的分支，它与特征点法有很大的不同，理解光流法跟踪特征点的原理，理解直接法估计相机位姿，实现多层直接法的计算</p><h3 id="ch8-x2F-optical-flow-cpp"><a href="#ch8-x2F-optical-flow-cpp" class="headerlink" title="ch8&#x2F;optical_flow.cpp"></a>ch8&#x2F;optical_flow.cpp</h3><p>光流是一种描述像素随时间在图像之间运动的方法：</p><p>光流法有两个假设：（1）灰度不变假设：同一个空间点的像素灰度值，在各个图像中是固定不变的；（2）假设某个窗口内的像素具有相同的运动</p><p>一、本讲的代码使用了三种方法来追踪图像上的特征点</p><ul><li>第一种：使用OpenCV中的LK光流；</li><li>第二种：用高斯牛顿实现光流：单层光流；</li><li>第三种：用高斯牛顿实现光流：多层光流。</li></ul><p>其中高斯牛顿法，即最小化灰度误差估计最优的像素偏移。在具体函数实现中（即calculateOpticalFlow)，求解这样一个问题：</p><p><img src="/pic/%E9%80%89%E5%8C%BA_136.png"></p><p><strong>Opencv中的LK光流</strong>：使用  cv::calculateOpticalFlowPyrLK函数：</p><ul><li>提供前后两张图像及对应的特征点，即可得到追踪后的点，以及各点的状态、误差;</li><li>根据status变量是否为1来确定对应的点是否被正确追踪到。</li><li>cv::calcOpticalFlowPyrLK(img1,img2,pt1,pt2,status,error);</li></ul><p><strong>单层光流</strong>：用高斯牛顿法实现光流，光流也可以看成一个优化问题，通过最小化灰度误差估计最优的像素偏移</p><p><strong>多层光流</strong>：因为单层光流在相机运动较快的情况下，容易达到一个局部极小值，因此引入图像金字塔。当原始图像的像素运动较大时，在金字塔顶层看来，运动仍然是一个小的运动范围</p><p>二、代码注释</p><pre class="line-numbers language-c++" data-language="c++"><code class="language-c++">#include&lt;opencv2&#x2F;opencv.hpp&gt;#include&lt;string&gt;#include&lt;chrono&gt;#include&lt;Eigen&#x2F;Core&gt;#include&lt;Eigen&#x2F;Dense&gt;using namespace std;using namespace cv;string file_1&#x3D;&quot;.&#x2F;LK1.png&quot;;   &#x2F;&#x2F;第一张图像的路径,可能需要写成绝对路径string file_2&#x3D;&quot;.&#x2F;LK2.png&quot;;  &#x2F;&#x2F;第二张图像的路径  &#x2F;&#x2F;使用高斯牛顿法实现光流&#x2F;&#x2F;定义一个光流追踪类class OpticalFlowTracker&#123; public:    OpticalFlowTracker(    &#x2F;&#x2F;带参构造函数，并初始化      const Mat &amp;img1_,      const Mat &amp;img2_,      const vector&lt;KeyPoint&gt;&amp;kp1_,      vector&lt;KeyPoint&gt;&amp;kp2_,      vector&lt;bool&gt;&amp;success_,      bool inverse_&#x3D;true,bool has_initial_&#x3D;false):      img1(img1_),img2(img2_),kp1(kp1_),kp2(kp2_),success(success_),inverse(inverse_),      has_initial(has_initial_) &#123;&#125;            &#x2F;&#x2F;计算光流的函数      void calculateOpticalFlow(const Range &amp;range);  &#x2F;&#x2F;range是一个区间，应该看作一个窗口        private:    const Mat &amp;img1;    const Mat &amp;img2;    const vector&lt;KeyPoint&gt; &amp;kp1;    vector&lt;KeyPoint&gt; &amp;kp2;    vector&lt;bool&gt; &amp;success;    bool inverse&#x3D;true;    bool has_initial&#x3D;false;  &#125;;  &#x2F;&#x2F;单层光流的函数声明  void OpticalFlowSingleLevel(    const Mat &amp;img1,    const Mat &amp;img2,    const vector&lt;KeyPoint&gt;&amp;kp1,    vector&lt;KeyPoint&gt;&amp;kp2,    vector&lt;bool&gt;&amp;success,    bool inverse&#x3D;false,    bool has_initial_guess&#x3D;false  );  &#x2F;&#x2F;多层光流的函数声明  void OpticalFlowMultiLevel(    const Mat &amp;img1,    const Mat &amp;img2,    const vector&lt;KeyPoint&gt;&amp;kp1,    vector&lt;KeyPoint&gt;&amp;kp2,    vector&lt;bool&gt; &amp;success,    bool inverse&#x3D;false       );    &#x2F;&#x2F;从图像中获取一个灰度值  &#x2F;&#x2F;采用双线性内插法，来估计一个点的像素：  &#x2F;&#x2F;f(x,y)&#x3D;f(0,0)(1-x)(1-y)+f(1,0)x(1-y)+f(0,1)(1-x)y+f(1,1)xy  inline float GetPixelValue(const cv::Mat &amp;img,float x,float y)  &#123;    &#x2F;&#x2F;边缘检测    if(x&lt;0)      x&#x3D;0;    if(y&lt;0)      y&#x3D;0;    if(x&gt;&#x3D;img.cols)      x&#x3D;img.cols-1;    if(y&gt;&#x3D;img.rows)      y&#x3D;img.rows-1;    uchar *data&#x3D;&amp;img.data[int(y)*img.step+int(x)];  &#x2F;&#x2F;img.step:表示图像矩阵中每行包含的字节数;int(x)将x转换为int类型        float xx&#x3D;x-floor(x);   &#x2F;&#x2F;floor(x)函数：向下取整函数，即返回一个不大于x的最大整数    float yy&#x3D;y-floor(y);        return float(      (1-xx)*(1-yy)*data[0]+      xx*(1-yy)*data[1]+      (1-xx)*yy*data[img.step]+      xx*yy*data[img.step+1]      );  &#125;    &#x2F;&#x2F;主函数  int main(int argc,char**argv)  &#123;    Mat img1&#x3D;imread(file_1,0);  &#x2F;&#x2F;以灰度读取图像，重点    Mat img2&#x3D;imread(file_2,0);    &#x2F;&#x2F;特征点检测    vector&lt;KeyPoint&gt;kp1;  &#x2F;&#x2F;关键点 存放在容器kp1中    Ptr&lt;GFTTDetector&gt; detector&#x3D;GFTTDetector::create(500,0.01,20); &#x2F;&#x2F;通过GFTTD来获取角点，参数：最大角点数目500;角点可以接受的最小特征值0.01;角点之间的最小距离20    detector-&gt;detect(img1,kp1);&#x2F;&#x2F;类似于ORB特征点的提取过程        &#x2F;&#x2F;接下来实现在第二张图像中追踪这些角点，即追踪 kp1    &#x2F;&#x2F;第一种方法：单层光流    vector&lt;KeyPoint&gt;kp2_single;    vector&lt;bool&gt;success_single;    OpticalFlowSingleLevel(img1,img2,kp1,kp2_single,success_single);        &#x2F;&#x2F;第二种方法：多层光流    vector&lt;KeyPoint&gt;kp2_multi;    vector&lt;bool&gt;success_multi;    chrono::steady_clock::time_point t1&#x3D;chrono::steady_clock::now();    OpticalFlowMultiLevel(img1,img2,kp1,kp2_multi,success_multi,true);    chrono::steady_clock::time_point t2&#x3D;chrono::steady_clock::now();    auto time_used&#x3D;chrono::duration_cast&lt;chrono::duration&lt;double&gt;&gt;(t2-t1);     &#x2F;&#x2F;输出使用高斯牛顿法所花费的时间    cout &lt;&lt; &quot;optical flow by gauss-newton: &quot; &lt;&lt; time_used.count() &lt;&lt; endl;        &#x2F;&#x2F;使用OpenCV中的LK光流    vector&lt;Point2f&gt;pt1,pt2;    for(auto &amp;kp:kp1)     &#x2F;&#x2F;kp1中存放的是第一张图像中的角点，通过遍历，将kp1存放在pt1中      pt1.push_back(kp.pt);    vector&lt;uchar&gt;status;    vector&lt;float&gt;error;    t1&#x3D;chrono::steady_clock::now();    &#x2F;&#x2F;调用cv::calculateOpticalFlowPyrLK函数：    &#x2F;&#x2F;提供前后两张图像及对应的特征点，即可得到追踪后的点，以及各点的状态、误差;    &#x2F;&#x2F;根据status变量是否为1来确定对应的点是否被正确追踪到。    cv::calcOpticalFlowPyrLK(img1,img2,pt1,pt2,status,error);    t2&#x3D;chrono::steady_clock::now();    time_used&#x3D;chrono::duration_cast&lt;chrono::duration&lt;double&gt;&gt;(t2-t1);    cout &lt;&lt; &quot;optical flow by opencv: &quot; &lt;&lt; time_used.count() &lt;&lt; endl;  &#x2F;&#x2F;输出使用opencv所花费的时间        &#x2F;&#x2F;下面一部分代码实现绘图的功能    &#x2F;&#x2F;第一张图像：单层光流的效果图     Mat img2_single;     &#x2F;&#x2F;将输入图像从一个空间转换到另一个色彩空间     cv::cvtColor(img2,img2_single,CV_GRAY2BGR); &#x2F;&#x2F;cvtColor(）函数实现的功能：将img2灰度图转换成彩色图img2_single输出     for(int i&#x3D;0;i&lt;kp2_single.size();i++)     &#123;       if(success_single[i])   &#x2F;&#x2F;判断是否追踪成功       &#123; &#x2F;&#x2F;circle():画圆：参数：源图像，画圆的圆心坐标，圆的半径，圆的颜色，线条的粗细程度 &#x2F;&#x2F;kp2_single[i].pt：用来取第i个角点的坐标；Scalar(0,250,0)：设置颜色，遵循B G R ，所以此图中为绿色 cv::circle(img2_single,kp2_single[i].pt,2,cv::Scalar(0,250,0),2); &#x2F;&#x2F;line():绘制直线：参数：要画的线所在的图像，直线起点，直线终点，直线的颜色（绿色） cv::line(img2_single,kp1[i].pt,kp2_single[i].pt,cv::Scalar(0,250,0));       &#125;     &#125;          &#x2F;&#x2F;第二张图像：多层光流的效果图     Mat img2_multi;     cv::cvtColor(img2,img2_multi,CV_GRAY2BGR);      for(int i&#x3D;0;i&lt;kp2_multi.size();i++)     &#123;       if(success_multi[i])          &#123; cv::circle(img2_multi,kp2_multi[i].pt,2,cv::Scalar(250,0,0),2);  cv::line(img2_multi,kp1[i].pt,kp2_multi[i].pt,cv::Scalar(250,0,0));       &#125;     &#125;          &#x2F;&#x2F;第三张图像：使用OpenCV中的LK光流     Mat img2_CV;     cv::cvtColor(img2,img2_CV,CV_GRAY2BGR);      for(int i&#x3D;0;i&lt;pt2.size();i++)     &#123;       if(status[i])          &#123;  cv::circle(img2_CV,pt2[i],2,cv::Scalar(0,0,250),2);  cv::line(img2_CV,pt1[i],pt2[i],cv::Scalar(0,0,250));       &#125;     &#125;          &#x2F;&#x2F;     cv::imshow(&quot;tracked single level&quot;,img2_single);     cv::imshow(&quot;tracked multi level&quot;,img2_multi);     cv::imshow(&quot;tracked by opencv&quot;,img2_CV);     cv::waitKey(0);     return 0;  &#125;<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p><em><strong>具体功能实现如下：</strong></em></p><pre class="line-numbers language-c++" data-language="c++"><code class="language-c++"> &#x2F;&#x2F;接下来这一部分：具体函数的实现 &#x2F;&#x2F;第一个：单层光流函数的实现 void OpticalFlowSingleLevel(   const Mat &amp;img1,   const Mat &amp;img2,   const vector&lt;KeyPoint&gt;&amp;kp1,   vector&lt;KeyPoint&gt;&amp;kp2,   vector&lt;bool&gt;&amp;success,   bool inverse,   bool has_initial) &#123;   &#x2F;&#x2F;resize()函数：调整图像的大小；size（）函数：获取kp1的长度   &#x2F;&#x2F;初始化   kp2.resize(kp1.size());   success.resize(kp1.size());   &#x2F;&#x2F;是否追踪成功的标志      OpticalFlowTracker tracker(img1,img2,kp1,kp2,success,inverse,has_initial);  &#x2F;&#x2F;创建类的对象tracker   &#x2F;&#x2F;调用parallel_for_并行调用OpticalFlowTracker::calculateOpticalFlow，该函数计算指定范围内特征点的光流   &#x2F;&#x2F;range():从指定的第一个值开始，并在到达指定的第二个值后终止   parallel_for_(Range(0,kp1.size()),std::bind(&amp;OpticalFlowTracker::calculateOpticalFlow,&amp;tracker,placeholders::_1)); &#125;  &#x2F;&#x2F;类外实现成员函数 void OpticalFlowTracker::calculateOpticalFlow(const Range &amp;range) &#123;   &#x2F;&#x2F;定义参数   int half_patch_size&#x3D;4;  &#x2F;&#x2F;窗口的大小8×8   int iterations&#x3D;10;  &#x2F;&#x2F;每个角点迭代10次   for(size_t i&#x3D;range.start;i&lt;range.end;i++)   &#123;     auto kp&#x3D;kp1[i];   &#x2F;&#x2F;将第一张图像中的第i个关键点kp1[i]存放在 kp 中     double dx&#x3D;0,dy&#x3D;0; &#x2F;&#x2F;初始化     if(has_initial)     &#123;dx&#x3D;kp2[i].pt.x-kp.pt.x;   &#x2F;&#x2F;第i个点在第二张图像中的位置与第一张图像中的位置的差值dy&#x3D;kp2[i].pt.y-kp.pt.y;     &#125;          double cost&#x3D;0,lastCost&#x3D;0;     bool succ&#x3D;true;          &#x2F;&#x2F;高斯牛顿方程     &#x2F;&#x2F;高斯牛顿迭代     Eigen::Matrix2d H &#x3D; Eigen::Matrix2d::Zero();   &#x2F;&#x2F;定义H，并进行初始化。     Eigen::Vector2d b &#x3D; Eigen::Vector2d::Zero();   &#x2F;&#x2F;定义b，并初始化.     Eigen::Vector2d J;   &#x2F;&#x2F;定义雅克比矩阵2×1     for(int iter&#x3D;0;iter&lt;iterations;iter++)     &#123;if(inverse&#x3D;&#x3D;false)&#123;  H&#x3D;Eigen::Matrix2d::Zero();  b&#x3D;Eigen::Vector2d::Zero();&#125;else&#123;  b&#x3D;Eigen::Vector2d::Zero();&#125;cost&#x3D;0;&#x2F;&#x2F;假设在这个8×8的窗口内像素具有同样的运动&#x2F;&#x2F;计算cost和Jfor(int x&#x3D;-half_patch_size;x&lt;half_patch_size;x++)  for(int y&#x3D;-half_patch_size;y&lt;half_patch_size;y++)  &#123;    &#x2F;&#x2F;GetPixelValue（）计算某点的灰度值    &#x2F;&#x2F;计算残差：I(x,y)-I(x+dx,y+dy)    double error&#x3D;GetPixelValue(img1,kp.pt.x+x,kp.pt.y+y)-GetPixelValue(img2,kp.pt.x+x+dx,kp.pt.y+y+dy);;    if(inverse&#x3D;&#x3D;false)    &#123;      &#x2F;&#x2F;雅克比矩阵为第二个图像在x+dx,y+dy处的梯度      J&#x3D;-1.0*Eigen::Vector2d(0.5*(GetPixelValue(img2,kp.pt.x+dx+x+1,kp.pt.y+dy+y)-                                 GetPixelValue(img2,kp.pt.x+dx+x-1,kp.pt.y+dy+y)),     0.5*(GetPixelValue(img2,kp.pt.x+dx+x,kp.pt.y+dy+y+1)-         GetPixelValue(img2,kp.pt.x+dx+x,kp.pt.y+dy+y-1)));    &#125;    else if(iter&#x3D;&#x3D;0)  &#x2F;&#x2F;如果是第一次迭代，梯度为第一个图像的梯度，反向光流法      &#x2F;&#x2F;在反向光流中，I(x,y)的梯度是保持不变的，可以在第一次迭代时保留计算的结果，在后续的迭代中使用。      &#x2F;&#x2F;当雅克比矩阵不变时，H矩阵不变，每次迭代只需要计算残差。    &#123;      J&#x3D;-1.0*Eigen::Vector2d(0.5*(GetPixelValue(img1,kp.pt.x+x+1,kp.pt.y+y)-                                 GetPixelValue(img1,kp.pt.x+x-1,kp.pt.y+y)),     0.5*(GetPixelValue(img1,kp.pt.x+x,kp.pt.y+y+1)-         GetPixelValue(img1,kp.pt.x+x,kp.pt.y+y-1)));    &#125;    &#x2F;&#x2F;计算H和b    b+&#x3D;-error*J;    cost+&#x3D;error*error;    if(inverse&#x3D;&#x3D;false||iter&#x3D;&#x3D;0)    &#123;      H+&#x3D;J*J.transpose();    &#125;  &#125;  &#x2F;&#x2F;计算增量update，求解线性方程Hx&#x3D;b  Eigen::Vector2d update&#x3D;H.ldlt().solve(b);  if(std::isnan(update[0]))  &#x2F;&#x2F;判断增量  &#123;    &#x2F;&#x2F;有时当我们遇到一个黑色或白色的方块，H是不可逆的，即高斯牛顿方程无解    cout&lt;&lt;&quot;update is nan&quot;&lt;&lt;endl;    succ&#x3D;false;   &#x2F;&#x2F;追踪失败    break;  &#125;  if(iter&gt;0&amp;&amp;cost&gt;lastCost)  &#123;    break;  &#125;  dx+&#x3D;update[0];  dy+&#x3D;update[1];  lastCost&#x3D;cost;  succ&#x3D;true;  if(update.norm()&lt;1e-2)  &#123;    break;  &#125;     &#125;     success[i]&#x3D;succ;     kp2[i].pt&#x3D;kp.pt+Point2f(dx,dy);   &#125; &#125;&#x2F;&#x2F;迭代完成  &#x2F;&#x2F;第二个：多层光流函数的实现 void OpticalFlowMultiLevel(   const Mat &amp;img1,   const Mat &amp;img2,   const vector&lt;KeyPoint&gt;&amp;kp1,   vector&lt;KeyPoint&gt;&amp;kp2,   vector&lt;bool&gt;&amp;success,   bool inverse) &#123;   int pyramids&#x3D;4; &#x2F;&#x2F;建立4层金字塔   double pyramid_scale&#x3D;0.5;  &#x2F;&#x2F;金字塔每层缩小0.5   double scales[]&#x3D;&#123;1.0,0.5,0.25,0.125&#125;;      &#x2F;&#x2F;建立金字塔   chrono::steady_clock::time_point t1&#x3D;chrono::steady_clock::now();  &#x2F;&#x2F;计算时间   vector&lt;Mat&gt;pyr1,pyr2;   for(int i&#x3D;0;i&lt;pyramids;i++)   &#123;     if(i&#x3D;&#x3D;0)     &#123;&#x2F;&#x2F;将两张图像存放在pyr1,pyr2中pyr1.push_back(img1);    pyr2.push_back(img2);     &#125;     else     &#123;Mat img1_pyr,img2_pyr;&#x2F;&#x2F;对图像进行缩放，参数：原图，输出图像，输出图像大小，Size（宽度，高度）cv::resize(pyr1[i-1],img1_pyr,cv::Size(pyr1[i-1].cols*pyramid_scale,pyr1[i-1].rows*pyramid_scale));cv::resize(pyr2[i-1],img2_pyr,cv::Size(pyr2[i-1].cols*pyramid_scale,pyr2[i-1].rows*pyramid_scale));pyr1.push_back(img1_pyr);pyr2.push_back(img2_pyr);     &#125;   &#125;   chrono::steady_clock::time_point t2&#x3D;chrono::steady_clock::now();   auto time_used&#x3D;chrono::duration_cast&lt;chrono::duration&lt;double&gt;&gt;(t2-t1);   cout&lt;&lt;&quot;build pyramid time:&quot;&lt;&lt;time_used.count()&lt;&lt;endl;      &#x2F;&#x2F;计算光流时，先从顶层的图像开始计算   vector&lt;KeyPoint&gt;kp1_pyr,kp2_pyr;   for(auto &amp;kp:kp1)   &#123;     auto kp_top&#x3D;kp;     kp_top.pt *&#x3D;scales[pyramids-1];   &#x2F;&#x2F;顶层     kp1_pyr.push_back(kp_top);     kp2_pyr.push_back(kp_top);   &#125;      for(int level&#x3D;pyramids-1;level&gt;&#x3D;0;level--)   &#123;     success.clear();     t1&#x3D;chrono::steady_clock::now();     OpticalFlowSingleLevel(pyr1[level],pyr2[level],kp1_pyr,kp2_pyr,success,inverse,true);     t2&#x3D;chrono::steady_clock::now();     auto time_used&#x3D;chrono::duration_cast&lt;chrono::duration&lt;double&gt;&gt;(t2-t1);     cout&lt;&lt;&quot;track pyr&quot;&lt;&lt;level&lt;&lt;&quot;cost time:&quot;&lt;&lt;time_used.count()&lt;&lt;endl;     if(level&gt;0)     &#123;for(auto &amp;kp:kp1_pyr)  kp.pt&#x2F;&#x3D;pyramid_scale;for(auto &amp;kp:kp2_pyr)  kp.pt&#x2F;&#x3D;pyramid_scale;     &#125;   &#125;   for(auto &amp;kp:kp2_pyr)     kp2.push_back(kp); &#125;<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p><a href="https://blog.csdn.net/weixin_51547017/article/details/115359316?utm_medium=distribute.pc_relevant.none-task-blog-2~default~baidujs_baidulandingword~default-0-115359316-blog-126934983.235%5Ev28%5Epc_relevant_recovery_v2&spm=1001.2101.3001.4242.1&utm_relevant_index=3">视觉SLAM第八讲视觉里程计2— LK光流—代码详细讲解</a></p><p><img src="/pic/%E9%80%89%E5%8C%BA_138.png" alt="运行结果"></p><p><img src="/pic/%E9%80%89%E5%8C%BA_137.png" alt="运行结果"></p><h3 id="ch8-x2F-direct-method-cpp"><a href="#ch8-x2F-direct-method-cpp" class="headerlink" title="ch8&#x2F;direct_method.cpp"></a>ch8&#x2F;direct_method.cpp</h3><p>在光流中，我们会首先追踪特征点的位置，再根据这些位置确定相机的运动。这样一种两步走的方案，很难保证全局最优。直接法通过在后一步中调整前一步的结果</p><p><a href="https://blog.csdn.net/pj18862486309/article/details/107829914?spm=1001.2101.3001.6650.6&utm_medium=distribute.pc_relevant.none-task-blog-2~default~BlogCommendFromBaidu~Rate-6-107829914-blog-126993782.235%5Ev28%5Epc_relevant_recovery_v2&depth_1-utm_source=distribute.pc_relevant.none-task-blog-2~default~BlogCommendFromBaidu~Rate-6-107829914-blog-126993782.235%5Ev28%5Epc_relevant_recovery_v2&utm_relevant_index=10">视觉十四讲：第八讲_直接法</a></p><p><img src="/pic/%E9%80%89%E5%8C%BA_139.png" alt="直接法的讨论"></p><p>代码注释待更新~~~~</p>]]></content>
      
      
      <categories>
          
          <category> slam </category>
          
          <category> c++ </category>
          
      </categories>
      
      
        <tags>
            
            <tag> c++ </tag>
            
            <tag> slam </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>slam14讲源代码的记录（前五讲）</title>
      <link href="/2023/04/06/slam14/"/>
      <url>/2023/04/06/slam14/</url>
      
        <content type="html"><![CDATA[<h2 id="第二讲"><a href="#第二讲" class="headerlink" title="第二讲"></a>第二讲</h2><p>本讲主要是cmake的使用以及一些库的链接方式</p><pre class="line-numbers language-cmake" data-language="cmake"><code class="language-cmake"><span class="token comment">#声明cmake的最低版本</span><span class="token keyword">cmake_minimum_required</span><span class="token punctuation">(</span><span class="token property">VERSION</span> <span class="token number">2.8</span><span class="token punctuation">)</span><span class="token comment">#声明一个cmake工程</span><span class="token keyword">project</span><span class="token punctuation">(</span>HelloSLAM<span class="token punctuation">)</span><span class="token comment">#设置编译模式</span><span class="token keyword">set</span><span class="token punctuation">(</span><span class="token variable">CMAKE_BUILD_TYPE</span><span class="token string">"Debug"</span><span class="token punctuation">)</span><span class="token comment">#添加可执行程序</span><span class="token keyword">add_executable</span><span class="token punctuation">(</span>helloSLAM helloSLAM.cpp<span class="token punctuation">)</span><span class="token comment">#下一部分</span><span class="token comment">#添加库文件</span><span class="token keyword">add_library</span><span class="token punctuation">(</span>hello <span class="token namespace">STATIC</span> libHelloSLAM.cpp<span class="token punctuation">)</span>   <span class="token comment">#静态链接库,会生成.a文件,可默认不加STATIC或者SHARED</span><span class="token comment"># add_library(hello_shared SHARED libHelloSLAM.cpp)  #动态链接库，会生成.so文件</span><span class="token keyword">add_executable</span><span class="token punctuation">(</span>useHello useHello.cpp<span class="token punctuation">)</span><span class="token keyword">target_link_libraries</span><span class="token punctuation">(</span>useHello hello<span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p><img src="/pic/11.jpg"></p><p><img src="/pic/12.jpg"></p><p><a href="https://cmake.org/cmake/help/latest/genindex.html#">CMake指令查询网址</a></p><blockquote><p>补充</p></blockquote><p>添加头文件include_directories()可使用具体路径，例如：</p><p>#添加Eigen头文件<br>include_directories(“&#x2F;usr&#x2F;include&#x2F;eigen3”)</p><p>静态库<br>• 原理:在编译时将源代码复制到程序中，运行时不用库文件依旧可以运行。<br>• 优点:运行已有代码，运行时不用再用库;无需加载库，运行更快<br>• 缺点:占用更多的空间和磁盘;静态库升级，需要重新编译程序<br>共享库(常用)<br>• 原理:编译时仅仅是记录用哪一个库里面的哪一个符号，不复制相关代码<br>• 优点:不复制代码，占用空间小;多个程序可以同时调用一个库;升级方便，无需重新编译 • 缺点:程序运行需要加载库，耗费一定时间</p><table><thead><tr><th align="center">操作系统</th><th align="center">静态库</th><th align="center">共享库</th></tr></thead><tbody><tr><td align="center">Windows</td><td align="center">lib</td><td align="center">.dll</td></tr><tr><td align="center">Linux</td><td align="center">.a</td><td align="center">.so</td></tr><tr><td align="center">Mac OS</td><td align="center">.a</td><td align="center">dylib</td></tr></tbody></table><h2 id="第三讲"><a href="#第三讲" class="headerlink" title="第三讲"></a>第三讲</h2><p>第三讲主要是三维空间刚体运动</p><p><strong>Eigen库的使用</strong></p><h3 id="ch3-x2F-useEigen-x2F-eigenMatrix-cpp中的需要记住的"><a href="#ch3-x2F-useEigen-x2F-eigenMatrix-cpp中的需要记住的" class="headerlink" title="ch3&#x2F;useEigen&#x2F;eigenMatrix.cpp中的需要记住的"></a>ch3&#x2F;useEigen&#x2F;eigenMatrix.cpp中的需要记住的</h3><pre class="line-numbers language-c++" data-language="c++"><code class="language-c++">using namespace Eigen;Vector3d v_3d;      &#x2F;&#x2F;实际上是MAtrix&lt;double,3,1&gt;Matrix3d matrix_33&#x3D;Matrix3d::Zero();&#x2F;&#x2F; Eigen::Matrix&lt;double, 3, 3&gt;Matrix&lt;float,2,3&gt; matrix_23;&#x2F;&#x2F;可以直接这样输入数据matrix_23&lt;&lt;1,2,3,4,5,6;&#x2F;&#x2F;相乘不能混用，需要显式转换Matrix&lt;double,2,1&gt; result &#x3D;matrix_23.cast&lt;double&gt;()*v_3d;matrix_33.transpose()；&#x2F;&#x2F;转置matrix_33.trace()&#x2F;&#x2F;迹matrix_33.inverse() ;&#x2F;&#x2F;求逆matrix_33.determinant();&#x2F;&#x2F;求行列式&#x2F;&#x2F;实对称矩阵一定可以对角化SelfAdjointEigenSolver&lt;Matrix3d&gt; eigen_solver(matrix_33.transpose()*matrix_33);eigen_solver.eigenvalues()；&#x2F;&#x2F;特征值eigen_solver.eigenvectors()；&#x2F;&#x2F;特征向量&#x2F;&#x2F;求解 matrix_NN * x &#x3D; v_Nd 这个方程.QR分解x&#x3D;matrix_NN.colPivHouseholderQr().solve(v_Nd);<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h3 id="ch3-x2F-useGeometry-x2F-eigenGeometry-cpp中"><a href="#ch3-x2F-useGeometry-x2F-eigenGeometry-cpp中" class="headerlink" title="ch3&#x2F;useGeometry&#x2F;eigenGeometry.cpp中"></a>ch3&#x2F;useGeometry&#x2F;eigenGeometry.cpp中</h3><pre class="line-numbers language-c++" data-language="c++"><code class="language-c++">Matrix3d rotation_matrix&#x3D;Matrix3d::Identity();&#x2F;&#x2F;单位阵,旋转矩阵AngleAxisd rotation_vector(M_PI&#x2F;4,Vector3d(0,0,1));&#x2F;&#x2F;旋转向量，使用的AngleAxisd&#x2F;&#x2F;旋转角以及旋转轴cout &lt;&lt; &quot;rotation_vector&quot; &lt;&lt; &quot;angle is: &quot; &lt;&lt; rotation_vector.angle() * (180 &#x2F; M_PI)                                 &lt;&lt; &quot; axis is: &quot; &lt;&lt; rotation_vector.axis().transpose() &lt;&lt; endl;&#x2F;&#x2F;旋转向量变旋转矩阵rotation_matrix &#x3D; rotation_vector.toRotationMatrix();&#x2F;&#x2F;旋转矩阵变旋转向量rotation_vector.fromRotationMatrix(rotation_matrix);&#x2F;&#x2F; 欧拉角: 可以将旋转矩阵直接转换成欧拉角Vector3d euler_angles &#x3D; rotation_matrix.eulerAngles(2, 1, 0); &#x2F;&#x2F; ZYX顺序，即yaw-pitch-roll顺序&#x2F;&#x2F;欧式变换使用Isometry,这里注意需要使用头文件 #include&lt;Eigen&#x2F;Geometry&gt;  Isometry3d T &#x3D; Isometry3d::Identity();                &#x2F;&#x2F; 虽然称为3d，实质上是4＊4的矩阵  T.rotate(rotation_vector);                                     &#x2F;&#x2F; 按照rotation_vector进行旋转  T.pretranslate(Vector3d(1, 3, 4));                     &#x2F;&#x2F; 把平移向量设成(1,3,4)  cout &lt;&lt; &quot;Transform matrix &#x3D; \n&quot; &lt;&lt; T.matrix() &lt;&lt; endl;&#x2F;&#x2F;四元数:拥有一个实部，3个虚部，Quaterniond是（s(q0),q1,q2,q3）Quaterniond q&#x3D;Quaterniond(rotation_vector);cout &lt;&lt; &quot;quaternion from rotation vector &#x3D; &quot; &lt;&lt; q.coeffs().transpose()&lt;&lt; endl;   &#x2F;&#x2F; 请注意coeffs的顺序是(x,y,z,w),w为实部，前三者为虚部<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><blockquote><p>补充：pretranslate 是在旋转之前的坐标轴上进行的平移操作，而 translate 是在旋转之后，pretanslate 相当于左乘，translate 相当于右乘。<br><a href="https://www.guyuehome.com/36653">Eigen 中 pretanslate 和 translate 的区别</a></p></blockquote><p>###ch3&#x2F;examples&#x2F;plotTrajectory.cpp中需要注意的</p><p>使用ifstream流来读取文件</p><p>说明：</p><p>1.ifstream类的对象创建成功的时候会返回非空值，借此判断是否创建文件对象成功</p><p>2.ifstream有个函数eof()用来判断文件是否读到尾部,没读到尾部返回false，否则返回true。</p><p>若尾部有回车，那么最后一条记录会读取两次。</p><p>若尾部没有回车，那么最后一条记录只会读取一次</p><p>3.iftream的对象假设为fin，fin在读取数据的时候会根据你的输出对象来选择输出的方式。</p><p>例程</p><pre class="line-numbers language-c++" data-language="c++"><code class="language-c++">#include&lt;iostream&gt;#include&lt;fstream&gt;using namespace std;int main()&#123;ifstream fin(&quot;abc.txt&quot;);&#x2F;&#x2F;读取文件的名字，可以相对或绝对if(!fin) &#123;cout&lt;&lt;&quot;open fail.&quot;&lt;&lt;endl;exit(1);&#125;else&#123;while(!fin.eof())&#123;char a[20],b[20],c[20];fin&gt;&gt;a&gt;&gt;b&gt;&gt;c;&#x2F;&#x2F;读取的时候遇见空格才会跳跃。cout&lt;&lt;a&lt;&lt;&quot;&quot;&lt;&lt;b&lt;&lt;&quot;  &quot;&lt;&lt;c&lt;&lt;&quot;  &quot;&lt;&lt;endl;&#125;fin.close();&#125;&#125;<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>如果STL容器中的元素是Eigen库数据结构，例如这里定义一个vector容器，元素是Matrix4d ，如下所示：</p><blockquote><p>vector<a href="Eigen::Matrix4d">Eigen::Matrix4d</a></p></blockquote><p>这个错误也是和上述一样的提示，编译不会出错，只有在运行的时候出错。解决的方法很简单，定义改成下面的方式：</p><blockquote><p>vector&lt;Eigen::Matrix4d,Eigen::aligned_allocator<a href="Eigen::Matrix4d">Eigen::Matrix4d</a>&gt;;</p></blockquote><p>其实上述的这段代码才是标准的定义容器方法，只是我们一般情况下定义容器的元素都是C++中的类型，所以可以省略，这是因为在C++11标准中，aligned_allocator管理C++中的各种数据类型的内存方法是一样的，可以不需要着重写出来。但是在Eigen管理内存和C++11中的方法是不一样的，所以需要单独强调元素的内存分配和管理。</p><p>对于轨迹文件的处理过程如下：</p><pre class="line-numbers language-c++" data-language="c++"><code class="language-c++">while (!fin.eof()) &#123;  double time, tx, ty, tz, qx, qy, qz, qw;  fin &gt;&gt; time &gt;&gt; tx &gt;&gt; ty &gt;&gt; tz &gt;&gt; qx &gt;&gt; qy &gt;&gt; qz &gt;&gt; qw;  Isometry3d Twr(Quaterniond(qw, qx, qy, qz)); &#x2F;&#x2F; 对比旋转向量的T.rotate(rotation_vector);            &#x2F;&#x2F; 按照rotation_vector进行旋转  Twr.pretranslate(Vector3d(tx, ty, tz));  poses.push_back(Twr);&#125;<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p><img src="/pic/%E9%80%89%E5%8C%BA_132.png" alt="运动轨迹"></p><p><a href="https://blog.csdn.net/weixin_41756645/article/details/122958689">旋转矩阵、变换矩阵、欧式变换</a></p><h2 id="第四讲"><a href="#第四讲" class="headerlink" title="第四讲"></a>第四讲</h2><p>主要是李群以及李代数，Sophus的使用</p><h3 id="ch4-x2F-useSophus-cpp中"><a href="#ch4-x2F-useSophus-cpp中" class="headerlink" title="ch4&#x2F;useSophus.cpp中"></a>ch4&#x2F;useSophus.cpp中</h3><pre class="line-numbers language-c++" data-language="c++"><code class="language-c++">&#x2F;&#x2F;李群的表示  &#x2F;&#x2F; 沿Z轴转90度的旋转矩阵  Matrix3d R &#x3D; AngleAxisd(M_PI &#x2F; 2, Vector3d(0, 0, 1)).toRotationMatrix();  &#x2F;&#x2F; 或者四元数  Quaterniond q(R);  Sophus::SO3d SO3_R(R);              &#x2F;&#x2F; Sophus::SO3d可以直接从旋转矩阵构造  Sophus::SO3d SO3_q(q);              &#x2F;&#x2F; 也可以通过四元数构造    &#x2F;&#x2F; 对SE(3)操作大同小异  Vector3d t(1, 0, 0);           &#x2F;&#x2F; 沿X轴平移1  Sophus::SE3d SE3_Rt(R, t);           &#x2F;&#x2F; 从R,t构造SE(3)  Sophus::SE3d SE3_qt(q, t);            &#x2F;&#x2F; 从q,t构造SE(3)  &#x2F;&#x2F;通过.matrix()查看李群的矩阵   &#x2F;&#x2F;通过对数映射转化为李代数，这里的log直接把李群转化为李代数，不是反对称矩阵  Vector3d so3 &#x3D; SO3_R.log();  cout &lt;&lt; &quot;so3 &#x3D; &quot; &lt;&lt; so3.transpose() &lt;&lt; endl;  &#x2F;&#x2F; hat 为向量到反对称矩阵  cout &lt;&lt; &quot;so3 hat&#x3D;\n&quot; &lt;&lt; Sophus::SO3d::hat(so3) &lt;&lt; endl;    &#x2F;&#x2F; 李代数se(3) 是一个六维向量，方便起见先typedef一下  typedef Eigen::Matrix&lt;double, 6, 1&gt; Vector6d;  Vector6d se3 &#x3D; SE3_Rt.log();  cout &lt;&lt; &quot;se3 &#x3D; &quot; &lt;&lt; se3.transpose() &lt;&lt; endl;  &#x2F;&#x2F; 观察输出，会发现在Sophus中，se(3)的平移在前，旋转在后.需要注意  &#x2F;&#x2F; 同样的，有hat和vee两个算符  cout &lt;&lt; &quot;se3 hat &#x3D; \n&quot; &lt;&lt; Sophus::SE3d::hat(se3) &lt;&lt; endl;  &#x2F;&#x2F; 增量扰动模型的更新，李代数求导    Vector3d update_so3(1e-4, 0, 0); &#x2F;&#x2F;假设更新量为这么多  Sophus::SO3d SO3_updated &#x3D; Sophus::SO3d::exp(update_so3) * SO3_R;  cout &lt;&lt; &quot;SO3 updated &#x3D; \n&quot; &lt;&lt; SO3_updated.matrix() &lt;&lt; endl;    Vector6d update_se3; &#x2F;&#x2F;更新量,这里不是太懂  update_se3.setZero();  update_se3(0, 0) &#x3D; 1e-4;  Sophus::SE3d SE3_updated &#x3D; Sophus::SE3d::exp(update_se3) * SE3_Rt;  cout &lt;&lt; &quot;SE3 updated &#x3D; &quot; &lt;&lt; endl &lt;&lt; SE3_updated.matrix() &lt;&lt; endl;<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p><a href="https://xsin.gitee.io/2019/01/14/SlSophus%E5%87%BD%E6%95%B0%E7%9A%84%E4%BD%BF%E7%94%A8/">Slam中Sophus函数的使用</a></p><h3 id="ch4-x2F-example-x2F-trajectoryError-cpp中比较真实轨迹与估计轨迹之间的误差"><a href="#ch4-x2F-example-x2F-trajectoryError-cpp中比较真实轨迹与估计轨迹之间的误差" class="headerlink" title="ch4&#x2F;example&#x2F;trajectoryError.cpp中比较真实轨迹与估计轨迹之间的误差"></a>ch4&#x2F;example&#x2F;trajectoryError.cpp中比较真实轨迹与估计轨迹之间的误差</h3><pre class="line-numbers language-c++" data-language="c++"><code class="language-c++">&#x2F;&#x2F;vector（动态数组）类型的TrajectoryType存储位姿typedef vector&lt;Sophus::SE3d, Eigen::aligned_allocator&lt;Sophus::SE3d&gt;&gt; TrajectoryType;&#x2F;&#x2F;通过下面这样读取至李群中  while (!fin.eof()) &#123;    double time, tx, ty, tz, qx, qy, qz, qw;    fin &gt;&gt; time &gt;&gt; tx &gt;&gt; ty &gt;&gt; tz &gt;&gt; qx &gt;&gt; qy &gt;&gt; qz &gt;&gt; qw;    Sophus::SE3d p1(Eigen::Quaterniond(qw, qx, qy, qz), Eigen::Vector3d(tx, ty, tz));    trajectory.push_back(p1);  &#125;  &#x2F;&#x2F;轨迹误差的计算方法如下    double rmse &#x3D; 0;  for (size_t i &#x3D; 0; i &lt; estimated.size(); i++) &#123;    Sophus::SE3d p1 &#x3D; estimated[i], p2 &#x3D; groundtruth[i];    double error &#x3D; (p2.inverse() * p1).log().norm();    rmse +&#x3D; error * error;  &#125;  rmse &#x3D; rmse &#x2F; double(estimated.size());  rmse &#x3D; sqrt(rmse);  cout &lt;&lt; &quot;RMSE &#x3D; &quot; &lt;&lt; rmse &lt;&lt; endl;<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p><img src="/pic/%E9%80%89%E5%8C%BA_129.png" alt="轨迹相差图"></p><p><a href="https://blog.csdn.net/qq_28087491/article/details/113540037">Pangolin可视化绘图库的使用</a></p><h2 id="第五讲"><a href="#第五讲" class="headerlink" title="第五讲"></a>第五讲</h2><p>本讲主要是相机与图像,内参外参畸变参数等，Opencv使用以及摄像头标定等</p><h3 id="ch5-x2F-imageBasics-x2F-imageBasics-cpp中操作Opencv图像中需要注意的"><a href="#ch5-x2F-imageBasics-x2F-imageBasics-cpp中操作Opencv图像中需要注意的" class="headerlink" title="ch5&#x2F;imageBasics&#x2F;imageBasics.cpp中操作Opencv图像中需要注意的"></a>ch5&#x2F;imageBasics&#x2F;imageBasics.cpp中操作Opencv图像中需要注意的</h3><pre class="line-numbers language-c++" data-language="c++"><code class="language-c++">  image &#x3D; cv::imread(argv[1]); &#x2F;&#x2F;cv::imread函数读取指定路径下的图像  &#x2F;&#x2F; 文件顺利读取, 首先输出一些基本信息  cout &lt;&lt; &quot;图像宽为&quot; &lt;&lt; image.cols &lt;&lt; &quot;,高为&quot; &lt;&lt; image.rows &lt;&lt; &quot;,通道数为&quot; &lt;&lt; image.channels() &lt;&lt; endl;  cv::imshow(&quot;image&quot;, image);      &#x2F;&#x2F; 用cv::imshow显示图像  cv::waitKey(0);                  &#x2F;&#x2F; 暂停程序,等待一个按键输入  &#x2F;&#x2F; 使用 std::chrono 来给算法计时  chrono::steady_clock::time_point t1 &#x3D; chrono::steady_clock::now();    sleep(1);    chrono::steady_clock::time_point t2 &#x3D; chrono::steady_clock::now();  chrono::duration&lt;double&gt; time_used &#x3D; chrono::duration_cast &lt; chrono::duration &lt; double &gt;&gt; (t2 - t1);  cout &lt;&lt; &quot;遍历图像用时：&quot; &lt;&lt; time_used.count() &lt;&lt; &quot; 秒。&quot; &lt;&lt; endl;&#x2F;&#x2F;time_point表示一个时间点，用来获取1970.1.1以来的秒数和当前的时间。time_point必须要clock来计时，time_point有一个函数time_since_epoch()用来获得1970年1月1日到time_point时间经过的duration&#x2F;&#x2F;时钟间隔Duration&#x2F;&#x2F; 遍历图像, 请注意以下遍历方式亦可使用于随机像素访问for (size_t y &#x3D; 0; y &lt; image.rows; y++) &#123;    &#x2F;&#x2F; 用cv::Mat::ptr获得图像的行指针    unsigned char *row_ptr &#x3D; image.ptr&lt;unsigned char&gt;(y);  &#x2F;&#x2F; row_ptr是第y行的头指针    for (size_t x &#x3D; 0; x &lt; image.cols; x++) &#123;      &#x2F;&#x2F; 访问位于 x,y 处的像素      unsigned char *data_ptr &#x3D; &amp;row_ptr[x * image.channels()]; &#x2F;&#x2F; data_ptr 指向待访问的像素数据      &#x2F;&#x2F; 输出该像素的每个通道,如果是灰度图就只有一个通道      for (int c &#x3D; 0; c !&#x3D; image.channels(); c++) &#123;        unsigned char data &#x3D; data_ptr[c]; &#x2F;&#x2F; data为I(x,y)第c个通道的值      &#125;    &#125;  &#125;    &#x2F;&#x2F; 关于 cv::Mat 的拷贝  &#x2F;&#x2F; 直接赋值并不会拷贝数据  cv::Mat image_another &#x3D; image;  &#x2F;&#x2F; 修改 image_another 会导致 image 发生变化  image_another(cv::Rect(0, 0, 100, 100)).setTo(0); &#x2F;&#x2F; 将左上角100*100的块置零  cv::imshow(&quot;image&quot;, image);  cv::waitKey(0);  &#x2F;&#x2F; 使用clone函数来拷贝数据  cv::Mat image_clone &#x3D; image.clone();  image_clone(cv::Rect(0, 0, 100, 100)).setTo(255);  cv::imshow(&quot;image&quot;, image);  cv::imshow(&quot;image_clone&quot;, image_clone);  cv::waitKey(0);<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><blockquote><p>补充</p></blockquote><p>CMakeLists.txt文件代码</p><pre class="line-numbers language-CMake" data-language="CMake"><code class="language-CMake">cmake_minimum_required( VERSION 2.8 )project(imageBasics) set( CMAKE_CXX_FLAGS &quot;-std&#x3D;c++11 -O3&quot;)find_package(OpenCV)include_directories($&#123;OpenCV&#125;)add_executable(imageBasics imageBasics.cpp)# 链接OpenCV库target_link_libraries(imageBasics $&#123;OpenCV_LIBS&#125;)add_executable(undistortImage undistortImage.cpp)# 链接OpenCV库target_link_libraries(undistortImage $&#123;OpenCV_LIBS&#125;)<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p><a href="https://blog.csdn.net/qq_43428547/article/details/88956911">二维数组与指针(详解)</a></p><p>数组名与指针的关系</p><pre class="line-numbers language-c++" data-language="c++"><code class="language-c++">a;&#x2F;&#x2F;代表数组首行地址，一般用a[0][0]的地址表示&amp;a;&#x2F;&#x2F;代表整个数组的地址，一般用a[0][0]地址表示a[i];代表了第i行起始元素的地址(网上说是代表了第i行的地址，但我觉得不是，在讲数组与指针的关系时我会验证给大家看)&amp;a[i];代表了第i行的地址，一般用a[i][0]的地址表示a[i]+j;&#x2F;&#x2F;代表了第i行第j个元素地址,a[i]就是j&#x3D;&#x3D;0的情况a[i][j];&#x2F;&#x2F;代表了第i行第j个元素&amp;a[i][j];&#x2F;&#x2F;代表了第i行第j个元素的地址*a;&#x2F;&#x2F;代表数组a首元素地址也就是a[0]或者&amp;a[0][0]*(a+i);&#x2F;&#x2F;代表了第i行首元素的地址,*a是i&#x3D;0的情况*(a+i)+j;&#x2F;&#x2F;代表了第i行j个元素的地址**a;&#x2F;&#x2F;代表a的首元素的值也就是a[0][0]*(*(a+i)+j);&#x2F;&#x2F;代表了第i行第j个元素<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p><a href="https://blog.csdn.net/u010420283/article/details/111946293?utm_medium=distribute.pc_relevant.none-task-blog-2~default~baidujs_baidulandingword~default-1-111946293-blog-121178263.235%5Ev27%5Epc_relevant_default_base1&spm=1001.2101.3001.4242.2&utm_relevant_index=4">c++中几种记时函数实例</a></p><p>几种及时函数如下：</p><pre class="line-numbers language-c++" data-language="c++"><code class="language-c++">&#x2F;&#x2F;方法1,标准库#include &lt;sys&#x2F;time.h&gt; struct timeval tv;gettimeofday(&amp;tv,NULL);auto b1&#x3D;(unsigned long long)tv.tv_sec*1000+(unsigned long long)tv.tv_usec&#x2F;1000;sleep(1);gettimeofday(&amp;tv,NULL);auto b2&#x3D;(unsigned long long)tv.tv_sec*1000+(unsigned long long)tv.tv_usec&#x2F;1000;cout&lt;&lt;&quot;method 1, cost time is:&quot;&lt;&lt;(b2-b1)&lt;&lt;endl;  &#x2F;&#x2F;方法2，chrono#include&lt;chrono&gt;using namespace std::chrono; auto t1&#x3D; duration_cast&lt;milliseconds&gt;(steady_clock::now().time_since_epoch()).count();sleep(1);auto t2&#x3D;duration_cast&lt;milliseconds&gt;(steady_clock::now().time_since_epoch()).count();cout&lt;&lt;&quot;method 2, cost time is:&quot;&lt;&lt;(t2-t1)&lt;&lt;endl; &#x2F;&#x2F;方法2副本，该代码只能求两段时间差，不能得到打印出当前时刻。auto begin&#x3D;system_clock::now();sleep(1);auto end&#x3D;system_clock::now();cout&lt;&lt;&quot;method 2-1, cost time is:&quot;&lt;&lt;duration_cast&lt;milliseconds&gt;(end - begin).count()&lt;&lt;endl;   &#x2F;&#x2F;方法3#include &lt;unistd.h&gt;clock_t c1&#x3D;clock();sleep(1);clock_t c2&#x3D;clock();cout&lt;&lt;&quot;method 3, cost time is:&quot;&lt;&lt;(c2-c1)&lt;&lt;endl; <span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h3 id="ch5-x2F-imageBasics-x2F-undistortImage-cpp中图像去畸变"><a href="#ch5-x2F-imageBasics-x2F-undistortImage-cpp中图像去畸变" class="headerlink" title="ch5&#x2F;imageBasics&#x2F;undistortImage.cpp中图像去畸变"></a>ch5&#x2F;imageBasics&#x2F;undistortImage.cpp中图像去畸变</h3><p><img src="/pic/%E9%80%89%E5%8C%BA_102.png" alt="去畸变公式"></p><pre class="line-numbers language-c++" data-language="c++"><code class="language-c++">&#x2F;&#x2F;主要知道内参以及去畸变参数  &#x2F;&#x2F; 畸变参数  double k1 &#x3D; -0.28340811, k2 &#x3D; 0.07395907, p1 &#x3D; 0.00019359, p2 &#x3D; 1.76187114e-05;  &#x2F;&#x2F; 内参  double fx &#x3D; 458.654, fy &#x3D; 457.296, cx &#x3D; 367.215, cy &#x3D; 248.375;  &#x2F;&#x2F; 计算去畸变后图像的内容  for (int v &#x3D; 0; v &lt; rows; v++) &#123;    for (int u &#x3D; 0; u &lt; cols; u++) &#123;      &#x2F;&#x2F; 按照公式，计算点(u,v)对应到畸变图像中的坐标(u_distorted, v_distorted)      double x &#x3D; (u - cx) &#x2F; fx, y &#x3D; (v - cy) &#x2F; fy;      double r &#x3D; sqrt(x * x + y * y);      double x_distorted &#x3D; x * (1 + k1 * r * r + k2 * r * r * r * r) + 2 * p1 * x * y + p2 * (r * r + 2 * x * x);      double y_distorted &#x3D; y * (1 + k1 * r * r + k2 * r * r * r * r) + p1 * (r * r + 2 * y * y) + 2 * p2 * x * y;      double u_distorted &#x3D; fx * x_distorted + cx;      double v_distorted &#x3D; fy * y_distorted + cy;      &#x2F;&#x2F; 赋值 (最近邻插值)      if (u_distorted &gt;&#x3D; 0 &amp;&amp; v_distorted &gt;&#x3D; 0 &amp;&amp; u_distorted &lt; cols &amp;&amp; v_distorted &lt; rows) &#123;        image_undistort.at&lt;uchar&gt;(v, u) &#x3D; image.at&lt;uchar&gt;((int) v_distorted, (int) u_distorted);      &#125; else &#123;        image_undistort.at&lt;uchar&gt;(v, u) &#x3D; 0;      &#125;    &#125;  &#125;<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p><strong>Opencv提供了去畸变函数cv::Undistort().</strong></p><ul><li><p>函数功能：直接对图像进行畸变矫正。</p></li><li><p>其内部调用了initUndistortRectifyMap和remap函数。</p></li></ul><pre class="line-numbers language-c++" data-language="c++"><code class="language-c++">CV_EXPORTS_W void undistort( InputArray src, &#x2F;&#x2F;原始图像                            OutputArray dst,&#x2F;&#x2F;矫正图像                             InputArray cameraMatrix,&#x2F;&#x2F;原相机的内参矩阵                             InputArray distCoeffs,&#x2F;&#x2F;相机矫正参数                             InputArray newCameraMatrix &#x3D; noArray() );&#x2F;&#x2F;新相机内参矩阵<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>直接使用Opencv中的函数主要是对参数写成矩阵形式加进去就可以了</p><pre class="line-numbers language-c++" data-language="c++"><code class="language-c++">#include &lt;opencv2&#x2F;opencv.hpp&gt;#include &lt;string&gt;using namespace std;int main(int argc, char **argv) &#123;    const cv::Mat K &#x3D; ( cv::Mat_&lt;double&gt; ( 3,3 ) &lt;&lt; 458.654, 0.0, 367.215, 0.0, 457.296, 248.375, 0.0, 0.0, 1.0 ); &#x2F;&#x2F; 相机内参矩阵    &#x2F;&#x2F;k1 &#x3D; -0.28340811, k2 &#x3D; 0.07395907, p1 &#x3D; 0.00019359, p2 &#x3D; 1.76187114e-05;    const cv::Mat D &#x3D; (cv::Mat_&lt;double&gt; ( 5,1 ) &lt;&lt;  -0.28340811, 0.07395907, 0.0, 0.00019359, 1.76187114e-05);    &#x2F;&#x2F;double k1 &#x3D; -0.28340811, k2 &#x3D; 0.07395907, k3&#x3D;0,p1 &#x3D; 0.00019359, p2 &#x3D; 1.76187114e-05;    const string str &#x3D; &quot;&#x2F;home&#x2F;chy&#x2F;slambook2&#x2F;ch5&#x2F;imageBasics&#x2F;distorted.png&quot;;  cv::Mat image &#x3D; cv::imread(str, 0);   &#x2F;&#x2F; 图像是灰度图，CV_8UC1  int rows &#x3D; image.rows, cols &#x3D; image.cols;  cv::Mat image_undistort &#x3D; cv::Mat(rows, cols, CV_8UC1);   &#x2F;&#x2F; 去畸变以后的图  cv::undistort(image,image_undistort,K,D,K);  &#x2F;&#x2F; 画图去畸变后图像  cv::imshow(&quot;distorted&quot;, image);  cv::imshow(&quot;undistorted&quot;, image_undistort);  cv::waitKey();  return 0;&#125;<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p><a href="https://blog.csdn.net/cannonfodder9527/article/details/129002265">SLAM14讲第5讲去畸变方法改进</a></p><p><strong>使用 getOptimalNewCameraMatrix + initUndistortRectifyMap + remap 矫正图像</strong></p><p><a href="https://blog.csdn.net/qq_18894441/article/details/122983176">关于OpenCV中的去畸变 c++</a></p><h3 id="ch5-x2F-stereo-x2F-stereoVision-cpp中双目视觉"><a href="#ch5-x2F-stereo-x2F-stereoVision-cpp中双目视觉" class="headerlink" title="ch5&#x2F;stereo&#x2F;stereoVision.cpp中双目视觉"></a>ch5&#x2F;stereo&#x2F;stereoVision.cpp中双目视觉</h3><p><img src="/pic/%E9%80%89%E5%8C%BA_091.png" alt="双目视觉求深度z"></p><p>使用SGBM算法计算左右图像的视差</p><pre class="line-numbers language-c++" data-language="c++"><code class="language-c++">cv::Ptr&lt;cv::StereoSGBM&gt; sgbm &#x3D; cv::StereoSGBM::create(       0, 96, 9, 8 * 9 * 9, 32 * 9 * 9, 1, 63, 10, 100, 32);    &#x2F;&#x2F; sgbm经典参数配置   cv::Mat disparity_sgbm, disparity;   sgbm-&gt;compute(left, right, disparity_sgbm);&#x2F;&#x2F;输入前面两张图，第三个参数是输出   disparity_sgbm.convertTo(disparity, CV_32F, 1.0 &#x2F; 16.0f);  &#x2F;&#x2F;将disparity_sgbm变成32F类型的disparity，这里的disparity才是视差图。  如果Mat类型数据的深度不满足上面的要求，则需要使用convertTo()函数来进行转换。convertTo()函数负责转换数据类型不同的Mat       for (int v &#x3D; 0; v &lt; left.rows; v++)       for (int u &#x3D; 0; u &lt; left.cols; u++) &#123;           if (disparity.at&lt;float&gt;(v, u) &lt;&#x3D; 0.0 || disparity.at&lt;float&gt;(v, u) &gt;&#x3D; 96.0) continue;           Vector4d point(0, 0, 0, left.at&lt;uchar&gt;(v, u) &#x2F; 255.0); &#x2F;&#x2F; 前三维为xyz,第四维为颜色           &#x2F;&#x2F; 根据双目模型计算 point 的位置           double x &#x3D; (u - cx) &#x2F; fx;           double y &#x3D; (v - cy) &#x2F; fy;           double depth &#x3D; fx * b &#x2F; (disparity.at&lt;float&gt;(v, u));           point[0] &#x3D; x * depth;           point[1] &#x3D; y * depth;           point[2] &#x3D; depth;           pointcloud.push_back(point);       &#125;<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p><strong>sgbm参数解释：</strong></p><pre class="line-numbers language-c++" data-language="c++"><code class="language-c++">cv::Ptr&lt;cv::StereoSGBM&gt; sgbm &#x3D; cv::StereoSGBM::create(    0, 96, 9, 8 * 9 * 9, 32 * 9 * 9, 1, 63, 10, 100, 32);    &#x2F;&#x2F; sgbm经典参数配置<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><p><img src="/pic/%E9%80%89%E5%8C%BA_092.png"></p><p><strong>convertTo函数</strong></p><p> 用于计算距离的视差图（CV_32F）和用于肉眼看的视差图（CV_8U）使用的格式不同，并且用于计算的视差图无需进行裁剪和归一化，这些只是为了显示的可读性和美观。所以，在对sgbm进行compute之后得到视差图disparity_sgbm，除以16得到用于计算的视差图disparity（除以16是因为每个像素值由一个16bit表示，其中低位的4位存储的是视差值得小数部分，所以真实视差值应该是该值除以16</p><p><a href="https://blog.csdn.net/weixin_70026476/article/details/127351340">参考:实践部分双目视觉代码讲解</a></p><p>这部分Pangolin的讲解也可以参考上面的链接</p><p><img src="/pic/%E9%80%89%E5%8C%BA_127.png" alt="深度图"></p><p><img src="/pic/%E9%80%89%E5%8C%BA_128.png" alt="点云图"></p><h3 id="ch5-x2F-rgbd-x2F-joinMap-cpp中"><a href="#ch5-x2F-rgbd-x2F-joinMap-cpp中" class="headerlink" title="ch5&#x2F;rgbd&#x2F;joinMap.cpp中"></a>ch5&#x2F;rgbd&#x2F;joinMap.cpp中</h3><p><a href="https://blog.csdn.net/zhiwei121/article/details/95033924">可参考</a></p><p>已知相机内外参，五张RGB图以及他们的深度信息计算任何一个像素的世界坐标系下的位置从而建立一个点云地图</p><pre class="line-numbers language-c++" data-language="c++"><code class="language-c++">&#x2F;&#x2F;起个别名，方便后面使用typedef vector&lt;Sophus::SE3d, Eigen::aligned_allocator&lt;Sophus::SE3d&gt;&gt; TrajectoryType;typedef Eigen::Matrix&lt;double, 6, 1&gt; Vector6d;&#x2F;&#x2F;通过fmt占位符读取图像，具体看下面fmt讲解链接，要先include &lt;boost&#x2F;format.hpp&gt;这个头文件。boost::format fmt(&quot;.&#x2F;%s&#x2F;%d.%s&quot;); &#x2F;&#x2F;图像文件格式colorImgs.push_back(cv::imread((fmt % &quot;color&quot; % (i + 1) % &quot;png&quot;).str()));depthImgs.push_back(cv::imread((fmt % &quot;depth&quot; % (i + 1) % &quot;pgm&quot;).str(), -1)); &#x2F;&#x2F; 使用-1读取原始图像&#x2F;&#x2F; for 在C++11的新特性,具体看下面链接讲解double data[7] &#x3D; &#123;0&#125;;for (auto &amp;d:data)     fin &gt;&gt; d; &#x2F;&#x2F; 计算点云并拼接    &#x2F;&#x2F; 相机内参     double cx &#x3D; 325.5;    double cy &#x3D; 253.5;    double fx &#x3D; 518.0;    double fy &#x3D; 519.0;    double depthScale &#x3D; 1000.0;    vector&lt;Vector6d, Eigen::aligned_allocator&lt;Vector6d&gt;&gt; pointcloud;&#x2F;&#x2F; reserve为容器预先分配内存空间，并未初始化空间元素    pointcloud.reserve(1000000);    for (int i &#x3D; 0; i &lt; 5; i++) &#123;        cout &lt;&lt; &quot;转换图像中: &quot; &lt;&lt; i + 1 &lt;&lt; endl;        cv::Mat color &#x3D; colorImgs[i];        cv::Mat depth &#x3D; depthImgs[i];        Sophus::SE3d T &#x3D; poses[i];        for (int v &#x3D; 0; v &lt; color.rows; v++)            for (int u &#x3D; 0; u &lt; color.cols; u++) &#123;                          &#x2F;*通过用Mat中的ptr模板函数 返回一个unsigned short类型的指针。v表示行 根据内部计算返回data头指针 + 偏移量来计算v行的头指针             * 图像为单通道的   depth.ptr&lt;unsigned short&gt; ( v ) 来获取行指针*&#x2F;                unsigned int d &#x3D; depth.ptr&lt;unsigned short&gt;(v)[u]; &#x2F;&#x2F; 深度值                if (d &#x3D;&#x3D; 0) continue; &#x2F;&#x2F; 为0表示没有测量到                Eigen::Vector3d point;                point[2] &#x3D; double(d) &#x2F; depthScale;&#x2F;&#x2F;实际尺度的一个缩放因子                point[0] &#x3D; (u - cx) * point[2] &#x2F; fx;                point[1] &#x3D; (v - cy) * point[2] &#x2F; fy;                Eigen::Vector3d pointWorld &#x3D; T * point;&#x2F;&#x2F;将相机坐标系转换为世界坐标系                Vector6d p;                &#x2F;&#x2F;head&lt;n&gt;()函数是对于Eigen库中的向量类型而言的，表示提取前n个元素                &#x2F;&#x2F;方法一                p.head&lt;3&gt;() &#x3D; pointWorld;                p[5] &#x3D; color.data[v * color.step + u * color.channels()];   &#x2F;&#x2F; blue                p[4] &#x3D; color.data[v * color.step + u * color.channels() + 1]; &#x2F;&#x2F; green                p[3] &#x3D; color.data[v * color.step + u * color.channels() + 2]; &#x2F;&#x2F; red                &#x2F;&#x2F;方法二：                &#x2F;&#x2F;程序上方读取了一张图片color                    p[5]&#x3D;color.at&lt;cv::Vec3b&gt;(v, u)[0];  &#x2F;&#x2F;B                     p[4]&#x3D;color.at&lt;cv::Vec3b&gt;(v, u)[1];  &#x2F;&#x2F;G                    p[3]&#x3D;color.at&lt;cv::Vec3b&gt;(v, u)[2];  &#x2F;&#x2F;R                                pointcloud.push_back(p);            &#125;    &#125;vector&lt;Vector6d, Eigen::aligned_allocator&lt;Vector6d&gt;&gt; pointcloud;pointcloud.reserve(1000000);<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p><a href="https://blog.csdn.net/hhl317/article/details/118937838">c++中的boost::format,fmt使用方法</a></p><p><a href="https://blog.csdn.net/try_again_later/article/details/81566850">boost::format 以及 for 新特性</a></p><p><a href="https://blog.csdn.net/CxC2333/article/details/107735638?spm=1001.2101.3001.6650.3&utm_medium=distribute.pc_relevant.none-task-blog-2~default~CTRLIST~Rate-3-107735638-blog-107092078.235%5Ev28%5Epc_relevant_recovery_v2&depth_1-utm_source=distribute.pc_relevant.none-task-blog-2~default~CTRLIST~Rate-3-107735638-blog-107092078.235%5Ev28%5Epc_relevant_recovery_v2&utm_relevant_index=6">Opencv关于成员函数data，step，at的使用</a></p><pre class="line-numbers language-c++" data-language="c++"><code class="language-c++">&#x2F;*                                            备注： 3通道的图像的遍历方式总结 * 对于单通道来说 每个像素占8位 3通道则是每个矩阵元素是一个Vec3b 即一个三维的向量 向量内部元素为8位数的unsigned char类型 * 1、使用at遍历图像 * for(v)row *  for(u)col *      image.at&lt;Vec3b&gt;（v,u）[0] 表示第一个通道的像素的值 *      image.at&lt;Vec3b&gt;(v,u)[1] *      image.at&lt;Vec3b&gt;(v,u)[2] * 2、使用迭代器方式 (实际上就是一个指针指向了 cv::Mat矩阵元素) * cv::MatIterator_&lt;Vec3b&gt;begin,end; * for( begin &#x3D; image.begin&lt;Vec3b&gt;(), end &#x3D; image.end&lt;Vec3b&gt;() ; begin !&#x3D; end;  ) *      (*begin)[0] &#x3D; ... *      (*begin)[1] &#x3D; ... *      (*begin)[2] &#x3D; ... * * 3、用指针的方式操作 * for(v) *  for(u) *      image.ptr&lt;Vec3b&gt;(v)[u][0] 表示第一个通道 *      image.ptr&lt;Vec3b&gt;(v)[u][0] 表示第二通道 *              . *              . *              . * *&#x2F;<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p><img src="/pic/%E9%80%89%E5%8C%BA_126.png" alt="点云图"></p>]]></content>
      
      
      <categories>
          
          <category> slam </category>
          
          <category> c++ </category>
          
      </categories>
      
      
        <tags>
            
            <tag> c++ </tag>
            
            <tag> slam </tag>
            
            <tag> CMake </tag>
            
            <tag> Sophus </tag>
            
            <tag> Eigen </tag>
            
            <tag> Pangolin </tag>
            
            <tag> 图像去畸变 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>orb_slam2改进代码的相关汇总</title>
      <link href="/2023/04/05/orb-slam2/"/>
      <url>/2023/04/05/orb-slam2/</url>
      
        <content type="html"><![CDATA[<h1 id="orb-slam2改进代码的相关汇总"><a href="#orb-slam2改进代码的相关汇总" class="headerlink" title="orb_slam2改进代码的相关汇总"></a>orb_slam2改进代码的相关汇总</h1><h2 id="点线特征相关"><a href="#点线特征相关" class="headerlink" title="点线特征相关"></a>点线特征相关</h2><p><strong>添加了线特征。从3D密集SLAM进行表面重建的增量3D线段提取</strong></p><p><a href="https://github.com/atlas-jj/ORB_Line_SLAM">Add line feature based ORB-SLAM2</a></p><p><strong>RGB-D模式下添加了点线融合</strong></p><p><a href="https://github.com/maxee1900/RGBD-PL-SLAM">RGBD-SLAM with Point and Line Features, developed based on ORB_SLAM2</a></p><p><strong>单目线特征</strong></p><p><a href="https://github.com/lanyouzibetty/ORB-SLAM2_with_line">ORB-SLAM2_with_line, Monocular ORB-SLAM with Line Features</a></p><p><strong>双目点线融合,在弱纹理环境中传统点特征方法失效的情况下拥有较高的运行鲁棒性</strong></p><p><a href="https://github.com/rubengooj/pl-slam">PL-SLAM: a Stereo SLAM System through the Combination of Points and Line Segments</a></p><h2 id="用新的特征点替代ORB"><a href="#用新的特征点替代ORB" class="headerlink" title="用新的特征点替代ORB"></a>用新的特征点替代ORB</h2><p><strong>使用了一种更好的特征选择方法</strong></p><p><a href="https://github.com/ivalab/gf_orb_slam2">GF-ORB-SLAM2, Real-Time SLAM for Monocular, Stereo and RGB-D Cameras, with Loop Detectionand Relocalization</a></p><p><strong>用SuperPoint 替代ORB来进行特征提取</strong></p><p><a href="https://github.com/KinglittleQ/SuperPoint_SLAM">SuperPoint-SLAM</a></p><p><strong>设计的GCNv2具有与ORB功能相同的描述符格式,可以替代关键点提取,精度更高,适合在嵌入式低功耗平台上运行</strong></p><p><a href="https://github.com/jiexiong2016/GCNv2_SLAM">GCNv2: Efficient Correspondence Prediction for Real-Time SLAM</a></p><h2 id="直接法代替特征点法"><a href="#直接法代替特征点法" class="headerlink" title="直接法代替特征点法"></a>直接法代替特征点法</h2><p><strong>使用SVO中直接法来跟踪代替耗时的特征点提取匹配,在保持同样精度的情况下,是原始ORB-SLAM2速度的3倍</strong></p><p><a href="https://github.com/gaoxiang12/ORB-YGZ-SLAM">ORB-YGZ-SLAM, average 3x speed up and keep almost same accuracy v.s. ORB-SLAM2, use directtracking in SVO to accelerate the feature matching</a></p><h2 id="融合其他传感器"><a href="#融合其他传感器" class="headerlink" title="融合其他传感器"></a>融合其他传感器</h2><p><strong>双目VIO版本,加入了LK光流和滑动窗口BA优化</strong></p><p><a href="https://github.com/gaoxiang12/ygz-stereo-inertial">YGZ-stereo-inertial SLAM, LK optical flow + sliding window bundle adjustment</a></p><p><strong>京胖实现的VI-ORB-SLAM2</strong></p><p><a href="https://github.com/jingpang/LearnVIORB">VIORB, An implementation of Visual Inertial ORBSLAM based on ORB-SLAM2</a></p><p><strong>支持鱼眼,不需要rectify和裁剪输入图</strong></p><p><a href="https://github.com/lsyads/fisheye-ORB-SLAM">Fisheye-ORB-SLAM, A real-time robust monocular visual SLAM system based on ORB-SLAM for fisheye cameras, without rectifying or cropping the input images</a></p><h2 id="地图相关"><a href="#地图相关" class="headerlink" title="地图相关"></a>地图相关</h2><p><strong>添加保存和导入地图功能</strong></p><p><a href="https://github.com/AlejandroSilvestri/osmap">Osmap, Save and load orb-slam2 maps</a></p><p><a href="https://github.com/Jiankai-Sun/ORB_SLAM2_Enhanced">ORB_SLAM2 with map load&#x2F;save function</a></p><p><strong>添加了地图可视化</strong></p><p><a href="https://github.com/AlejandroSilvestri/Osmap-viewer">Viewer for maps from ORB-SLAM2 Osmap</a></p><p><strong>高翔实现的添加稠密点云地图</strong></p><p><a href="https://github.com/gaoxiang12/ORBSLAM2_with_pointcloud_map">ORBSLAM2_with_pointcloud_map</a></p><p><strong>在高翔基础上添加了稠密闭环地图</strong></p><p><a href="https://github.com/tiantiandabaojian/ORB-SLAM2_RGBD_DENSE_MAP">ORB-SLAM2_RGBD_DENSE_MAP, modified from Xiang Gao’s “ORB_SLAM2_modified”. It is added a dense loopclosing map model</a></p><h2 id="动态环境"><a href="#动态环境" class="headerlink" title="动态环境"></a>动态环境</h2><p><strong>适合动态环境,增加了动态物体检测和背景修复的能力</strong></p><p><a href="https://github.com/BertaBescos/DynaSLAM">DynaSLAM, is a SLAM system robust in dynamic environments for monocular, stereo and RGB-Dsetups(Mask-Rcnn)</a></p><p><a href="https://github.com/bijustin/YOLO-DynaSLAM">YOLO版本的DynaSLAM</a></p><p><strong>使用语义分割网络和运动一致性检查的方法（光流法）相结合的方法减少视觉SLAM中动态物体造成的影响</strong></p><p><a href="https://github.com/ivipsourcecode/DS-SLAM">DS-SLAM: A Semantic Visual SLAM towards Dynamic Environments</a></p><p><strong>基于语义的实时动态vSLAM算法RDS-SLAM，跟踪线程不再需要等待语义结果</strong></p><p><a href="https://github.com/yubaoliu/RDS-SLAM">RDS-SLAM: Real-Time Dynamic SLAM Using Semantic Segmentation Methods</a></p><p><strong>一种用于动态环境下资源受限机器人的实时RGB-D惯性里程计系统-Dynamic-VINS</strong></p><p><a href="https://github.com/HITSZ-NRSL/Dynamic-VINS">RGB-D Inertial Odometry for a Resource-Restricted Robot in Dynamic Environments</a></p><p><em><strong>动态slam汇总大全</strong></em></p><p><a href="https://github.com/oceanechy/Awesome_Dynamic_SLAM">动态slam汇总大全</a></p><h2 id="语义相关"><a href="#语义相关" class="headerlink" title="语义相关"></a>语义相关</h2><p><strong>动态语义SLAM 目标检测+VSLAM+光流&#x2F;多视角几何动态物体检测+octomap地图+目标数据库</strong></p><p><a href="https://github.com/Ewenwan/ORB_SLAM2_SSD_Semantic">ORB_SLAM2_SSD_Semantic, 动态语义SLAM 目标检测+VSLAM+光流&#x2F;多视角几何动态物体检测+octomap地图+目标数据库</a></p><p><strong>用YOLO v3的语义信息来增加跟踪性能</strong></p><p><a href="https://github.com/Eralien/TE-ORB_SLAM2">TE-ORB_SLAM2, Tracking Enhanced ORB-SLAM2</a></p><p><strong>通过手持RGB-D相机进行SLAM,ORB-SLAM2作为后端,用PSPNet做语义预测并将语义融入octomap</strong></p><p><a href="https://github.com/floatlazer/semantic_slam">Semantic SLAM,Real time semantic slam in ROS with a hand held RGB-D camera orb-slam2 with semantic labelling</a></p><p><strong>用深度学习的场景理解来增强传统特征检测方法,基于贝叶斯SegNet 和ORB-SLAM2,用于长时间定位</strong></p><p><a href="https://github.com/navganti/SIVO">SIVO: Semantically Informed Visual Odometry and Mapping</a></p><blockquote><p>参考计算机视觉life内容，仅供学习使用</p></blockquote>]]></content>
      
      
      <categories>
          
          <category> slam </category>
          
      </categories>
      
      
        <tags>
            
            <tag> slam </tag>
            
            <tag> orb_slam2 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>slam14简单总结捋思路</title>
      <link href="/2023/04/05/slam14-2/"/>
      <url>/2023/04/05/slam14-2/</url>
      
        <content type="html"><![CDATA[<p><img src="/pic/%E9%80%89%E5%8C%BA_093.png"></p><p><img src="/pic/%E9%80%89%E5%8C%BA_094.png"></p><p><img src="/pic/%E9%80%89%E5%8C%BA_095.png"></p><p>实际操作 图像去畸变 双目模型生成点云 RGB-D生成点云</p><p><img src="/pic/%E9%80%89%E5%8C%BA_096.png"></p><p><img src="/pic/%E9%80%89%E5%8C%BA_097.png"></p><p><img src="/pic/%E9%80%89%E5%8C%BA_098.png"></p><p><img src="/pic/%E9%80%89%E5%8C%BA_099.png"></p><p><img src="/pic/%E9%80%89%E5%8C%BA_100.png"></p><p><img src="/pic/%E9%80%89%E5%8C%BA_101.png"></p>]]></content>
      
      
      <categories>
          
          <category> slam </category>
          
          <category> c++ </category>
          
      </categories>
      
      
        <tags>
            
            <tag> c++ </tag>
            
            <tag> slam </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Opencv系列（五）</title>
      <link href="/2023/04/04/opencv5/"/>
      <url>/2023/04/04/opencv5/</url>
      
        <content type="html"><![CDATA[<h2 id="3D相关HEF矩阵计算"><a href="#3D相关HEF矩阵计算" class="headerlink" title="3D相关HEF矩阵计算"></a>3D相关HEF矩阵计算</h2><h3 id="F矩阵含义"><a href="#F矩阵含义" class="headerlink" title="F矩阵含义"></a>F矩阵含义</h3><ul><li>相邻两帧一组对应像素点的约束关系</li><li>像素点在下一帧极线上的位置</li></ul><p><img src="/pic/%E9%80%89%E5%8C%BA_056.png" alt="运动示意图"></p><p><img src="/pic/%E9%80%89%E5%8C%BA_057.png" alt="约束关系"></p><p><img src="/pic/%E9%80%89%E5%8C%BA_058.png" alt="极线位置"></p><p><img src="/pic/%E9%80%89%E5%8C%BA_059.png" alt="八点法前的归一化"></p><p><img src="/pic/%E9%80%89%E5%8C%BA_060.png" alt="八点法的讨论"></p><p><img src="/pic/%E9%80%89%E5%8C%BA_061.png" alt="最小二乘求解(≥8点)"></p><p><img src="/pic/%E9%80%89%E5%8C%BA_062.png" alt="最小二乘求解的讨论"></p><p><img src="/pic/%E9%80%89%E5%8C%BA_063.png" alt="无敌的RANSAC(≥8点)"></p><p><img src="/pic/%E9%80%89%E5%8C%BA_064.png" alt="无敌的RANSAC的讨论"></p><p><img src="/pic/%E9%80%89%E5%8C%BA_065.png" alt="无敌的RANSAC的讨论"></p><p><img src="/pic/%E9%80%89%E5%8C%BA_066.png" alt="无敌的RANSAC的讨论"></p><p><img src="/pic/%E9%80%89%E5%8C%BA_067.png" alt="Opencv中的函数"></p><h3 id="E矩阵含义"><a href="#E矩阵含义" class="headerlink" title="E矩阵含义"></a>E矩阵含义</h3><p>本质矩阵是归一化平面下的基本矩阵的特殊形式</p><p><img src="/pic/%E9%80%89%E5%8C%BA_068.png" alt="约束关系"></p><p><img src="/pic/%E9%80%89%E5%8C%BA_069.png" alt="具体求解方法(不展开)"></p><p><img src="/pic/%E9%80%89%E5%8C%BA_070.png" alt="关于E矩阵的讨论"></p><p><img src="/pic/%E9%80%89%E5%8C%BA_071.png" alt="Opencv中的函数"></p><p><img src="/pic/%E9%80%89%E5%8C%BA_072.png" alt="根据Essential Matrix恢复位姿信息R与t)"></p><p><img src="/pic/%E9%80%89%E5%8C%BA_073.png" alt="根据Essential Matrix恢复位姿信息R与t)"></p><p><img src="/pic/%E9%80%89%E5%8C%BA_074.png" alt="Opencv中的函数"></p><p><img src="/pic/%E9%80%89%E5%8C%BA_075.png" alt="根据E估计深度(三角化)"></p><h3 id="H矩阵意义"><a href="#H矩阵意义" class="headerlink" title="H矩阵意义"></a>H矩阵意义</h3><p><img src="/pic/%E9%80%89%E5%8C%BA_076.png"></p><p><img src="/pic/%E9%80%89%E5%8C%BA_077.png" alt="Opencv中的函数"></p><blockquote><p>注：以上截图内容参考计算机视觉life</p></blockquote><h2 id="位姿求取"><a href="#位姿求取" class="headerlink" title="位姿求取"></a>位姿求取</h2><h3 id="求取位姿的方法"><a href="#求取位姿的方法" class="headerlink" title="求取位姿的方法"></a>求取位姿的方法</h3><ol><li>对极约束：2D-2D，通过二维图像点的对应关系，恢复出在两帧之间摄像机的运动。</li><li>PnP：3D-2D,求解3D到2D点对运动的方法。描述了当知道n个3D空间点及其投影位置时，如何估计相机的位姿。</li><li>ICP：3D-3D，配对好的3D点,已知世界参考系下的3D点和相机参考系下的3D点</li></ol><h3 id="PnP概念"><a href="#PnP概念" class="headerlink" title="PnP概念"></a>PnP概念</h3><p>如果场景的三维结构已知，利用多个控制点在三维场景中的坐标及其在图像中的透视投影坐标即可<br>求解出相机坐标系与世界坐标系之间的绝对位姿关系，包括绝对平移向量t以及旋转矩阵R，该类求<br>解方法统称为N点透视位姿求解（Perspective-N-Point，PNP问题）。这里的控制点是指准确知道三<br>维空间坐标位置，同时也知道对应图像平面坐标的点。<br><strong>已知条件：</strong></p><ul><li>n3D参考点(3D reference points)坐标; </li><li>与这n个3D点对应的、投影在图像上的2D参考点(2D reference points)坐标; </li><li>相机的的内参K;</li></ul><p><strong>求解：</strong></p><ul><li>相机的位姿。</li></ul><h3 id="PnP应用场景"><a href="#PnP应用场景" class="headerlink" title="PnP应用场景"></a>PnP应用场景</h3><p><strong>场景主要有两个</strong></p><ol><li>求解相机的位姿，一般应用于AR，人脸跟踪等；<br>通常输入的是物体在世界坐标系下的3D点以及这些3D点在图像上投影的2D点，因此求得的是相机（相机坐标系）相对于真实物体（世界坐标系）的位姿</li></ol><p><img src="/pic/%E9%80%89%E5%8C%BA_078.png"></p><ol start="2"><li>求取前一帧到当前帧的相机位姿变化，一般用于slam中；</li></ol><p>通常输入的是上一帧中的3D点（在上一帧的相机坐标系下表示的点）和这些3D点在当前帧中的投影得到的2D点，所以它求得的是当前帧相对于上一帧的位姿变换</p><p><img src="/pic/%E9%80%89%E5%8C%BA_079.png"></p><h3 id="Opencv中的函数"><a href="#Opencv中的函数" class="headerlink" title="Opencv中的函数"></a>Opencv中的函数</h3><p><strong>solvePnP</strong></p><p><img src="/pic/%E9%80%89%E5%8C%BA_080.png"></p><p>flags有如下的求解方法：</p><pre class="line-numbers language-c++" data-language="c++"><code class="language-c++">enum &#123; SOLVEPNP_ITERATIVE &#x3D; 0,       SOLVEPNP_EPNP      &#x3D; 1, &#x2F;&#x2F;!&lt; EPnP: Efficient Perspective-n-Point Camera Pose Estimation @cite lepetit2009epnp       SOLVEPNP_P3P       &#x3D; 2, &#x2F;&#x2F;!&lt; Complete Solution Classification for the Perspective-Three-Point Problem @cite gao2003complete       SOLVEPNP_DLS       &#x3D; 3, &#x2F;&#x2F;!&lt; A Direct Least-Squares (DLS) Method for PnP  @cite hesch2011direct       SOLVEPNP_UPNP      &#x3D; 4, &#x2F;&#x2F;!&lt; Exhaustive Linearization for Robust Camera Pose and Focal Length Estimation @cite penate2013exhaustive       SOLVEPNP_AP3P      &#x3D; 5, &#x2F;&#x2F;!&lt; An Efficient Algebraic Solution to the Perspective-Three-Point Problem @cite Ke17       SOLVEPNP_MAX_COUNT      &#x2F;&#x2F;!&lt; Used for count&#125;;<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><blockquote><p>提示</p></blockquote><ol><li>SOLVEPNP_ITERATIVE，默认值，它通过迭代求出重投影误差最小的解作为问题的最优解。该方法<br>实质是迭代求出重投影误差最小的解，这个解显然不一定是正解。</li></ol><p><strong>使用范围：</strong>4对共面匹配点，小于4组点不准确</p><ol start="2"><li>SOLVEPNP_P3P，是使用非常经典的Gao的P3P问题求解算法。P3P算法要求输入的控制点个数只能是4对。3对点求出4组可能的解，通过对第4对点进行重投影，返回重投影误差最小的，确定最优解。可以使用任意4对匹配点求解，不要共面，特征点数量不为4时报错.</li></ol><p><strong>使用范围：</strong>4对不共面匹配点</p><ol start="3"><li>SOLVEPNP_EPNP，该方法使用EfficientPNP方法，求解问题EPnP使用大于等于3组点，是目前最有效的PnP求解方法。</li></ol><p><strong>使用范围：</strong>最少需要4对不共面的匹配点（对于共面的情况只需要3对）,点太少能运行，但不准确</p><p><strong>注：</strong>方法 SOLVEPNP_DLS 和 SOLVEPNP_UPNP 不能使用，因为当前的实现是不稳定的，有时会给出完全错误的结果。如果传递这两个标志之一，则将使用 SOLVEPNP_EPNP 方法。<br>建议：小于4组匹配点时，用SOLVEPNP_ITERATIVE；在4组匹配点时，用SOLVEPNP_P3P；大于4组匹配点时，用SOLVEPNP_EPNP。</p><p><strong>solvePnPRansac</strong></p><p>solvePnP的一个缺点是对异常值不够鲁棒，当我们用相机定位真实世界的点，可能存在错配，对误<br>匹配进行Ransac过滤，RANSAC是“Random Sample Consensus（随机抽样一致）”的缩写</p><p><img src="/pic/%E9%80%89%E5%8C%BA_081.png"></p><p><a href="https://docs.opencv.org/4.0.1/d9/d0c/group__calib3d.html#ga549c2075fac14829ff4a58bc931c033d">可查看官方文档</a></p><h2 id="扩展-PnP问题求解原理"><a href="#扩展-PnP问题求解原理" class="headerlink" title="扩展-PnP问题求解原理"></a>扩展-PnP问题求解原理</h2><p>目前主要有直接线性变换（DLT），P3P，EPnP，UPnP以及非线性优化方法。</p><p>关于PnP的求解问题</p><p><img src="/pic/%E9%80%89%E5%8C%BA_082.png"></p><p>把（3）式带入（1）式和（2）式，整理得：</p><p><img src="/pic/%E9%80%89%E5%8C%BA_083.png"></p><p>我们可以看出，fx fy u0 v0是相机内参，上一节中已经求出，Xw Yw x y是一组3D&#x2F;2D点的坐标，所以未知数有R11 R12 R13 R21 R22 R23 R31 R32 R33 T1 T2 T3一共12个，由于旋转矩阵是正交矩阵，每行每列都是单位向量且两两正交，所以R的自由度为3，秩也是3，比如知道R11 R12 R21就能求出剩下的Rxx。加上平移向量的3个未知数，一共6个未知数，而每一组2D&#x2F;3D点提供的x y Xw Yw Zw可以确立两个方程，所以3组2D&#x2F;3D点的坐标能确立6个方程从而解出6个未知数。故PnP需要知道至少3组2D&#x2F;3D点。</p><h3 id="DLT"><a href="#DLT" class="headerlink" title="DLT"></a>DLT</h3><p>通过2D-3D的关系直接构建方程，直接求解：<br>DLT主要是通过构建一个12维的增广矩阵（R|t），然后通过投影矩阵构建一个方程：</p><p>通过最后一行，消去s，最后构建一个12维的线性方程组，通过6对匹配点（一对点两个方程）来求解中间的矩阵。 最后将[R|t]左侧3<em>3矩阵块进行QR分解，用一个旋转矩阵去近似（将3</em>3矩阵空间投影到SE(3)流形上）。</p><p><img src="/pic/%E9%80%89%E5%8C%BA_085.png"></p><p>优点：求解简单<br>缺点：直接将 T 矩阵看成了 12 个未知数，忽略了它们之间的联系。因为旋转矩阵 R ∈ SO(3)，用 DLT 求出的解不一定满足该约束，它是一个一般矩阵。寻找一个最好的旋转矩阵对它进行近似</p><h3 id="P3P（ICP-3D-3D）"><a href="#P3P（ICP-3D-3D）" class="headerlink" title="P3P（ICP:3D-3D）"></a>P3P（ICP:3D-3D）</h3><p>采用一种变换的形式，把2D-3D匹配关系，变换成3D-3D点的匹配关系：<br>将世界坐标系下的ABC三点和图像坐标系下的abc三点匹配，其中AB，BC，AC的⻓度已知，<br>&lt;a,b&gt;，&lt;b,c&gt;，&lt;a,c&gt;也是已知，通过余弦定理得出方程组</p><p><img src="/pic/%E9%80%89%E5%8C%BA_086.png"></p><p>类似于分解 E 的情况，该方程最多可能得到四个解，但我们可以用验证点来计算最可能的解，得到<br>A, B, C 在相机坐标系下的 3D 坐标。然后，根据 3D-3D 的点对，使用类似ICP的坐标系对，计算相<br>机的运动 R, t。</p><p>优点：需要的匹配点少</p><p>缺点：P3P 只利用三个点的信息。当给定的配对点多于 3 组时，难以利用更多的信息；如果 3D 点<br>或 2D 点受噪声影响，或者存在误匹配，则算法失效。</p><p>可参考论文《<a href="https://cmp.felk.cvut.cz/~zimmerk/track3D/papers/p3p.pdf">CompleteSolution Classification for the Perspective-Three-Point Problem</a>》</p><h3 id="EPnP"><a href="#EPnP" class="headerlink" title="EPnP"></a>EPnP</h3><p>将世界坐标系中的3D坐标表示为一组虚拟的控制点的加权和。</p><p><img src="/pic/%E9%80%89%E5%8C%BA_087.png"></p><p>可参考论文《<a href="https://github.com/cvlab-epfl/EPnP">EPnP: Efficient Perspective-n-Point Camera Pose Estimation</a>》</p>]]></content>
      
      
      <categories>
          
          <category> Opencv </category>
          
          <category> PnP </category>
          
          <category> HEF </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Opencv </tag>
            
            <tag> HEF矩阵 </tag>
            
            <tag> PnP </tag>
            
            <tag> EPnP </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Opencv系列（四）</title>
      <link href="/2023/04/03/opencv4/"/>
      <url>/2023/04/03/opencv4/</url>
      
        <content type="html"><![CDATA[<h1 id="Opencv相机标定"><a href="#Opencv相机标定" class="headerlink" title="Opencv相机标定"></a>Opencv相机标定</h1><p>基本理论知识请查看如下链接：</p><p><a href="https://zhuanlan.zhihu.com/p/520357612">相机模型</a></p><h2 id="张正友标定法"><a href="#张正友标定法" class="headerlink" title="张正友标定法"></a>张正友标定法</h2><p><img src="/pic/%E9%80%89%E5%8C%BA_055.png"></p><p><strong>张正友标定法假设</strong></p><ul><li>标定板的角点在一个平面上</li><li>世界坐标系的xy平面在标定板平面上，Z&#x3D;0</li><li>相机模型不考虑畸变</li></ul><h2 id="Opencv现有函数"><a href="#Opencv现有函数" class="headerlink" title="Opencv现有函数"></a>Opencv现有函数</h2> <pre class="line-numbers language-c++" data-language="c++"><code class="language-c++">cv::calibrateCamera(object_points, &#x2F;&#x2F;三维点坐标                        image_points_seq, &#x2F;&#x2F;像素坐标                        image_size, &#x2F;&#x2F;图像尺寸                        cameraMatrix, &#x2F;&#x2F;输出相机矩阵                        distCoeffs, &#x2F;&#x2F;输出畸变矩阵                        rvecsMat, tvecsMat, &#x2F;&#x2F;r,t                        0);cv::findChessboardCorners(imageInput,&#x2F;&#x2F;输入图像                          board_size,&#x2F;&#x2F;标定板上角点的行列                          image_points_buf&#x2F;&#x2F;输出角点的像素坐标                         )cv::cornerSubPix(view_gray,&#x2F;&#x2F;输入图像，最好是灰度图                 image_points_buf,&#x2F;&#x2F;输入输出角点的像素坐标                 cv::Size(5,5),&#x2F;&#x2F;搜索窗口的半径                 cv::Size(-1,-1),&#x2F;&#x2F;-1表示忽略                 cv::TermCriteria(cv::TermCriteria::MAX_ITER + cv::TermCriteria::EPS,30,&#x2F;&#x2F;最大迭代次数                    0.1)&#x2F;&#x2F;最小精度                );cv::drawChessboardCorners(view_gray, board_size, image_points_buf,                           true);&#x2F;&#x2F;如果角点全部找到，返回true<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p><a href="https://blog.csdn.net/dcrmg/article/details/52929669?ops_request_misc=&request_id=&biz_id=102&utm_term=boardsize%20%20opencv&utm_medium=distribute.pc_search_result.none-task-blog-2~all~sobaiduweb~default-4-.first_rank_v2_pc_rank_v29&spm=1018.2226.3001.4187">Opencv 张正友相机标定傻瓜教程</a></p>]]></content>
      
      
      <categories>
          
          <category> Opencv </category>
          
          <category> Camera </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Opencv </tag>
            
            <tag> 相机模型 </tag>
            
            <tag> 相机标定 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Opencv系列（三）</title>
      <link href="/2023/04/03/opencv3/"/>
      <url>/2023/04/03/opencv3/</url>
      
        <content type="html"><![CDATA[<h1 id="Opencv显示与绘制"><a href="#Opencv显示与绘制" class="headerlink" title="Opencv显示与绘制"></a>Opencv显示与绘制</h1><h2 id="OpenCV-的基础绘制功能"><a href="#OpenCV-的基础绘制功能" class="headerlink" title="OpenCV 的基础绘制功能"></a>OpenCV 的基础绘制功能</h2><p>OpenCV 中提供直线、椭圆、矩形、圆以及多边形的绘制功能。 在OpenCV 的图形绘制中我们会经常使用以下两种结构：</p><ul><li>cv::Point 和cv::Scalar</li></ul><blockquote><p>cv::Point 表示2D 平面上的点，通过指定其在图像上的坐标位置x 和y 来实现。<br>语法结构如下：<br>Point pt; pt.x &#x3D; 10； pt.y &#x3D; 8; 或者 Point pt &#x3D; Point(10, 8);</p></blockquote><blockquote><p>cv::Scalar 表示一个含有4 个元素的向量，OpenCV 中用来传递像素值。<br>语法结构如下：<br>Scalar( a, b, c )<br>其中Blue &#x3D; a, Green &#x3D; b, Red &#x3D; c。这里由于我们只有BGR 三个颜色值，所以我们只需要定义三个变量，最后一个元素可以省略。 </p></blockquote><p>下面开始介绍直线、椭圆、矩形、圆以及多边形分别的语法结构:</p><p><strong>直线</strong></p><pre class="line-numbers language-c++" data-language="c++"><code class="language-c++">CV_EXPORTS_W void line(InputOutputArray img, Point pt1, Point pt2, const Scalar&amp; color,                     int thickness &#x3D; 1, int lineType &#x3D; LINE_8, int shift &#x3D; 0);<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><p>参数：</p><ul><li>img:  图像 </li><li>pt:  线段的起点 </li><li>pt2:  线段的终点 </li><li>color:  直线的颜色 </li><li>thickness:  直线的粗细 </li><li>lineType:  直线类型，具体可以参考下表 </li><li>shift:  点坐标中的小数点位数</li></ul><p><img src="/pic/%E9%80%89%E5%8C%BA_047.png"></p><p><img src="/pic/%E9%80%89%E5%8C%BA_048.png"></p><p>8 联通线是指下一个点连接上一个点的边或者角，4 联通线是指下一个点与上一个点边相连。4 联通线消除了8 联通线的断裂瑕疵</p><p><strong>椭圆</strong></p><pre class="line-numbers language-c++" data-language="c++"><code class="language-c++">CV_EXPORTS_W void ellipse(InputOutputArray img, Point center, Size axes,                        double angle, double startAngle, double endAngle,                        const Scalar&amp; color, int thickness &#x3D; 1,                        int lineType &#x3D; LINE_8, int shift &#x3D; 0);<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span></span></code></pre><p>参数： </p><ul><li>img:  图像 </li><li>center:  椭圆中心 </li><li>axes:  椭圆主轴尺寸的一半 </li><li>angle:  椭圆旋转角度 </li><li>startAngle:  椭圆弧的起始角度 </li><li>engAngle:  椭圆弧的终止角度 </li><li>color:  椭圆的颜色 </li><li>thickness:  椭圆轮廓的粗细，如果不设置默认填充</li><li>lineType:  椭圆边界线类型 shift:  中心坐标和轴值的小数点位数</li></ul><p>cv::ellipse 可以用来绘制椭圆线、实心椭圆、椭圆弧、椭圆扇面。如果想要绘制一个完整的椭圆，startAngle&#x3D;0、endAngle&#x3D;360。如果起始角度大于终止角度，他们会进行交换。下图 在绘制蓝色弧时各个参数的含义。</p><p><img src="/pic/%E9%80%89%E5%8C%BA_049.png" alt="椭圆弧绘制中各参数的意义"></p><p><strong>矩形</strong></p><pre class="line-numbers language-c++" data-language="c++"><code class="language-c++">CV_EXPORTS_W void rectangle(InputOutputArray img, Point pt1, Point pt2,                          const Scalar&amp; color, int thickness &#x3D; 1,                          int lineType &#x3D; LINE_8, int shift &#x3D; 0);<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre><p>参数： </p><ul><li>img:  图像</li><li>pt1:  矩形的顶点 </li><li>pt2:与pt1 相对的矩形顶点 </li><li>color:  矩形颜色或者亮度 </li><li>thickness:  矩形轮廓的粗细 </li><li>lineType:  线条类型 </li><li>shift:  点坐标中的小数点位数</li></ul><p><strong>多线段</strong></p><pre class="line-numbers language-c++" data-language="c++"><code class="language-c++">CV_EXPORTS void polylines(Mat&amp; img, const Point* const* pts, const int* npts,                          int ncontours, bool isClosed, const Scalar&amp; color,                          int thickness &#x3D; 1, int lineType &#x3D; LINE_8, int shift &#x3D; 0 );<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre><p>参数： </p><ul><li>img:  图像 </li><li>pts:  多边形曲线阵列 </li><li>isClosed:  所绘制多段线是否闭合 </li><li>color:  多段线颜色 </li><li>thickness:  多段线粗细 </li><li>lineType:  多段线种类 </li><li>shift:  点坐标中的小数点位数</li></ul><p><strong>多边形填充</strong></p><pre class="line-numbers language-c++" data-language="c++"><code class="language-c++">CV_EXPORTS void fillPoly(Mat&amp; img, const Point** pts,                         const int* npts, int ncontours,                         const Scalar&amp; color, int lineType &#x3D; LINE_8, int shift &#x3D; 0,                         Point offset &#x3D; Point() );<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span></span></code></pre><p>cv::fillPoly 可以填充由多边形包围的区域，可用于填充复杂区域。 </p><p>参数： </p><ul><li>img:  图像 </li><li>pts:  多边形数组，每个多边形都是一组点 </li><li>npts:  顶点个数 </li><li>ncontours:  多边形轮廓个数 </li><li>color:  多边形颜色 </li><li>lineType:  多边形边界粗细 </li><li>shift:  点坐标的小数点位数 </li><li>offset:  可选择轮廓点偏移量</li></ul><p><strong>实例</strong></p><pre class="line-numbers language-c++" data-language="c++"><code class="language-c++">#include &lt;opencv2&#x2F;core.hpp&gt; #include &lt;opencv2&#x2F;imgproc.hpp&gt; #include &lt;opencv2&#x2F;highgui.hpp&gt;#define w 400 using namespace cv; void MyEllipse( Mat img, double angle ); void MyFilledCircle( Mat img, Point center ); void MyPolygon( Mat img ); void MyLine( Mat img, Point start, Point end ); int main( void )&#123;&#x2F;&#x2F;创建两个窗口和两个图像来进行图形绘制    char atom_window[] &#x3D; &quot;Drawing 1: Atom&quot;;     char rook_window[] &#x3D; &quot;Drawing 2: Rook&quot;;     Mat atom_image &#x3D; Mat::zeros( w, w, CV_8UC3 );     Mat rook_image &#x3D; Mat::zeros( w, w, CV_8UC3 ); &#x2F;&#x2F;原子图像绘制，创建了MyEllipse 和MyFilledCircle 两个函数     MyEllipse( atom_image, 90 );     MyEllipse( atom_image, 0 );     MyEllipse( atom_image, 45 );     MyEllipse( atom_image, -45 );     MyFilledCircle( atom_image, Point( w&#x2F;2, w&#x2F;2) );    &#x2F;&#x2F;绘制国际象棋的车创建了MyPolygon 和MyLine 函数，且应用了矩形的绘制函数     MyPolygon( rook_image );     rectangle( rook_image,                Point( 0, 7*w&#x2F;8 ),                Point( w, w),                Scalar( 0, 255, 255 ),                FILLED,                LINE_8 );     MyLine( rook_image, Point( 0, 15*w&#x2F;16 ), Point( w, 15*w&#x2F;16 ) );     MyLine( rook_image, Point( w&#x2F;4, 7*w&#x2F;8 ), Point( w&#x2F;4, w ) );     MyLine( rook_image, Point( w&#x2F;2, 7*w&#x2F;8 ), Point( w&#x2F;2, w ) );     MyLine( rook_image, Point( 3*w&#x2F;4, 7*w&#x2F;8 ), Point( 3*w&#x2F;4, w ) );     imshow( atom_window, atom_image );     moveWindow( atom_window, 0, 200 );     imshow( rook_window, rook_image );     moveWindow( rook_window, w, 200 );     waitKey( 0 );     return(0); &#125; void MyEllipse( Mat img, double angle ) &#123;     int thickness &#x3D; 2;     int lineType &#x3D; 8;     ellipse( img,              Point( w&#x2F;2, w&#x2F;2 ),              Size( w&#x2F;4, w&#x2F;16 ),              angle,              0,              360,              Scalar( 255, 0, 0 ),              thickness,              lineType ); &#125; void MyFilledCircle( Mat img, Point center ) &#123;     circle( img,             center,             w&#x2F;32,             Scalar( 0, 0, 255 ),             FILLED,             LINE_8 ); &#125; void MyPolygon( Mat img ) &#123;     int lineType &#x3D; LINE_8;     Point rook_points[1][20];     rook_points[0][0]  &#x3D; Point(    w&#x2F;4,   7*w&#x2F;8 );     rook_points[0][1]  &#x3D; Point(  3*w&#x2F;4,   7*w&#x2F;8 );     rook_points[0][2]  &#x3D; Point(  3*w&#x2F;4,  13*w&#x2F;16 );     rook_points[0][3]  &#x3D; Point( 11*w&#x2F;16, 13*w&#x2F;16 );     rook_points[0][4]  &#x3D; Point( 19*w&#x2F;32,  3*w&#x2F;8 );     rook_points[0][5]  &#x3D; Point(  3*w&#x2F;4,   3*w&#x2F;8 );     rook_points[0][6]  &#x3D; Point(  3*w&#x2F;4,     w&#x2F;8 );     rook_points[0][7]  &#x3D; Point( 26*w&#x2F;40,    w&#x2F;8 );     rook_points[0][8]  &#x3D; Point( 26*w&#x2F;40,    w&#x2F;4 );     rook_points[0][9]  &#x3D; Point( 22*w&#x2F;40,    w&#x2F;4 );     rook_points[0][10] &#x3D; Point( 22*w&#x2F;40,    w&#x2F;8 );     rook_points[0][11] &#x3D; Point( 18*w&#x2F;40,    w&#x2F;8 );     rook_points[0][12] &#x3D; Point( 18*w&#x2F;40,    w&#x2F;4 );     rook_points[0][13] &#x3D; Point( 14*w&#x2F;40,    w&#x2F;4 );     rook_points[0][14] &#x3D; Point( 14*w&#x2F;40,    w&#x2F;8 );     rook_points[0][15] &#x3D; Point(    w&#x2F;4,     w&#x2F;8 );     rook_points[0][16] &#x3D; Point(    w&#x2F;4,   3*w&#x2F;8 );     rook_points[0][17] &#x3D; Point( 13*w&#x2F;32,  3*w&#x2F;8 );     rook_points[0][18] &#x3D; Point(  5*w&#x2F;16, 13*w&#x2F;16 );     rook_points[0][19] &#x3D; Point(    w&#x2F;4,  13*w&#x2F;16 );     const Point* ppt[1] &#x3D; &#123; rook_points[0] &#125;;     int npt[] &#x3D; &#123; 20 &#125;;     fillPoly( img,               ppt,               npt,               1,               Scalar( 255, 255, 255 ),               lineType ); &#125; void MyLine( Mat img, Point start, Point end ) &#123;     int thickness &#x3D; 2;     int lineType &#x3D; LINE_8;     line( img,           start,           end,           Scalar( 0, 0, 0 ),           thickness,           lineType ); &#125;<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p><img src="/pic/%E9%80%89%E5%8C%BA_050.png" alt="绘制结果"></p><h2 id="轮廓"><a href="#轮廓" class="headerlink" title="轮廓"></a>轮廓</h2><h3 id="如何在图像中寻找物体轮廓"><a href="#如何在图像中寻找物体轮廓" class="headerlink" title="如何在图像中寻找物体轮廓"></a>如何在图像中寻找物体轮廓</h3><p><strong>在二值图像中寻找轮廓</strong></p><pre class="line-numbers language-c++" data-language="c++"><code class="language-c++">CV_EXPORTS_W void findContours( InputOutputArray image, OutputArrayOfArrays contours,                              OutputArray hierarchy, int mode,                              int method, Point offset &#x3D; Point());<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre><p>参数： </p><ul><li>image:  8 位单通道图像，非零像素视为 1，零值像素视为 0，因此所有的输入图像均被视为二值图像。因此在应用时我们一般会利用 Canny,  compare,  InRange,  threshold, adaptiveThreshold 等函数处理得到二值图像。如果模式为RETR_CCOMP 或者RETR_FLOODFILL，输入也可以是32 位的灰度图像。 </li><li>contours: 检测到的轮廓，每个轮廓都储存为一组点向量，定义为std::vector&lt;std::vector<a href="cv::Point">cv::Point</a>&gt;。 </li><li>hierarchy:  可选输出向量，定义为 std::vector<a href="cv::Vec4i">cv::Vec4i</a>，包含有图像拓扑信息。其内部元素的个数等同于所检测轮廓的个数。Vec 4i 是Vec &lt;int,  4&gt;的别名，定义为向量内每一个元素包含4 个int 型变量。hierarchy 向量内第i 个轮廓的4 个int型变量（hierarchy[i][0], hierarchy[i][1], hierarchy[i][2], hierarchy[i][3]）分别表示其同一层级下的后一个轮廓，前一个轮廓，子轮廓以及父轮廓的索引编号。如果轮廓 i 没有对应的上述轮廓，则 hierarchy[i][0],  hierarchy[i][1],  hierarchy[i][2],  hierarchy[i][3]被置为-1。 </li><li>mode:  轮廓的检索模式。具体见下表二。 </li><li>method:  轮廓的近似方法。具体见表三。</li><li>offset: Point 偏移量</li></ul><p><img src="/pic/%E9%80%89%E5%8C%BA_051.png"></p><p><strong>轮廓的绘制</strong></p><pre class="line-numbers language-c++" data-language="c++"><code class="language-c++">CV_EXPORTS_W void drawContours( InputOutputArray image, InputArrayOfArrays contours,                              int contourIdx, const Scalar&amp; color,                              int thickness &#x3D; 1, int lineType &#x3D; LINE_8,                              InputArray hierarchy &#x3D; noArray(),                              int maxLevel &#x3D; INT_MAX, Point offset &#x3D; Point() );<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>参数： </p><ul><li>image:  目的图像。 </li><li>contours:  所有输入轮廓，每个轮廓储存为一组点向量。 </li><li>contourIdx:  指示要绘制的轮廓线参数，如果为负，则绘制所有轮廓线。 </li><li>color:  轮廓线颜色。 </li><li>thickness:  绘制轮廓线的粗细，如果为负（thickness&#x3D;FILLED）,则轮廓内部也会被绘制。当thickness&#x3D;FILLED 时，即使没有提供层级数据，也能成功绘制有孔的轮廓。 </li><li>lineType:  直线绘制算法。 </li><li>hierarchy:  可选层级信息，只有当想绘制多条轮廓时才需要用到。 </li><li>mexLevel:  所绘制轮廓的最大层级。如果是0，则只绘制指定的轮廓；如果是1，则绘制指定轮廓以及嵌套轮廓；如果是2，则绘制指定轮廓、嵌套轮廓以及嵌套轮廓的嵌套轮廓以此类推。该参数只在层级参数可用时可以使用。 </li><li>offset: Point 的偏移</li></ul><p> <strong>实例</strong></p> <pre class="line-numbers language-c++" data-language="c++"><code class="language-c++">#include &quot;opencv2&#x2F;imgcodecs.hpp&quot; #include &quot;opencv2&#x2F;highgui.hpp&quot; #include &quot;opencv2&#x2F;imgproc.hpp&quot; #include &lt;iostream&gt; using namespace cv; using namespace std; Mat src_gray; int thresh &#x3D; 100; RNG rng(12345); void thresh_callback(int, void* ); int main( int argc, char** argv ) &#123;    &#x2F;&#x2F;读取图像     Mat src &#x3D; imread( &quot;1.jpg&quot;, IMREAD_COLOR );     if( src.empty() )     &#123;         cout &lt;&lt; &quot;Could not open or find the image!\n&quot; &lt;&lt; endl;         cout &lt;&lt; &quot;Usage: &quot; &lt;&lt; argv[0] &lt;&lt; &quot; &lt;Input image&gt;&quot; &lt;&lt; endl;         return -1;&#125;     &#x2F;&#x2F;转化为灰度图像，模糊处理以去除噪声     cvtColor( src, src_gray, COLOR_BGR2GRAY );     blur( src_gray, src_gray, Size(3,3) );     &#x2F;&#x2F;创建名为Source 的窗口并在其中显示输入图像     const char* source_window &#x3D; &quot;Source&quot;;     namedWindow( source_window );     imshow( source_window, src );     const int max_thresh &#x3D; 255;     createTrackbar( &quot;Canny thresh:&quot;, source_window, &amp;thresh, max_thresh, thresh_callback );     thresh_callback( 0, 0 );     waitKey();     return 0; &#125; void thresh_callback(int, void* ) &#123;     Mat canny_output;     &#x2F;&#x2F;Canny 边缘检测     Canny( src_gray, canny_output, thresh, thresh*2 );     vector&lt;vector&lt;Point&gt; &gt; contours;     vector&lt;Vec4i&gt; hierarchy;     &#x2F;&#x2F;寻找轮廓的函数     findContours( canny_output, contours, hierarchy, RETR_TREE, CHAIN_APPROX_SIMPLE );     Mat drawing &#x3D; Mat::zeros( canny_output.size(), CV_8UC3 );     for( size_t i &#x3D; 0; i&lt; contours.size(); i++ )     &#123;         Scalar color &#x3D; Scalar( rng.uniform(0, 256), rng.uniform(0,256), rng.uniform(0,256) );     &#x2F;&#x2F;轮廓绘制         drawContours( drawing, contours, (int)i, color, 2, LINE_8, hierarchy, 0 );     &#125;     imshow( &quot;Contours&quot;, drawing ); &#125;<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p><img src="/pic/403.gif"></p><h3 id="凸包函数"><a href="#凸包函数" class="headerlink" title="凸包函数"></a>凸包函数</h3><pre class="line-numbers language-c++" data-language="c++"><code class="language-c++">CV_EXPORTS_W void convexHull( InputArray points, OutputArray hull,                              bool clockwise &#x3D; false, bool returnPoints &#x3D; true );<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><p>参数： </p><ul><li>points:  输入的二维点集，存储在std::vector 或者Mat 中 </li><li>hull:  输出找到的凸包。 </li><li>clockwise:  操作方向，当 clockwise&#x3D;true 时，输出凸包为顺时针方向。否则输出凸包方向为逆时针方向。 </li><li>returnPoints:  操作标识符，默认值为 true，此时返回各凸包的各点，否则返回凸包各点的索引。当输出数组为std::vector 时，此标识被忽略</li></ul> <pre class="line-numbers language-c++" data-language="c++"><code class="language-c++">#include &quot;opencv2&#x2F;imgcodecs.hpp&quot; #include &quot;opencv2&#x2F;highgui.hpp&quot; #include &quot;opencv2&#x2F;imgproc.hpp&quot; #include &lt;iostream&gt; using namespace cv; using namespace std; Mat src_gray; int thresh &#x3D; 100; RNG rng(12345); void thresh_callback(int, void* ); int main( int argc, char** argv ) &#123;     Mat src &#x3D; imread( &quot;11.png&quot; , IMREAD_COLOR);     if( src.empty() )     &#123;         cout &lt;&lt; &quot;Could not open or find the image!\n&quot; &lt;&lt; endl;         cout &lt;&lt; &quot;Usage: &quot; &lt;&lt; argv[0] &lt;&lt; &quot; &lt;Input image&gt;&quot; &lt;&lt; endl;         return -1;     &#125;     cvtColor( src, src_gray, COLOR_BGR2GRAY );     blur( src_gray, src_gray, Size(3,3) );     const char* source_window &#x3D; &quot;Source&quot;;     namedWindow( source_window );     imshow( source_window, src );     const int max_thresh &#x3D; 255;     createTrackbar( &quot;Canny thresh:&quot;, source_window, &amp;thresh, max_thresh, thresh_callback );     thresh_callback( 0, 0 );     waitKey();     return 0; &#125; void thresh_callback(int, void* ) &#123;     Mat canny_output;     Canny( src_gray, canny_output, thresh, thresh*2 );     vector&lt;vector&lt;Point&gt; &gt; contours;     findContours( canny_output, contours, RETR_TREE, CHAIN_APPROX_SIMPLE );     &#x2F;&#x2F;利用凸包函数找到图形中的凸包     vector&lt;vector&lt;Point&gt; &gt;hull( contours.size() );     for( size_t i &#x3D; 0; i &lt; contours.size(); i++ )     &#123;         convexHull( contours[i], hull[i] );     &#125;     &#x2F;&#x2F;绘制轮廓与凸包     Mat drawing &#x3D; Mat::zeros( canny_output.size(), CV_8UC3 );     for( size_t i &#x3D; 0; i&lt; contours.size(); i++ )     &#123;         Scalar color &#x3D; Scalar( rng.uniform(0, 256), rng.uniform(0,256), rng.uniform(0,256) );         drawContours( drawing, contours, (int)i, color );         drawContours( drawing, hull, (int)i, color );     &#125;     imshow( &quot;Hull demo&quot;, drawing ); &#125;<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>]]></content>
      
      
      <categories>
          
          <category> Opencv </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Opencv </tag>
            
            <tag> 显示与绘制 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Opencv系列（二）</title>
      <link href="/2023/04/02/opencv2/"/>
      <url>/2023/04/02/opencv2/</url>
      
        <content type="html"><![CDATA[<h1 id="Opencv图像滤波"><a href="#Opencv图像滤波" class="headerlink" title="Opencv图像滤波"></a>Opencv图像滤波</h1><h2 id="连通域-amp-直方图"><a href="#连通域-amp-直方图" class="headerlink" title="连通域&amp;直方图"></a>连通域&amp;直方图</h2><p><img src="/pic/%E9%80%89%E5%8C%BA_042.png"></p><p><strong>计算图像直方图</strong></p><pre class="line-numbers language-c++" data-language="c++"><code class="language-c++">void cv::calcHist(       const Mat *     images,           &#x2F;&#x2F;输入图像    int     nimages,                  &#x2F;&#x2F;源图像的个数（通常为1）    const int *     channels,         &#x2F;&#x2F;列出通道    InputArray  mask,                 &#x2F;&#x2F;输入掩码（需处理的像素）    OutputArray     hist,             &#x2F;&#x2F;输出直方图    int     dims,                     &#x2F;&#x2F;直方图的维度（通道数量）    const int *     histSize,         &#x2F;&#x2F;每个维度位数    const float **  ranges,           &#x2F;&#x2F;每个维度的范围    bool    uniform &#x3D; true,           &#x2F;&#x2F;true表示箱子间距相同    bool    accumulate &#x3D; false        &#x2F;&#x2F;是否在多次调用时进行累积    )<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p><strong>二值函数</strong></p><p>cv::threshold()</p><blockquote><p>〖原理〗：通过将所有像素与某个阈值（第三个参数）进行比较赋值，将图像表示为只有两种像素值的图像（例子：学生排队）。</p></blockquote><p>阈值是图像分割的标尺。</p><pre class="line-numbers language-c++" data-language="c++"><code class="language-c++">#include&lt;opencv2&#x2F;imgproc.hpp&gt;&#x2F;&#x2F;创建二值图像double cv::threshold(       InputArray  src,          &#x2F;&#x2F;输入图像（多通道、8位或32位浮点类型）       OutputArray     dst,      &#x2F;&#x2F;输出图像    double  thresh,           &#x2F;&#x2F;指定阈值    double  maxval,           &#x2F;&#x2F;设定最大值（常取255）    int     type              &#x2F;&#x2F;阈值类型    )&#x2F;*type，详见参数介绍1、THRESH_BINARY：将所有大于thresh的像素赋值为maxval，将其他像素赋值为0；2、THRESH_BINARY_INY：将所有大于thresh的像素赋值为0，将其他像素赋值为maxval；3、THRESH_TRUNC：截断，将所有大于thresh的像素赋值为thresh，其他像素值不变；4、THRESH_TOZERO：所有大于thresh的像素值保持不变，将其他像素赋值为0；5、THRESH_TOZERO_INV：所有大于thresh的像素值赋值为0，其他像素值保持不变.6、THRESH_OTSU:使用Otsu算法去寻找到最优的阈值7、THRESH_TRIANGLE：使用三角化方法寻找到最有的阈值*&#x2F;<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><blockquote><p>【应用】：可用于区分图像的前景和背景（通常前景的像素值大于背景的像素值）；</p></blockquote><h2 id="形态学运算变换图像"><a href="#形态学运算变换图像" class="headerlink" title="形态学运算变换图像"></a>形态学运算变换图像</h2><p><strong>概念</strong></p><ul><li>形态学是一种滤波器，用结构元素探测图像中每个像素的操作过程称为形态学滤波器的应用过程；</li><li>结构元素是一堆像素的组合，原则上可以是任何形状，通常是正方形、圆形或菱形，中心点为原点（锚点）。<br> <strong>作用</strong></li><li>可用于强化或消除特殊形状</li></ul><p><img src="/pic/%E9%80%89%E5%8C%BA_043.png"></p><p><strong>腐蚀与膨胀</strong></p><pre class="line-numbers language-c++" data-language="c++"><code class="language-c++">&#x2F;&#x2F;腐蚀图像&#x2F;&#x2F;原理：在某个像素上应用结构元素时，结构元素的锚点与该像素对齐，腐蚀就是把当前像素替换成所定义像素集合中的最小像素值。void cv::erode  (       InputArray  src,           &#x2F;&#x2F;输入图像：灰度图像&amp;彩色图像    OutputArray     dst,       &#x2F;&#x2F;输出图像    InputArray  kernel,           &#x2F;&#x2F;结构元素，默认cv::Mat()，3x3的正方形    Point   anchor &#x3D; Point(-1,-1),  &#x2F;&#x2F;结构元素的锚点位置，默认为中心    int     iterations &#x3D; 1,         &#x2F;&#x2F;腐蚀次数    int     borderType &#x3D; BORDER_CONSTANT,  &#x2F;&#x2F;边界类型（像素外推的方法）    const Scalar &amp;  borderValue &#x3D; morphologyDefaultBorderValue()   &#x2F;&#x2F;连续边界的边界值)&#x2F;&#x2F;膨胀图像&#x2F;&#x2F;原理：把当前像素替换成所定义像素集合中的最大像素值。void cv::dilate (       InputArray  src,    OutputArray     dst,    InputArray  kernel,    Point   anchor &#x3D; Point(-1,-1),    int     iterations &#x3D; 1,    int     borderType &#x3D; BORDER_CONSTANT,    const Scalar &amp;  borderValue &#x3D; morphologyDefaultBorderValue() )       &#x2F;&#x2F;增加腐蚀&#x2F;膨胀次数或者使用更大的结构元素，都会增加腐蚀&#x2F;膨胀的效果。&#x2F;&#x2F;以腐蚀和膨胀操作作为基础，执行高级的形态变换void cv::morphologyEx   (       InputArray  src,    OutputArray     dst,    int     op,             &#x2F;&#x2F;形态学操作类型    InputArray  kernel,    Point   anchor &#x3D; Point(-1,-1),    int     iterations &#x3D; 1,    int     borderType &#x3D; BORDER_CONSTANT,         const Scalar &amp;  borderValue &#x3D; morphologyDefaultBorderValue() )   <span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p><strong>形态学梯度运算提取图像边缘</strong></p><pre class="line-numbers language-c++" data-language="c++"><code class="language-c++">&#x2F;&#x2F;以腐蚀和膨胀操作作为基础，执行高级的形态变换void cv::morphologyEx   (       InputArray  src,    OutputArray     dst,    int     op,             &#x2F;&#x2F;形态学操作类型    InputArray  kernel,    Point   anchor &#x3D; Point(-1,-1),    int     iterations &#x3D; 1,    int     borderType &#x3D; BORDER_CONSTANT,         const Scalar &amp;  borderValue &#x3D; morphologyDefaultBorderValue() )   <span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p><strong>图形分割</strong></p><pre class="line-numbers language-c++" data-language="c++"><code class="language-c++">CV_EXPORTS_W void watershed( InputArray image, InputOutputArray markers );&#x2F;&#x2F; cv::InputArray image：待分割的源图像；&#x2F;&#x2F; cv::InputOutputArray markers：标记图像；即这个参数用于存放函数调后的输出结果，需和源图片有一样的尺寸和类型。<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre><p><a href="https://blog.csdn.net/weixin_43863869/article/details/128534217">利用分水岭算法实现图像分割</a></p><h2 id="图像滤波"><a href="#图像滤波" class="headerlink" title="图像滤波"></a>图像滤波</h2><ul><li>概念：即选择性地提取图像中某些方面的内容，这些内容通常在特定的应用环境下传达了重要信息。</li><li>作用：滤波器是一种放大（也可以不改变）图像中某些频段，同时滤掉（或减弱）其他频段的算子，分为低通滤波器&amp;高通滤波器。</li><li>示例：去噪（噪声点）、重采样</li></ul><h3 id="频域分析"><a href="#频域分析" class="headerlink" title="频域分析"></a>频域分析</h3><p>描述图像的两种形式：</p><ul><li>频域：观察图像内容强度值（灰度值）变化的频率（蓝天 VS 杂货间），图像中精致的细节对应着高频；</li></ul><p><img src="/pic/%E9%80%89%E5%8C%BA_044.png"></p><ul><li>空域：观察图像内容灰度分布来描述图像特征（直方图）</li></ul><p><strong>频域分析</strong>：把图像分解成从低频到高频的频率成分。图像强度值变化慢的区域只包含低频率，强度值变化快的区域产生高频率。</p><p>二维图像的频率分为垂直频率和水平频率。</p><h3 id="低通滤波器"><a href="#低通滤波器" class="headerlink" title="低通滤波器"></a>低通滤波器</h3><p>目的：消除图像中的高频部分，减少图像变化的幅度（把前景变得光滑；把前景和背景之间的差异变小）。</p><p>常用方法：把每个像素的值替换成它周围像素的平均值，线性滤波。</p><p><strong>块滤波器（box filter）——&gt;卷积核（掩膜）</strong></p><pre class="line-numbers language-c++" data-language="c++"><code class="language-c++">&#x2F;&#x2F;典型示例一：均值滤波器void cv::blur   (       InputArray  src,                  &#x2F;&#x2F;输入图像    OutputArray     dst,              &#x2F;&#x2F;输出图像    Size    ksize,                    &#x2F;&#x2F;卷积核大小（值为系统默认指定？）    Point   anchor &#x3D; Point(-1,-1),     &#x2F;&#x2F;锚点位置    int     borderType &#x3D; BORDER_DEFAULT &#x2F;&#x2F;边界类型)   void cv::boxFilter  (       InputArray  src,    OutputArray     dst,    int     ddepth,    Size    ksize,    Point   anchor &#x3D; Point(-1,-1),    bool    normalize &#x3D; true,    int     borderType &#x3D; BORDER_DEFAULT )   void cv::filter2D   (       InputArray  src,    OutputArray     dst,    int     ddepth,    InputArray  kernel,                        &#x2F;&#x2F;卷积核值    Point   anchor &#x3D; Point(-1,-1),    double  delta &#x3D; 0,    int     borderType &#x3D; BORDER_DEFAULT )   &#x2F;&#x2F;典型示例二：高斯滤波器void cv::GaussianBlur   (       InputArray  src,    OutputArray     dst,    Size    ksize,                   &#x2F;&#x2F;滤波器尺寸,必须为奇数，否则会引发错误    double  sigmaX,                  &#x2F;&#x2F;控制高斯曲线水平方向形状的参数    double  sigmaY &#x3D; 0,              &#x2F;&#x2F;控制高斯曲线垂直方向形状的参数    int     borderType &#x3D; BORDER_DEFAULT     )  <span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p><strong>图像降采样</strong></p><ul><li>降低图像精度的过程称为缩减像素采样（downsampling）；</li><li>提升图像精度的过程称为提升像素采样（upsampling）</li></ul><p>难点：重采样的过程需要尽可能地保持图像质量</p><p><strong>中值滤波器</strong></p><pre class="line-numbers language-c++" data-language="c++"><code class="language-c++">&#x2F;&#x2F;原理：非线性滤波，把当前像素和它的邻域组成一个集合，然后计算出这个集合的中间值，以此作为当前像素的值（用邻域内集合的中位数代替当前像素值）void cv::medianBlur (       InputArray  src,    OutputArray     dst,    int     ksize                    &#x2F;&#x2F;注意这里的ksize类型是 int类型)<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h3 id="高通滤波器"><a href="#高通滤波器" class="headerlink" title="高通滤波器"></a>高通滤波器</h3><p><strong>定向滤波器（边缘检测）</strong></p><ul><li>二维图像分为水平方向和垂直方向；</li><li>比较经典的卷积核称为算子</li></ul><p><img src="/pic/%E9%80%89%E5%8C%BA_045.png"></p><p><img src="/pic/%E9%80%89%E5%8C%BA_046.png"></p><blockquote><p>Q:如何辨别x方向和y方向的滤波？（分别沿x&#x2F;y方向去找灰度值发生急剧变化的边缘处）</p></blockquote><pre class="line-numbers language-c++" data-language="c++"><code class="language-c++">&#x2F;&#x2F;Sobel滤波器：只对垂直或水平方向的图像频率起作用void cv::Sobel  (       InputArray  src,    OutputArray     dst,    int     ddepth,             &#x2F;&#x2F;位深，-1代表输出图像和源图像的位深相同    int     dx,                 &#x2F;&#x2F;x方向的微分，几阶导数    int     dy,                 &#x2F;&#x2F;y方向的微分，几阶导数    int     ksize &#x3D; 3,          &#x2F;&#x2F;sobel内核尺寸，只能为 1,3,5或7    double  scale &#x3D; 1,    double  delta &#x3D; 0,    int     borderType &#x3D; BORDER_DEFAULT )   &#x2F;&#x2F;Schar滤波器void cv::Scharr (       InputArray  src,    OutputArray     dst,    int     ddepth,           &#x2F;&#x2F;注意，此处位深最好选用CV_16S,否则会丢失很多信息    int     dx,    int     dy,    double  scale &#x3D; 1,    double  delta &#x3D; 0,    int     borderType &#x3D; BORDER_DEFAULT )      &#x2F;&#x2F;Laplacian算子    void cv::Laplacian  (       InputArray  src,    OutputArray     dst,    int     ddepth,    int     ksize &#x3D; 1,    double  scale &#x3D; 1,    double  delta &#x3D; 0,    int     borderType &#x3D; BORDER_DEFAULT )       <span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h3 id="Canny边缘检测算子"><a href="#Canny边缘检测算子" class="headerlink" title="Canny边缘检测算子"></a>Canny边缘检测算子</h3><pre class="line-numbers language-c++" data-language="c++"><code class="language-c++">&#x2F;&#x2F;Canny边缘检测算法void cv::Canny  (       InputArray  image,           &#x2F;&#x2F;8位的输入图像    OutputArray     edges,       &#x2F;&#x2F;输出图像，一般是二值图像    double  threshold1,          &#x2F;&#x2F;低阈值，常取高阈值的1&#x2F;2或1&#x2F;3    double  threshold2,          &#x2F;&#x2F;高阈值    int     apertureSize &#x3D; 3,    &#x2F;&#x2F;sobel算子的size，通常取值3    bool    L2gradient &#x3D; false   &#x2F;&#x2F;选择true表示用L2归一化，选择false表示用L1来归一化)<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>]]></content>
      
      
      <categories>
          
          <category> Opencv </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Opencv </tag>
            
            <tag> 直方图 </tag>
            
            <tag> 腐蚀膨胀 </tag>
            
            <tag> 图像滤波 </tag>
            
            <tag> Canny </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Opencv系列（一）</title>
      <link href="/2023/04/01/opencv/"/>
      <url>/2023/04/01/opencv/</url>
      
        <content type="html"><![CDATA[<h1 id="Opencv"><a href="#Opencv" class="headerlink" title="Opencv"></a>Opencv</h1><p><strong>main modules</strong></p><ul><li><p>core</p><p> 定义基本数据结构的紧凑模块，包括稠密的多维数组 Mat 和所有其他模块使用的基本功能。</p></li><li><p>imgproc</p><p> 一个图像处理模块，包括线性和非线性图像滤波、几何图像变换（调整大小、仿射和透视变换、通用的基于表的重映射）、色彩空间转换、直方图等。</p></li><li><p>imgcodecs</p><p> 图像文件的读取和写入</p></li><li><p>videoio</p><p> 视频输入和输出</p></li><li><p>highgui</p><p> GUI界面</p></li><li><p>calib3d</p><p> 相机标定和三维重建</p></li><li><p>features2d</p><p> 2d特征的检测，描述，匹配以及在图像上绘制2d特征点和匹配对</p></li><li><p>objdetect</p><p> 用于目标检测的基于Haar 特征的级联分类器  </p></li><li><p>dnn</p><p> 用于构建深度神经网络，主要是测试网络的输出，不支持网络训练</p></li><li><p>ml</p><p> 一组用于统计分类、回归和数据聚类的类和函数。</p><ul><li>flann<br>  FLANN库的opencv接口（功能不完整）</li></ul></li><li><p>photo</p><p> 照片处理算法，包括修补，去噪，HDR成像等</p></li><li><p>stitching</p><p> 图像拼接</p></li><li><p>gapi</p><p> OpenCV Graph API（或 G-API）是一个新的 OpenCV 模块，旨在使常规图像处理快速且便携。这两个目标是通过引入新的基于图的执行模型来实现的</p></li></ul><p><strong>contrib modules</strong></p><ul><li><p>alphamat</p><p> 从背景图像中提取具有软边界的前景</p></li><li><p>aruco</p><p> ArUco 标记是二进制方形基准标记，可用于相机姿态估计。他们的主要好处是他们的检测是鲁棒、快速和简单的。</p><p> aruco 模块包括这些类型的标记的检测以及使用它们进行姿势估计和相机校准的工具。 </p></li><li><p>bgsegm</p><p> 背景分割</p></li><li><p>bioinspired</p><p> 视网膜模型及其在图像处理中的应用</p></li><li><p>ccalib</p><p> 多相机和广角相机标定</p></li><li><p>cnn_3dobj</p><p> 用于3D物体分类和位姿估计的卷积神经网络 </p></li><li><p>cvv</p><p> 应用于计算机视觉类应用的交互式Debug </p></li><li><p>dnn_objdetect</p><p> 使用卷积神经网络进行目标检测</p></li><li><p>dnn_superres</p><p> 使用卷积神经网络进行图像放大（提高分辨率）</p></li><li><p>face</p><p>  人脸识别的相关算法</p></li><li><p>fuzzy</p><p> 模糊数学理论在图像处理中的应用，主要是F变换 </p></li><li><p>hdf</p><p> hdf5文件的输入和输出</p></li><li><p>Julia</p><p> OpenCV的Julia语言封装</p></li><li><p>line_descriptor</p><p> 从图像中检测直线</p></li><li><p>mcc</p><p> 图像色彩校正</p></li><li><p>phase_unwrapping</p><p> 二维相位展开</p></li><li><p>sfm</p><p> 运动结构恢复</p></li><li><p>stereo</p><p> 稠密立体匹配</p></li><li><p>structured_light</p><p> 结构光反射图案的解析</p></li><li><p>Text</p><p> Tesseract文字识别框架</p></li><li><p>tracking</p><p> 图像中的物体追踪</p></li><li><p>viz</p><p> 可视化窗口（类似Qt）</p></li><li><p>ximgproc</p><p> 拓展图像处理模块。包含结构森林，变化域滤波器，导向滤波，自适应流行滤波器，联合双边滤波器和超像素。</p></li><li><p>xphoto</p><p> 白平衡调整</p></li></ul><h1 id="Opencv如何对像素进行操作"><a href="#Opencv如何对像素进行操作" class="headerlink" title="Opencv如何对像素进行操作"></a>Opencv如何对像素进行操作</h1><h2 id="取"><a href="#取" class="headerlink" title="取"></a>取</h2><pre class="line-numbers language-c++" data-language="c++"><code class="language-c++">Mat cv::imread  (   const String &amp;  filename,                    int     flags &#x3D; IMREAD_COLOR                 )   <span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre><p>filename （必须的）：需要读取的图像名，你也可以写成读取的路径：绝对路劲和相对路径都可</p><p>flag （可选）：flag时读取图像的格式。<br>如果你没有flag选项就按照原始的图像格式，如果有flag选项就按照flag格式读取<br>flag可以是数字，也可以是具体的类型（枚举)</p><pre class="line-numbers language-c++" data-language="c++"><code class="language-c++">enum ImreadModes &#123;       IMREAD_UNCHANGED            &#x3D; -1, &#x2F;&#x2F;!&lt; If set, return the loaded image as is (with alpha channel, otherwise it gets cropped).       IMREAD_GRAYSCALE            &#x3D; 0,  &#x2F;&#x2F;!&lt; If set, always convert image to the single channel grayscale image.       IMREAD_COLOR                &#x3D; 1,  &#x2F;&#x2F;!&lt; If set, always convert image to the 3 channel BGR color image.       IMREAD_ANYDEPTH             &#x3D; 2,  &#x2F;&#x2F;!&lt; If set, return 16-bit&#x2F;32-bit image when the input has the corresponding depth, otherwise convert it to 8-bit.       IMREAD_ANYCOLOR             &#x3D; 4,  &#x2F;&#x2F;!&lt; If set, the image is read in any possible color format.       IMREAD_LOAD_GDAL            &#x3D; 8,  &#x2F;&#x2F;!&lt; If set, use the gdal driver for loading the image.       IMREAD_REDUCED_GRAYSCALE_2  &#x3D; 16, &#x2F;&#x2F;!&lt; If set, always convert image to the single channel grayscale image and the image size reduced 1&#x2F;2.       IMREAD_REDUCED_COLOR_2      &#x3D; 17, &#x2F;&#x2F;!&lt; If set, always convert image to the 3 channel BGR color image and the image size reduced 1&#x2F;2.       IMREAD_REDUCED_GRAYSCALE_4  &#x3D; 32, &#x2F;&#x2F;!&lt; If set, always convert image to the single channel grayscale image and the image size reduced 1&#x2F;4.       IMREAD_REDUCED_COLOR_4      &#x3D; 33, &#x2F;&#x2F;!&lt; If set, always convert image to the 3 channel BGR color image and the image size reduced 1&#x2F;4.       IMREAD_REDUCED_GRAYSCALE_8  &#x3D; 64, &#x2F;&#x2F;!&lt; If set, always convert image to the single channel grayscale image and the image size reduced 1&#x2F;8.       IMREAD_REDUCED_COLOR_8      &#x3D; 65, &#x2F;&#x2F;!&lt; If set, always convert image to the 3 channel BGR color image and the image size reduced 1&#x2F;8.       IMREAD_IGNORE_ORIENTATION   &#x3D; 128 &#x2F;&#x2F;!&lt; If set, do not rotate the image according to EXIF&#39;s orientation flag.     &#125;;     <span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h2 id="存"><a href="#存" class="headerlink" title="存"></a>存</h2><pre class="line-numbers language-c++" data-language="c++"><code class="language-c++">bool cv::imwrite    (   const String &amp;  filename,                        InputArray  img,                        const std::vector&lt; int &gt; &amp;params&#x3D;std::vector&lt; int &gt;()                     )   <span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span></span></code></pre><p>filename （必须）同上<br>img  （必须）表示需要保存的Mat类型的图像数据<br>通常，使用此功能只能保存 8 位单通道或 3 通道（具有“BGR”通道顺序）图像，除去一些特殊情况。</p><pre class="line-numbers language-c++" data-language="c++"><code class="language-c++">enum ImwriteFlags &#123;       IMWRITE_JPEG_QUALITY        &#x3D; 1,  &#x2F;&#x2F;!&lt; For JPEG, it can be a quality from 0 to 100 (the higher is the better). Default value is 95.       IMWRITE_JPEG_PROGRESSIVE    &#x3D; 2,  &#x2F;&#x2F;!&lt; Enable JPEG features, 0 or 1, default is False.       IMWRITE_JPEG_OPTIMIZE       &#x3D; 3,  &#x2F;&#x2F;!&lt; Enable JPEG features, 0 or 1, default is False.       IMWRITE_JPEG_RST_INTERVAL   &#x3D; 4,  &#x2F;&#x2F;!&lt; JPEG restart interval, 0 - 65535, default is 0 - no restart.       IMWRITE_JPEG_LUMA_QUALITY   &#x3D; 5,  &#x2F;&#x2F;!&lt; Separate luma quality level, 0 - 100, default is 0 - don&#39;t use.       IMWRITE_JPEG_CHROMA_QUALITY &#x3D; 6,  &#x2F;&#x2F;!&lt; Separate chroma quality level, 0 - 100, default is 0 - don&#39;t use.       IMWRITE_PNG_COMPRESSION     &#x3D; 16, &#x2F;&#x2F;!&lt; For PNG, it can be the compression level from 0 to 9. A higher value means a smaller size and longer compression time. If specified, strategy is changed to IMWRITE_PNG_STRATEGY_DEFAULT (Z_DEFAULT_STRATEGY). Default value is 1 (best speed setting).       IMWRITE_PNG_STRATEGY        &#x3D; 17, &#x2F;&#x2F;!&lt; One of cv::ImwritePNGFlags, default is IMWRITE_PNG_STRATEGY_RLE.       IMWRITE_PNG_BILEVEL         &#x3D; 18, &#x2F;&#x2F;!&lt; Binary level PNG, 0 or 1, default is 0.       IMWRITE_PXM_BINARY          &#x3D; 32, &#x2F;&#x2F;!&lt; For PPM, PGM, or PBM, it can be a binary format flag, 0 or 1. Default value is 1.       IMWRITE_EXR_TYPE            &#x3D; (3 &lt;&lt; 4) + 0, &#x2F;* 48 *&#x2F; &#x2F;&#x2F;!&lt; override EXR storage type (FLOAT (FP32) is default)       IMWRITE_WEBP_QUALITY        &#x3D; 64, &#x2F;&#x2F;!&lt; For WEBP, it can be a quality from 1 to 100 (the higher is the better). By default (without any parameter) and for quality above 100 the lossless compression is used.       IMWRITE_PAM_TUPLETYPE       &#x3D; 128,&#x2F;&#x2F;!&lt; For PAM, sets the TUPLETYPE field to the corresponding string value that is defined for the format       IMWRITE_TIFF_RESUNIT &#x3D; 256,&#x2F;&#x2F;!&lt; For TIFF, use to specify which DPI resolution unit to set; see libtiff documentation for valid values       IMWRITE_TIFF_XDPI &#x3D; 257,&#x2F;&#x2F;!&lt; For TIFF, use to specify the X direction DPI       IMWRITE_TIFF_YDPI &#x3D; 258 &#x2F;&#x2F;!&lt; For TIFF, use to specify the Y direction DPI     &#125;;<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h2 id="ROI区域"><a href="#ROI区域" class="headerlink" title="ROI区域"></a>ROI区域</h2><blockquote><p>定义<br>有事需要让一个处理函数只在图像的某个部分起作用，所以需要定义图像的子区域，也就是ROI区域（region of interest）感兴趣区域</p></blockquote><pre class="line-numbers language-c++" data-language="c++"><code class="language-c++">int  main()&#123;cv::Mat image&#x3D; cv::imread(&quot;3.png&quot;);cv::Mat logo&#x3D; cv::imread(&quot;2.jpg&quot;);cv::Mat imageROI(image,                                    cv::Rect(0,                                    0,                                    logo.cols,                                    logo.rows));imshow(&quot;1&quot;,imageROI);&#x2F;&#x2F;将logo替换image中的感兴区域imageROIlogo.copyTo(imageROI);imshow(&quot;2&quot;,logo);imshow(&quot;3&quot;,image);cv::waitKey(0);return 0;&#125;<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><blockquote><p>扩展：除了使用起点和终点位置，还可以通过列数和行数实现ROI区域定义：</p></blockquote><pre class="line-numbers language-c++" data-language="c++"><code class="language-c++">cv:: Mat imageROI &#x3D; image(cv::Range(0,logo.rows),                                  cv::Range(0,logo.cols));<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><h2 id="指针遍历"><a href="#指针遍历" class="headerlink" title="指针遍历"></a>指针遍历</h2><p>像素遍历就是将图像的所有像素都访问一次。由于图像像素数量非常庞大，高效遍历就十分必要</p><pre class="line-numbers language-c++" data-language="c++"><code class="language-c++">int nl&#x3D; image.rows; &#x2F;&#x2F; 行数 &#x2F;&#x2F; 每行的元素数量 int nc&#x3D; image.cols * image.channels();  for (int j&#x3D;0; j&lt;nl; j++) &#123;  &#x2F;&#x2F; 取得行 j 的地址,这里以uchar图像类型作为例子 uchar* data&#x3D; image.ptr&lt;uchar&gt;(j);  for (int i&#x3D;0; i&lt;nc; i++) &#123; &#x2F;&#x2F; 处理每个像素 data[i] &#125; &#x2F;&#x2F; 一行结束 &#125;<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>例子：减色算法，对每个像素做减色，对于8为无符号字符类型的彩色图有256x256x256中颜色，减色就是减色颜色的种类</p><pre class="line-numbers language-c++" data-language="c++"><code class="language-c++">&#x2F;*减色算法：假设 N 是减色因子，将图像中每个像素的值除以 N（这里假定使用整数除法，不保留余数）。然后将结果乘以 N，得到 N 的倍数，并且刚好不超过原始像素 值。加上 N &#x2F; 2，就得到相邻的 N 倍数之间的中间值。对所有 8 位通道值重复这个过程，就会得到  (256 &#x2F; N) × (256 &#x2F; N) × (256 &#x2F; N)种可能的颜色值*&#x2F;void colorReduce(cv::Mat image, int div &#x3D;64)&#123;    int nl &#x3D;image.rows;    int nc &#x3D;image.cols;    for(int j&#x3D;0;j&lt;nl;j++)&#123;        uchar* data&#x3D;image.ptr&lt;uchar&gt;(j);        for(int i&#x3D;0;i&lt;nc;i++)&#123;            data[i]&#x3D; data[i]&#x2F;div*div+div&#x2F;2;                    &#125;    &#125;&#125;int  main()&#123;    cv::Mat image&#x3D;cv::imread(&quot;1.jpg&quot;);    colorReduce(image);    cv::imshow(&quot;1&quot;,image);    cv::waitKey();    return 0;&#125;<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h2 id="迭代器遍历"><a href="#迭代器遍历" class="headerlink" title="迭代器遍历"></a>迭代器遍历</h2><pre class="line-numbers language-c++" data-language="c++"><code class="language-c++">&#x2F;&#x2F;创建迭代器bagin和end，注意要给出图像的数据类型，此处以cv::Vec3b为例&#x2F;&#x2F;在opencv中cv::Vec3b向量包含三个无符号字符类型的数据，可以用于描述彩色图的三通道。cv::Mat_&lt;cv::Vec3b&gt;::iterator it&#x3D; image.begin&lt;cv::Vec3b&gt;(); cv::Mat_&lt;cv::Vec3b&gt;::iterator itend&#x3D; image.end&lt;cv::Vec3b&gt;();  &#x2F;&#x2F; 扫描全部像素 for ( ; it!&#x3D; itend; ++it) &#123; &#x2F;&#x2F;处理每个像素 (*it)[0] (*it)[1] (*it)[2] &#125; &#x2F;&#x2F;或者while (it!&#x3D; itend) &#123;  &#x2F;&#x2F; 处理每个像素 ---------------------  ... &#x2F;&#x2F; 像素处理结束 ---------------------  ++it; &#125; <span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h1 id="Opencv点检测"><a href="#Opencv点检测" class="headerlink" title="Opencv点检测"></a>Opencv点检测</h1><h2 id="Harris角点检测"><a href="#Harris角点检测" class="headerlink" title="Harris角点检测"></a>Harris角点检测</h2><h3 id="角点定义"><a href="#角点定义" class="headerlink" title="角点定义"></a>角点定义</h3><p>角点是图像中某些属性较为突出的像素点，例如像素值最大或者最小的点、线段的顶点、孤立的边缘点等，图中圆圈包围的线段的拐点就是一些常见的角点。常用的角点有以下几种。  </p><ul><li>灰度梯度的最大值对应的像素点；  </li><li>两条直线或者曲线的交点；  </li><li>一阶梯度的导数最大值和梯度方向变化率最大的像素点；  </li><li>一阶导数值最大，但是二阶导数值为0的像素点；</li></ul><h3 id="Harris算法原理"><a href="#Harris算法原理" class="headerlink" title="Harris算法原理"></a>Harris算法原理</h3><p>Harris角点是最经典的角点之一，其从像素值变化度对角点进行定义，像素值的局部最大峰值即为Harris角点。Harris角点的检测过程如图9-3所示，首先以某个像素为中心构建一个矩形滑动窗口，滑动窗口覆盖图像像素值通过线性叠加得到得到滑动窗口所有像素值的衡量系数，该系数与滑动窗口范围内的像素值成正比，当滑动窗口范围内像素值整体变大时，该衡量系数也变大。在图像中以每个像素为中心向各个方向移动滑动窗口，当滑动窗口无论向哪个方向移动像素值衡量系数都缩小时，滑动窗口中心点对应的像素点即为Haris角点</p><p><img src="/pic/%E9%80%89%E5%8C%BA_032.png"></p><p>角点检测最原始的想法就是取某个像素的一个邻域窗口，当这个窗口在各个方向上进行小范围移动时，观察窗口内平均的像素灰度值的变化（即E(u,v)，Window-averaged change of intensity）。从上图可知，我们可以将一幅图像大致分为三个区域（‘flat’，‘edge’，‘corner’），这三个区域变化是不一样的。</p><p><img src="/pic/%E9%80%89%E5%8C%BA_035.png"></p><p>其中：  </p><ul><li>u、v是窗口在水平，竖直方向的偏移；  </li><li>w(x,y)表示滑动窗口权重函数，可以是常数，也可以是高斯函数；</li></ul><p><img src="/pic/%E9%80%89%E5%8C%BA_034.png"></p><p>图中蓝线圈出的地方我们称之为Harris角点的梯度协方差矩阵，记为M。其中，Ix和Iy分别为X方向和Y方向的梯度。  由于E(x,y)取值与M相关，进一步对其进行简化，定义Harris角点评价系数R为：</p><blockquote><p>R&#x3D;det(M)-k(tr(M))^2</p></blockquote><p>其中k为常值权重系数，det(M)&#x3D;λ1λ2,tr(M)&#x3D;λ1+λ2,λ1和λ2是梯度协方差矩阵M的特征向量，将特征向量代入得:</p><blockquote><p>R&#x3D;λ1λ2-k(λ1+λ2)^2</p></blockquote><p>当R较大时，说明两个特征向量较相似或者接近，则该点为角点；当R&lt;0时，说明两个特征向量相差较大，则该点位于直线上；当|R|较小，说明两个特征值较小，则该点位于平面。</p><h3 id="Opencv实现"><a href="#Opencv实现" class="headerlink" title="Opencv实现"></a>Opencv实现</h3><p><strong>cornerHarris()——计算角点Harris评价系数R</strong></p><pre class="line-numbers language-c++" data-language="c++"><code class="language-c++">void cv::cornerHarris( InputArray src, OutputArray dst, int blockSize, int ksize, double k, int borderType &#x3D; BORDER_DEFAULT)<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><ul><li>src:待检测Harris角点的输入图像，图像必须是CV_8U或者CV_32F的单通道灰度图像；</li><li>dst:存放Harris评价系数R的矩阵，数据类型为CV_32F的单通道图像，与输入图像具有相同的尺寸</li><li>blockSize:邻域大小（窗口大小），通常取2；</li><li>ksize：Sobel算子的半径，用于得到图像梯度信息，该参数需要是奇数，多使用3或者5；</li><li>k:计算Harris评价系数R的权重系数，一般取值为0.02~0.04;</li><li>borderType：像素外推算法标志，这里使用默认。</li></ul><p><strong>drawKeypoints()——一次性绘制所有的角点（关键词）</strong></p><blockquote><p>绘制关键点</p></blockquote><pre class="line-numbers language-c++" data-language="c++"><code class="language-c++">void drawKeypoints(  InputArray image,  const std::vector&lt;KeyPoint&gt;&amp; keypoints, InputOutputArray outImage, const Scalar&amp; color&#x3D;Scalar::all(-1), int flags&#x3D;DrawMatchesFlags::DEFAULT )   &#x2F;&#x2F;KeyPoint类数据 class KeyPoint&#123; float angle      &#x2F;&#x2F;关键点的角度 int class_id     &#x2F;&#x2F;关键点的分类号 int octave       &#x2F;&#x2F;特征点来源（“金字塔”） Point2f pt       &#x2F;&#x2F;关键点坐标 float response   &#x2F;&#x2F;最强关键点的响应 float size       &#x2F;&#x2F;关键点邻域的直径 <span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><ul><li>image:绘制关键点的原图像，图像可以是单通道的灰度图像和三通道的彩色图像；</li><li>keypoints:来自原图像中的关键点向量，vector向量中存放着表示关键点的KeyPoint类型的数据；</li><li>outImage：绘制关键点后的输出图像；</li><li>color：关键点空心圆的颜色，默认使用随机颜色绘制空心圆；</li><li>flag:绘制功能选择标志，其实就是设置特征点的那些信息需要绘制，那些不需要绘制，有以下几种模式可选：</li></ul><table><thead><tr><th align="center">标志参数</th><th align="center">简记</th><th align="center">含义</th></tr></thead><tbody><tr><td align="center">DEFAULT</td><td align="center">0</td><td align="center">只绘制特征点的坐标点,显示在图像上就是一个个小圆点,每个小圆点的圆心坐标都是特征点的坐标。</td></tr><tr><td align="center">DRAW_OVER_OUTIMG</td><td align="center">1</td><td align="center">函数不创建输出的图像,而是直接在输出图像变量空间绘制,要求本身输出图像变量就是一个初始化好了的,size与type都是已经初始化好的变量</td></tr><tr><td align="center">NOT_DRAW_SINGLE_POINTS</td><td align="center">2</td><td align="center">单点的特征点不被绘制</td></tr><tr><td align="center">DRAW_RICH_KEYPOINTS</td><td align="center">4</td><td align="center">绘制特征点的时候绘制的是一个个带有方向的圆,这种方法同时显示图像的坐标,size，和方向,是最能显示特征的一种绘制方式</td></tr></tbody></table><blockquote><p>Harris 算法实现步骤：<br>（1）计算图像在两个方向上的梯度<br>（2）计算两个方向梯度乘积<br>（3）使用高斯函数进行加权平均，生成矩阵元素和<br>（4）计算每个像素Harris响应值，并对小于某一个阈值的像素置0<br>（5）在阈值的邻域内进行非最大值抑制，局部最大值即为Harris角点Harris算法优劣：  </p></blockquote><blockquote><p>（1）优点：计算简单，提取的特征点均匀且合理稳定（对图像旋转、亮度变化、噪声影响和视点变换不敏感）；<br>（2）缺点：a.对尺度很敏感，不具有尺度不变性；b.提取的角点精度是像素级的；c.需要设计对应的描述子和匹配算法；</p></blockquote><pre class="line-numbers language-c++" data-language="c++"><code class="language-c++">int  main()&#123;    cv::Mat image&#x3D;cv::imread(&quot;1.png&quot;);    cv::Mat grayImage;    cv::cvtColor(image,grayImage,CV_BGR2GRAY);    cv::Mat dstImage;    cv::cornerHarris(grayImage,dstImage,2,3,0.01);    cv::imshow(&quot;直接显示&quot;,dstImage);    cv::Mat thredImage;threshold(dstImage, thredImage, 0.0001, 255, CV_THRESH_BINARY);imshow(&quot;【阀值后显示】&quot;, thredImage);    cv::waitKey();    return 0;&#125;<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><blockquote><p>绘制匹配点</p></blockquote><pre class="line-numbers language-c++" data-language="c++"><code class="language-c++">void cv::drawMatches (InputArray img1,const std::vector&lt; KeyPoint &gt; &amp; keypoints1,InputArray img2,const std::vector&lt; KeyPoint &gt; &amp; keypoints2,const std::vector&lt; DMatch &gt; &amp; matches1to2,InputOutputArray outImg,const Scalar &amp; matchColor &#x3D; Scalar::all(-1),const Scalar &amp; singlePointColor &#x3D; Scalar::all(-1),const std::vector&lt; char &gt; &amp; matchesMask &#x3D; std::vector&lt; char &gt;(),DrawMatchesFlags flags &#x3D; DrawMatchesFlags::DEFAULT)<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><ul><li>img1:第一个源图像，</li><li>keypoints1:第一个源图像的关键点，</li><li>img2:第二个源图像，</li><li>keypoints2:第二个源图像的关键点，</li><li>matches1to2:从第一张图像匹配到第二张图像，</li><li>outimg: 输出图像。它的内容取决于定义在输出图像中绘制的内容的标志值，</li><li>matchColor:匹配的颜色（线和连接的关键点），</li><li>singlePointColor:单个关键点（圆圈）的颜色，表示关键点不匹配，</li><li>matchesMask:确定绘制哪些匹配项的掩码。如果掩码为空，则绘制所有匹配项。</li><li>flags:标志设置绘图功能</li></ul><h2 id="SIFT特征点检测"><a href="#SIFT特征点检测" class="headerlink" title="SIFT特征点检测"></a>SIFT特征点检测</h2><h3 id="SIFT综述"><a href="#SIFT综述" class="headerlink" title="SIFT综述"></a>SIFT综述</h3><p>尺度不变特征转换(SIFT)是一种电脑视觉的算法用来侦测与描述影像中的局部性特征，它在空间尺度中寻找极值点，并提取出其位置、尺度、旋转不变量，此算法由 David Lowe在1999年所发表，2004年完善总结。</p><p>Lowe将SIFT算法分解为如下四步：  </p><ul><li>尺度空间极值检测：搜索所有尺度上的图像位置。通过高斯微分函数来识别潜在的对于尺度和旋转不变的兴趣点。  </li><li>关键点定位：在每个候选的位置上，通过一个拟合精细的模型来确定位置和尺度。关键点的选择依据于它们的稳定程度。  </li><li>方向确定：基于图像局部的梯度方向，分配给每个关键点位置一个或多个方向。所有后面的对图像数据的操作都相对于关键点的方向、尺度和位置进行变换，从而提供对于这些变换的不变性。  </li><li>关键点描述：在每个关键点周围的邻域内，在选定的尺度上测量图像局部的梯度。这些梯度被变换成一种表示，这种表示允许比较大的局部形状的变形和光照变化。</li></ul><h3 id="SIFT算法的OpenCV实现"><a href="#SIFT算法的OpenCV实现" class="headerlink" title="SIFT算法的OpenCV实现"></a>SIFT算法的OpenCV实现</h3><p>OpenCV中的SIFT函数主要有两个接口。</p><p>构造函数：</p><pre class="line-numbers language-c++" data-language="c++"><code class="language-c++">SIFT::SIFT(int nfeatures&#x3D;0, int nOctaveLayers&#x3D;3, double contrastThreshold&#x3D;0.04, double edgeThreshold&#x3D;10, double sigma&#x3D;1.6) <span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><ul><li>nfeatures：特征点数目（算法对检测出的特征点排名，返回最好的nfeatures个特征点）。</li><li>nOctaveLayers：金字塔中每组的层数（算法中会自己计算这个值，后面会介绍）。</li><li>contrastThreshold：过滤掉较差的特征点的对阈值。contrastThreshold越大，返回的特征点越少。</li><li>edgeThreshold：过滤掉边缘效应的阈值。edgeThreshold越大，特征点越多（被多滤掉的越少）。</li><li>sigma：金字塔第0层图像高斯滤波系数，也就是σ。</li></ul><p>重载操作符：</p><pre class="line-numbers language-c++" data-language="c++"><code class="language-c++">void SIFT::operator()(InputArray img, InputArray mask, vector&lt;KeyPoint&gt;&amp; keypoints, OutputArray descriptors, bool useProvidedKeypoints&#x3D;false) <span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre><ul><li>img：8bit灰度图像 mask：图像检测区域（可选）</li><li>keypoints：特征向量矩阵</li><li>descipotors：特征点描述的输出向量（如果不需要输出，需要传cv::noArray()）。</li><li>useProvidedKeypoints：是否进行特征点检测。ture，则检测特征点；false，只计算图像特征描述。</li></ul><h3 id="SURF特征"><a href="#SURF特征" class="headerlink" title="SURF特征"></a>SURF特征</h3><p>SURF（Speeded Up Robust Features）是对SIFT的一种改进，主要特点是快速。SURF与SIFT主要有以下几点不同处理：</p><blockquote><p>1、SIFT在构造DOG金字塔以及求DOG局部空间极值比较耗时，SURF的改进是使用Hessian矩阵变换图像，极值的检测只需计算Hessian矩阵行列式，作为进一步优化，使用一个简单的方程可以求出Hessian行列式近似值，使用盒状模糊滤波（box  blur）求高斯模糊近似值。<br>2、 SURF不使用降采样，通过保持图像大小不变，但改变盒状滤波器的大小来构建尺度金字塔。<br>3、在计算关键点主方向以及关键点周边像素方向的方法上，SURF不使用直方图统计，而是使用哈尔(haar)小波转换。SIFT的KPD达到128维，导致KPD的比较耗时，SURF使用哈尔(haar)小波转换得到的方向，让SURF的KPD降到64维，减少了一半，提高了匹配速度</p></blockquote><h2 id="ORB特征"><a href="#ORB特征" class="headerlink" title="ORB特征"></a>ORB特征</h2><p>ORB特征由关键点和描述子两部分组成，关键点称为“Oriented FAST”，是一种改进的FAST 角点。它的描述子称为BRIEF(Binary Robust Independent Elementary Feature)。</p><p>提取 ORB 特征分为如下两个步骤：  </p><ul><li>FAST 角点提取：找出图像中的“角点”。相较于原始的 FAST，ORB 中计算了特征点的主方向，为BRIEF 描述子增加了旋转不变特性。  </li><li>BRIEF 描述子的计算：对前一步提取出特征点的周围图像区域进行描述。ORB 对 BRIEF 进行了改进，主要是在BRIEF 中使用了先前计算的方向信息。</li></ul><h3 id="FAST关键点"><a href="#FAST关键点" class="headerlink" title="FAST关键点"></a>FAST关键点</h3><p>FAST 是一种角点，主要检测局部像素灰度变化明显的地方，以速度快著称。它的思想是：如果一个像素与邻域的像素差别较大（过亮或过暗），那么它可能是角点。检测步骤如下：</p><ul><li>在图像中选取像素 p，假设它的亮度为Ip。</li><li>设置一个阈值 T（比如，Ip的20%）。</li><li>以像素 p 为中心，选取半径为3的圆上的16个像素点。</li><li>假如选取的圆上有连续的 N 个点的亮度大于 Ip+T 或小于 Ip−T，那么像素p 可以被认为是特征点（N通常取12，即为 FAST-12。其他常用的N取值为9和11，它们分别被称为FAST-9和FAST-11）。</li><li>循环以上四步，对每一个像素执行相同的操作。</li></ul><p>在FAST-12算法中，可以进行预测试操作，以快速地排除绝大多数不是角点的像素。</p><blockquote><p>具体操作为，对于每个像素，直接检测邻域圆上的第 1, 5, 9, 13 个像素的亮度。只有当这 4个像素中有 3 个同时大于 Ip+T或小于 Ip−T 时，当前像素才有可能是一个角点，否则应该直接排除。这大大加速了角点检测。</p></blockquote><p>还需要用非极大值抑制(Non-maximal suppression)，在一定区域内仅保留响应极大值的角点，避免角点集中的问题。  </p><p><img src="/pic/%E9%80%89%E5%8C%BA_037.png" alt="FAST特征点"></p><p>FAST特征点的计算仅仅是比较像素间亮度的差异，所以速度非常快。它的缺点是重复性不强，分布不均匀，不具有方向信息。同时，由于它固定取半径为3的圆，存在尺度问题：远处看着像是角点的地方，接近后看可能就不是角点了。</p><blockquote><p>针对 FAST 角点不具有方向性和尺度的弱点，ORB添加了尺度和旋转的描述。尺度不变性由构建图像金字塔解决，在金字塔的每一层上检测角点。特征的旋转是由灰度质心法(Intensity Centroid)实现。</p></blockquote><p><img src="/pic/%E9%80%89%E5%8C%BA_038.png" alt="使用金字塔可以匹配不同缩放倍率下的图像"></p><p>图像金字塔如上图，金字塔底层是原始图像，每往上一层，就对图像进行一个固定倍率的缩放，这样就有了不同分辨率的图像。较小的图像可以看成是远处看过来的场景。在特征匹配算法中，我们可以匹配不同层上的图像，从而实现尺度不变性。例如，如果相机在后退，那么我们应该能够在上一个图像金字塔的上层和下一个图像的下层中找到匹配。</p><p><img src="/pic/%E9%80%89%E5%8C%BA_039.png"></p><p>通过以上方法，FAST 角点便具有了尺度与旋转的描述，从而大大提升了其在不同图像之间表述的鲁棒性。所以在 ORB中，把这种改进后的 FAST 称为 Oriented FAST。</p><h3 id="BRIEF描述子"><a href="#BRIEF描述子" class="headerlink" title="BRIEF描述子"></a>BRIEF描述子</h3><p>在提取 Oriented FAST 关键点后，对每个点计算其描述子，ORB 使用改进的BRIEF特征描述。BRIEF 是一种二进制描述子，其描述向量由许多个 0 和 1 组成，这里的 0 和 1 编码了关键点附近两个随机像素（比如p和q）的大小关系：如果p 比 q 大，则取 1，反之就取 0。如果我们取了 128个这样的 p, q，最后就得到 128 维由 0、1 组成的向量。关于一对随机点的选择方法，ORB论文原作者测试了以下5种方法，发现方法（2）比较好：</p><p><img src="/pic/%E9%80%89%E5%8C%BA_040.png"></p><p>原始的 BRIEF 描述子不具有旋转不变性，因此在图像发生旋转时容易丢失。而 ORB 在 FAST 特征点提取阶段计算了关键点的方向，所以可以利用方向信息，计算了旋转之后的“Steer BRIEF”特征使 ORB 的描述子具有较好的旋转不变性。</p><p><strong>ORB类定义</strong></p><pre class="line-numbers language-c++" data-language="c++"><code class="language-c++">CV_WRAP static Ptr&lt;ORB&gt; create(int nfeatures&#x3D;500, float scaleFactor&#x3D;1.2f, int nlevels&#x3D;8, int edgeThreshold&#x3D;31,    int firstLevel&#x3D;0, int WTA_K&#x3D;2, int scoreType&#x3D;ORB::HARRIS_SCORE, int patchSize&#x3D;31, int fastThreshold&#x3D;20);<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><p>其中：</p><ul><li>nfeatures:需要的特征点总数；</li><li>scaleFactor:尺度因子；</li><li>nlevels:金字塔层数；</li><li>edgeThreshold:边界阈值；</li><li>firstLevel:起始层； </li><li>WTA_K：描述子形成方法,WTA_K&#x3D;2表示，采用两两比较；</li><li>scoreType:角点响应函数，可以选择Harris或者Fast的方法；</li><li>patchSize:特征点邻域大小</li></ul><pre class="line-numbers language-c++" data-language="c++"><code class="language-c++">int  main()&#123;    cv::Mat image&#x3D;cv::imread(&quot;1.jpg&quot;);    cv::Mat grayImage;    cv::cvtColor(image,grayImage,CV_BGR2GRAY);    vector&lt;KeyPoint&gt; keypoints_1;    Mat descriptors_1;    Ptr&lt;FeatureDetector&gt; detector &#x3D; ORB::create();    Ptr&lt;DescriptorExtractor&gt; descriptor &#x3D; ORB::create();     &#x2F;&#x2F;-- 第一步:检测 Oriented FAST 角点位置    detector-&gt;detect ( grayImage,keypoints_1 );    &#x2F;&#x2F;-- 第二步:根据角点位置计算 BRIEF 描述子    descriptor-&gt;compute ( grayImage, keypoints_1, descriptors_1 );    Mat outimg1;    Mat outimg2;    &#x2F;&#x2F;-- 第三步:显示特征点    drawKeypoints( grayImage, keypoints_1, outimg1, Scalar::all(-1), DrawMatchesFlags::DEFAULT );    drawKeypoints( grayImage, keypoints_1, outimg2, Scalar::all(-1), DrawMatchesFlags::DRAW_RICH_KEYPOINTS );    imshow(&quot;ORB特征点&quot;,outimg1);    imshow(&quot;ORB特征方向圆&quot;,outimg2);    cv::waitKey();    return 0;&#125;<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p><img src="/pic/%E9%80%89%E5%8C%BA_041.png"></p><p><strong>特征提取及形成描述子</strong></p><pre class="line-numbers language-c++" data-language="c++"><code class="language-c++">void ORB::operator()( InputArray _image, InputArray _mask, vector&lt;KeyPoint&gt;&amp; _keypoints,                        OutputArray _descriptors, bool useProvidedKeypoints)<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><ul><li>_image:输入图像；</li><li>_mask:掩码图像;</li><li>_keypoints:输入角点；</li><li>_descriptors:如果为空，只寻找特征点，不计算特征描述子；</li><li>_useProvidedKeypoints:如果为true,函数只计算特征描述子</li></ul><pre class="line-numbers language-c++" data-language="c++"><code class="language-c++">#include &lt;iostream&gt;#include &lt;chrono&gt;using namespace std;#include &lt;opencv2&#x2F;core&#x2F;core.hpp&gt;#include &lt;opencv2&#x2F;highgui&#x2F;highgui.hpp&gt;int main ( int argc, char** argv )&#123;    &#x2F;&#x2F; 读取argv[1]指定的图像    cv::Mat image;    image &#x3D; cv::imread ( argv[1] );     &#x2F;&#x2F; 判断图像文件是否正确读取    if ( image.data &#x3D;&#x3D; nullptr ) &#x2F;&#x2F;数据不存在,可能是文件不存在    &#123;        cerr&lt;&lt;&quot;文件&quot;&lt;&lt;argv[1]&lt;&lt;&quot;不存在.&quot;&lt;&lt;endl;        return 0;    &#125;    &#x2F;&#x2F; 文件顺利读取, 首先输出一些基本信息 H*W  rows*cols    cout&lt;&lt;&quot;图像宽为&quot;&lt;&lt;image.cols&lt;&lt;&quot;,高为&quot;&lt;&lt;image.rows&lt;&lt;&quot;,通道数为&quot;&lt;&lt;image.channels()&lt;&lt;endl;    cv::imshow ( &quot;image&quot;, image );      &#x2F;&#x2F; 用cv::imshow显示图像    cv::waitKey ( 0 );                  &#x2F;&#x2F; 暂停程序,等待一个按键输入    &#x2F;&#x2F; 判断image的类型    if ( image.type() !&#x3D; CV_8UC1 &amp;&amp; image.type() !&#x3D; CV_8UC3 )    &#123;        &#x2F;&#x2F; 图像类型不符合要求        cout&lt;&lt;&quot;请输入一张彩色图或灰度图.&quot;&lt;&lt;endl;        return 0;    &#125;    &#x2F;&#x2F; 遍历图像, 请注意以下遍历方式亦可使用于随机像素访问    &#x2F;&#x2F; 使用 std::chrono 来给算法计时    chrono::steady_clock::time_point t1 &#x3D; chrono::steady_clock::now();    for ( size_t y&#x3D;0; y&lt;image.rows; y++ )    &#123;        &#x2F;&#x2F; 用cv::Mat::ptr获得图像的行指针        unsigned char* row_ptr &#x3D; image.ptr&lt;unsigned char&gt; ( y );  &#x2F;&#x2F; row_ptr是第y行的头指针        for ( size_t x&#x3D;0; x&lt;image.cols; x++ )        &#123;            &#x2F;&#x2F; 访问位于 x,y 处的像素            unsigned char* data_ptr &#x3D; &amp;row_ptr[ x*image.channels() ]; &#x2F;&#x2F; data_ptr 指向待访问的像素数据            &#x2F;&#x2F; 输出该像素的每个通道,如果是灰度图就只有一个通道            for ( int c &#x3D; 0; c !&#x3D; image.channels(); c++ )            &#123;                unsigned char data &#x3D; data_ptr[c]; &#x2F;&#x2F; data为I(x,y)第c个通道的值            &#125;        &#125;    &#125;    chrono::steady_clock::time_point t2 &#x3D; chrono::steady_clock::now();    chrono::duration&lt;double&gt; time_used &#x3D; chrono::duration_cast&lt;chrono::duration&lt;double&gt;&gt;( t2-t1 );    cout&lt;&lt;&quot;遍历图像用时：&quot;&lt;&lt;time_used.count()&lt;&lt;&quot; 秒。&quot;&lt;&lt;endl;    &#x2F;&#x2F; 关于 cv::Mat 的拷贝    &#x2F;&#x2F; 直接赋值并不会拷贝数据    cv::Mat image_another &#x3D; image;    &#x2F;&#x2F; 修改 image_another 会导致 image 发生变化    image_another ( cv::Rect ( 0,0,100,100 ) ).setTo ( 0 ); &#x2F;&#x2F; 将左上角100*100的块置零    cv::imshow ( &quot;image&quot;, image );    cv::waitKey ( 0 );    &#x2F;&#x2F; 使用clone函数来拷贝数据    cv::Mat image_clone &#x3D; image.clone();    image_clone ( cv::Rect ( 0,0,100,100 ) ).setTo ( 255 );    cv::imshow ( &quot;image&quot;, image );    cv::imshow ( &quot;image_clone&quot;, image_clone );    cv::waitKey ( 0 );    &#x2F;&#x2F; 对于图像还有很多基本的操作,如剪切,旋转,缩放等,限于篇幅就不一一介绍了,请参看OpenCV官方文档查询每个函数的调用方法.    cv::destroyAllWindows();    return 0;&#125;cmake_minimum_required( VERSION 2.8 )project( imageBasics )# 添加c++ 11标准支持set( CMAKE_CXX_FLAGS &quot;-std&#x3D;c++11&quot; )# 寻找OpenCV库set(OpenCV_DIR  ~&#x2F;ssd&#x2F;software&#x2F;opencv3.3.1&#x2F;build)   #添加OpenCVConfig.cmake的搜索路径find_package( OpenCV 3 REQUIRED )# 添加头文件include_directories( $&#123;OpenCV_INCLUDE_DIRS&#125; )add_executable( imageBasics imageBasics.cpp )# 链接OpenCV库target_link_libraries( imageBasics $&#123;OpenCV_LIBS&#125; )         <span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>]]></content>
      
      
      <categories>
          
          <category> Opencv </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Opencv </tag>
            
            <tag> Harris </tag>
            
            <tag> SIFT </tag>
            
            <tag> SURF </tag>
            
            <tag> ORB </tag>
            
            <tag> FAST </tag>
            
            <tag> BRIEF描述子 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>服务器开发与工具使用（vscode,anaconda,DL等）</title>
      <link href="/2023/03/31/net/"/>
      <url>/2023/03/31/net/</url>
      
        <content type="html"><![CDATA[<h1 id="服务器"><a href="#服务器" class="headerlink" title="服务器"></a>服务器</h1><p>首先拥有一个属于自己的服务器账号，包括服务器ip，端口，用户名以及密码等。</p><p>接着，需要安装ssh,一般linux系统都自带ssh，windows一般自带ssh客户端。如未安装，可查看以下教程安装</p><p><a href="https://blog.csdn.net/weixin_50964512/article/details/123588745">ssh安装与配置，详解版</a></p><p><a href="https://blog.csdn.net/qq_33594636/article/details/128849482">Windows安装和启动SSH服务</a></p><h2 id="软件安装"><a href="#软件安装" class="headerlink" title="软件安装"></a>软件安装</h2><p><strong>xftp7以及xshell7的安装</strong></p><ul><li><p>Xftp7是一个功能强大但轻量级的SFTP&#x2F;FTP客户机，用于需要在网络上安全地传输文件的用户。通过使用 拖放、直接编辑、增强的同步、传输调度和更直观的选项卡界面等特性，文件传输得到了简化。</p></li><li><p>Xshell 是一个强大的安全终端模拟软件，它支持SSH1、SHH2、以及 Microsoft Windows 平台的TELNET协议。Xshell 通过互联网到远程主机的安全连接以及它创新性的设计和特色帮助用户在复杂的网络环境中享受他们的工作。</p></li><li><p>Xshell 可以在 Windows 界面下用来访问远端不同系统下的服务器，从而比较好的达到远程控制终端的目的。除此之外，其还有丰富的外观配色方案以及样式选择。</p></li></ul><p>下载需要填个邮箱，之后它会给你的邮箱发个下载链接，点击下载链接安装即可。</p><p><a href="https://www.xshell.com/zh/free-for-home-school/">学生版下载网址</a></p><p><a href="https://www.xshell.com/">官网网址</a></p><p><strong>vscode软件安装</strong></p><p>Visual Studio Code 简称 VSCode ，2015 年由微软公司发布。</p><p>可用于 Windows，macOS 和 Linux。它具有对 JavaScript，TypeScript 和 Node.js 的内置支持，并具有丰富的其他语言（例如 C++，C＃，Java，Python，PHP，Go</p><p><a href="https://code.visualstudio.com/">vscode下载官网</a></p><p><a href="https://blog.csdn.net/weixin_44950987/article/details/128129613">vscode安装教程</a></p><p><strong>vscode链接远程服务器</strong></p><p>在vscode软件里安装插件<em><strong>Remote-SSH</strong></em>，安装完成后需要添加服务器连接配置，具体操作如下链接：</p><p><a href="https://blog.csdn.net/zhaxun/article/details/120568402">vscode连接远程服务器</a></p><blockquote><p>总结：xftp7用来传数据，xshell7与vscode都可以通过命令行形式对远程服务器进行操控，其中vscode依靠它强大的性能可以直接对服务器中个人程序进行图形化编写，运行，调试等</p></blockquote><h1 id="服务器中开发环境配置"><a href="#服务器中开发环境配置" class="headerlink" title="服务器中开发环境配置"></a>服务器中开发环境配置</h1><h2 id="Anaconda-x2F-miniconda安装"><a href="#Anaconda-x2F-miniconda安装" class="headerlink" title="Anaconda&#x2F;miniconda安装"></a>Anaconda&#x2F;miniconda安装</h2><p>对于需要进行深度学习的同学，需要安装Anaconda以管理自己的服务器环境，对于本实验室的服务器，可直接访问&#x2F;share&#x2F;software来获得此前下载过的Anaconda各版本</p><p>如果想自己安装不同版本的anaconda，请查看如下教程：</p><p><a href="https://zhuanlan.zhihu.com/p/32925500">Anaconda介绍，安装及使用教程</a></p><p><a href="https://blog.csdn.net/qq_42257666/article/details/121383450">anaconda安装配置教程</a></p><p><strong>conda是Anaconda提供的一个管理版本和Python环境的工具，我们一般都使用它创建虚拟环境，能够隔离不同的Python软件包环境，也不会影响到其他用户。强烈建议每个用户单独将它安装在用户目录用于Python环境管理。不建议使用系统自带的Python环境。</strong></p><p><a href="https://zhuanlan.zhihu.com/p/537439636">conda的使用,创建虚拟环境</a></p><p>安装完成之后，可以替换镜像源为清华源，安装软件包时速度一般能更快，换源教程如下：</p><p><a href="https://blog.csdn.net/qq_43115981/article/details/129064657">anaconda换源</a></p><h2 id="在自己创建的虚拟环境中安装PyTorch"><a href="#在自己创建的虚拟环境中安装PyTorch" class="headerlink" title="在自己创建的虚拟环境中安装PyTorch"></a>在自己创建的虚拟环境中安装PyTorch</h2><p>参照官网，选择好对应的cuda版本和python语言以及linux系统即可。</p><p>注意使用conda安装时，会自动安装cudatoolkit与cudnn，</p><p><a href="https://pytorch.org/">pytorch官网</a></p><p><a href="https://pytorch.org/get-started/previous-versions/">pytorch以往版本</a></p><p>安装好pytorch后，就可以开始自己的深度学习编程之旅了~~~</p><h1 id="深度学习教程"><a href="#深度学习教程" class="headerlink" title="深度学习教程"></a>深度学习教程</h1><p>这里推荐一些深度学习教程</p><p><a href="https://zh-v2.d2l.ai/">动手学深度学习</a></p><p><a href="hhttps://www.bilibili.com/video/BV1FT4y1E74V/?p=3&vd_source=c3ce2553ecbf3f37d2f4be6f466233fa">吴恩达深度学习</a></p><p><a href="https://www.bilibili.com/video/BV1nJ411z7fe/?spm_id_from=333.337.search-card.all.click&vd_source=c3ce2553ecbf3f37d2f4be6f466233fa">斯坦福李飞飞cs231n计算机视觉课程【附中文字幕】</a></p><p>强烈推荐B站UP主：<a href="https://space.bilibili.com/18161609/channel/series">霹雳吧啦Wz</a></p><p>他对深度学习基础的网络进行了详细的讲解并附上源码，包括图像分类、语义分割、关键点检测、目标检测，实例分割等，强烈推荐学习</p><p><a href="https://github.com/WZMIAOMIAO/deep-learning-for-image-processing">源码地址</a></p><p>另外推荐一些开源的深度学习开源工具箱，零基础上手就可以跑起来</p><p><a href="https://github.com/open-mmlab">OpenMMLab 平台</a></p><p><a href="https://github.com/open-mmlab/mmdetection">MMDetection基于 PyTorch 的目标检测开源工具箱</a></p><p><a href="https://github.com/open-mmlab/mmyolo">mmyolo基于 PyTorch 和 MMDetection 的 YOLO 系列算法开源工具箱</a></p>]]></content>
      
      
      <categories>
          
          <category> net </category>
          
          <category> DL </category>
          
      </categories>
      
      
        <tags>
            
            <tag> net </tag>
            
            <tag> ssh </tag>
            
            <tag> DL </tag>
            
            <tag> vscode </tag>
            
            <tag> anaconda </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>ROS介绍及快速体验</title>
      <link href="/2023/03/30/ros/"/>
      <url>/2023/03/30/ros/</url>
      
        <content type="html"><![CDATA[<h1 id="ROS概念"><a href="#ROS概念" class="headerlink" title="ROS概念"></a>ROS概念</h1><p><strong>ROS全称Robot Operating System(机器人操作系统)</strong></p><ul><li><p>ROS是适用于机器人的开源元操作系统</p></li><li><p>ROS集成了大量的工具，库，协议，提供类似OS所提供的功能，简化对机器人的控制</p></li><li><p>还提供了用于在多台计算机上获取，构建，编写和运行代码的工具和库，ROS在某些方面类似于“机器人框架”</p></li><li><p>ROS设计者将ROS表述为“ROS &#x3D; Plumbing + Tools + Capabilities + Ecosystem”，即ROS是通讯机制、工具软件包、机器人高层技能以及机器人生态系统的集合体</p></li></ul><p><img src="/pic/%E9%80%89%E5%8C%BA_028.png"></p><h1 id="ROS框架"><a href="#ROS框架" class="headerlink" title="ROS框架"></a>ROS框架</h1><p>ROS 框架主要分成三个层级，分别是 ROS 文件系统、ROS 计算图和 ROS 社区。</p><h2 id="ROS文件系统"><a href="#ROS文件系统" class="headerlink" title="ROS文件系统"></a>ROS文件系统</h2><p>ROS 的文件系统主要介绍了硬盘上 ROS 文件的组织形式。其中，我们必须了解的主要有以下几个方面：</p><ul><li>软件包（Package）：ROS 软件包是 ROS 软件框架的独立单元。ROS 软件包可能包含源代码、第三方软件库、配置文件等。ROS 软件包可以复用和共享。</li><li>软件包清单（Package Manifest）：清单文件（package.xml）列出了软件包的所有详细信息，包括名称、描述、许可信息以及最重要的依赖关系。</li><li>消息（msg）类型：消息的描述存储在软件包的 msg 文件夹下。ROS 消息是一组通过 ROS 的消息传递系统进行数据发送的数据结构。消息的定义存储在扩展名为 .msg 的文件里。</li><li>服务（srv）类型：服务的描述使用扩展名 .srv 存储在 srv 文件夹下。该文件定义了 ROS 内服务请求和响应的数据结构。</li></ul><p><img src="/pic/%E9%80%89%E5%8C%BA_029.png"></p><pre class="line-numbers language-none"><code class="language-none">WorkSpace --- 自定义的工作空间    |--- build:编译空间，用于存放CMake和catkin的缓存信息、配置信息和其他中间文件。    |--- devel:开发空间，用于存放编译后生成的目标文件，包括头文件、动态&amp;静态链接库、可执行文件等。    |--- src: 源码        |-- package：功能包(ROS基本单元)包含多个节点、库与配置文件，包名所有字母小写，只能由字母、数字与下划线组成            |-- CMakeLists.txt 配置编译规则，比如源文件、依赖项、目标文件            |-- package.xml 包信息，比如:包名、版本、作者、依赖项...(以前版本是 manifest.xml)            |-- scripts 存储python文件            |-- src 存储C++源文件            |-- include 头文件            |-- msg 消息通信格式文件            |-- srv 服务通信格式文件            |-- action 动作格式文件            |-- launch 可一次性运行多个节点             |-- config 配置信息        |-- CMakeLists.txt: 编译的基本配置<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h2 id="ROS计算图"><a href="#ROS计算图" class="headerlink" title="ROS计算图"></a>ROS计算图</h2><p>ros 程序运行之后，不同的节点之间是错综复杂的，ROS 中提供了一个实用的工具:rqt_graph。</p><p>rqt_graph能够创建一个显示当前系统运行情况的动态图形。ROS 分布式系统中不同进程需要进行数据交互，计算图可以以点对点的网络形式表现数据交互过程。rqt_graph是rqt程序包中的一部分。</p><p>ROS 计算图中的基本功能包括节点、ROS 控制器、参数服务器、消息和服务：</p><ul><li><strong>节点（Node）</strong>：ROS 节点是使用 ROS 功能处理数据的进程。节点的基本功能是计算。例如，节点可以对激光扫描仪数据进行处理，以检查是否存在碰撞。ROS 节点的编写需要 ROS 客户端库文件（如roscpp和rospy）的支持。</li><li><strong>ROS 控制器（Master）</strong>：ROS 节点可以通过名为 ROS 控制器的程序相互连接。此程序提供计算图其他节点的名称、注册和查找信息。如果不运行这个控制器，节点之间将无法相互连接和发送消息。</li><li><strong>参数服务器（Parameter server）</strong>：ROS 参数是静态值，存储在叫作参数服务器的全局位置。所有节点都可以从参数服务器访问这些值。我们甚至可以将参数服务器的范围设置为 private 以访问单个节点，或者设置为 public 以访问所有节点。</li><li><strong>ROS主题（Topic）</strong>：ROS 节点使用命名总线（叫作 ROS 主题）彼此通信。数据以消息的形式流经主题。通过主题发送消息称为发布，通过主题接收数据称为订阅。</li><li><strong>消息（Message）</strong>：ROS 消息是一种数据类型，可以由基本数据类型（如整型、浮点型、布尔类型等）组成。ROS 消息流经 ROS 主题。一个主题一次只能发送&#x2F;接收一种类型的消息。我们可以创建自己的消息定义并通过主题发送它。</li><li><strong>服务（Service）</strong>：我们看到使用 ROS 主题的发布&#x2F;订阅模型是一种非常灵活的通信模式，这是一种一对多的通信模式，意味着一个主题可以被任意数量的节点订阅。在某些情况下，可能还需要一种<strong>请求&#x2F;应答</strong>类型的交互方式，它可以用于分布式系统。这种交互方式可以使用 ROS 服务实现。ROS 服务的工作方式与 ROS 主题类似，因为它们都有消息类型定义。使用该消息定义可以将服务请求发送到另一个提供该服务的节点。服务的结果将作为应答发送。该节点必须等待，直到从另一个节点接收到结果。</li><li><strong>ROS 消息记录包（Bag）</strong>：这是一种用于保存和回放 ROS 主题的文件格式。ROS 消息记录包是记录传感器数据和处理数据的重要工具。这些包之后可以用于离线测试算法</li></ul><blockquote><p>演示</p></blockquote><p>首先，按照前面所示，运行案例</p><p>然后，启动新终端，键入: rqt_graph 或 rosrun rqt_graph rqt_graph，可以看到类似下图的网络拓扑图，该图可以显示不同节点之间的关系。</p><p><img src="/pic/%E9%80%89%E5%8C%BA_030.png"></p><h2 id="ROS快速体验"><a href="#ROS快速体验" class="headerlink" title="ROS快速体验"></a>ROS快速体验</h2><h3 id="HelloWorld实现简介"><a href="#HelloWorld实现简介" class="headerlink" title="HelloWorld实现简介"></a>HelloWorld实现简介</h3><p>ROS中涉及的编程语言以C++和Python为主，ROS中的大多数程序两者都可以实现,ROS中的程序即便使用不同的编程语言，实现流程也大致类似，以当前HelloWorld程序为例，实现流程大致如下：</p><ul><li>先创建一个工作空间；</li><li>再创建一个功能包；</li><li>编辑源文件；</li><li>编辑配置文件；</li><li>编译并执行。</li></ul><p><strong>1.创建工作空间并初始化</strong></p><pre class="line-numbers language-none"><code class="language-none">mkdir -p 自定义空间名称&#x2F;srccd 自定义空间名称catkin_make<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre><p><strong>2.进入 src 创建 ros 包并添加依赖</strong></p><pre class="line-numbers language-none"><code class="language-none">cd srccatkin_create_pkg 自定义ROS包名 roscpp rospy std_msgs<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><p>上述命令，会在工作空间下生成一个功能包，该功能包依赖于** <em>roscpp、rospy 与 std_msgs</em>**，其中roscpp是使用C++实现的库，而rospy则是使用python实现的库，std_msgs是标准消息库，创建ROS功能包时，一般都会依赖这三个库实现。</p><p><em><strong>补充</strong></em> ：在ROS中，虽然实现同一功能时，C++和Python可以互换，但是具体选择哪种语言，需要视需求而定，因为两种语言相较而言:C++运行效率高但是编码效率低，而Python则反之，基于二者互补的特点，ROS设计者分别设计了roscpp与rospy库，前者旨在成为ROS的高性能库，而后者则一般用于对性能无要求的场景，旨在提高开发效率。</p><h3 id="HelloWorld-C-版"><a href="#HelloWorld-C-版" class="headerlink" title="HelloWorld(C++版)"></a>HelloWorld(C++版)</h3><p><strong>1.进入 ros 包的 src 目录编辑源文件</strong></p><pre class="line-numbers language-none"><code class="language-none">cd 自定义的包<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>C++源码实现(文件名自定义)</p><pre class="line-numbers language-c++" data-language="c++"><code class="language-c++">#include &quot;ros&#x2F;ros.h&quot;int main(int argc, char *argv[])&#123;    &#x2F;&#x2F;执行 ros 节点初始化    ros::init(argc,argv,&quot;hello&quot;);    &#x2F;&#x2F;创建 ros 节点句柄(非必须)    ros::NodeHandle n;    &#x2F;&#x2F;控制台输出 hello world    ROS_INFO(&quot;hello world!&quot;);    return 0;&#125;<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p><strong>2.编辑 ros 包下的 Cmakelist.txt文件</strong></p><pre class="line-numbers language-c++" data-language="c++"><code class="language-c++">add_executable(步骤3的源文件名  src&#x2F;步骤3的源文件名.cpp)target_link_libraries(步骤3的源文件名  $&#123;catkin_LIBRARIES&#125;)<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p><strong>3.进入工作空间目录并编译</strong></p><pre class="line-numbers language-none"><code class="language-none">cd 自定义空间名称catkin_make<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><p>生成 build devel ….</p><p><strong>4.执行</strong></p><p><em>先启动命令行1：</em></p><pre class="line-numbers language-none"><code class="language-none">roscore<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p><em>再启动命令行2：</em></p><pre class="line-numbers language-none"><code class="language-none">cd 工作空间source .&#x2F;devel&#x2F;setup.bashrosrun 包名 C++节点<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre><p>命令行输出: HelloWorld!</p><p><em><strong>补充</strong></em>：source ~&#x2F;工作空间&#x2F;devel&#x2F;setup.bash可以添加进.bashrc文件，使用上更方便</p><p><strong>添加方式1: 直接使用 gedit 或 vi 编辑 .bashrc 文件，最后添加该内容</strong></p><p><strong>添加方式2:echo “source ~&#x2F;工作空间&#x2F;devel&#x2F;setup.bash” &gt;&gt; ~&#x2F;.bashrc</strong></p><blockquote><p>参考文献</p></blockquote><p><a href="http://c.biancheng.net/view/9853.html">ROS机器人操作系统简介</a></p><p><a href="http://www.autolabor.com.cn/book/ROSTutorials/chapter1/15-ben-zhang-xiao-jie/153-rosji-suan-tu.html">机器人入门教程</a></p>]]></content>
      
      
      <categories>
          
          <category> ROS </category>
          
          <category> c++ </category>
          
      </categories>
      
      
        <tags>
            
            <tag> c++ </tag>
            
            <tag> ros </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>RANSAC、LMEDS以及Hough变换原理</title>
      <link href="/2023/03/28/ransac/"/>
      <url>/2023/03/28/ransac/</url>
      
        <content type="html"><![CDATA[<h1 id="RANSAC算法简介"><a href="#RANSAC算法简介" class="headerlink" title="RANSAC算法简介"></a>RANSAC算法简介</h1><p>RANSAC是”RANdom SAmple Consensus”（随机采样一致）的缩写。它可以从一组包含“局外点”的观测数据集中，通过迭代方式估计数学模型的参数，它是一种不确定的算法—-它有一定的概率得出一个合理的结果；为了提高概率必须提高迭代次数。</p><blockquote><p>RANSAC基本假设</p></blockquote><ul><li>数据由<strong>局内点</strong>组成， 例如，数据的分布可以用一些模型（比如直线方程）参数来解释；</li><li><strong>局外点</strong>是不能适应该模型的参数；</li><li>除此之外的数据属于噪声；</li></ul><p>局外点产生的原因有：噪声的极值；错误的测量方法；对数据的错误假设等；</p><blockquote><p>RANSAC概述</p></blockquote><p>RANSAC算法的输入时一组观测数据，一个可以解释或者适应于观测数据的参数化模型，一些可行的参数。</p><p>RANSAC通过反复选择数据中的一组随机子集来达成目标。被选取的子集被假设为局内点，并用下述方法进行验证：</p><ul><li>有一个模型适应于假定的局内点，即所有的未知参数都能从假设的局内点计算得出；</li><li>用1中得到的模型去测试所有的其它数据，如果某个点适用于估计的模型，认为他也是局内点；</li><li>如果有足够多的点呗归类为假设的局内点，那么估计的模型就足够合理；</li><li>然后，用所有假设的局内点去重新估计模型，因为它仅仅被初始的假设局内点估计过；</li><li>最后，通过估计局内点与模型的错误率来评估模型；</li></ul><p>这个过程被重复执行固定的次数，每次产生的模型要么因为局内点太少而被抛弃，要么因为比现有的模型更好而被选用。</p><p>关于模型好坏算法实现上有两种方式：</p><ul><li>规定一个点数，达到这个点数后，算这些点与模型间的误差，找误差最小的模型。 对应下面算法一</li><li>规定一个误差，找匹配模型并小于这个误差的所有点，匹配的点最多的模型，就是最好模型。 对应下面算法二</li></ul><p>算法伪代码一：</p><pre class="line-numbers language-伪代码" data-language="伪代码"><code class="language-伪代码">输入：data ---- 一组观测数据model ---- 适应于数据的模型n ---- 适用于模型的最少数据个数k ---- 算法的迭代次数t ---- 用于决定数据是否适应于模型的阈值d ---- 判定模型是否适用于数据集的数据数目输出：best_model —— 跟数据最匹配的模型参数（如果没有找到，返回null）best_consensus_set —— 估计出模型的数据点best_error —— 跟数据相关的估计出的模型的错误iterations &#x3D; 0best_model &#x3D; nullbest_consensus_set &#x3D; nullbest_error &#x3D; 无穷大while( iterations &lt; k )    maybe_inliers &#x3D;  从数据集中随机选择n个点    maybe_model &#x3D; 适合于maybe_inliers的模型参数    consensus_set &#x3D; maybe_inliers    for (每个数据集中不属于maybe_inliers的点)        if （如果点适合于maybe_model，并且错误小于t）           将该点添加到consensus_set    if (consensus_set中的点数大于d)        已经找到了好的模型， 现在测试该模型到底有多好       better_model &#x3D; 适用于consensus_set中所有点的模型参数       this_error &#x3D;  better_model 究竟如何适合这些点的度量        if （this_error &lt; best_error）        发现比以前好的模型，保存该模型直到更好的模型出现        best_model &#x3D; better_model        best_consensus_set &#x3D; consensus_set        best_error &#x3D; this_error    iterations ++函数返回best_model, best_consensus_set, best_error<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>RANSAC算法的可能变化包括以下几种：</p><ul><li>如果发现一种足够好的模型（该模型有足够下的错误率）， 则跳出主循环，这样节约不必要的计算；设置一个错误率的阈值，小于这个值就跳出循环；</li><li>可以直接从maybe_model计算this_error，而不从consensus_set重新估计模型，这样可能会节约时间，但是可能会对噪音敏感。</li></ul><p>算法伪代码二：</p><pre class="line-numbers language-伪代码" data-language="伪代码"><code class="language-伪代码">输入：data ---- 一组观测数据numForEstimate ----- 初始模型需要的点数delta ------ 判定点符合模型的误差probability ----- 表示迭代过程中从数据集内随机选取出的点均为局内点的概率输出：best_model —— 跟数据最匹配的模型参数（如果没有找到，返回null）best_consensus_set —— 估计出模型的数据点k &#x3D; 1000&#x2F;&#x2F;设置初始值iterations &#x3D; 0best_model &#x3D; nullbest_consensus_set &#x3D; nullwhile( iterations &lt; k )    maybe_inliers &#x3D;  从数据集中随机选择numForEstimate个点    maybe_model &#x3D; 适合于maybe_inliers的模型参数，比如直线，取两个点，得直线方程    for (每个数据集中不属于maybe_inliers的点)        if （如果点适合于maybe_model，并且错误小于delta）            将该点添加到maybe_inliers    if(maybe_inliers的点数 &gt; best_consensus_set 的点数）&#x2F;&#x2F;找到更好的模型        best_model &#x3D; maybe_model        best_consensus_set  &#x3D; maybe_inliers        根据公式k&#x3D;log(1-p)&#x2F;log(1-pow(w,n))重新计算k    iterations ++函数返回best_model, best_consensus_set,<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h2 id="RANSAC参数"><a href="#RANSAC参数" class="headerlink" title="RANSAC参数"></a>RANSAC参数</h2><p>我们不得不根据特定的问题和数据集通过实验来确定参数t和d。然而参数k（迭代次数）可以从理论结果推断。当我们从估计模型参数时，用p表示一些迭代过程中从数据集内随机选取出的点均为局内点的概率；此时，结果模型很可能有用，因此p也表征了算法产生有用结果的概率。用w表示每次从数据集中选取一个局内点的概率，如下式所示： w &#x3D; 局内点的数目 &#x2F; 数据集的数目 通常情况下，我们事先并不知道w的值，但是可以给出一些鲁棒的值。假设估计模型需要选定n个点，wn是所有n个点均为局内点的概率；1 − wn是n个点中至少有一个点为局外点的概率，此时表明我们从数据集中估计出了一个不好的模型。 (1 − wn)k表示算法永远都不会选择到n个点均为局内点的概率，它和1-p相同。因此，<br>$$<br>1-p&#x3D;\left(1-w^n\right)^k<br>$$</p><p>其中</p><ul><li><p>p 表示置信度confidence</p></li><li><p>w 表示数据集中inlier占的比例</p></li><li><p>n 表示采样点数</p></li><li><p>k 表示需要迭代采样的最少次数</p></li><li><p>1 − wn 表示采样一次，n个点中至少有一个outlier的概率</p></li><li><p>(1 − wn)k 表示采样k次，n个点中至少有一个outlier的概率</p></li><li><p>因为p为采样k次，能有至少一次n个点都是inlier的概率</p></li><li><p>所以(1 − p)和(1 − wn)k相等时，k 为需要迭代采样的最少次数</p></li></ul><p>下面是k的解析解：</p><p>$$<br>k&#x3D;\frac{\log (1-p)}{\log \left(1-w^n\right)}<br>$$</p><p>值得注意的是，这个结果假设n个点都是独立选择的；也就是说，某个点被选定之后，它可能会被后续的迭代过程重复选定到。这种方法通常都不合理，由此推导出的k值被看作是选取不重复点的上限。例如，要从上图中的数据集寻找适合的直线，RANSAC算法通常在每次迭代时选取2个点，计算通过这两点的直线maybe_model，要求这两点必须唯一。</p><p>为了得到更可信的参数，标准偏差或它的乘积可以被加到k上。k的标准偏差定义为：</p><p>$$<br>S D(k)&#x3D;\frac{\sqrt{1-w^n}}{w^n}<br>$$</p><blockquote><p>RANSAC的函数接口 参照opencv来说主要需要3-4个参数（第四个不是必须的）</p></blockquote><ul><li>误差阈值ransacThreshold：区分inlier和outliner的依据</li><li>置信度confidence：设置之后代表RANSAC采样n次过程中会出现（至少一次）采样点数据集中的点都为内点的概率。这个值设置的太大，会增加采样次数。太小，会使结果不太理想。一般取0.95-0.99</li><li>最大采样迭代次数maxIters：为了防止一直在采样计算</li><li>最大精细迭代次数refineIters：在采样之后，选取最优解。可以增加精确优化，比如使用LM算法获得更优解。</li></ul><p>以上4个参数，有三个是经验值。其中最大采样迭代次数maxIters是可以有数值解的。</p><h2 id="RANSAC优点与缺点"><a href="#RANSAC优点与缺点" class="headerlink" title="RANSAC优点与缺点"></a>RANSAC优点与缺点</h2><p>RANSAC的优点是它能鲁棒的估计模型参数。例如，它能从包含大量局外点的数据集中估计出高精度的参数。</p><p>RANSAC的缺点是它计算参数的迭代次数没有上限；如果设置迭代次数的上限，得到的结果可能不是最优的结果，甚至可能得到错误的结果。RANSAC只有一定的概率得到可信的模型，概率与迭代次数成正比。RANSAC的另一个缺点是它要求设置跟问题相关的阀值。</p><p>RANSAC只能从特定的数据集中估计出一个模型，如果存在两个（或多个）模型，RANSAC不能找到别的模型。如果有多个模型，可以先估算出一个，然后用剩余的数据重新运算，重复这个过程，直到没有模型。</p><h2 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h2><p><a href="https://www.cnblogs.com/xrwang/archive/2011/03/09/ransac-1.html">王先荣RANSAC介绍</a></p><p><a href="https://zhuanlan.zhihu.com/p/402727549">RANSAC详解，保姆级教程</a></p><p><a href="https://zhuanlan.zhihu.com/p/45532306">计算机视觉基本原理</a></p><h2 id="RANSAC应用"><a href="#RANSAC应用" class="headerlink" title="RANSAC应用"></a>RANSAC应用</h2><p>OpenCV中使用RANSAC算法实现多张图像拼接思路：</p><ul><li><p>获取图像的特征点，将每张图片的特征点保存到一个vector中；</p></li><li><p>通过特征点匹配的方法，找到每张图片的共有特征点，并将其保存到一个vector中；</p></li><li><p>通过RANSAC算法求解出拼接的变换矩阵；</p></li><li><p>根据变换矩阵对每张图片进行仿射变换；</p></li><li><p>将拼接后的图片进行裁剪；</p></li><li><p>将裁剪后的图片拼接起来，最终得到拼接后的图片。</p></li></ul><p><a href="https://blog.csdn.net/qq_39312146/article/details/129053592">OpenCV中使用RANSAC算法实现多张图像拼接</a></p><p>OpenCV中的solvePnPRansac函数和findHomography函数都具有RANSAC特性，该特性使算法对少量的错误数据鲁棒。<br>这两个函数利用RANSACPointSetRegistrator类实现RANSAC算法，但这个类并没有对外开放，因此只能通过阅读OpenCV源代码学习RANSAC算法的实现和使用。</p><p>类的实现在ptsetreg.cpp中，可通过调用precomp.hpp文件中的createRANSACPointSetRegistrator函数使用。此外，该文件还提供了createLMeDSPointSetRegistrator函数调用最小中值算法。</p><p><a href="https://blog.csdn.net/HopefulLight/article/details/78775974">在OpenCV中使用RANSAC</a></p><h1 id="LMEDS算法概述（最小中值法：Least-Median-of-Squares）"><a href="#LMEDS算法概述（最小中值法：Least-Median-of-Squares）" class="headerlink" title="LMEDS算法概述（最小中值法：Least Median of Squares）"></a>LMEDS算法概述（最小中值法：Least Median of Squares）</h1><blockquote><p>经典步骤</p></blockquote><ul><li><p>随机采样</p></li><li><p>计算模型参数</p></li><li><p>计算相对模型的点集偏差err，并求出偏差中值Med(err)</p></li><li><p>迭代2. 3.步直至获得符合阈值的最优解：Med(err)最小</p></li><li><p>精确优化模型参数（LM算法迭代优化）</p></li></ul><blockquote><p>LMedS的函数接口 参照opencv来说主要需要2-3个参数（第三个不是必须的）</p></blockquote><ul><li><p>置信度confidence：设置之后代表RANSAC采样n次过程中会出现（至少一次）采样点数据集中的点都为内点的概率，这个值设置的太大，会增加采样次数。太小，会使结果不太理想。一般取0.95-0.99</p></li><li><p>最大采样迭代次数maxIters：为了防止一直在采样计算</p></li><li><p>最大精细迭代次数refineIters：在采样之后，选取最优解。可以增加精确优化，比如使用LM算法获得更优解。</p></li></ul><p>注意： 相对于RANSAC，LMedS有一个优点：不需要指定 - 误差阈值ransacThreshold：区分inlier和outliner的依据</p><blockquote><p>RANSAC与LMEDS两者的区别</p></blockquote><p><em><strong>RANSAC的阈值在具有物理意义或者几何意义的时候比较容易确定，但是当阈值不具有这些特征的时候，就成了一个不太好调整的参数了。这时LMedS可以自适应迭代获得最优解。</strong></em></p><blockquote><p>此外，LMedS也能自适应获得inlier和outliner,公式如下：</p></blockquote><p>$$<br>\hat{\sigma}&#x3D;1.4826\left(1+\frac{5}{n-p}\right) \operatorname{med}_i \sqrt{r_i^2}<br>$$</p><p>其中</p><ul><li><p>n 表示点集的个数</p></li><li><p>p 表示计算模型一次采样的点个数</p></li><li><p>ri2  表示误差</p></li><li><p>med(ri2 ) 表示误差中值</p></li></ul><p> 筛选条件为：</p> <!-- $$w_i=\left\{\begin{array}{cc}1 & \frac{\left|r_i\right|}{\hat{\sigma}} \leq 2.5 \\ 0 & \frac{\left|r_i\right|}{\hat{\sigma}}>2.5\end{array}\right.$$ --><p><img src="/pic/%E9%80%89%E5%8C%BA_027.png"></p><p><strong>由于LMedS会需要对整个点集的err求中值，当点集很大的时候，求中值的过程会很消耗时间</strong></p><p><a href="https://blog.csdn.net/billbliss/article/details/78592216">RANSAC LMedS 详细分析</a></p><h1 id="霍夫变换-Hough"><a href="#霍夫变换-Hough" class="headerlink" title="霍夫变换(Hough)"></a>霍夫变换(Hough)</h1><p><strong>霍夫变换</strong>是一种特征提取(feature extraction)，被广泛应用在图像分析（image analysis）、计算机视觉(computer vision)以及数位影像处理(digital image processing)。霍夫变换是用来辨别找出物件中的特征，例如：线条。他的算法流程大致如下，给定一个物件、要辨别的形状的种类，算法会在参数空间(parameter space)中执行投票来决定物体的形状，而这是由累加空间(accumulator space)里的局部最大值(local maximum)来决定。</p><p>经典的霍夫变换是侦测图片中的直线，之后，霍夫变换不仅能识别直线，也能够识别任何形状，常见的有圆形、椭圆形。1981年，因为DanaH.Ballard的一篇期刊论文”Generalizing the Hough transform to detect arbitrary shapes”，让霍夫变换开始流行于计算机视觉界。</p><p><a href="https://zhuanlan.zhihu.com/p/47649796">霍夫变换-神奇的特征提取方法</a></p><p><a href="https://zhuanlan.zhihu.com/p/203292567">通俗易懂-霍夫变换原理</a></p><p><a href="https://zhuanlan.zhihu.com/p/386048978">霍夫变换及代码实现</a></p>]]></content>
      
      
      <categories>
          
          <category> Opencv </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Opencv </tag>
            
            <tag> RANSAC </tag>
            
            <tag> LMEDS </tag>
            
            <tag> Hough </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>视觉及其融合惯性SLAM技术发展综述</title>
      <link href="/2023/03/27/slam/"/>
      <url>/2023/03/27/slam/</url>
      
        <content type="html"><![CDATA[<h1 id="SLAM技术"><a href="#SLAM技术" class="headerlink" title="SLAM技术"></a>SLAM技术</h1><p>同步定位与建图（Simultaneous Localization and Mapping，简称SLAM）问题可以描述为：机器人在未知环境中从一个未知位置开始移动,在移动过程中根据位置和地图进行自身定位，同时在自身定位的基础上建造增量式地图，实现机器人的自主定位和导航。</p><blockquote><p>SLAM 技术主要呈现以下 3 点发展趋势。</p></blockquote><ul><li>理论优化改进：由于应用场景需求的多样化，结合惯性、异源图像等多传感器的信息融合模式成为 SLAM 主流，促进了以紧耦合为主的信息融合理论发展，而随着大场景 SLAM 应用需求及图优化理论的推进，逐步形成了<strong>基于扩展卡尔曼滤波</strong>框架的改进滤波器优化架构，和以<strong>光束法平差（BA）</strong>为主的非线性优化架构两种研究趋势。</li><li>新型技术引入：随着<strong>深度学习</strong>技术在计算机视觉中的广泛应用，视觉 SLAM 呈现出由传统几何变换方式逐步转向结合深度学习的智能融合趋势。一方面<strong>视觉图像与语义信息</strong>的紧密联系，使得集成语义信息的视觉 SLAM 得到更多探索；另一方面为减少对传统方式依赖，利用<strong>神经网络架构</strong>替代 SLAM 的部分模块或端到端<strong>强化学习</strong>的模式得以广泛研究。</li><li>应用领域推广：视觉 SLAM 目前在<em>智能家居、自动驾驶、无人机</em>等领域得到了不同层次的应用，随着硬件性能的提升，视觉 SLAM</li></ul><p><img src="/pic/%E5%9B%BE%E7%89%871.png" alt="视觉、惯性SLAM系统框架结构"></p><p><img src="/pic/%E5%9B%BE%E7%89%872.png" alt="视觉SLAM构建地图类型"></p><p><img src="/pic/%E5%9B%BE%E7%89%873.png"></p><p><img src="/pic/%E5%9B%BE%E7%89%874.png"></p><p><img src="/pic/%E5%9B%BE%E7%89%875.png"></p><p><img src="/pic/%E5%9B%BE%E7%89%876.png"></p><p><img src="/pic/%E5%9B%BE%E7%89%877.png" alt="标准视觉SLAM Pipeline"></p><p><img src="/pic/%E5%9B%BE%E7%89%878.png" alt="极具影响力的视觉SLAM方法"></p><blockquote><p>VSLAM 常用数据集：表内的GT 是指真值的可用性</p></blockquote><p><img src="/pic/%E5%9B%BE%E7%89%879.png"></p><p><img src="/pic/%E5%9B%BE%E7%89%8710.png" alt="各种论文中用于评估的一些主流视觉SLAM数据集的实例"></p>]]></content>
      
      
      
        <tags>
            
            <tag> slam </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>图像变换(image_transformer)</title>
      <link href="/2023/03/27/image-transformer/"/>
      <url>/2023/03/27/image-transformer/</url>
      
        <content type="html"><![CDATA[<h1 id="图像变换"><a href="#图像变换" class="headerlink" title="图像变换"></a>图像变换</h1><p>基本的图像变换有<strong>刚性变换(等距变换、欧式变换)、相似变换、仿射变换、射影变换(透视变换、投影变换)</strong></p><blockquote><p>刚性变化：只对图像进行平移与旋转，形状保持不变</p></blockquote><p>欧式变换（等距变换）保持了向量的<strong>长度和夹角</strong>，相当于我们把一个刚体原封不动地进行移动或旋转，不改变它自身的样子</p><p><img src="/pic/%E9%80%89%E5%8C%BA_020.png" alt="刚体变换矩阵"></p><blockquote><p>相似变换： 等距变换与一个均匀缩放的复合；等距变换+ 均匀缩放，类似相似三角形，比例不变</p></blockquote><p>相似变换比欧氏变换多了一个自由度，它允许物体进行均匀的放缩，其矩阵表示形式为：</p><p><img src="/pic/%E9%80%89%E5%8C%BA_021.png" alt="相似变换矩阵"></p><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">import</span> cv2<span class="token keyword">import</span> matplotlib<span class="token punctuation">.</span>pyplot <span class="token keyword">as</span> plt<span class="token keyword">import</span> numpy <span class="token keyword">as</span> npimg <span class="token operator">=</span> cv2<span class="token punctuation">.</span>imread<span class="token punctuation">(</span><span class="token string">'/hone/chy/pic/git.png'</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span>img <span class="token operator">=</span> cv2<span class="token punctuation">.</span>cvtColor<span class="token punctuation">(</span>img<span class="token punctuation">,</span> cv2<span class="token punctuation">.</span>COLOR_BGR2RGB<span class="token punctuation">)</span><span class="token comment"># print(img.shape)</span><span class="token comment"># 得到相似变换的矩阵  # center：旋转中心 angle：旋转角度   scale：缩放比例</span>M <span class="token operator">=</span> cv2<span class="token punctuation">.</span>getRotationMatrix2D<span class="token punctuation">(</span>center <span class="token operator">=</span> <span class="token punctuation">(</span>img<span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token operator">/</span><span class="token number">2</span><span class="token punctuation">,</span>img<span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token operator">/</span><span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">,</span>                              angle <span class="token operator">=</span> <span class="token number">30</span><span class="token punctuation">,</span>                              scale <span class="token operator">=</span> <span class="token number">0.5</span><span class="token punctuation">)</span> <span class="token comment"># 原图像按照相似矩阵进行相似变换  三个参数：原图像，相似矩阵，画布面积</span>img_rotate <span class="token operator">=</span> cv2<span class="token punctuation">.</span>warpAffine<span class="token punctuation">(</span>img<span class="token punctuation">,</span> M<span class="token punctuation">,</span> <span class="token punctuation">(</span>img<span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">,</span> img<span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">)</span>plt<span class="token punctuation">.</span>figure<span class="token punctuation">(</span>figsize<span class="token operator">=</span><span class="token punctuation">(</span><span class="token number">8</span><span class="token punctuation">,</span><span class="token number">8</span><span class="token punctuation">)</span><span class="token punctuation">)</span>plt<span class="token punctuation">.</span>subplot<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span><span class="token number">2</span><span class="token punctuation">,</span><span class="token number">1</span><span class="token punctuation">)</span>plt<span class="token punctuation">.</span>imshow<span class="token punctuation">(</span>img<span class="token punctuation">)</span>plt<span class="token punctuation">.</span>subplot<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span><span class="token number">2</span><span class="token punctuation">,</span><span class="token number">2</span><span class="token punctuation">)</span>plt<span class="token punctuation">.</span>imshow<span class="token punctuation">(</span>img_rotate<span class="token punctuation">)</span>plt<span class="token punctuation">.</span>show<span class="token punctuation">(</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p><img src="/pic/%E9%80%89%E5%8C%BA_019.png" alt="相似变换结果"></p><blockquote><p>仿射变换：非齐次坐标下的一个非奇异线性变换与一个平移变换的复合，（即第三行是0,0,1）; 旋转+平移+缩放+切变，保持平行性</p></blockquote><p>仿射变换只要求 A 是一个可逆矩阵，而不必是正交矩阵。仿射变换也叫正交投影。经过仿射变换之后，立方体就不再是方的了，但是各个面仍是平行四边形 </p><ul><li>性质：Parallel lines are still parallel lines（不再具有保角性，具有保平行性）</li><li>三个非共线的点对（6 parameters）确定一个仿射变换。</li></ul><p><img src="/pic/%E9%80%89%E5%8C%BA_022.png" alt="仿射变换矩阵"></p><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">import</span> cv2<span class="token keyword">import</span> matplotlib<span class="token punctuation">.</span>pyplot <span class="token keyword">as</span> plt<span class="token keyword">import</span> numpy <span class="token keyword">as</span> np<span class="token comment"># 3 Src(原始) Points + 3 Dst(目标) Points</span><span class="token comment"># cols：列/长  rows：行/宽</span>img <span class="token operator">=</span> cv2<span class="token punctuation">.</span>imread<span class="token punctuation">(</span><span class="token string">'lenna.jpg'</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span>img <span class="token operator">=</span> cv2<span class="token punctuation">.</span>cvtColor<span class="token punctuation">(</span>img<span class="token punctuation">,</span> cv2<span class="token punctuation">.</span>COLOR_BGR2RGB<span class="token punctuation">)</span><span class="token comment"># print(img.shape)</span>cols <span class="token operator">=</span> img<span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span>rows <span class="token operator">=</span> img<span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span>pt1 <span class="token operator">=</span> np<span class="token punctuation">.</span>float32<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">,</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token punctuation">[</span>cols<span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">,</span> rows<span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">)</span>pt2 <span class="token operator">=</span> np<span class="token punctuation">.</span>float32<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token punctuation">[</span>cols<span class="token operator">*</span><span class="token number">0.3</span><span class="token punctuation">,</span> rows<span class="token operator">*</span><span class="token number">0.3</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token punctuation">[</span>cols<span class="token operator">*</span><span class="token number">0.8</span><span class="token punctuation">,</span> rows<span class="token operator">*</span><span class="token number">0.2</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token punctuation">[</span>cols<span class="token operator">*</span><span class="token number">0.1</span><span class="token punctuation">,</span> rows<span class="token operator">*</span><span class="token number">0.9</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token comment"># [[0,0], [cols, 0], [0, rows]] --> [[cols*0.3, rows*0.3], [cols*0.8, rows*0.2], [cols*0.1, rows*0.9]]</span>M <span class="token operator">=</span> cv2<span class="token punctuation">.</span>getAffineTransform<span class="token punctuation">(</span>pt1<span class="token punctuation">,</span> pt2<span class="token punctuation">)</span>       <span class="token comment"># 仿射变换矩阵</span>dst <span class="token operator">=</span> cv2<span class="token punctuation">.</span>warpAffine<span class="token punctuation">(</span>img<span class="token punctuation">,</span> M<span class="token punctuation">,</span> <span class="token punctuation">(</span>cols<span class="token punctuation">,</span> rows<span class="token punctuation">)</span><span class="token punctuation">)</span>plt<span class="token punctuation">.</span>figure<span class="token punctuation">(</span>figsize<span class="token operator">=</span><span class="token punctuation">(</span><span class="token number">8</span><span class="token punctuation">,</span><span class="token number">8</span><span class="token punctuation">)</span><span class="token punctuation">)</span>plt<span class="token punctuation">.</span>subplot<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span><span class="token number">2</span><span class="token punctuation">,</span><span class="token number">1</span><span class="token punctuation">)</span>plt<span class="token punctuation">.</span>imshow<span class="token punctuation">(</span>img<span class="token punctuation">)</span>plt<span class="token punctuation">.</span>subplot<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span><span class="token number">2</span><span class="token punctuation">,</span><span class="token number">2</span><span class="token punctuation">)</span>plt<span class="token punctuation">.</span>imshow<span class="token punctuation">(</span>dst<span class="token punctuation">)</span>plt<span class="token punctuation">.</span>show<span class="token punctuation">(</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p><img src="/pic/%E9%80%89%E5%8C%BA_024.png" alt="仿射变换结果"></p><blockquote><p>射影变换:次坐标的一般非奇异线性变换 。射影变换可以分解为相似变换，仿射变换，射影变换的复合，不保留平行性，保留重合关系、长度的交比</p></blockquote><p><img src="/pic/%E9%80%89%E5%8C%BA_026.png" alt="射影变换矩阵"></p><p>它左上角为可逆矩阵 A，右上为平移 t，左下缩放 a^{T} 。由于采用齐坐标，当 v \neq 0 时吗我们可以对整个矩阵除于 v得到一个右下角为 1 的矩阵；否则，则得到右下角为 0 的矩阵。因此，2D 的射影变换一共有8个自由度，3D则共有15个自由度。</p><ul><li>性质：Lines are still lines（不保角，不保平行，保直线性）</li><li>四个非共线的点对（8 parameters）确定一个透视变换</li></ul><p><img src="/pic/%E9%80%89%E5%8C%BA_023.png" alt="射影变换矩阵"></p><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">import</span> cv2<span class="token keyword">import</span> matplotlib<span class="token punctuation">.</span>pyplot <span class="token keyword">as</span> plt<span class="token keyword">import</span> numpy <span class="token keyword">as</span> npimg <span class="token operator">=</span> cv2<span class="token punctuation">.</span>imread<span class="token punctuation">(</span><span class="token string">'lenna.jpg'</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span>img <span class="token operator">=</span> cv2<span class="token punctuation">.</span>cvtColor<span class="token punctuation">(</span>img<span class="token punctuation">,</span> cv2<span class="token punctuation">.</span>COLOR_BGR2RGB<span class="token punctuation">)</span>width <span class="token operator">=</span> img<span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span>height <span class="token operator">=</span> img<span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span>pts1 <span class="token operator">=</span> np<span class="token punctuation">.</span>float32<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">,</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token punctuation">[</span>width<span class="token punctuation">,</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">,</span>height<span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token punctuation">[</span>width<span class="token punctuation">,</span>height<span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">)</span>pts2 <span class="token operator">=</span> np<span class="token punctuation">.</span>float32<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token punctuation">[</span>width<span class="token operator">*</span><span class="token number">0.1</span><span class="token punctuation">,</span>height<span class="token operator">*</span><span class="token number">0.1</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token punctuation">[</span>width<span class="token operator">*</span><span class="token number">0.9</span><span class="token punctuation">,</span> width<span class="token operator">*</span><span class="token number">0.1</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token punctuation">[</span>height<span class="token operator">*</span><span class="token number">0.2</span><span class="token punctuation">,</span>height<span class="token operator">*</span><span class="token number">0.8</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token punctuation">[</span>width<span class="token operator">*</span><span class="token number">0.7</span><span class="token punctuation">,</span>height<span class="token operator">*</span><span class="token number">0.7</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">)</span>M_warp <span class="token operator">=</span> cv2<span class="token punctuation">.</span>getPerspectiveTransform<span class="token punctuation">(</span>pts1<span class="token punctuation">,</span> pts2<span class="token punctuation">)</span>     <span class="token comment"># 单应性矩阵</span>img_warp <span class="token operator">=</span> cv2<span class="token punctuation">.</span>warpPerspective<span class="token punctuation">(</span>img<span class="token punctuation">,</span> M_warp<span class="token punctuation">,</span> <span class="token punctuation">(</span>width<span class="token punctuation">,</span> height<span class="token punctuation">)</span><span class="token punctuation">)</span>plt<span class="token punctuation">.</span>figure<span class="token punctuation">(</span>figsize<span class="token operator">=</span><span class="token punctuation">(</span><span class="token number">8</span><span class="token punctuation">,</span><span class="token number">8</span><span class="token punctuation">)</span><span class="token punctuation">)</span>plt<span class="token punctuation">.</span>subplot<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span><span class="token number">2</span><span class="token punctuation">,</span><span class="token number">1</span><span class="token punctuation">)</span>plt<span class="token punctuation">.</span>imshow<span class="token punctuation">(</span>img<span class="token punctuation">)</span>plt<span class="token punctuation">.</span>subplot<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span><span class="token number">2</span><span class="token punctuation">,</span><span class="token number">2</span><span class="token punctuation">)</span>plt<span class="token punctuation">.</span>imshow<span class="token punctuation">(</span>img_warp<span class="token punctuation">)</span>plt<span class="token punctuation">.</span>show<span class="token punctuation">(</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p><img src="/pic/%E9%80%89%E5%8C%BA_025.png" alt="射影变换结果"></p>]]></content>
      
      
      <categories>
          
          <category> cs231A </category>
          
      </categories>
      
      
        <tags>
            
            <tag> transformer </tag>
            
            <tag> opencv </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>对极几何(Epipolar_Geometry)</title>
      <link href="/2023/03/27/epipolar-geometry/"/>
      <url>/2023/03/27/epipolar-geometry/</url>
      
        <content type="html"><![CDATA[<h1 id="对极几何基本用处"><a href="#对极几何基本用处" class="headerlink" title="对极几何基本用处"></a>对极几何基本用处</h1><h2 id="立体匹配"><a href="#立体匹配" class="headerlink" title="立体匹配"></a>立体匹配</h2><p>对于已知两视角空间位置关系的情况下，由于对极几何这个几何模型限定的约束条件，使得在立体图像对上搜索空间上的分别在两个图像中的位置只需要相应的对极线上找，把原来的二维搜搜问题，直接简化为一维搜索，双目测距就是这方面得应用之一。</p><h2 id="确定两个摄像点的相对位置与姿态问题"><a href="#确定两个摄像点的相对位置与姿态问题" class="headerlink" title="确定两个摄像点的相对位置与姿态问题"></a>确定两个摄像点的相对位置与姿态问题</h2><p>在未知视角位置的情况下，通过搜索图像对中的匹配点，可以求得两个位置和姿态得相对关系，这一点常用在机器人导航、地图得生成、三维重建等方面。</p><blockquote><p>基本概念</p></blockquote><ul><li>极点（Epipoles）：两个相机得基线与两个成像平面得交点</li><li>极线（Epipolar Lines）：空间中点在成像平面上的投影点与极点的连线</li><li>极平面（Epipolar Plane）：空间中的点与两个相机的光轴中心点所组成的平面</li></ul><blockquote><p>本质矩阵</p></blockquote><p><img src="/pic/duiji2.png"></p><blockquote><p>对极约束(E)</p></blockquote><p><img src="/pic/duiji.png"></p><blockquote><p>基础矩阵</p></blockquote><p><img src="/pic/duiji3.png"></p><blockquote><p>对极约束(F)</p></blockquote><p><img src="/pic/duiji4.png"></p>]]></content>
      
      
      <categories>
          
          <category> cs231A </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 对极几何 </tag>
            
            <tag> 对极约束 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Git 基础用法</title>
      <link href="/2023/03/26/git/"/>
      <url>/2023/03/26/git/</url>
      
        <content type="html"><![CDATA[<h1 id="Git介绍"><a href="#Git介绍" class="headerlink" title="Git介绍"></a>Git介绍</h1><p>Git是一个开源的<strong>分布式版本控制系统</strong>，可以有效、高速地处理从很小到非常大的项目版本管理。也是Linus Torvalds为了帮助管理Linux内核开发而开发的一个开放源码的版本控制软件。与常用的版本控制工具<strong>CVS, Subversion</strong>等不同，它采用了<strong>分布式版本库</strong>的方式，不必服务器端软件支持（wingeddevil注：这得分是用什么样的服务端，使用http协议或者git协议等不太一样。并且在push和pull的时候和服务器端还是有交互的。），使源代码的发布和交流极其方便。 Git 的速度很快，这对于诸如 Linux kernel 这样的大项目来说自然很重要。 Git 最为出色的是它的合并跟踪（merge tracing）能力。</p><p>Torvalds 开始着手开发 Git 是为了作为一种过渡方案来替代 BitKeeper</p><p><em>分布式相比于集中式的最大区别在于开发者可以提交到本地，每个开发者通过克隆（git clone），在本地机器上拷贝一个完整的Git仓库</em></p><h2 id="Git基本工作过程"><a href="#Git基本工作过程" class="headerlink" title="Git基本工作过程"></a>Git基本工作过程</h2><blockquote><p>9个常见操作，具体如下</p></blockquote><ul><li><p>1.新建项目文件夹（只做一次）</p></li><li><p>2.进入文件夹 (重要)</p></li><li><p>3.初始化仓库：git init（只做一次）</p></li><li><p>4.编码</p></li><li><p>5.添加文件信息： git add .</p></li><li><p>6.确认添加信息：git commit -m”描述信息”</p></li><li><p>7.查看详细日志信息：git log</p></li><li><p>8.查看简略日志信息：git log –oneline</p></li><li><p>9.版本回滚:git reset –hard 版本号</p></li></ul><img src="/pic/git.png"><h2 id="其他的常用操作"><a href="#其他的常用操作" class="headerlink" title="其他的常用操作"></a>其他的常用操作</h2><h3 id="git-init"><a href="#git-init" class="headerlink" title="git init"></a>git init</h3><blockquote><p>用法：git clone [url]</p></blockquote><p>该命令可用于通过指定的URL获取一个代码库。</p><h3 id="git-config"><a href="#git-config" class="headerlink" title="git config"></a>git config</h3><blockquote><p>用法：git config –global user.name “[name]”</p></blockquote><blockquote><p>用法：git config –global user.email “[email address]”</p></blockquote><p>该命令将分别设置提交代码的用户名和电子邮件地址。</p><h3 id="git-add"><a href="#git-add" class="headerlink" title="git add"></a>git add</h3><blockquote><p>用法：git add [file]</p></blockquote><p>该命令可以将一个文件添加至stage(暂存区)。</p><blockquote><p>用法：git add *</p></blockquote><p>该命令可以将多个文件添加至stage(暂存区)。</p><h3 id="git-commit"><a href="#git-commit" class="headerlink" title="git commit"></a>git commit</h3><blockquote><p>用法：git commit -m “[ Type in the commit message]”</p></blockquote><p>该命令可以在版本历史记录中永久记录文件。</p><blockquote><p>用法：git commit -a</p></blockquote><p>该命令将提交git add命令添加的所有文件，并提交git add命令之后更改的所有文件。 </p><h3 id="git-diff"><a href="#git-diff" class="headerlink" title="git diff"></a>git diff</h3><blockquote><p>用法：git diff</p></blockquote><p>该命令可以显示尚未添加到stage的文件的变更。</p><blockquote><p>用法：git diff –staged</p></blockquote><p>该命令可以显示添加到stage的文件与当前最新版本之间的差异。</p><blockquote><p>用法：git diff [first branch] [second branch]</p></blockquote><p>该命令可以显示两个分支之间的差异。</p><h3 id="git-reset"><a href="#git-reset" class="headerlink" title="git reset"></a>git reset</h3><blockquote><p>用法：git reset [file]</p></blockquote><p>该命令将从stage中撤出指定的文件，但可以保留文件的内容。</p><blockquote><p>用法：git reset [commit]</p></blockquote><p>该命令可以撤销指定提交之后的所有提交，并在本地保留变更。</p><h3 id="git-status"><a href="#git-status" class="headerlink" title="git status"></a>git status</h3><blockquote><p>用法：git status</p></blockquote><p>该命令将显示所有需要提交的文件。</p><h3 id="git-rm"><a href="#git-rm" class="headerlink" title="git rm"></a>git rm</h3><blockquote><p>用法：git rm [file]</p></blockquote><p>该命令将删除工作目录中的文件，并将删除动作添加到stage。</p><h3 id="git-log"><a href="#git-log" class="headerlink" title="git log"></a>git log</h3><blockquote><p>用法：git log</p></blockquote><p>该命令可用于显示当前分支的版本历史记录。</p><blockquote><p>用法：git log –follow[file]</p></blockquote><p>该命令可用于显示某个文件的版本历史记录，包括文件的重命名。</p><h3 id="git-show"><a href="#git-show" class="headerlink" title="git show"></a>git show</h3><blockquote><p>用法：git show [commit]</p></blockquote><p>该命令经显示指定提交的元数据以及内容变更。</p><h3 id="git-tag"><a href="#git-tag" class="headerlink" title="git tag"></a>git tag</h3><blockquote><p>用法：git tag [commitID]</p></blockquote><p>该命令可以给指定的提交添加标签。</p><h3 id="git-branch"><a href="#git-branch" class="headerlink" title="git branch"></a>git branch</h3><blockquote><p>用法：git branch</p></blockquote><p>该命令将显示当前代码库中所有的本地分支。</p><blockquote><p>用法：git branch [branch name]</p></blockquote><p>该命令将创建一个分支。</p><blockquote><p>用法：git branch -d [branch name]</p></blockquote><p>该命令将删除指定的分支。</p><h3 id="git-checkout"><a href="#git-checkout" class="headerlink" title="git checkout"></a>git checkout</h3><blockquote><p>用法：git checkout [branch name]</p></blockquote><p>你可以通过该命令切换分支。</p><blockquote><p>用法：git checkout -b [branch name]</p></blockquote><p>你可以通过该命令创建一个分支，并切换到新分支上。</p><h3 id="git-merge"><a href="#git-merge" class="headerlink" title="git merge"></a>git merge</h3><blockquote><p>用法：git merge [branch name]</p></blockquote><p>该命令可以将指定分支的历史记录合并到当前分支。</p><h3 id="git-remote"><a href="#git-remote" class="headerlink" title="git remote"></a>git remote</h3><blockquote><p>用法：git remote add [variable name] [Remote Server Link]</p></blockquote><p>你可以通过该命令将本地的代码库连接到远程服务器。</p><h3 id="git-push"><a href="#git-push" class="headerlink" title="git push"></a>git push</h3><blockquote><p>用法：git push [variable name] master</p></blockquote><p>该命令可以将主分支上提交的变更发送到远程代码库。</p><blockquote><p>用法：git push [variable name] [branch]</p></blockquote><p>该命令可以将指定分支上的提交发送到远程代码库。</p><blockquote><p>用法：git push –all [variable name]</p></blockquote><p>该命令可以将所有分支发送到远程代码库。</p><blockquote><p>用法：git pull [Repository Link]</p></blockquote><p>该命令将获取远程服务器上的变更，并合并到你的工作目录。</p><h3 id="git-stash"><a href="#git-stash" class="headerlink" title="git stash"></a>git stash</h3><blockquote><p>用法：git stash save</p></blockquote><p>该命令将临时保存所有修改的文件。</p><blockquote><p>用法：git stash pop</p></blockquote><p>该命令将恢复最近一次stash（储藏）的文件。</p><blockquote><p>用法：git stash list</p></blockquote><p>该命令将显示stash的所有变更。</p><blockquote><p>用法：git stash drop</p></blockquote><p>该命令将丢弃最近一次stash的变更。</p><h2 id="Git小游戏"><a href="#Git小游戏" class="headerlink" title="Git小游戏"></a>Git小游戏</h2><p>游戏链接为<a href="https://oschina.gitee.io/learn-git-branching/">GIt小游戏</a></p><p>推荐一个图解答案<a href="https://blog.csdn.net/GDUT_xin/article/details/125537967">答案</a></p><pre class="line-numbers language-none"><code class="language-none">---基础篇1.git commitgit commitgit commit2.git branchgit branch bugFixgit checkout bugFix3.git mergegit branch bugFixgit checkout bugFixgit commit -m “commit bugFix”git checkout maingit commit -m “commit main”git merge bugFix4.git rebasegit branch bugFixgit checkout bugFixgit commit -m “bugFix”git checkout maingit commit -m “main commit”git checkout bugFixgit rebase main高级篇1.分离HEADgit checkout c42.相对引用(^)git checkout main^git checkout c33.相对引用2(~)git branch -f main c6git checkout HEAD~1git branch -f bugFix HEAD~14.撤销变更git reset HEAD~1git chekout pushedgit revert HEAD移动提交记录1.git cherry-pickgit cherry-pick c3 c4 c72.交互式rebasegit rebase -i overHere （打开控制面板）omit c2 （点击c2）c4 c5交换位置 （拉取）杂项1.只取一个提交记录git checkout maingit cherry-pick c42.提交的技巧#1git rebase -i HEAD~2 交换2和3的位置git commit --amendgit rebase -i HEAD~2 恢复2和3的位置git checkout miangit rebase caption main3.提交的技巧#2git checkout maingit cherry-pick c2git commit --amendgit cherry-pick c34.git taggit tag v0 c1git tag v1 c2git checkout c25. git descridegit commit高级话题1.多次rebasegit rebase main bugFixgit rebase bugFix sidegit rebase side anothergit rebase another main2.两个父节点git branch bugWork main^ ^ 2^3.纠缠不清的分支git checkout onegit cherry-pick c4 c3 c2git checkout twogit cherry-pick c5 c4 c3 c2git branch -f three c2Push &amp; Pull —— Git 远程仓库！1.git clonegit clone2.远程分支git commitgit checkout o&#x2F;maingit commit3.git fetchgit fetch4.git pullgit pull5.模拟团队合作git clonegit fakeTeamwork 2git commitgit pull6.git pushgit commitgit commitgit push7.偏离的提及历史git clonegit fakeTeamworkgit commitgit pull --rebasegit push8.锁定的main（locked main）git reset --hard o&#x2F;maingit checkout -b feature c2git push origin feature关于 origin 和它的周边 —— Git 远程仓库高级操作1.推送主分支git fetchgit rebase o&#x2F;main side1git rebase side1 side2git rebase side2 side3git rebase side3 maingit push2.合并远程仓库git checkout maingit pullgit merge side1git merge side2git merge side3git push3.远程追踪git checkout -b side o&#x2F;maingit commitgit pull --rebasegit push4.git push的参数git push origin maingit push origin foo5.git push的参数2git push origin main^:foogit push origin foo:main6.git fetch的参数git fetch origin main~1:foogit fetch origin foo:maingit checkout foogit merge main7.没有source的sourcegit fetch origin :bargit push origin :foo8.git pull的参数git pull origin bar:foogit pull origin main:side---<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>]]></content>
      
      
      <categories>
          
          <category> git </category>
          
      </categories>
      
      
        <tags>
            
            <tag> git基础 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>你好</title>
      <link href="/2023/03/25/chy/"/>
      <url>/2023/03/25/chy/</url>
      
        <content type="html"><![CDATA[<h1 id="hexo-theme-matery"><a href="#hexo-theme-matery" class="headerlink" title="hexo-theme-matery"></a>hexo-theme-matery</h1><p><a href="http://hits.dwyl.io/blinkfox/hexo-theme-matery"><img src="http://hits.dwyl.io/blinkfox/hexo-theme-matery.svg" alt="HitCount"></a> <a href="https://gitter.im/hexo-theme-matery/Lobby?utm_source=badge"><img src="https://img.shields.io/gitter/room/blinkfox/hexo-theme-matery.svg" alt="Gitter"></a> <a href="https://github.com/blinkfox/hexo-theme-matery/issues"><img src="https://img.shields.io/github/issues/blinkfox/hexo-theme-matery.svg" alt="GitHub issues"></a> <a href="https://github.com/blinkfox/hexo-theme-matery/blob/master/LICENSE"><img src="https://img.shields.io/github/license/blinkfox/hexo-theme-matery.svg" alt="GitHub license"></a> <a href="https://codeload.github.com/blinkfox/hexo-theme-matery/zip/master"><img src="https://img.shields.io/badge/downloads-master-green.svg" alt="Download"></a> <a href="http://hexo.io/"><img src="https://img.shields.io/badge/hexo-%3E%3D%205.0.0-blue.svg" alt="Hexo Version"></a> <a href="https://github.com/blinkfox/hexo-theme-matery/network"><img src="https://img.shields.io/github/forks/blinkfox/hexo-theme-matery.svg" alt="GitHub forks"></a> <a href="https://github.com/blinkfox/hexo-theme-matery/stargazers"><img src="https://img.shields.io/github/stars/blinkfox/hexo-theme-matery.svg" alt="GitHub stars"></a></p><p><a href="README.md">🇺🇸English Document</a> | <a href="http://blinkfox.com/">国内访问示例 (http://blinkfox.com)</a> | <a href="https://blinkfox.github.io/">Github 部署演示示例 (https://blinkfox.github.io)</a> </p><p>QQ 交流群1（已满）: <a href="https://jq.qq.com/?_wv=1027&k=5zMDYHT"><code>926552981</code></a> | QQ 交流群2（已满）: <a href="https://jq.qq.com/?_wv=1027&k=53q2Ayp"><code>971887688</code></a> | QQ 交流群3（推荐）: <a href="https://qm.qq.com/cgi-bin/qm/qr?k=fC1-kU-_aTn4q-JQq4GsYKr4WcKdgfGa&jump_from=webapi"><code>670694035</code></a></p><blockquote><p>这是一个采用 <code>Material Design</code> 和响应式设计的 Hexo 博客主题。</p></blockquote><h2 id="特性"><a href="#特性" class="headerlink" title="特性"></a>特性</h2><ul><li>简单漂亮，文章内容美观易读</li><li><a href="https://material.io/">Material Design</a> 设计</li><li>响应式设计，博客在桌面端、平板、手机等设备上均能很好的展现</li><li>首页轮播文章及每天动态切换 <code>Banner</code> 图片</li><li>瀑布流式的博客文章列表（文章无特色图片时会有 <code>24</code> 张漂亮的图片代替）</li><li>时间轴式的归档页</li><li><strong>词云</strong>的标签页和<strong>雷达图</strong>的分类页</li><li>丰富的关于我页面（包括关于我、文章统计图、我的项目、我的技能、相册等）</li><li>可自定义的数据的友情链接页面</li><li>支持文章置顶和文章打赏</li><li>支持 <code>MathJax</code></li><li>支持中文繁简转换</li><li><code>TOC</code> 目录</li><li>可设置复制文章内容时追加版权信息</li><li>可设置阅读文章时做密码验证</li><li><a href="https://gitalk.github.io/">Gitalk</a>、<a href="https://imsun.github.io/gitment/">Gitment</a>、<a href="https://valine.js.org/">Valine</a> 和 <a href="https://disqus.com/">Disqus</a> 评论模块（推荐使用 <code>Gitalk</code>）</li><li>集成了<a href="http://busuanzi.ibruce.info/">不蒜子统计</a>、谷歌分析（<code>Google Analytics</code>）和文章字数统计等功能</li><li>支持在首页的音乐播放和视频播放功能</li><li>支持<code>emoji</code>表情，用<code>markdown emoji</code>语法书写直接生成对应的能<strong>跳跃</strong>的表情。</li><li>支持 <a href="http://www.daovoice.io/">DaoVoice</a>、<a href="https://www.tidio.com/">Tidio</a> 在线聊天功能。</li></ul><table><thead><tr><th>表头</th><th>表头</th></tr></thead><tbody><tr><td>单元格</td><td>单元格</td></tr><tr><td>单元格</td><td>单元格</td></tr></tbody></table><table><thead><tr><th align="left">左对齐</th><th align="right">右对齐</th><th align="center">居中对齐</th></tr></thead><tbody><tr><td align="left">单元格</td><td align="right">单元格</td><td align="center">单元格</td></tr><tr><td align="left">单元格</td><td align="right">单元格</td><td align="center">单元格</td></tr></tbody></table><h2 id="贡献者"><a href="#贡献者" class="headerlink" title="贡献者"></a>贡献者</h2><p>感谢下面列出的贡献者，没有他们，hexo-theme-matery 不会这么完美。</p><ul><li><a href="https://github.com/HarborZeng">@HarborZeng</a></li><li><a href="https://github.com/shw2018">@shw2018</a></li><li><a href="https://github.com/L1cardo">@L1cardo</a></li><li><a href="https://github.com/Five-great">@Five-great</a></li></ul><h2 id="下载"><a href="#下载" class="headerlink" title="下载"></a>下载</h2><p>本主题<strong>推荐你使用 Hexo 5.0.0 及以上的版本</strong>。如果，你已经有一个自己的 <a href="https://hexo.io/zh-cn/">Hexo</a> 博客了，建议你将 Hexo 升级到最新稳定的版本。</p><p>点击 <a href="https://codeload.github.com/blinkfox/hexo-theme-matery/zip/master">这里</a> 下载 <code>master</code> 分支的最新稳定版的代码，解压缩后，将 <code>hexo-theme-matery</code> 的文件夹复制到你 Hexo 的 <code>themes</code> 文件夹中即可。</p><p>当然你也可以在你的 <code>themes</code> 文件夹下使用 <code>git clone</code> 命令来下载:</p><pre class="line-numbers language-bash" data-language="bash"><code class="language-bash"><span class="token function">git</span> clone https://github.com/blinkfox/hexo-theme-matery.git<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><h2 id="配置"><a href="#配置" class="headerlink" title="配置"></a>配置</h2><h3 id="切换主题"><a href="#切换主题" class="headerlink" title="切换主题"></a>切换主题</h3><p>修改 Hexo 根目录下的 <code>_config.yml</code> 的  <code>theme</code> 的值：<code>theme: hexo-theme-matery</code></p><h4 id="config-yml-文件的其它修改建议"><a href="#config-yml-文件的其它修改建议" class="headerlink" title="_config.yml 文件的其它修改建议:"></a><code>_config.yml</code> 文件的其它修改建议:</h4><ul><li>请修改 <code>_config.yml</code> 的 <code>url</code> 的值为你的网站主 <code>URL</code>（如：<code>http://xxx.github.io</code>）。</li><li>建议修改两个 <code>per_page</code> 的分页条数值为 <code>6</code> 的倍数，如：<code>12</code>、<code>18</code> 等，这样文章列表在各个屏幕下都能较好的显示。</li><li>如果你是中文用户，则建议修改 <code>language</code> 的值为 <code>zh-CN</code>。</li></ul><h3 id="新建分类-categories-页"><a href="#新建分类-categories-页" class="headerlink" title="新建分类 categories 页"></a>新建分类 categories 页</h3><p><code>categories</code> 页是用来展示所有分类的页面，如果在你的博客 <code>source</code> 目录下还没有 <code>categories/index.md</code> 文件，那么你就需要新建一个，命令如下：</p><pre class="line-numbers language-bash" data-language="bash"><code class="language-bash">hexo new page <span class="token string">"categories"</span><span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>编辑你刚刚新建的页面文件 <code>/source/categories/index.md</code>，至少需要以下内容：</p><pre class="line-numbers language-yaml" data-language="yaml"><code class="language-yaml"><span class="token punctuation">---</span><span class="token key atrule">title</span><span class="token punctuation">:</span> categories<span class="token key atrule">date</span><span class="token punctuation">:</span> <span class="token datetime number">2018-09-30 17:25:30</span><span class="token key atrule">type</span><span class="token punctuation">:</span> <span class="token string">"categories"</span><span class="token key atrule">layout</span><span class="token punctuation">:</span> <span class="token string">"categories"</span><span class="token punctuation">---</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title></title>
      <link href="/2023/03/23/hello-world/"/>
      <url>/2023/03/23/hello-world/</url>
      
        <content type="html"><![CDATA[<p>Welcome to <a href="https://hexo.io/">Hexo</a>! This is your very first post. Check <a href="https://hexo.io/docs/">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href="https://hexo.io/docs/troubleshooting.html">troubleshooting</a> or you can ask me on <a href="https://github.com/hexojs/hexo/issues">GitHub</a>.</p><h2 id="Quick-Start"><a href="#Quick-Start" class="headerlink" title="Quick Start"></a>Quick Start</h2><h3 id="Create-a-new-post"><a href="#Create-a-new-post" class="headerlink" title="Create a new post"></a>Create a new post</h3><pre class="line-numbers language-bash" data-language="bash"><code class="language-bash">$ hexo new <span class="token string">"My New Post"</span><span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>More info: <a href="https://hexo.io/docs/writing.html">Writing</a></p><h3 id="Run-server"><a href="#Run-server" class="headerlink" title="Run server"></a>Run server</h3><pre class="line-numbers language-bash" data-language="bash"><code class="language-bash">$ hexo server<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>More info: <a href="https://hexo.io/docs/server.html">Server</a></p><h3 id="Generate-static-files"><a href="#Generate-static-files" class="headerlink" title="Generate static files"></a>Generate static files</h3><pre class="line-numbers language-bash" data-language="bash"><code class="language-bash">$ hexo generate<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>More info: <a href="https://hexo.io/docs/generating.html">Generating</a></p><h3 id="Deploy-to-remote-sites"><a href="#Deploy-to-remote-sites" class="headerlink" title="Deploy to remote sites"></a>Deploy to remote sites</h3><pre class="line-numbers language-bash" data-language="bash"><code class="language-bash">$ hexo deploy<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>More info: <a href="https://hexo.io/docs/one-command-deployment.html">Deployment</a></p>]]></content>
      
      
      
    </entry>
    
    
  
  
</search>
