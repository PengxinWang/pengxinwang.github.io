<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>图像生成：目标放置</title>
      <link href="/2024/06/27/object-place/"/>
      <url>/2024/06/27/object-place/</url>
      
        <content type="html"><![CDATA[<h1 id="阅读论文"><a href="#阅读论文" class="headerlink" title="阅读论文"></a>阅读论文</h1><h2 id="一-TopNet-Transformer-based-Object-Placement-Network-for-Image-Compositing-CVPR2023"><a href="#一-TopNet-Transformer-based-Object-Placement-Network-for-Image-Compositing-CVPR2023" class="headerlink" title="一.TopNet: Transformer-based Object Placement Network for Image Compositing(CVPR2023)"></a>一.TopNet: Transformer-based Object Placement Network for Image Compositing(CVPR2023)</h2><p><a href="https://www.semanticscholar.org/paper/TopNet%3A-Transformer-Based-Object-Placement-Network-Zhu-Lin/9e09cb666982d407d434a3ce8d56d8bcb6eb4e38#citing-papers">论文链接</a></p><p><a href="https://blog.csdn.net/qq_41994006/article/details/134592288">参考链接</a></p><h3 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h3><p>作者调研自动放置目标到背景进行图像合成的问题。<strong>提供背景图、分割的目标</strong>，&#x3D;&#x3D;训练模型预测合理放置信息（位置及尺寸）&#x3D;&#x3D;。<em>当前工作主要是生成候选框或者使用滑窗搜索，但是不能在背景图中建模局部信息</em>。本文通过transformer学习目标特征与所有局部背景特征之间相关性。稀疏对比损失用于进一步训练模型。通过网络前向生成3D heatmap表明所有合理位置&#x2F;尺度组合。训练时可以使用具体标注也可使用现有inpaint模型，已超过SOTA方法。用户研究表明训练的模型可泛化到真实图片。</p><br/><h3 id="引言"><a href="#引言" class="headerlink" title="引言"></a>引言</h3><p><img src="/pic/9b892e0137735285ccd5c26347c0bb37.png" alt="截图"></p><p>如图1，</p><ul><li>现有方法【26】直接预测多个变换或边界框，表明提供目标的位置和尺度，但仅推荐top，不提供其他可能位置及尺度。</li><li>现有方法【29】使用检索模型评估给定位置的合理性，并以滑动窗口的方式评估位置和尺度的网格，这导致推理速度慢。</li><li>本文作者提出的TopNet，<strong>将目标放置转化为稠密点预测问题</strong>：通过一次网络前向生成<strong>包含位置、尺度的稠密网格评估</strong>。之前方法仅在全局层级结合前景及背景，而TopNet学习全局前景特征与局部背景特征之间相关性，可高效评估所有可能放置位置。<br>作者训练TopNet时仅提供一个边界框，因此<strong>使用稀疏对比损失</strong>，真值位置&#x2F;尺寸有一个相对高的得分，同时最小化其他组合及比真值得分高的组合，通过在预测的<strong>3D热度图上寻找局部最大值生成候选边界框位置</strong>。</li></ul><h4 id="贡献"><a href="#贡献" class="headerlink" title="贡献"></a>贡献</h4><ul><li>1、一种新颖的基于transformer的结构建模目标图与来自背景图的局部信息之间相关关系；</li><li>2、稀疏对比损失训练稠密预测网络；</li><li>3、在inpaint数据集和标定数据集充分实验验证达到SOTA</li></ul><h3 id="算法"><a href="#算法" class="headerlink" title="算法"></a>算法</h3><h3 id="架构"><a href="#架构" class="headerlink" title="架构"></a>架构</h3><p>提供一张背景$I_b$及前景$I_o$，模型预测的3D热度图$H\in{\mathbb{R}^{h_b\times w_b\times c}} $，$c &#x3D; 16$，表示尺度值s，0.15-0.9，间隔0.05，每个空间位置与放置边界框的中心有关。<br>推理时，首先对$H$归一化，寻找top-1或top-k候选框。</p><p><img src="/pic/8e6e82f044a770d26467b28bc4b8eb39.png" alt="截图"></p><p>如图2，使用两个编码器学习背景和目标特征，为确定特定位置的目标尺寸是否合适，</p><ul><li>背景图中局部信息可提供细节信息，因此保留来自背景encoder中最后一个卷积层或transformer层的局部特征&#x2F;token；</li><li>对于前景相对简单，保留全局特征。</li><li>使用多层transformer学习目标全局特征与背景局部特征之间相关性，class token替换为目标全局特征，最后一层所有patch token送入上采样decoder；对于transformer降采样后的特征进行concat及reshape，而后经过4个卷积层进行上采样。</li></ul><h3 id="损失函数"><a href="#损失函数" class="headerlink" title="损失函数"></a>损失函数</h3><blockquote><p>对密集定位和尺度评价的主要挑战是稀疏的监督信号。</p></blockquote><ul><li>CAIS[27]或OPA[12]中的小尺度标注仅为每个样本提供一个正位置边界框，没有对其他位置&#x2F;尺度的监督。</li><li>对于没有显式注释的大规模数据集，生成监督的一种方法是屏蔽背景图像中的原始对象，并使用现成的手绘模型生成纯背景图像。然后可以考虑原目标的边界框作为ground-truth的放置，但是监督仍然是稀疏的——只有一个位置和尺度。</li></ul><blockquote><p>简单的思路。</p></blockquote><p>监督模型的一个简单想法是为每个位置尺度组合(即3D热图中的每个数据点)分配一个真实值。简单二元赋值将唯一的ground-truth组合(GT数据点)考虑为1，所有其他位置尺度组合考虑为0。</p><p>更平滑的分配是高斯分配，它根据每个数据点与三维空间中地面真值点之间的距离给出分数。它考虑分数的局部性，即接近地面真相的位置&#x2F;尺度仍然应该是很好的候选人。这些分配将所有远离地面真相的位置&#x2F;尺度视为低分的负点。然而，这种假设在大多数情况下并不成立。给定一个特定的背景场景，某些物体可以在许多不同尺度的位置兼容。</p><p><strong>我们假设背景图像中存在多个良好的候选边界框，并提出在ground-truth位置&#x2F;尺度上最大化分数，同时在其他位置&#x2F;尺度上允许具有高分的局部峰值</strong>,假设三维热图$H\in{\mathbb{R}^{h_b\times w_b\times c}} $中的groundtruth坐标为$(x_{gt},y_{gt},z_{gt})$。第一个损失项表示为:</p><p><img src="/pic/00791df0f793ad2ca914199e9ba6067e.png" alt="截图"></p><p>真值处得分最大，其中对真值附近的点$M$为0，其余位置为0.1</p><p><img src="/pic/621001dbe8bb7cd669e683b6f311f058.png" alt="截图"></p><p>鼓励最低得分为0，因为对于某些背景和物体图像，总是存在不良的位置或比例，防止模型预测所有位置&#x2F;尺度的高分。</p><p><img src="/pic/d39ddd38c0a0c29e82478a314e80fd8a.png" alt="截图"></p><h3 id="评估"><a href="#评估" class="headerlink" title="评估"></a>评估</h3><ul><li>TOP-K IOU。将top5 IOU作为评估指标，即在基本事实和前5个预测边界框之间的最佳IOU。</li><li>归一化分数。对于具有热图分数的方法，ground-truth可能不一定要有最高分，但它应该是最好的。因此，我们应用标准化分数作为评估指标之一。首先将热图得分归一化，最小值和最大值分别为3.1节中的H -，称为归一化得分(normalized score, NS)。地真位置&#x2F;比例尺的NS可能不为1，但与其他位置和比例尺相比，NS应该相对较高。因此，**我们计算平均NS和高于一定阈值的NS部分(例如0.9)**。当仅评估位置时，NS比IOU更合理，因为小的空间移动可能导致IOU为0。</li></ul><h4 id="跟SOTA进行对比"><a href="#跟SOTA进行对比" class="headerlink" title="跟SOTA进行对比"></a>跟SOTA进行对比</h4><p><strong>Regression</strong>表示直接预测真值框；<br><strong>†Retrieval</strong>表示通过检索寻找合理位置；<br><strong>Classifier</strong>表示通过分类器预测合成图是否合理，为检索方案的进一步扩充；<br><strong>PlaceNet</strong>表示通过对抗训练预测框是否合理。<br>表1展示top-5 IOU评估结果，该方法远超过现有方法，同时速度也比较快；</p><p><img src="/pic/2e74c49debca041eeabe03bf0e433f89.png" alt="截图"></p><h2 id="二-DiffPop-Plausibility-Guided-Object-Placement-Diffusion-for-Image-Composition"><a href="#二-DiffPop-Plausibility-Guided-Object-Placement-Diffusion-for-Image-Composition" class="headerlink" title="二.DiffPop: Plausibility-Guided Object Placement Diffusion for Image Composition"></a>二.DiffPop: Plausibility-Guided Object Placement Diffusion for Image Composition</h2><p><a href="https://www.semanticscholar.org/paper/DiffPop%3A-Plausibility-Guided-Object-Placement-for-Liu-Zhou/16f3a46289caa44f18d0d8035a51154aabfc5048">论文链接</a></p><h3 id="摘要-1"><a href="#摘要-1" class="headerlink" title="摘要"></a>摘要</h3><p>作者解决了逼真图像合成的挑战性任务中<strong>可信对象放置的问题</strong>。提出了DiffPop框架，这是第一个<strong>利用似然引导的去噪扩散概率模型</strong>来学习多个目标和相应场景图像之间的尺度和空间关系的框架。首先，训练了一个非引导扩散模型，以自监督的方式直接学习物体的放置参数。然后，开发了一个人在环Pipeline，<strong>利用人对扩散生成的合成图像进行标记，为训练结构合理性分类器提供弱监督</strong>。该分类器进一步用于引导扩散采样过程产生合理的目标放置。实验结果验证了该方法在新的cityscape - op数据集和公共OPA数据集上生成可信和多样化的合成图像的优越性，并展示了其在数据增强和多目标放置任务等应用中的潜力。</p><h3 id="引言-1"><a href="#引言-1" class="headerlink" title="引言"></a>引言</h3><h4 id="目标放置"><a href="#目标放置" class="headerlink" title="目标放置"></a>目标放置</h4><p>图像合成包括通过将特定的前景对象与背景图像相结合来创建看起来逼真的合成图像。在计算机视觉领域，Niu等人[NCL  21]将图像合成分为四个分支:<strong>物体放置、图像混合、图像协调和阴影生成</strong>。这些分支解决了图像合成过程中遇到的各种挑战。在本文中，我们重点研究了目标放置任务，该任务旨在为<strong>前景目标确定合适的比例和位置</strong>。</p><p>传统的目标放置方法采用明确的规则为前景目标找到合适的位置和尺度。</p><p>另一方面，基于学习的物体放置方法通常预测或生成仿射变换矩阵，以确定前景物体在背景图像上的位置和比例。</p><ul><li><p>Lin等人[LYW  18]介绍了一种新的GAN架构，该架构利用空间Transformer网络(STN)作为生成器，根据生成的转换参数对前景对象进行转换，从而生成逼真的合成图像。这种基于深度学习的方法极大地推动了物体放置领域的发展。</p></li><li><p>Lee等人[LLG18]提出了一种端到端vae - gan，通过自监督和无监督训练生成对象的变换矩阵和形状。该方法降低了GAN训练过程中模式崩溃的风险，在后续工作中得到了广泛的应用。</p></li><li><p>Tripathi等人[TCA 19]在GAN训练过程中加入了一个额外的判别器网络，以促进下游任务的目标数据增强。</p></li><li><p>Zhang等[ZWM∗20]利用通过预训练的实例分割和图像绘制方法获得的自监督数据对来确保目标放置的多样性。</p></li><li><p>Liu等人[LLZ 21]创建了一个名为OPA的专用对象放置数据集，并引入了SimOPA分类器来评估对象放置。</p></li><li><p>Zhou等人[ZLNZ22]将目标放置问题转化为图节点补全任务，利用二值分类损失训练判别器网络，充分利用了带标签的负样本。</p></li><li><p>SAC-GAN [ZMZ  22]结合了目标和背景图像的边缘和语义信息，以提高合成结果的结构一致性。</p></li><li><p><strong>TopNet [ZLC23]提出使用transformer来学习目标特征与局部背景特征之间的关系，从而改进了目标尺度和位置的生成。</strong></p><h4 id="扩散模型"><a href="#扩散模型" class="headerlink" title="扩散模型"></a>扩散模型</h4></li><li><p>Ho等[HJA20]引入了去噪扩散概率模型(DDPM)，这是一种生成模型，通过不断向真实样本中添加噪声并使用网络去噪来优化网络。这种方法使网络能够生成真实的样本。</p></li><li><p>Song等[SME20]对扩散模型进行了改进，提高了采样速度。</p></li><li><p>Nichol等[ND21]进一步增强了扩散模型生成高质量样品的能力。</p></li><li><p>对于条件图像合成，Dharival等[DN21]通过在扩散模型的采样过程中加入分类器引导来提高采样质量。他们利用分类器的梯度来平衡生成样本的多样性和合理性。</p></li><li><p>Liu等[LPA * 23]引入了一个统一的语义扩散引导框架，允许通过语言、图像或两者进行引导。</p></li><li><p>Ho等人[HS22]联合训练条件扩散模型和无条件扩散模型，将得到的条件和无条件分数估计值结合起来，实现样本质量和多样性之间的平衡。这种方法将扩散模型从分类器引导采样的限制中解放出来。</p></li><li><p>Nichol等人[NDR * 21]提出了GLIDE模型，该模型能够生成以文本为条件的高质量图像。他们证明无条件指导在基于语言的条件反射中优于CLIP指导。</p></li><li><p>Ramesh等人[RDN∗22]提出了unCLIP，它利用CLIP的特征空间和扩散模型以零射击的方式从文本描述生成图像。</p></li><li><p>撒哈拉等人[SCS * 22]引入了Imagen，这是一个框架，它结合了大型Transformer语言模型和扩散模型，使网络具有从文本提示生成图像的能力。</p></li><li><p>Robin等人[RBL * 22]将扩散模型直接应用于潜在空间，大大节省了文本到图像生成的计算资源。</p></li><li><p>[HZO * 23]扩散模型在图像合成领域的应用。他们迭代地将背景图像中的上下文信息注入到插入的前景对象中，从而可以控制前景对象的变化程度。</p></li></ul><p>与上述方法相比，我们专注于可信性引导的对象放置，即我们<strong>首先基于非引导扩散模型产生的弱注释图像训练一个可信性分类器，然后使用分类器指导扩散采样过程以产生可信的结果</strong>。</p><h4 id="贡献-1"><a href="#贡献-1" class="headerlink" title="贡献"></a>贡献</h4><ul><li>我们提出了DiffPop，这是第一个以合理性为导向的扩散框架，旨在为图像构图生成合理的对象放置。具体来说，我们学习了一个结构合理性分类器，为基于扩散的物体放置生成过程提供指导。</li><li>我们采用human-in-the-loop策略获得图像级弱监督来训练可信性分类器。我们创建了一个新的数据集cityscenes - op，该数据集可以用于训练似然引导扩散模型，用于在比OPA数据集更复杂和结构背景的场景中放置物体。</li><li>实验结果表明，我们的方法在cityscape - op和OPA数据集的可行性和多样性方面实现了最先进的对象放置性能。我们的方法在创建用于数据增强和多对象放置的合成图像方面也显示出有希望的结果。</li></ul><p>与上述方法相比，我们的方法基于扩散模型，提供了多样性和稳定的训练。我们的方法<strong>有效地利用正样本和负样本来指导扩散模型生成更合理的尺度和位置</strong>，从而形成逼真的图像。此外，我们的导引扩散框架还可以扩展到<strong>同时放置多个物体</strong>，这是以前的方法无法实现的。</p><h3 id="算法-1"><a href="#算法-1" class="headerlink" title="算法"></a>算法</h3><p>给定一个场景图像作为背景，一个物体斑块作为前景，我们寻求学习物体放置的比例和空间分布，从而获得逼真的图像构图。我们的DiffPop框架的训练管道包含两个阶段，如图2所示:</p><ul><li>在阶段1中，训练一个无引导的物体放置去噪扩散模型来学习物体尺度和位置的分布;</li><li>在阶段2中，使用先前训练模型生成的人工标记的合成图像训练结构可信性分类器。</li></ul><p>在推理时，如图3所示，给定背景图像和目标patch，我们<strong>使用分类器引导扩散生成二维变换(尺度和位置)以合理放置目标，并采用复制-粘贴方案进行图像合成</strong>。</p><p><img src="/pic/2ff12f8cef2ccffb7c3dbea79646854a.png" alt="截图"></p><p><img src="/pic/00a87d764703dd3a43adb638d7fb99f2.png" alt="截图"></p><br/><p>生成的效果</p><p><img src="/pic/fa99b388f8de8c87c1dd1ebeb7bc9541.png" alt="截图"></p><h4 id="无引导的对象放置去噪扩散"><a href="#无引导的对象放置去噪扩散" class="headerlink" title="无引导的对象放置去噪扩散"></a>无引导的对象放置去噪扩散</h4><p>首先训练一个无引导的去噪扩散模型，用于学习给定数据集中<strong>对象尺度和位置</strong>的分布。</p><p><strong>扩散过程</strong>。前向扩散过程是一个预定义的马尔可夫链，它对对象放置$x\in{\mathbb{R}^D} $ 进行操作，其中 x &#x3D; [s, v,h]，s 是对象-图像对的相对尺度，(v,h) 是垂直和对象相对于图像的水平偏移，如图 2（上图）所示。为了启动扩散过程，我们<strong>从从底层分布 q(x0) 采样的干净对象放置 x0 开始。然后，我们逐渐将高斯噪声添加到 x0，从而产生一系列中间对象放置变量 x1:T，遵循线性增加噪声方差的预定时间表，表示为 β1:T</strong>。扩散过程的联合分布 q(xt |xt−1) 表述为：</p><p><img src="/pic/e498c6787469821ebcf38f0db648355e.png" alt="截图"></p><p><strong>去噪过程</strong>。去噪过程也称为生成过程，被参数化为具有可学习反向高斯转换的马尔可夫链。给定从标准多元高斯分布采样的噪声对象放置，表示为 xT ∼N(0, I)，作为初始状态。<strong>目标是在每个时间步纠正每个状态 xt，使用学习到的高斯转移 pθ (xt−1|xt ) 生成更清晰的版本 xt−1。这种转变由表示为 θ 的可学习网络决定</strong>。通过迭代地应用这个相反的过程直到达到最大步数T，获得代表期望的干净对象放置的最终状态x0。生成过程的联合分布表示为 pθ (x0:T)，表达如下：</p><p><img src="/pic/902a9f9d2149a2181d48c964c64fe3df.png" alt="截图"></p><p>其中参数 µθ (xt ,t) 和 Σθ (xt ,t) 分别表示 xt−1 高斯分布的预测均值和协方差。这些参数是通过将xt作为去噪网络θ的输入获得的。为简单起见，我们为 Σθ (xt ,t) 设置预定义常数，如 DDPM [HJA20] 中所示。随后，根据贝叶斯定理，可以通过减去预测噪声来重新参数化 µθ (xt ,t)：</p><p><img src="/pic/ce83c8e0876b1a4836510d01756d7e19.png" alt="截图"></p><p><strong>网络训练目标</strong>。我们从图像中提取真实的对象位置，以自我监督的方式训练我们的位置去噪网络。网络 θ 是一个简单的 4 层 MLP，输入和输出大小为 N×3。我们根据 DDPM [HJA20] 的 ϵ 预测来训练网络，损失为 ℓ2：</p><p><img src="/pic/8ec100f30394892f72d9b0a6c714ccff.png" alt="截图"></p><h4 id="合理性引导的对象放置扩散"><a href="#合理性引导的对象放置扩散" class="headerlink" title="合理性引导的对象放置扩散"></a>合理性引导的对象放置扩散</h4><p>由于无引导扩散模型仅学习对象放置分布，因此它<strong>没有考虑场景级结构一致性</strong>，并且可能无法根据给定对象和场景图像生成合理的放置。受到[DN21]中分类器引导的条件生成的启发，我们训练了<strong>一个基于带注释的结构合理性的分类器</strong>，并使用其梯度来引导扩散采样过程实现场<strong>景级结构一致性</strong>。为了训练这样的合理性分类器：</p><ul><li>我们要么从现有数据集中获取现有的正&#x2F;负标签（例如 OPA），</li><li>要么采用人机交互策略来解决合理性测量的注释。</li></ul><p><strong>人机交互的合理性标签</strong>。像 Cityscapes 这样的现有数据集最初并不是为对象放置任务而设计的，导致缺乏用于训练对象组合网络的真实注释。尽管自监督训练方案 [ZWM<em>20,ZMZ</em>22] 可用于从正例中学习对象放置分布，但对于训练二元分类器进行合理性测量所必需的负例通常会丢失。为了解决这个问题，我们采用人机交互策略，<strong>根据图像的合理性和真实性标准，手动为无引导对象放置扩散模型生成的合成图像分配正标签和负标签</strong>，如下所示如图2（下）所示。简而言之，正标签意味着插入的对象和背景场景之间结构一致，并且整体图像是合理的，反之亦然。这些人工注释可以为学习合理性分类器提供必要的弱监督，该分类器可以测量无引导扩散模型的结果，并进一步用于指导扩散采样过程。&#x3D;&#x3D;需要人工&#x3D;&#x3D;</p><p><strong>结构合理性分类器</strong>。为了引导扩散模型生成合理的对象放置，我们训练了结构合理性分类器 Cs 并使用其梯度来指导扩散采样过程。 Cs 简单地定义为 ResNet-18 主干二元分类器，旨在判断场景级别合成图像的结构合理性。分类器将<strong>语义场景布局</strong>与<strong>对象掩模</strong>相结合作为<strong>输入</strong>，并以监督方式使用手动注释的正&#x2F;负标签进行训练。对于输入场景的语义布局，我们直接利用数据集提供的语义图并将其处理为二进制掩码，而每个掩码对应一个类别。为了从输入对象掩码和处理后的二进制场景布局中获得复合布局，我们使用空间变换网络（STN）[JSZ*15]提出的空间扭曲以及由无引导扩散模型生成的仿射变换矩阵 At 来变换 2D 对象块， 在哪里</p><p><img src="/pic/eb3a3ab8357e7a8d8c51b5f1723bbdb0.png" alt="截图"></p><p><strong>分类器使用非引导扩散的结果进行独立训练，并将在引导扩散过程中被冻结。</strong></p><br/><p><strong>分类器引导的扩散</strong>。一旦分类器 Cs 训练完毕，我们就使用<strong>它的梯度来指导对象放置扩散模型的采样过程</strong>（图 4）。具体来说，采样公式定义如下：</p><p><img src="/pic/05d00c7aa3646603410fe5d8bb1125de.png" alt="截图"></p><br/><p>其中，λ是引导尺度因子，决定分类器Cs的梯度对扩散模型采样的影响程度，φ是Cs的参数。具体来说，我们根据时间 t 的采样结果生成一个变换，并利用它来变换给定的对象块并将其与目标背景布局相结合。然后，将复合布局（详细信息请参见第 4.6 节）输入到 Cs 中以获得合理性得分，即二元分类的概率。我们计算分数的梯度。 t时刻的采样结果，梯度用于指导t时刻t−1的采样结果。经过T次迭代后，可以得到最终的位置x0&#x3D;[s0,h0,v0]。最后，我们使用(s0,h0,v0)形成的仿射变换矩阵A0（见式8）对物体进行变换，并将变换后的物体粘贴到背景图像上以获得合成图像。</p><p><img src="/pic/83de10faf4e66f2e5fd677ea81f7cf9a.png" alt="截图"></p><h3 id="实验"><a href="#实验" class="headerlink" title="实验"></a>实验</h3><p><img src="/pic/4bd70f7e2ca6ebdf640b5a02b612ac69.png" alt="截图"></p><p><img src="/pic/895acf45f3c73a9e60568edf320344e0.png" alt="截图"></p><p><img src="/pic/ca8102ed0c2fcbb5a1f62e10cdec2b9e.png" alt="截图"></p><h2 id="三-Learning-to-Place-Objects-into-Scenes-by-Hallucinating-Scenes-around-Objects"><a href="#三-Learning-to-Place-Objects-into-Scenes-by-Hallucinating-Scenes-around-Objects" class="headerlink" title="三.Learning to Place Objects into Scenes by Hallucinating Scenes around Objects"></a>三.Learning to Place Objects into Scenes by Hallucinating Scenes around Objects</h2><h3 id="摘要-2"><a href="#摘要-2" class="headerlink" title="摘要"></a>摘要</h3><p>修改图像以将新对象添加到场景中的能力是一个强大的图像编辑控件。然而，现有的基于扩散的图像编辑方法不支持对象插入。核心挑战是在给定场景图像的情况下预测物体在场景中的位置。为了解决这个问题，我们提出了DreamPlace，这是一个两步的方法，它将给定类的对象插入到图像中:1)预测对象在图像中的可能位置，2)在该位置对对象进行绘制。我们只使用合成数据来训练我们的物体放置模型，利用基于扩散的图像绘制来产生围绕给定物体的场景的新图像。DreamPlace使用它的学习放置模型，可以产生比类似的基于扩散的基线在质量上更真实的对象插入编辑。<strong>此外，对于存在基准注释的有限对象类别集，我们的学习对象放置模型尽管完全是在生成的数据上训练的，但与在大型手动注释数据集(&gt;80k注释样本)上训练的最先进的监督方法相比，其对象放置的准确性要高出35%。</strong></p><br/><h3 id="引言-2"><a href="#引言-2" class="headerlink" title="引言"></a>引言</h3><p>由于缺乏对对象放置位置的自动控制，控制基于扩散的图像生成的现有方法在执行对象插入编辑的能力方面受到限制。(见图1)。最先进的基于迭代编辑的范例(3)也经常忽略向现有场景添加对象的命令，特别是当对象在图像中很小的时候。其他编辑方法(29;24)在插入对象方面表现得更好，但他们假设了明确的指导，即在何处绘制对象及其方向和形状(例如，遮罩)。</p><p>我们引入DreamPlace，这是一种修改图像以包含给定类的新对象的算法(例如，在包含桌子的场景图像中添加笔记本电脑，或者在描绘咖啡桌的图像中添加杯子)。DreamPlace将问题分为两个阶段:1)学习一个模型，该模型预测给定类的对象可能属于图像中的哪个位置(对象应该“放置”在场景中的哪个位置);2)利用基于扩散的图像绘制来生成新的图像像素，在预测位置描绘物体，同时确保与周围场景环境的视觉和谐。</p><h4 id="动机思路"><a href="#动机思路" class="headerlink" title="动机思路"></a>动机思路</h4><p>在这种设置中，挑战在于:仅给定场景的图像，我们如何预测物体的位置?我们的见解是，尽管现有的文本引导扩散模型不直接支持对象附加编辑，但它们在互联网规模的数据中进行了训练(25)，在更大的场景背景下，已经学会了对合理对象放置的强大先验。给定我们希望添加的对象类的图像，我们通过使用基于扩散的图像绘制来产生包含该对象的可信完整场景的幻觉图像，从而利用这一先验。然后，我们使用这些生成的图像作为一个大而多样的训练数据集，通过弱监督学习来学习对象放置预测器。换句话说，我们通过使用现成的扩散模型在物体周围放置场景来学习在场景中放置物体。</p><p><img src="/pic/cef7c3ab0610f38fcbcb87bfb5c4bdf6.png" alt="截图"></p><p>我们将DreamPlace与基于扩散的基线和对象放置基线进行比较。使用其学习对象放置指导，DreamPlace在一系列常见的桌面、室内、街景和空中对象类(第4.1节)上生成比先前基于扩散的方法在质量上更逼真的对象插入编辑。此外，对于具有标记评估数据的一小部分对象类别(桌面对象，如杯子，笔记本电脑等)，我们预测的对象放置(纯粹从合成训练数据中学习)在数量上优于先前在大型人类注释放置数据集上训练的最先进的模型(13)。DreamPlace实现了跨桌面对象类别的面积交叉点(IoA)指标平均增加25.2%，从可信表面到预测边界框的平均位移(µD)平均提高35.1%(第4.2节)。</p><br/><h3 id="相关工作"><a href="#相关工作" class="headerlink" title="相关工作"></a>相关工作</h3><p><strong>文本引导图像扩散模型</strong>(21;20;22)能够合成高质量的图像和最近的扩展，例如(2;1;29日;30;24;28)旨在控制扩散模型。这些方法能够<strong>在给定明确位置或布局的情况下将对象添加到场景中</strong>;他们这样做是通过绘画(21;2;28日;29日;11)或以掩模、层或分割的形式进行空间调节(29;1;24;30)。然而，当给定一个没有明确输入在哪里(以及如何)添加对象的场景时，它们并不能解决问题。例如，美术师需要绘制编辑的形状和位置。这很重要，因为当对象编辑的目标是为其他下游任务生成或增加数据时，要求人工指定添加对象的位置是不可伸缩的。**(3)支持基于指令的图像编辑，但文本指定的编辑在插入新对象时表现不佳(图1)**。我们的目标是使用扩散模型启用对象插入编辑，但让系统完全决定对象的放置位置和方式。</p><p>类似的目标插入任务在计算机视觉文献中也有研究。前景目标搜索(32;34)用于图像合成，检索给定图像、指定位置(例如，放置框)和可放置对象的数据库的语义兼容和正确放置的对象。物体放置(对于2D图像)是预测图像中物体可能到达的合理位置的任务。方法包<strong>括监督(33;27个;31日;15;35)和弱监督方法(34;5)</strong>.前者使用人类注释的数据集(如OPA(13))进行训练。</p><p>然而，与前景对象搜索和绘制不同，OPA(13)不模拟对象的几何形状或姿态;因此，虽然在OPA上训练的模型可以学习到餐具(如盘子)经常出现在桌面上，但最终的合成结果通常是不现实的(图7)。我们观察到，在OPA上训练的模型对更多样化和更真实的场景的泛化能力很差;<strong>OPA中的标签通常是低质量的或有偏差的(详见图6)<strong>，这突出了手动收集数据的困难。弱监督方法采用暴力处理大型图像数据集(18;7)为了找到包含感兴趣的对象和足够的场景上下文的相关图像(34)，或者他们在有限的领域工作，如街道图像(31;5). DreamPlace的不同之处在于，它为一组特定的对象类生成图像数据集;这意味着DreamPlace不需要明确地预处理或过滤数以百万计的图像，也不需要将自己限制在容易获得宽场景图像的领域。然而，</strong>DreamPlace确实依赖于预训练的扩散模型来生成足够可信的训练场景</strong>。</p><p>DreamPlace与(23;26日;6)利用扩散模型合成训练数据，用于分类或图像相似任务。Instruct Pix2Pix(3)使用大型语言模型(LLM)(4)和Stable Diffusion(21)生成图像编辑示例的配对数据集。DreamPlace采用了与(3)类似的方法，但针对对象插入任务的挑战。</p><h3 id="算法-2"><a href="#算法-2" class="headerlink" title="算法"></a>算法</h3><p>DreamPlace使用两步流程将给定类别的对象插入给定的输入图像。</p><ul><li>首先，放置网络P分析空场景图像，并提出物体的粗略位置和比例，表示为边界框(3.1节)。</li><li>使用预训练的Stable Diffusion (21) in-painting模型(我们称之为D)生成的合成数据来训练P，然后使用D来合成(inpaint)所提出的边界框中的新对象(第3.2节)。我们将描述DreamPlace的关键细节，并将完整的实现细节委托给附录A。</li></ul><h4 id="放置模型"><a href="#放置模型" class="headerlink" title="放置模型"></a>放置模型</h4><p>给定一个描述场景的图像，放置模块的目标是在图像中提出一个表示粗放置指令的边界框。由于对象类的列表是已知的，我们为每个类生成数据，并为每个对象类训练独立的、专门的放置模型。我们训练放置网络P，在给定RGB图像$x\in{\mathbb{R}^{128\times 128\times 3}} $和二进制掩码$m\in{0,1}^{128\times128} $的情况下预测可信度评分，该掩码为提议对象编码一个边界框。P是ResNet-18，具有4个输入通道- x和m连接-并使用正负对进行训练;也就是说，<strong>一个掩码编码一个可信的盒子vs一个随机生成的盒子</strong>。训练中使用的损失是二元交叉熵，这在分类任务中很常见。</p><p>为了在测试时推断整个图像的位置，我们使用网格和不同的框尺度(类似于(34))生成m的候选点，并将预测的分数聚合到2D热图中。然后，<strong>我们根据盒子的置信度得分和该区域热图的平均值的加权和对盒子进行排序</strong>。得分最高的方框是预测的可信区域。A.1节提供了进一步的细节和直觉。</p><h5 id="训练数据集的生成"><a href="#训练数据集的生成" class="headerlink" title="训练数据集的生成"></a>训练数据集的生成</h5><p>为了训练P，我们合成了一个由<strong>图像和相应的物体位置</strong>(位置和尺度)组成的大型训练对数据集。从互联网或现有数据集中获得这样的图像是具有挑战性的(34)，这激发了我们的生成方法。</p><p>我们使用文本-图像扩散绘画模型D，它以图像$x\in{\mathbb{R}^{512\times 512\times 3}} $，二进制绘画掩码$m\in{0,1}^{512\times512} $和文本提示符作为输入。作为启动映像，我们下载少量实例分割的种子对象(例如，50台笔记本电脑)。这些对象不需要场景背景，并且很容易从任何主要的网络搜索引擎中抓取。我们将对象和它的分割蒙版随机粘贴在一个512 × 512的画布上，并使用D来绘制蒙版外面的像素。描述性文本提示有助于扩散模型生成真实连贯的图像;</p><br/><p>因此，我们查询一个大语言模型(LLM)(16)，以获取定义对象类及其上下文的关系的文本提示:例如，“桌子上的笔记本电脑”。给定上述输入，D生成一张包含种子对象的512 × 512画图。最后，我们使用LaMa(12)来移除种子对象(通过inpainting)。最终的修复图像和种子对象的边界框掩码被用作放置网络的伪标记训练对x和m。</p><p><img src="/pic/5e7506cd1a004cec948bb3b4a60a2cfb.png" alt="截图"></p><br/><p>图3演示了完整的数据生成管道。我们对<strong>不同的文本提示、种子对象和随机生成的画布</strong>重复此过程，以合成一个大型训练集。我们发现提高生成图像质量的一个小优化是逐步淘汰上下文(例如，淘汰两次);第一轮外画生成中等宽度的上下文，第二轮在最终图像中生成剩余的上下文。由于每轮外画只需要在最终图像中合成一小部分像素，因此它避免了D忽略种子对象并在外画区域太大时生成新的大对象的常见失败情况(参见附录中图5中的示例)。</p><br/><h4 id="使用文本引导图像扩散的对象合成"><a href="#使用文本引导图像扩散的对象合成" class="headerlink" title="使用文本引导图像扩散的对象合成"></a>使用文本引导图像扩散的对象合成</h4><p>给定由P产生的建议对象放置边界框，我们使用扩散模型D(通过文本引导的绘制)在建议框(x1, y1, x2, y2坐标)内合成所需类别的对象。这是通过将提案框填充50%，裁剪并使用d绘制未填充的区域来完成的。文本提示符指定要放置的类别;例如，要添加一台笔记本电脑，提示符将是“一台笔记本电脑”。图4显示了插入对象的示例。我们发现，在粗糙的放置指导下，稳定扩散(D)能够在保留场景其余部分的同时，对合理放置和协调的物体进行涂漆。</p><p><img src="/pic/6adaab1fd7719f0243dba1009bed05c5.png" alt="截图"></p><h3 id="实验-1"><a href="#实验-1" class="headerlink" title="实验"></a>实验</h3><p>我们根据<strong>基于扩散的基线</strong>(第4.1节)以及<strong>最先进的对象放置基线</strong>(在OPA(13)上进行培训)评估DreamPlace放置对象的能力;4.2节)。</p><h4 id="基于扩散方法的定性比较"><a href="#基于扩散方法的定性比较" class="headerlink" title="基于扩散方法的定性比较"></a>基于扩散方法的定性比较</h4><p>我们将DreamPlace与两个简单的稳定扩散(SD)绘制基线和directtpix2pix(3)进行比较。首先，我们考虑使用SD绘制覆盖大部分图像的大框-即让SD决定位置。其次，我们考虑SD给定一个随机选择的位置和规模。这两个原始的基线证实了SD本身缺乏放置对象的规划能力，并且随机盒子不太可能是合理的放置。指导Pix2Pix(3)是最近的方法，为有针对性的图像编辑使用文本;我们表明，我们表明，对对象放置进行编辑也是不够的，这也是本文的重点。</p><p>我们在办公室、客厅、城市街道等真实世界场景的各种互联网图像上测试了这些方法。我们选择了常见的对象作为评估类别，包括笔记本电脑、碗、杯子和键盘等桌面对象；汽车和行人等街景对象；以及绘画、钟表和鸟等其他类别。这些类别可以由稳定扩散生成，并且通常只占场景的很小一部分。</p><p>图4显示了定性结果；扩展结果见附录中的图12-13)。使用SD修复大区域会显著修改场景，生成通常不再描述原始场景的新图像。随机放置会导致插画对象位于不太可能的位置且大小不正确。当提示描述添加到场景的对象级别时，指令Pix2Pix失败。DreamPlace引导的修复能够在合理位置添加小型对象，同时仍保留场景上下文。</p><h4 id="目标放置基线的定性比较"><a href="#目标放置基线的定性比较" class="headerlink" title="目标放置基线的定性比较"></a>目标放置基线的定性比较</h4><p>我们还将DreamPlace与最近发布的对象放置基线进行了定量比较：**Terse(27)、PlaceNet(31)和GracoNet(33)**。这些方法预测给定场景图像的位置和比例(例如，框)，并且直接在OPA(13)数据集上训练。GALA(34)等弱监督方法的代码和数据尚未公开提供。</p><br/><p>我们的评估数据集包括桌面对象放置的100个场景和四种类型的对象(笔记本电脑、键盘、杯子和碗)。为了为场景的放置产生一个地面真实标签来计算定量度量，我们<strong>手动地为对象在这些图像上的放置标注看似合理的表面</strong>。附录中的图9显示了测试场景和标签的示例。由于无法列举评估数据集中所有好的放置框，因此我们<strong>使用两个自动但客观的指标来量化性能</strong>。虽然这并不能充分说明好的排名，但在这些指标上表现不佳意味着排名不佳。</p><ul><li>指标1：面积上的交点。我们希望一个好的对象放置与可行区域有面积相交-即放置在曲面上。较小的交叉点或没有交叉点将意味着该对象是浮动的。<strong>我们不关心对象未占用的曲面面积。</strong></li><li>指标2：平均移动量。假设一个物体不在合理的区域内。然后，我们测量<strong>它与最近的合理区域的接近程度</strong>：即移动位置所需的距离。我们通过从预测的对象边界框的底边到场景中任何可能的表面的平均距离(以图像维度的分数表示)来近似这一点。</li><li>指标 3：视觉质量评估。我们<strong>在不同的放置建议方法的指导下对 SD 生成的图像进行定性评估</strong>。该指标是合理的输出图像的百分比，由两个标准定义：(1) 新对象是否与表面合理交互，(2) 对象是否具有合理的比例。表 2 列出了 DreamPlace 和基线 (27; 31; 33) 的结果。我们还包括随机安置建议作为基线。总体而言，DreamPlace 生成的合格图像比例较高。由于 DreamPlace 提出了更准确的编辑区域，因此在对象插入期间也减少了场景级别的更改。示例和分析请参见附录中的图10。</li></ul><br/><p>我们与在整个 OPA 数据集上训练并按类别专门训练的基线进行比较。 DreamPlace 与后者最具可比性，因为我们专门针对每个对象类，但为了完整性我们将两者都包含在内。结果如表 1 所示。总体而言，DreamPlace 生成更加本地化和准确的放置位置，在这两个指标上都优于所有先前的方法。图 10 根据 DreamPlace 生成的基线和热图推断出的边界框。</p><p><img src="/pic/196438d76a5d2d5f2f8b14725235e443.png" alt="截图"></p><p>对于端到端评估，我们测试了 Stable Diffusion 在给定来自 DreamPlace 和基线的建议边界框的情况下修复最终对象的能力；我们预计糟糕的提案也会妨碍合成该对象的能力。</p><h3 id="讨论"><a href="#讨论" class="headerlink" title="讨论"></a>讨论</h3><p>DreamPlace 是一个概念证明，<strong>表明现有的文本图像扩散模型可以用作弱标记数据集生成器和对象合成器来执行对象放置任务</strong>。我们确定了未来工作的一些改进措施。</p><ul><li>首先，可以扩展对象集。目前，DreamPlace 需要一个实例分段器来为数据生成提供对象种子；零样本模型（10）将支持任意对象。</li><li>其次，更新的对象生成方法 (29) 可以用作稳定扩散 (21) 的替代品。</li><li>第三，DreamPlace 并未针对速度和效率进行优化——热图生成成本高昂。最近的对象放置文献 (35) 的优化或训练通用的多类 DreamPlace 模型可以提供效率增益。</li></ul>]]></content>
      
      
      <categories>
          
          <category> DL </category>
          
      </categories>
      
      
        <tags>
            
            <tag> CV </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>面试总结</title>
      <link href="/2024/05/25/mianshi/"/>
      <url>/2024/05/25/mianshi/</url>
      
        <content type="html"><![CDATA[<h1 id="百度一面问题"><a href="#百度一面问题" class="headerlink" title="百度一面问题"></a>百度一面问题</h1><h2 id="问题一：介绍一下openvins，以及它如何进行定位的"><a href="#问题一：介绍一下openvins，以及它如何进行定位的" class="headerlink" title="问题一：介绍一下openvins，以及它如何进行定位的"></a>问题一：介绍一下openvins，以及它如何进行定位的</h2><h3 id="OpenVINS-介绍"><a href="#OpenVINS-介绍" class="headerlink" title="OpenVINS 介绍"></a>OpenVINS 介绍</h3><p><strong>OpenVINS</strong> 是一种开源的视觉惯性导航系统（Visual-Inertial Navigation System，VINS），由University of Delaware开发。OpenVINS旨在提供一个高性能、模块化的VINS框架，用于机器人和无人机的实时定位和导航。它结合了视觉和惯性传感器的数据，通过非线性优化算法实现高精度的姿态估计和轨迹跟踪。</p><h3 id="OpenVINS-的定位方法"><a href="#OpenVINS-的定位方法" class="headerlink" title="OpenVINS 的定位方法"></a>OpenVINS 的定位方法</h3><p>OpenVINS 的定位主要依赖于视觉和惯性传感器数据的融合。它通过以下几个主要步骤实现精确的定位和导航：</p><ol><li><strong>传感器数据预处理（Sensor Data Preprocessing）</strong></li><li><strong>状态估计（State Estimation）</strong></li><li><strong>优化与滤波（Optimization and Filtering）</strong></li><li><strong>关键帧管理（Keyframe Management）</strong></li><li><strong>回环检测与闭环优化（Loop Closure and Relocalization）</strong></li></ol><h3 id="1-传感器数据预处理"><a href="#1-传感器数据预处理" class="headerlink" title="1. 传感器数据预处理"></a>1. 传感器数据预处理</h3><p>在传感器数据预处理中，OpenVINS会对视觉传感器（如单目或双目相机）和惯性传感器（如IMU）的数据进行同步和校准：</p><ul><li><strong>时间同步（Time Synchronization）</strong>：确保从不同传感器采集的数据在同一时间戳上进行处理。</li><li><strong>校准（Calibration）</strong>：使用预先标定的相机内参和IMU参数，校正传感器数据中的畸变和误差。</li></ul><h3 id="2-状态估计"><a href="#2-状态估计" class="headerlink" title="2. 状态估计"></a>2. 状态估计</h3><p>OpenVINS使用扩展卡尔曼滤波器（Extended Kalman Filter，EKF）进行状态估计。状态向量通常包括位置、速度、姿态以及IMU的偏置。</p><ul><li><p><strong>状态向量</strong>：(\mathbf{x} &#x3D; [\mathbf{p}_w, \mathbf{v}_w, \mathbf{q}_w, \mathbf{b}_a, \mathbf{b}_g])</p><ul><li>(\mathbf{p}_w)：位置</li><li>(\mathbf{v}_w)：速度</li><li>(\mathbf{q}_w)：姿态</li><li>(\mathbf{b}_a)、(\mathbf{b}_g)：IMU的加速度计和陀螺仪偏置</li></ul></li><li><p><strong>预测步骤（Prediction Step）</strong>：</p><ul><li>使用IMU数据，根据运动模型预测下一个时刻的状态和协方差。</li></ul></li><li><p><strong>更新步骤（Update Step）</strong>：</p><ul><li>使用视觉传感器的数据，更新状态和协方差矩阵。通常，通过提取特征点和匹配这些特征点来实现。</li></ul></li></ul><h3 id="3-优化与滤波"><a href="#3-优化与滤波" class="headerlink" title="3. 优化与滤波"></a>3. 优化与滤波</h3><p>OpenVINS结合滑动窗口优化（Sliding Window Optimization）和扩展卡尔曼滤波（EKF），进行状态估计和轨迹优化。</p><ul><li><strong>滑动窗口优化</strong>：<ul><li>在滑动窗口内，使用非线性优化方法（如Levenberg-Marquardt）最小化重投影误差，优化当前帧和关键帧的状态。</li><li>重投影误差：图像中观察到的特征点与3D特征点投影位置之间的差距。</li></ul></li></ul><h3 id="4-关键帧管理"><a href="#4-关键帧管理" class="headerlink" title="4. 关键帧管理"></a>4. 关键帧管理</h3><p>在关键帧管理中，OpenVINS根据视差变化和特征点的分布选择关键帧。</p><ul><li><strong>关键帧选择（Keyframe Selection）</strong>：<ul><li>当视差变化超过一定阈值或观察到的新特征点数量达到一定标准时，选择当前帧作为关键帧。</li><li>关键帧用于长期的状态估计和轨迹优化。</li></ul></li></ul><h3 id="5-回环检测与闭环优化"><a href="#5-回环检测与闭环优化" class="headerlink" title="5. 回环检测与闭环优化"></a>5. 回环检测与闭环优化</h3><p>回环检测用于检测机器人是否回到了之前经过的位置，从而消除累积误差。</p><ul><li><p><strong>回环检测（Loop Closure Detection）</strong>：</p><ul><li>使用特征描述子（如ORB特征）在历史关键帧中进行匹配，检测到回环时进行闭环优化。</li></ul></li><li><p><strong>闭环优化（Loop Closure Optimization）</strong>：</p><ul><li>使用全局优化方法（如Pose Graph Optimization）调整整个轨迹，消除累积误差，提升定位精度。</li></ul></li></ul><h3 id="OpenVINS-的代码结构"><a href="#OpenVINS-的代码结构" class="headerlink" title="OpenVINS 的代码结构"></a>OpenVINS 的代码结构</h3><p>OpenVINS 的代码结构模块化，主要包括以下几个部分：</p><ul><li><strong>Sensor Manager</strong>：处理传感器数据的输入和预处理。</li><li><strong>Estimator</strong>：实现EKF和滑动窗口优化，进行状态估计和轨迹优化。</li><li><strong>Visualizer</strong>：可视化工具，展示系统的运行状态和轨迹。</li><li><strong>Utility</strong>：辅助函数和工具，包括数学运算、数据存储等。</li></ul><h3 id="示例代码"><a href="#示例代码" class="headerlink" title="示例代码"></a>示例代码</h3><p>以下是一个简化的示例，展示如何使用OpenVINS进行定位：</p><pre class="line-numbers language-cpp" data-language="cpp"><code class="language-cpp"><span class="token macro property"><span class="token directive-hash">#</span><span class="token directive keyword">include</span> <span class="token string">&lt;ros/ros.h></span></span><span class="token macro property"><span class="token directive-hash">#</span><span class="token directive keyword">include</span> <span class="token string">"ov_msckf/MSCKF.hpp"</span></span><span class="token macro property"><span class="token directive-hash">#</span><span class="token directive keyword">include</span> <span class="token string">"ov_core/imu_buffer.hpp"</span></span><span class="token macro property"><span class="token directive-hash">#</span><span class="token directive keyword">include</span> <span class="token string">"ov_core/feature_buffer.hpp"</span></span><span class="token keyword">int</span> <span class="token function">main</span><span class="token punctuation">(</span><span class="token keyword">int</span> argc<span class="token punctuation">,</span> <span class="token keyword">char</span><span class="token operator">*</span><span class="token operator">*</span> argv<span class="token punctuation">)</span> <span class="token punctuation">&#123;</span>    ros<span class="token double-colon punctuation">::</span><span class="token function">init</span><span class="token punctuation">(</span>argc<span class="token punctuation">,</span> argv<span class="token punctuation">,</span> <span class="token string">"openvins_node"</span><span class="token punctuation">)</span><span class="token punctuation">;</span>    ros<span class="token double-colon punctuation">::</span>NodeHandle nh<span class="token punctuation">;</span>    <span class="token comment">// 创建MSCKF实例</span>    ov_msckf<span class="token double-colon punctuation">::</span>MSCKF msckf<span class="token punctuation">;</span>    <span class="token comment">// 配置IMU和相机参数</span>    msckf<span class="token punctuation">.</span><span class="token function">loadParameters</span><span class="token punctuation">(</span>nh<span class="token punctuation">)</span><span class="token punctuation">;</span>    <span class="token comment">// 订阅IMU和图像数据</span>    ros<span class="token double-colon punctuation">::</span>Subscriber imu_sub <span class="token operator">=</span> nh<span class="token punctuation">.</span><span class="token function">subscribe</span><span class="token punctuation">(</span><span class="token string">"imu"</span><span class="token punctuation">,</span> <span class="token number">1000</span><span class="token punctuation">,</span> <span class="token operator">&amp;</span>ov_msckf<span class="token double-colon punctuation">::</span>MSCKF<span class="token double-colon punctuation">::</span>imuCallback<span class="token punctuation">,</span> <span class="token operator">&amp;</span>msckf<span class="token punctuation">)</span><span class="token punctuation">;</span>    ros<span class="token double-colon punctuation">::</span>Subscriber img_sub <span class="token operator">=</span> nh<span class="token punctuation">.</span><span class="token function">subscribe</span><span class="token punctuation">(</span><span class="token string">"camera/image_raw"</span><span class="token punctuation">,</span> <span class="token number">1000</span><span class="token punctuation">,</span> <span class="token operator">&amp;</span>ov_msckf<span class="token double-colon punctuation">::</span>MSCKF<span class="token double-colon punctuation">::</span>imageCallback<span class="token punctuation">,</span> <span class="token operator">&amp;</span>msckf<span class="token punctuation">)</span><span class="token punctuation">;</span>    <span class="token comment">// 运行ROS循环</span>    ros<span class="token double-colon punctuation">::</span><span class="token function">spin</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">;</span>    <span class="token keyword">return</span> <span class="token number">0</span><span class="token punctuation">;</span><span class="token punctuation">&#125;</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><p>OpenVINS 是一个功能强大且灵活的视觉惯性导航系统，通过结合视觉和惯性传感器的数据，实现高精度的定位和导航。其核心技术包括传感器数据预处理、状态估计、优化与滤波、关键帧管理和回环检测与闭环优化。了解和掌握OpenVINS的原理和实现，可以为从事机器人和无人机导航的开发人员提供强有力的工具。</p><h2 id="问题二：手写kmeans聚类算法"><a href="#问题二：手写kmeans聚类算法" class="headerlink" title="问题二：手写kmeans聚类算法"></a>问题二：手写kmeans聚类算法</h2><p>当然，可以手写一个简单的K-means聚类算法。K-means是一种常用的无监督学习算法，用于将数据集分成K个簇。它的基本思想是通过最小化簇内数据点到簇中心的距离平方和来进行聚类。</p><h3 id="K-means聚类算法的步骤"><a href="#K-means聚类算法的步骤" class="headerlink" title="K-means聚类算法的步骤"></a>K-means聚类算法的步骤</h3><ol><li><strong>初始化：</strong> 随机选择K个初始簇中心（也叫质心）。</li><li><strong>分配：</strong> 对于数据集中的每个点，计算它与每个簇中心的距离，并将它分配给最近的簇。</li><li><strong>更新：</strong> 重新计算每个簇的簇中心，即计算簇内所有点的平均值。</li><li><strong>重复：</strong> 重复步骤2和3，直到簇中心不再变化或达到最大迭代次数。</li></ol><h3 id="Python实现"><a href="#Python实现" class="headerlink" title="Python实现"></a>Python实现</h3><p>以下是一个使用Python实现的K-means聚类算法：</p><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">import</span> numpy <span class="token keyword">as</span> np<span class="token keyword">class</span> <span class="token class-name">KMeans</span><span class="token punctuation">:</span>    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> k<span class="token operator">=</span><span class="token number">3</span><span class="token punctuation">,</span> max_iter<span class="token operator">=</span><span class="token number">100</span><span class="token punctuation">,</span> tol<span class="token operator">=</span><span class="token number">1e-4</span><span class="token punctuation">)</span><span class="token punctuation">:</span>        self<span class="token punctuation">.</span>k <span class="token operator">=</span> k        self<span class="token punctuation">.</span>max_iter <span class="token operator">=</span> max_iter        self<span class="token punctuation">.</span>tol <span class="token operator">=</span> tol    <span class="token keyword">def</span> <span class="token function">fit</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> X<span class="token punctuation">)</span><span class="token punctuation">:</span>        n_samples<span class="token punctuation">,</span> n_features <span class="token operator">=</span> X<span class="token punctuation">.</span>shape        <span class="token comment"># 随机初始化质心</span>        np<span class="token punctuation">.</span>random<span class="token punctuation">.</span>seed<span class="token punctuation">(</span><span class="token number">42</span><span class="token punctuation">)</span>  <span class="token comment"># 保证结果可重复</span>        random_indices <span class="token operator">=</span> np<span class="token punctuation">.</span>random<span class="token punctuation">.</span>choice<span class="token punctuation">(</span>n_samples<span class="token punctuation">,</span> self<span class="token punctuation">.</span>k<span class="token punctuation">,</span> replace<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">)</span>        self<span class="token punctuation">.</span>centroids <span class="token operator">=</span> X<span class="token punctuation">[</span>random_indices<span class="token punctuation">]</span>        <span class="token keyword">for</span> i <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span>self<span class="token punctuation">.</span>max_iter<span class="token punctuation">)</span><span class="token punctuation">:</span>            <span class="token comment"># 分配每个点到最近的质心</span>            self<span class="token punctuation">.</span>labels <span class="token operator">=</span> self<span class="token punctuation">.</span>_assign_clusters<span class="token punctuation">(</span>X<span class="token punctuation">)</span>            <span class="token comment"># 计算新的质心</span>            new_centroids <span class="token operator">=</span> self<span class="token punctuation">.</span>_compute_centroids<span class="token punctuation">(</span>X<span class="token punctuation">)</span>            <span class="token comment"># 检查质心是否收敛</span>            <span class="token keyword">if</span> np<span class="token punctuation">.</span><span class="token builtin">all</span><span class="token punctuation">(</span>np<span class="token punctuation">.</span><span class="token builtin">abs</span><span class="token punctuation">(</span>new_centroids <span class="token operator">-</span> self<span class="token punctuation">.</span>centroids<span class="token punctuation">)</span> <span class="token operator">&lt;</span> self<span class="token punctuation">.</span>tol<span class="token punctuation">)</span><span class="token punctuation">:</span>                <span class="token keyword">break</span>            self<span class="token punctuation">.</span>centroids <span class="token operator">=</span> new_centroids    <span class="token keyword">def</span> <span class="token function">_assign_clusters</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> X<span class="token punctuation">)</span><span class="token punctuation">:</span>        distances <span class="token operator">=</span> self<span class="token punctuation">.</span>_compute_distances<span class="token punctuation">(</span>X<span class="token punctuation">)</span>        <span class="token keyword">return</span> np<span class="token punctuation">.</span>argmin<span class="token punctuation">(</span>distances<span class="token punctuation">,</span> axis<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span>    <span class="token keyword">def</span> <span class="token function">_compute_centroids</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> X<span class="token punctuation">)</span><span class="token punctuation">:</span>        centroids <span class="token operator">=</span> np<span class="token punctuation">.</span>zeros<span class="token punctuation">(</span><span class="token punctuation">(</span>self<span class="token punctuation">.</span>k<span class="token punctuation">,</span> X<span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">)</span>        <span class="token keyword">for</span> i <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span>self<span class="token punctuation">.</span>k<span class="token punctuation">)</span><span class="token punctuation">:</span>            points_in_cluster <span class="token operator">=</span> X<span class="token punctuation">[</span>self<span class="token punctuation">.</span>labels <span class="token operator">==</span> i<span class="token punctuation">]</span>            centroids<span class="token punctuation">[</span>i<span class="token punctuation">]</span> <span class="token operator">=</span> np<span class="token punctuation">.</span>mean<span class="token punctuation">(</span>points_in_cluster<span class="token punctuation">,</span> axis<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">)</span>        <span class="token keyword">return</span> centroids    <span class="token keyword">def</span> <span class="token function">_compute_distances</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> X<span class="token punctuation">)</span><span class="token punctuation">:</span>        distances <span class="token operator">=</span> np<span class="token punctuation">.</span>zeros<span class="token punctuation">(</span><span class="token punctuation">(</span>X<span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">,</span> self<span class="token punctuation">.</span>k<span class="token punctuation">)</span><span class="token punctuation">)</span>        <span class="token keyword">for</span> i <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span>self<span class="token punctuation">.</span>k<span class="token punctuation">)</span><span class="token punctuation">:</span>            distances<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span> i<span class="token punctuation">]</span> <span class="token operator">=</span> np<span class="token punctuation">.</span>linalg<span class="token punctuation">.</span>norm<span class="token punctuation">(</span>X <span class="token operator">-</span> self<span class="token punctuation">.</span>centroids<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token punctuation">,</span> axis<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span>        <span class="token keyword">return</span> distances    <span class="token keyword">def</span> <span class="token function">predict</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> X<span class="token punctuation">)</span><span class="token punctuation">:</span>        <span class="token keyword">return</span> self<span class="token punctuation">.</span>_assign_clusters<span class="token punctuation">(</span>X<span class="token punctuation">)</span><span class="token comment"># 示例使用</span><span class="token keyword">if</span> __name__ <span class="token operator">==</span> <span class="token string">"__main__"</span><span class="token punctuation">:</span>    <span class="token keyword">from</span> sklearn<span class="token punctuation">.</span>datasets <span class="token keyword">import</span> make_blobs    <span class="token keyword">import</span> matplotlib<span class="token punctuation">.</span>pyplot <span class="token keyword">as</span> plt    <span class="token comment"># 生成示例数据</span>    X<span class="token punctuation">,</span> y <span class="token operator">=</span> make_blobs<span class="token punctuation">(</span>n_samples<span class="token operator">=</span><span class="token number">300</span><span class="token punctuation">,</span> centers<span class="token operator">=</span><span class="token number">4</span><span class="token punctuation">,</span> cluster_std<span class="token operator">=</span><span class="token number">0.60</span><span class="token punctuation">,</span> random_state<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">)</span>    <span class="token comment"># 训练KMeans模型</span>    kmeans <span class="token operator">=</span> KMeans<span class="token punctuation">(</span>k<span class="token operator">=</span><span class="token number">4</span><span class="token punctuation">)</span>    kmeans<span class="token punctuation">.</span>fit<span class="token punctuation">(</span>X<span class="token punctuation">)</span>    <span class="token comment"># 预测簇标签</span>    labels <span class="token operator">=</span> kmeans<span class="token punctuation">.</span>predict<span class="token punctuation">(</span>X<span class="token punctuation">)</span>    <span class="token comment"># 可视化结果</span>    plt<span class="token punctuation">.</span>scatter<span class="token punctuation">(</span>X<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">,</span> X<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">,</span> c<span class="token operator">=</span>labels<span class="token punctuation">,</span> s<span class="token operator">=</span><span class="token number">50</span><span class="token punctuation">,</span> cmap<span class="token operator">=</span><span class="token string">'viridis'</span><span class="token punctuation">)</span>    plt<span class="token punctuation">.</span>scatter<span class="token punctuation">(</span>kmeans<span class="token punctuation">.</span>centroids<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">,</span> kmeans<span class="token punctuation">.</span>centroids<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">,</span> s<span class="token operator">=</span><span class="token number">200</span><span class="token punctuation">,</span> c<span class="token operator">=</span><span class="token string">'red'</span><span class="token punctuation">,</span> marker<span class="token operator">=</span><span class="token string">'x'</span><span class="token punctuation">)</span>    plt<span class="token punctuation">.</span>show<span class="token punctuation">(</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h3 id="代码说明"><a href="#代码说明" class="headerlink" title="代码说明"></a>代码说明</h3><ol><li><p><strong>初始化参数：</strong></p><ul><li><code>k</code>：簇的数量。</li><li><code>max_iter</code>：最大迭代次数。</li><li><code>tol</code>：容差，用于判断质心是否收敛。</li></ul></li><li><p><strong>fit方法：</strong></p><ul><li>随机选择K个初始质心。</li><li>在每次迭代中，首先根据当前质心将每个点分配到最近的质心，然后计算新的质心，最后检查质心是否收敛。</li></ul></li><li><p><strong>_assign_clusters方法：</strong></p><ul><li>计算每个点到每个质心的距离，并返回距离最近的质心索引。</li></ul></li><li><p><strong>_compute_centroids方法：</strong></p><ul><li>计算每个簇的质心，即簇内所有点的平均值。</li></ul></li><li><p><strong>_compute_distances方法：</strong></p><ul><li>计算每个点到每个质心的欧几里得距离。</li></ul></li><li><p><strong>predict方法：</strong></p><ul><li>使用训练好的模型对新数据进行簇标签预测。</li></ul></li></ol><h3 id="总结-1"><a href="#总结-1" class="headerlink" title="总结"></a>总结</h3><p>以上代码实现了一个简单的K-means聚类算法，能够对数据集进行聚类分析。通过这种方式，你可以理解K-means的基本原理和实现步骤。对于大规模数据集，可以进一步优化和扩展该算法，例如使用Mini-Batch K-means等变种算法。</p><h2 id="问题三：对于目前主流的一些三维目标检测网络有哪些，从输入数据的类型进行划分"><a href="#问题三：对于目前主流的一些三维目标检测网络有哪些，从输入数据的类型进行划分" class="headerlink" title="问题三：对于目前主流的一些三维目标检测网络有哪些，从输入数据的类型进行划分"></a>问题三：对于目前主流的一些三维目标检测网络有哪些，从输入数据的类型进行划分</h2><p>三维目标检测是自动驾驶、机器人和计算机视觉领域的关键任务。三维目标检测网络根据输入数据的类型可以分为以下几类：</p><ol><li><strong>基于点云（Point Cloud-based）</strong></li><li><strong>基于RGB-D图像（RGB-D Image-based）</strong></li><li><strong>基于多视图（Multi-view Image-based）</strong></li><li><strong>基于融合（Fusion-based）</strong></li></ol><h3 id="1-基于点云的三维目标检测网络"><a href="#1-基于点云的三维目标检测网络" class="headerlink" title="1. 基于点云的三维目标检测网络"></a>1. 基于点云的三维目标检测网络</h3><p><strong>输入数据类型：</strong> LiDAR点云</p><h4 id="代表性网络："><a href="#代表性网络：" class="headerlink" title="代表性网络："></a>代表性网络：</h4><ul><li><p><strong>PointNet&#x2F;PointNet++</strong></p><ul><li><strong>特点：</strong> 使用点的坐标作为输入，通过对每个点进行特征提取，再进行全局特征聚合。</li><li><strong>示例论文：</strong> “PointNet: Deep Learning on Point Sets for 3D Classification and Segmentation” (CVPR 2017)</li></ul></li><li><p><strong>VoxelNet</strong></p><ul><li><strong>特点：</strong> 将点云数据划分成体素，然后在体素中进行三维卷积操作，提取体素特征。</li><li><strong>示例论文：</strong> “VoxelNet: End-to-End Learning for Point Cloud Based 3D Object Detection” (CVPR 2018)</li></ul></li><li><p><strong>SECOND (Sparsely Embedded Convolutional Detection)</strong></p><ul><li><strong>特点：</strong> 使用稀疏卷积网络对体素化的点云进行处理，提高了计算效率和检测精度。</li><li><strong>示例论文：</strong> “SECOND: Sparsely Embedded Convolutional Detection” (Sensors 2018)</li></ul></li><li><p><strong>PointRCNN</strong></p><ul><li><strong>特点：</strong> 直接对点云进行操作，使用PointNet++提取点特征，通过RPN网络生成候选框，并进行分类和回归。</li><li><strong>示例论文：</strong> “PointRCNN: 3D Object Proposal Generation and Detection from Point Cloud” (CVPR 2019)</li></ul></li></ul><h3 id="2-基于RGB-D图像的三维目标检测网络"><a href="#2-基于RGB-D图像的三维目标检测网络" class="headerlink" title="2. 基于RGB-D图像的三维目标检测网络"></a>2. 基于RGB-D图像的三维目标检测网络</h3><p><strong>输入数据类型：</strong> RGB图像和深度图</p><h4 id="代表性网络：-1"><a href="#代表性网络：-1" class="headerlink" title="代表性网络："></a>代表性网络：</h4><ul><li><p><strong>Vote3Deep</strong></p><ul><li><strong>特点：</strong> 使用基于深度学习的方法处理RGB-D数据，通过对深度图进行体素化，并进行三维卷积操作。</li><li><strong>示例论文：</strong> “Vote3Deep: Fast Object Detection in 3D Point Clouds Using Efficient Convolutional Neural Networks” (IROS 2016)</li></ul></li><li><p><strong>Frustum PointNets</strong></p><ul><li><strong>特点：</strong> 先使用2D检测网络在RGB图像上检测物体，再在深度图中生成视锥体区域，使用PointNet对该区域进行三维检测。</li><li><strong>示例论文：</strong> “Frustum PointNets for 3D Object Detection from RGB-D Data” (CVPR 2018)</li></ul></li></ul><h3 id="3-基于多视图的三维目标检测网络"><a href="#3-基于多视图的三维目标检测网络" class="headerlink" title="3. 基于多视图的三维目标检测网络"></a>3. 基于多视图的三维目标检测网络</h3><p><strong>输入数据类型：</strong> 多视角RGB图像</p><h4 id="代表性网络：-2"><a href="#代表性网络：-2" class="headerlink" title="代表性网络："></a>代表性网络：</h4><ul><li><p><strong>MV3D (Multi-View 3D Networks)</strong></p><ul><li><strong>特点：</strong> 将来自不同视角（鸟瞰图、前视图、侧视图等）的特征进行融合，通过多视角特征提升检测精度。</li><li><strong>示例论文：</strong> “MV3D: Multi-View 3D Object Detection Network” (CVPR 2017)</li></ul></li><li><p><strong>MVTN (Multi-View Transformation Network)</strong></p><ul><li><strong>特点：</strong> 使用一个可学习的变换网络生成多视图表示，融合这些表示进行三维目标检测。</li><li><strong>示例论文：</strong> “Learning Multi-View 3D Object Detection Network for Autonomous Driving” (ICCV 2019)</li></ul></li></ul><h3 id="4-基于融合的三维目标检测网络"><a href="#4-基于融合的三维目标检测网络" class="headerlink" title="4. 基于融合的三维目标检测网络"></a>4. 基于融合的三维目标检测网络</h3><p><strong>输入数据类型：</strong> 多模态数据（如LiDAR点云和RGB图像）</p><h4 id="代表性网络：-3"><a href="#代表性网络：-3" class="headerlink" title="代表性网络："></a>代表性网络：</h4><ul><li><p><strong>AVOD (Aggregate View Object Detection)</strong></p><ul><li><strong>特点：</strong> 结合鸟瞰图和RGB图像特征进行三维目标检测，通过特征融合提升检测精度。</li><li><strong>示例论文：</strong> “Multi-View 3D Object Detection Network for Autonomous Driving” (CVPR 2018)</li></ul></li><li><p><strong>PointFusion</strong></p><ul><li><strong>特点：</strong> 结合RGB图像和点云数据进行特征融合，利用深度学习网络进行三维目标检测。</li><li><strong>示例论文：</strong> “PointFusion: Deep Sensor Fusion for 3D Bounding Box Estimation” (CVPR 2018)</li></ul></li><li><p><strong>PI-RCNN (Point Interaction Region-based Convolutional Neural Network)</strong></p><ul><li><strong>特点：</strong> 将点云和RGB图像特征进行交互融合，使用RPN生成候选框，并进行多阶段细化。</li><li><strong>示例论文：</strong> “PI-RCNN: An Efficient Multi-Sensor 3D Object Detector with Point-based Attentive Contour Feature Fusion” (IROS 2019)</li></ul></li></ul><h3 id="总结-2"><a href="#总结-2" class="headerlink" title="总结"></a>总结</h3><p>三维目标检测网络根据输入数据类型可以分为基于点云、基于RGB-D图像、基于多视图和基于融合的网络。每种类型的网络都有其独特的优势和应用场景。例如，基于点云的网络在处理LiDAR数据时表现出色，而基于融合的网络通过结合多种传感器数据，可以进一步提升检测精度和鲁棒性。选择合适的三维目标检测网络取决于具体应用需求和可用的传感器数据类型。</p><h2 id="问题四：目前这些三维检测网络一般会将高度方向上的维度进行拼接，融合到特征维度上，那是不是高度上的特征划分是2，还是4，或者是6是不是不那么重要，比如VoxelNet-那对于pointpillars这样的网络不在高度上进行划分格子，会不会对性能有影响"><a href="#问题四：目前这些三维检测网络一般会将高度方向上的维度进行拼接，融合到特征维度上，那是不是高度上的特征划分是2，还是4，或者是6是不是不那么重要，比如VoxelNet-那对于pointpillars这样的网络不在高度上进行划分格子，会不会对性能有影响" class="headerlink" title="问题四：目前这些三维检测网络一般会将高度方向上的维度进行拼接，融合到特征维度上，那是不是高度上的特征划分是2，还是4，或者是6是不是不那么重要，比如VoxelNet, 那对于pointpillars这样的网络不在高度上进行划分格子，会不会对性能有影响"></a>问题四：目前这些三维检测网络一般会将高度方向上的维度进行拼接，融合到特征维度上，那是不是高度上的特征划分是2，还是4，或者是6是不是不那么重要，比如VoxelNet, 那对于pointpillars这样的网络不在高度上进行划分格子，会不会对性能有影响</h2><p>在三维目标检测中，如何处理高度方向上的特征确实是一个重要问题。不同的网络在处理高度方向上的策略可能会影响其性能和效果。</p><h3 id="VoxelNet-与高度划分"><a href="#VoxelNet-与高度划分" class="headerlink" title="VoxelNet 与高度划分"></a>VoxelNet 与高度划分</h3><p><strong>VoxelNet</strong> 将点云划分为固定大小的体素（voxels），然后在每个体素内进行特征提取和聚合。VoxelNet在高度方向上的划分是通过设置体素网格的大小来实现的。例如，一个体素网格可以设定为 ( (x, y, z) ) 三个维度，其中 ( z ) 代表高度方向。</p><ul><li><strong>高度方向划分：</strong><ul><li>VoxelNet通常会根据需要设置体素的高度分辨率，例如可以将高度方向划分为若干层（如2层、4层或6层）。</li><li>这种划分可以捕捉高度方向上的特征变化，有助于提升检测性能。</li></ul></li></ul><h3 id="PointPillars-与高度划分"><a href="#PointPillars-与高度划分" class="headerlink" title="PointPillars 与高度划分"></a>PointPillars 与高度划分</h3><p><strong>PointPillars</strong> 是一种专门设计用于高效三维目标检测的网络。它通过将点云数据转换为伪图像（pseudo image），简化了三维点云处理的复杂性。</p><ul><li><strong>PointPillars的特点：</strong><ul><li>PointPillars不对高度方向进行显式的划分，而是将点云数据按照地面投影划分为柱状体（pillars）。</li><li>每个柱状体包含的点云信息会被转换为二维特征映射，这些特征映射在网络中进行处理。</li></ul></li></ul><h3 id="高度划分对性能的影响"><a href="#高度划分对性能的影响" class="headerlink" title="高度划分对性能的影响"></a>高度划分对性能的影响</h3><ul><li><p><strong>高度方向的特征划分：</strong></p><ul><li>对于像VoxelNet这样的网络，高度方向上的特征划分可以捕捉不同高度层次的细节信息，从而提升三维目标检测的精度。</li><li>过少的高度划分可能无法充分捕捉高度方向上的特征变化，而过多的划分则可能增加计算开销。</li></ul></li><li><p><strong>PointPillars的策略：</strong></p><ul><li>PointPillars通过简化高度方向的处理，将三维点云转化为二维特征映射，显著提升了计算效率。</li><li>这种策略在某些情况下可能会损失一些高度方向上的细节信息，但在实际应用中，PointPillars通常表现出良好的性能和效率平衡。</li></ul></li></ul><h3 id="性能权衡"><a href="#性能权衡" class="headerlink" title="性能权衡"></a>性能权衡</h3><ul><li><strong>VoxelNet</strong>：通过体素化和三维卷积处理高度方向上的特征，适合需要高度精度的三维目标检测任务，但计算开销较大。</li><li><strong>PointPillars</strong>：通过柱状体简化三维点云处理，适合实时性要求高的应用，计算效率高，但在处理高度方向的细节特征时可能不如VoxelNet精细。</li></ul><h3 id="实验与优化"><a href="#实验与优化" class="headerlink" title="实验与优化"></a>实验与优化</h3><ul><li>实际上，最优的高度划分策略依赖于具体的应用场景和数据集。建议在实际应用中进行实验，选择最适合的参数。</li><li>对于PointPillars，如果需要更高的高度方向分辨率，可以考虑在柱状体特征映射中引入更多的高度信息，但这需要仔细权衡计算效率和检测精度。</li></ul><h3 id="结论"><a href="#结论" class="headerlink" title="结论"></a>结论</h3><p>高度方向上的特征划分在三维目标检测中具有重要影响，但其重要性依赖于具体的网络架构和应用场景。VoxelNet通过体素化在高度方向上进行详细划分，有助于提升检测精度。PointPillars通过简化高度方向处理，提升了计算效率，但在某些应用中可能需要通过其他方式引入更多高度信息以提升性能。在实际应用中，需要根据具体需求和实验结果进行优化和选择。</p><h2 id="问题五：详细介绍一下CLIP-zero-shot-和DINO-SSL"><a href="#问题五：详细介绍一下CLIP-zero-shot-和DINO-SSL" class="headerlink" title="问题五：详细介绍一下CLIP(zero-shot)和DINO(SSL)"></a>问题五：详细介绍一下CLIP(zero-shot)和DINO(SSL)</h2><h3 id="CLIP-Contrastive-Language-Image-Pre-Training"><a href="#CLIP-Contrastive-Language-Image-Pre-Training" class="headerlink" title="CLIP (Contrastive Language-Image Pre-Training)"></a>CLIP (Contrastive Language-Image Pre-Training)</h3><h4 id="1-概述"><a href="#1-概述" class="headerlink" title="1. 概述"></a>1. 概述</h4><p><strong>CLIP</strong>（Contrastive Language-Image Pre-Training）是OpenAI提出的一种模型，通过对图像和文本进行对比学习，实现在大量未标注数据上进行预训练，并在下游任务中实现零样本学习（zero-shot learning）。</p><h4 id="2-模型架构"><a href="#2-模型架构" class="headerlink" title="2. 模型架构"></a>2. 模型架构</h4><p>CLIP包含两个主要组件：</p><ul><li><strong>图像编码器（Image Encoder）：</strong> 通常采用ResNet或Vision Transformer（ViT）等架构。</li><li><strong>文本编码器（Text Encoder）：</strong> 通常采用Transformer架构。</li></ul><p>图像和文本编码器分别将图像和文本转换为特征向量，这两个向量通过对比学习的方式进行训练。</p><h4 id="3-训练过程"><a href="#3-训练过程" class="headerlink" title="3. 训练过程"></a>3. 训练过程</h4><ol><li><p><strong>数据准备：</strong></p><ul><li>使用大量图像-文本对（如从网络上抓取的图像和其对应的描述）进行训练。</li></ul></li><li><p><strong>对比学习：</strong></p><ul><li>在每个训练步骤中，将一个批次的图像和文本输入模型。</li><li>图像编码器将图像转化为特征向量，文本编码器将文本转化为特征向量。</li><li>计算每对图像和文本向量之间的余弦相似度。</li><li>使用对比损失函数（contrastive loss）来最大化正确图像-文本对的相似度，最小化错误对的相似度。</li></ul></li><li><p><strong>损失函数：</strong></p><ul><li>使用对比损失（contrastive loss），如InfoNCE</li></ul></li></ol><h4 id="4-Zero-Shot-Learning"><a href="#4-Zero-Shot-Learning" class="headerlink" title="4. Zero-Shot Learning"></a>4. Zero-Shot Learning</h4><p>CLIP在预训练完成后，能够直接应用于下游任务，无需额外的微调：</p><ol><li><p><strong>文本提示（Prompting）：</strong></p><ul><li>给定一个分类任务，使用预定义的文本提示（如“a photo of a [class]”）生成各类别的文本描述。</li></ul></li><li><p><strong>相似度计算：</strong></p><ul><li>计算输入图像与所有类别文本描述之间的相似度。</li></ul></li><li><p><strong>分类：</strong></p><ul><li>根据相似度最大化的原则，将输入图像分类到对应的类别。</li></ul></li></ol><h4 id="5-优势"><a href="#5-优势" class="headerlink" title="5. 优势"></a>5. 优势</h4><ul><li><strong>高效：</strong> 利用大规模未标注数据进行训练，提高模型泛化能力。</li><li><strong>灵活：</strong> 能够处理多种下游任务，包括图像分类、图像检索等。</li><li><strong>零样本学习：</strong> 无需对特定任务进行微调，直接应用于新的任务和数据集。</li></ul><h3 id="DINO-Self-Supervised-Learning"><a href="#DINO-Self-Supervised-Learning" class="headerlink" title="DINO (Self-Supervised Learning)"></a>DINO (Self-Supervised Learning)</h3><h4 id="1-概述-1"><a href="#1-概述-1" class="headerlink" title="1. 概述"></a>1. 概述</h4><p><strong>DINO</strong>（Self-Distillation with No Labels）是一种自监督学习（Self-Supervised Learning）方法，由Facebook AI Research提出。DINO通过自蒸馏（self-distillation）技术，在没有标签的数据上进行训练，取得了优异的性能。</p><h4 id="2-模型架构-1"><a href="#2-模型架构-1" class="headerlink" title="2. 模型架构"></a>2. 模型架构</h4><p>DINO的主要组件是Vision Transformer（ViT）或卷积神经网络（CNN），主要用于图像编码。</p><h4 id="3-训练过程-1"><a href="#3-训练过程-1" class="headerlink" title="3. 训练过程"></a>3. 训练过程</h4><ol><li><p><strong>数据增强：</strong></p><ul><li>对输入图像应用不同的数据增强方法，生成不同视图（view）。</li></ul></li><li><p><strong>双学生网络（Teacher and Student Networks）：</strong></p><ul><li>使用两个相同结构的神经网络：教师网络（teacher network）和学生网络（student network）。</li><li>教师网络参数通过指数移动平均（EMA）更新，学生网络参数通过梯度下降更新。</li></ul></li><li><p><strong>自蒸馏：</strong></p><ul><li>输入不同视图到教师和学生网络，获取特征向量。</li><li>计算两个网络输出的相似度，使用对比学习的损失函数进行优化。</li></ul></li><li><p><strong>损失函数：</strong></p><ul><li>使用对比损失（如InfoNCE loss），最大化不同视图的相似度，最小化错误对的相似度。</li></ul></li></ol><h4 id="4-Self-Supervised-Learning"><a href="#4-Self-Supervised-Learning" class="headerlink" title="4. Self-Supervised Learning"></a>4. Self-Supervised Learning</h4><p>DINO在无标签数据上进行训练，学习数据的特征表示：</p><ol><li><p><strong>数据增强：</strong></p><ul><li>使用强增强和弱增强方法生成不同视图。</li></ul></li><li><p><strong>相似度计算：</strong></p><ul><li>计算不同视图的特征向量之间的相似度。</li></ul></li><li><p><strong>参数更新：</strong></p><ul><li>教师网络参数通过学生网络参数的指数移动平均（EMA）进行更新。</li><li>学生网络参数通过梯度下降进行更新。</li></ul></li></ol><h4 id="5-优势-1"><a href="#5-优势-1" class="headerlink" title="5. 优势"></a>5. 优势</h4><ul><li><strong>无需标签：</strong> 在没有标签的数据上进行训练，节省大量标注成本。</li><li><strong>性能优异：</strong> 在多种下游任务中，DINO与有监督学习方法的性能相当。</li><li><strong>鲁棒性：</strong> 自监督学习方法对数据的鲁棒性较强，能够捕捉到数据的本质特征。</li></ul><h3 id="总结-3"><a href="#总结-3" class="headerlink" title="总结"></a>总结</h3><p><strong>CLIP</strong>和<strong>DINO</strong>是两种不同的模型，分别用于解决不同类型的任务。CLIP通过对比学习在图像和文本之间建立联系，实现了零样本学习能力。而DINO则是一种自监督学习方法，通过自蒸馏技术在无标签数据上进行训练，学习数据的特征表示。这两种方法都在各自领域中展示了强大的能力，并推动了NLP和计算机视觉领域的发展。</p><h2 id="问题六：有哪些自监督学习的方法，同时介绍一下不同监督学习（监督，半监督，弱监督，自监督等）"><a href="#问题六：有哪些自监督学习的方法，同时介绍一下不同监督学习（监督，半监督，弱监督，自监督等）" class="headerlink" title="问题六：有哪些自监督学习的方法，同时介绍一下不同监督学习（监督，半监督，弱监督，自监督等）"></a>问题六：有哪些自监督学习的方法，同时介绍一下不同监督学习（监督，半监督，弱监督，自监督等）</h2><h3 id="监督学习与不同形式的监督学习"><a href="#监督学习与不同形式的监督学习" class="headerlink" title="监督学习与不同形式的监督学习"></a>监督学习与不同形式的监督学习</h3><h4 id="监督学习（Supervised-Learning）"><a href="#监督学习（Supervised-Learning）" class="headerlink" title="监督学习（Supervised Learning）"></a>监督学习（Supervised Learning）</h4><p><strong>定义：</strong> 监督学习使用带标签的数据集进行训练，其中每个训练样本都包括一个输入和一个期望的输出。</p><ul><li><strong>标注数据：</strong> 需要大量的标注数据，输入数据和对应的标签。</li><li><strong>目标：</strong> 学习一个函数，使其能够将输入映射到正确的输出。</li><li><strong>应用：</strong> 图像分类、语音识别、自然语言处理等。</li></ul><p><strong>示例：</strong> 使用图像分类器将输入的图像分类到正确的类别（如猫、狗等）。</p><h4 id="半监督学习（Semi-Supervised-Learning）"><a href="#半监督学习（Semi-Supervised-Learning）" class="headerlink" title="半监督学习（Semi-Supervised Learning）"></a>半监督学习（Semi-Supervised Learning）</h4><p><strong>定义：</strong> 半监督学习使用少量标注数据和大量未标注数据进行训练。</p><ul><li><strong>标注和未标注数据：</strong> 使用少量标注数据来引导模型学习，再利用大量未标注数据进行进一步训练。</li><li><strong>目标：</strong> 在只有少量标注数据的情况下，提升模型的性能。</li><li><strong>应用：</strong> 图像分类、语音识别等，适用于标注成本高的数据集。</li></ul><p><strong>示例：</strong> 使用少量标注的图片和大量未标注的图片来训练图像分类模型。</p><h4 id="弱监督学习（Weakly-Supervised-Learning）"><a href="#弱监督学习（Weakly-Supervised-Learning）" class="headerlink" title="弱监督学习（Weakly Supervised Learning）"></a>弱监督学习（Weakly Supervised Learning）</h4><p><strong>定义：</strong> 弱监督学习使用不完全、不准确或不精确的标签进行训练。</p><ul><li><strong>不完全标签：</strong> 训练数据只部分标注。</li><li><strong>不准确标签：</strong> 训练数据的标签可能包含噪声。</li><li><strong>不精确标签：</strong> 训练数据的标签可能是粗粒度的或不精确的。</li><li><strong>目标：</strong> 在标签数据不完美的情况下训练模型。</li><li><strong>应用：</strong> 自然语言处理、图像分类等。</li></ul><p><strong>示例：</strong> 使用带有噪声的标签或部分标注的数据来训练模型。</p><h4 id="自监督学习（Self-Supervised-Learning）"><a href="#自监督学习（Self-Supervised-Learning）" class="headerlink" title="自监督学习（Self-Supervised Learning）"></a>自监督学习（Self-Supervised Learning）</h4><p><strong>定义：</strong> 自监督学习从数据本身生成监督信号进行训练，无需额外的标注数据。</p><ul><li><strong>数据驱动标签：</strong> 利用数据的内在结构生成伪标签。</li><li><strong>目标：</strong> 学习数据的表示，使其对下游任务（如分类、回归）有用。</li><li><strong>应用：</strong> 图像处理、自然语言处理、时间序列分析等。</li></ul><p><strong>示例：</strong> 使用图像的颜色信息作为伪标签进行训练，使模型学习图像特征，然后在下游任务中应用这些特征。</p><h3 id="自监督学习的方法"><a href="#自监督学习的方法" class="headerlink" title="自监督学习的方法"></a>自监督学习的方法</h3><p>自监督学习通过设计预训练任务，使模型能够从数据中学习有用的表示。常见的方法包括：</p><h4 id="1-对比学习（Contrastive-Learning）"><a href="#1-对比学习（Contrastive-Learning）" class="headerlink" title="1. 对比学习（Contrastive Learning）"></a>1. 对比学习（Contrastive Learning）</h4><p>对比学习通过将样本映射到一个嵌入空间，使相似样本（正样本）在该空间中的距离更近，不相似样本（负样本）距离更远。</p><ul><li><p><strong>方法：</strong></p><ul><li><strong>SimCLR（Simple Framework for Contrastive Learning of Visual Representations）：</strong> 通过数据增强生成正样本对，使用对比损失函数（如NT-Xent）进行训练。</li><li><strong>MoCo（Momentum Contrast）：</strong> 使用一个动量编码器生成负样本队列，提高对比学习的效果。</li></ul></li><li><p><strong>应用：</strong> 图像分类、目标检测、图像分割等。</p></li></ul><p><strong>示例代码（SimCLR）：</strong></p><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">import</span> torch<span class="token keyword">import</span> torch<span class="token punctuation">.</span>nn <span class="token keyword">as</span> nn<span class="token keyword">import</span> torch<span class="token punctuation">.</span>nn<span class="token punctuation">.</span>functional <span class="token keyword">as</span> F<span class="token keyword">class</span> <span class="token class-name">SimCLR</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> encoder<span class="token punctuation">,</span> projection_dim<span class="token punctuation">)</span><span class="token punctuation">:</span>        <span class="token builtin">super</span><span class="token punctuation">(</span>SimCLR<span class="token punctuation">,</span> self<span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>        self<span class="token punctuation">.</span>encoder <span class="token operator">=</span> encoder        self<span class="token punctuation">.</span>projection_head <span class="token operator">=</span> nn<span class="token punctuation">.</span>Sequential<span class="token punctuation">(</span>            nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span>encoder<span class="token punctuation">.</span>out_dim<span class="token punctuation">,</span> <span class="token number">256</span><span class="token punctuation">)</span><span class="token punctuation">,</span>            nn<span class="token punctuation">.</span>ReLU<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span>            nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span><span class="token number">256</span><span class="token punctuation">,</span> projection_dim<span class="token punctuation">)</span>        <span class="token punctuation">)</span>        <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> x<span class="token punctuation">)</span><span class="token punctuation">:</span>        h <span class="token operator">=</span> self<span class="token punctuation">.</span>encoder<span class="token punctuation">(</span>x<span class="token punctuation">)</span>        z <span class="token operator">=</span> self<span class="token punctuation">.</span>projection_head<span class="token punctuation">(</span>h<span class="token punctuation">)</span>        <span class="token keyword">return</span> z<span class="token keyword">def</span> <span class="token function">nt_xent_loss</span><span class="token punctuation">(</span>z_i<span class="token punctuation">,</span> z_j<span class="token punctuation">,</span> temperature<span class="token operator">=</span><span class="token number">0.5</span><span class="token punctuation">)</span><span class="token punctuation">:</span>    batch_size <span class="token operator">=</span> z_i<span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span>    z <span class="token operator">=</span> torch<span class="token punctuation">.</span>cat<span class="token punctuation">(</span><span class="token punctuation">(</span>z_i<span class="token punctuation">,</span> z_j<span class="token punctuation">)</span><span class="token punctuation">,</span> dim<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">)</span>    sim_matrix <span class="token operator">=</span> F<span class="token punctuation">.</span>cosine_similarity<span class="token punctuation">(</span>z<span class="token punctuation">.</span>unsqueeze<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">,</span> z<span class="token punctuation">.</span>unsqueeze<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">)</span><span class="token punctuation">,</span> dim<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">)</span>    sim_matrix <span class="token operator">=</span> sim_matrix <span class="token operator">/</span> temperature        sim_i_j <span class="token operator">=</span> torch<span class="token punctuation">.</span>diag<span class="token punctuation">(</span>sim_matrix<span class="token punctuation">,</span> batch_size<span class="token punctuation">)</span>    sim_j_i <span class="token operator">=</span> torch<span class="token punctuation">.</span>diag<span class="token punctuation">(</span>sim_matrix<span class="token punctuation">,</span> <span class="token operator">-</span>batch_size<span class="token punctuation">)</span>        positives <span class="token operator">=</span> torch<span class="token punctuation">.</span>cat<span class="token punctuation">(</span><span class="token punctuation">(</span>sim_i_j<span class="token punctuation">,</span> sim_j_i<span class="token punctuation">)</span><span class="token punctuation">,</span> dim<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">)</span>    negatives <span class="token operator">=</span> sim_matrix<span class="token punctuation">[</span><span class="token operator">~</span>torch<span class="token punctuation">.</span>eye<span class="token punctuation">(</span><span class="token number">2</span> <span class="token operator">*</span> batch_size<span class="token punctuation">,</span> dtype<span class="token operator">=</span><span class="token builtin">bool</span><span class="token punctuation">)</span><span class="token punctuation">]</span><span class="token punctuation">.</span>view<span class="token punctuation">(</span><span class="token number">2</span> <span class="token operator">*</span> batch_size<span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span>        labels <span class="token operator">=</span> torch<span class="token punctuation">.</span>zeros<span class="token punctuation">(</span><span class="token number">2</span> <span class="token operator">*</span> batch_size<span class="token punctuation">)</span><span class="token punctuation">.</span>to<span class="token punctuation">(</span>z<span class="token punctuation">.</span>device<span class="token punctuation">)</span><span class="token punctuation">.</span><span class="token builtin">long</span><span class="token punctuation">(</span><span class="token punctuation">)</span>    logits <span class="token operator">=</span> torch<span class="token punctuation">.</span>cat<span class="token punctuation">(</span><span class="token punctuation">(</span>positives<span class="token punctuation">.</span>unsqueeze<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">,</span> negatives<span class="token punctuation">)</span><span class="token punctuation">,</span> dim<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span>        loss <span class="token operator">=</span> F<span class="token punctuation">.</span>cross_entropy<span class="token punctuation">(</span>logits<span class="token punctuation">,</span> labels<span class="token punctuation">)</span>    <span class="token keyword">return</span> loss<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h4 id="2-预文本生成（Pretext-Task）"><a href="#2-预文本生成（Pretext-Task）" class="headerlink" title="2. 预文本生成（Pretext Task）"></a>2. 预文本生成（Pretext Task）</h4><p>预文本生成任务通过设计特定的任务，使模型能够从数据中学习有用的表示。</p><ul><li><p><strong>方法：</strong></p><ul><li><strong>自编码器（Autoencoder）：</strong> 通过重建输入数据，使模型学习数据的压缩表示。</li><li><strong>Jigsaw Puzzle：</strong> 将图像打乱为多个块，要求模型重建原始图像顺序。</li><li><strong>RotNet：</strong> 随机旋转图像，让模型预测旋转的角度。</li></ul></li><li><p><strong>应用：</strong> 图像分类、目标检测、图像分割等。</p></li></ul><p><strong>示例代码（自编码器）：</strong></p><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">import</span> torch<span class="token keyword">import</span> torch<span class="token punctuation">.</span>nn <span class="token keyword">as</span> nn<span class="token keyword">class</span> <span class="token class-name">Autoencoder</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span><span class="token punctuation">:</span>        <span class="token builtin">super</span><span class="token punctuation">(</span>Autoencoder<span class="token punctuation">,</span> self<span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>        self<span class="token punctuation">.</span>encoder <span class="token operator">=</span> nn<span class="token punctuation">.</span>Sequential<span class="token punctuation">(</span>            nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span><span class="token number">784</span><span class="token punctuation">,</span> <span class="token number">256</span><span class="token punctuation">)</span><span class="token punctuation">,</span>            nn<span class="token punctuation">.</span>ReLU<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span>            nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span><span class="token number">256</span><span class="token punctuation">,</span> <span class="token number">64</span><span class="token punctuation">)</span><span class="token punctuation">,</span>            nn<span class="token punctuation">.</span>ReLU<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span>            nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span><span class="token number">64</span><span class="token punctuation">,</span> <span class="token number">32</span><span class="token punctuation">)</span>        <span class="token punctuation">)</span>        self<span class="token punctuation">.</span>decoder <span class="token operator">=</span> nn<span class="token punctuation">.</span>Sequential<span class="token punctuation">(</span>            nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span><span class="token number">32</span><span class="token punctuation">,</span> <span class="token number">64</span><span class="token punctuation">)</span><span class="token punctuation">,</span>            nn<span class="token punctuation">.</span>ReLU<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span>            nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span><span class="token number">64</span><span class="token punctuation">,</span> <span class="token number">256</span><span class="token punctuation">)</span><span class="token punctuation">,</span>            nn<span class="token punctuation">.</span>ReLU<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span>            nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span><span class="token number">256</span><span class="token punctuation">,</span> <span class="token number">784</span><span class="token punctuation">)</span><span class="token punctuation">,</span>            nn<span class="token punctuation">.</span>Sigmoid<span class="token punctuation">(</span><span class="token punctuation">)</span>        <span class="token punctuation">)</span>        <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> x<span class="token punctuation">)</span><span class="token punctuation">:</span>        x <span class="token operator">=</span> self<span class="token punctuation">.</span>encoder<span class="token punctuation">(</span>x<span class="token punctuation">)</span>        x <span class="token operator">=</span> self<span class="token punctuation">.</span>decoder<span class="token punctuation">(</span>x<span class="token punctuation">)</span>        <span class="token keyword">return</span> x<span class="token comment"># 示例使用</span>autoencoder <span class="token operator">=</span> Autoencoder<span class="token punctuation">(</span><span class="token punctuation">)</span>criterion <span class="token operator">=</span> nn<span class="token punctuation">.</span>MSELoss<span class="token punctuation">(</span><span class="token punctuation">)</span>optimizer <span class="token operator">=</span> torch<span class="token punctuation">.</span>optim<span class="token punctuation">.</span>Adam<span class="token punctuation">(</span>autoencoder<span class="token punctuation">.</span>parameters<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span> lr<span class="token operator">=</span><span class="token number">0.001</span><span class="token punctuation">)</span><span class="token comment"># 假设有一个DataLoader，加载数据</span><span class="token keyword">for</span> data <span class="token keyword">in</span> dataloader<span class="token punctuation">:</span>    inputs <span class="token operator">=</span> data<span class="token punctuation">.</span>view<span class="token punctuation">(</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">784</span><span class="token punctuation">)</span>    outputs <span class="token operator">=</span> autoencoder<span class="token punctuation">(</span>inputs<span class="token punctuation">)</span>        loss <span class="token operator">=</span> criterion<span class="token punctuation">(</span>outputs<span class="token punctuation">,</span> inputs<span class="token punctuation">)</span>    optimizer<span class="token punctuation">.</span>zero_grad<span class="token punctuation">(</span><span class="token punctuation">)</span>    loss<span class="token punctuation">.</span>backward<span class="token punctuation">(</span><span class="token punctuation">)</span>    optimizer<span class="token punctuation">.</span>step<span class="token punctuation">(</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h4 id="3-生成对抗网络（Generative-Adversarial-Networks-GANs）"><a href="#3-生成对抗网络（Generative-Adversarial-Networks-GANs）" class="headerlink" title="3. 生成对抗网络（Generative Adversarial Networks, GANs）"></a>3. 生成对抗网络（Generative Adversarial Networks, GANs）</h4><p>GANs通过生成器和判别器的对抗训练，使生成器能够生成与真实数据分布相似的数据，从而学习数据的表示。</p><ul><li><p><strong>方法：</strong></p><ul><li><strong>生成对抗网络（GANs）：</strong> 包括基本GAN、DCGAN、CycleGAN等。</li></ul></li><li><p><strong>应用：</strong> 图像生成、风格迁移、数据增强等。</p></li></ul><p><strong>示例代码（基本GAN）：</strong></p><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">import</span> torch<span class="token keyword">import</span> torch<span class="token punctuation">.</span>nn <span class="token keyword">as</span> nn<span class="token keyword">class</span> <span class="token class-name">Generator</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span><span class="token punctuation">:</span>        <span class="token builtin">super</span><span class="token punctuation">(</span>Generator<span class="token punctuation">,</span> self<span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>        self<span class="token punctuation">.</span>model <span class="token operator">=</span> nn<span class="token punctuation">.</span>Sequential<span class="token punctuation">(</span>            nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span><span class="token number">100</span><span class="token punctuation">,</span> <span class="token number">256</span><span class="token punctuation">)</span><span class="token punctuation">,</span>            nn<span class="token punctuation">.</span>ReLU<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span>            nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span><span class="token number">256</span><span class="token punctuation">,</span> <span class="token number">512</span><span class="token punctuation">)</span><span class="token punctuation">,</span>            nn<span class="token punctuation">.</span>ReLU<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span>            nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span><span class="token number">512</span><span class="token punctuation">,</span> <span class="token number">1024</span><span class="token punctuation">)</span><span class="token punctuation">,</span>            nn<span class="token punctuation">.</span>ReLU<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span>            nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span><span class="token number">1024</span><span class="token punctuation">,</span> <span class="token number">784</span><span class="token punctuation">)</span><span class="token punctuation">,</span>            nn<span class="token punctuation">.</span>Tanh<span class="token punctuation">(</span><span class="token punctuation">)</span>        <span class="token punctuation">)</span>        <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> x<span class="token punctuation">)</span><span class="token punctuation">:</span>        <span class="token keyword">return</span> self<span class="token punctuation">.</span>model<span class="token punctuation">(</span>x<span class="token punctuation">)</span><span class="token keyword">class</span> <span class="token class-name">Discriminator</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span><span class="token punctuation">:</span>        <span class="token builtin">super</span><span class="token punctuation">(</span>Discriminator<span class="token punctuation">,</span> self<span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>        self<span class="token punctuation">.</span>model <span class="token operator">=</span> nn<span class="token punctuation">.</span>Sequential<span class="token punctuation">(</span>            nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span><span class="token number">784</span><span class="token punctuation">,</span> <span class="token number">512</span><span class="token punctuation">)</span><span class="token punctuation">,</span>            nn<span class="token punctuation">.</span>LeakyReLU<span class="token punctuation">(</span><span class="token number">0.2</span><span class="token punctuation">)</span><span class="token punctuation">,</span>            nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span><span class="token number">512</span><span class="token punctuation">,</span> <span class="token number">256</span><span class="token punctuation">)</span><span class="token punctuation">,</span>            nn<span class="token punctuation">.</span>LeakyReLU<span class="token punctuation">(</span><span class="token number">0.2</span><span class="token punctuation">)</span><span class="token punctuation">,</span>            nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span><span class="token number">256</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">,</span>            nn<span class="token punctuation">.</span>Sigmoid<span class="token punctuation">(</span><span class="token punctuation">)</span>        <span class="token punctuation">)</span>        <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> x<span class="token punctuation">)</span><span class="token punctuation">:</span>        <span class="token keyword">return</span> self<span class="token punctuation">.</span>model<span class="token punctuation">(</span>x<span class="token punctuation">)</span><span class="token comment"># 示例使用</span>generator <span class="token operator">=</span> Generator<span class="token punctuation">(</span><span class="token punctuation">)</span>discriminator <span class="token operator">=</span> Discriminator<span class="token punctuation">(</span><span class="token punctuation">)</span>criterion <span class="token operator">=</span> nn<span class="token punctuation">.</span>BCELoss<span class="token punctuation">(</span><span class="token punctuation">)</span>optimizer_g <span class="token operator">=</span> torch<span class="token punctuation">.</span>optim<span class="token punctuation">.</span>Adam<span class="token punctuation">(</span>generator<span class="token punctuation">.</span>parameters<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span> lr<span class="token operator">=</span><span class="token number">0.0002</span><span class="token punctuation">)</span>optimizer_d <span class="token operator">=</span> torch<span class="token punctuation">.</span>optim<span class="token punctuation">.</span>Adam<span class="token punctuation">(</span>discriminator<span class="token punctuation">.</span>parameters<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span> lr<span class="token operator">=</span><span class="token number">0.0002</span><span class="token punctuation">)</span><span class="token comment"># 假设有一个DataLoader，加载数据</span><span class="token keyword">for</span> data <span class="token keyword">in</span> dataloader<span class="token punctuation">:</span>    real_data <span class="token operator">=</span> data<span class="token punctuation">.</span>view<span class="token punctuation">(</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">784</span><span class="token punctuation">)</span>    batch_size <span class="token operator">=</span> real_data<span class="token punctuation">.</span>size<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">)</span>        <span class="token comment"># 训练判别器</span>    z <span class="token operator">=</span> torch<span class="token punctuation">.</span>randn<span class="token punctuation">(</span>batch_size<span class="token punctuation">,</span> <span class="token number">100</span><span class="token punctuation">)</span>    fake_data <span class="token operator">=</span> generator<span class="token punctuation">(</span>z<span class="token punctuation">)</span>        real_labels <span class="token operator">=</span> torch<span class="token punctuation">.</span>ones<span class="token punctuation">(</span>batch_size<span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span>    fake_labels <span class="token operator">=</span> torch<span class="token punctuation">.</span>zeros<span class="token punctuation">(</span>batch_size<span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span>        outputs <span class="token operator">=</span> discriminator<span class="token punctuation">(</span>real_data<span class="token punctuation">)</span>    d_loss_real <span class="token operator">=</span> criterion<span class="token punctuation">(</span>outputs<span class="token punctuation">,</span> real_labels<span class="token punctuation">)</span>        outputs <span class="token operator">=</span> discriminator<span class="token punctuation">(</span>fake_data<span class="token punctuation">.</span>detach<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span>    d_loss_fake <span class="token operator">=</span> criterion<span class="token punctuation">(</span>outputs<span class="token punctuation">,</span> fake_labels<span class="token punctuation">)</span>        d_loss <span class="token operator">=</span> d_loss_real <span class="token operator">+</span> d_loss_fake    optimizer_d<span class="token punctuation">.</span>zero_grad<span class="token punctuation">(</span><span class="token punctuation">)</span>    d_loss<span class="token punctuation">.</span>backward<span class="token punctuation">(</span><span class="token punctuation">)</span>    optimizer_d<span class="token punctuation">.</span>step<span class="token punctuation">(</span><span class="token punctuation">)</span>        <span class="token comment"># 训练生成器</span>    outputs <span class="token operator">=</span> discriminator<span class="token punctuation">(</span>fake_data<span class="token punctuation">)</span>    g_loss <span class="token operator">=</span> criterion<span class="token punctuation">(</span>outputs<span class="token punctuation">,</span> real_labels<span class="token punctuation">)</span>        optimizer_g<span class="token punctuation">.</span>zero_grad<span class="token punctuation">(</span><span class="token punctuation">)</span>    g_loss<span class="token punctuation">.</span>backward<span class="token punctuation">(</span><span class="token punctuation">)</span>    optimizer_g<span class="token punctuation">.</span>step<span class="token punctuation">(</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h3 id="结论-1"><a href="#结论-1" class="headerlink" title="结论"></a>结论</h3><p>自监督学习通过设计预训练任务，从数据本身生成监督信号进行训练，不需要大量标注数据。常见的方法包括对比学习、预文本生成和生成对抗网络等。每种方法有其独特的应用场景和优势，在实际应用中选择合适的方法可以有效提升模型性能。</p><p>此外，不同形式的监督学习（如监督学习、半监督学习、弱监督学习和自监督学习）在不同的标注数据需求和应用场景下各具优势。了解这些方法及其应用场景，对于解决实际问题和提升模型性能至关重要。</p><h2 id="问题七：mae的方法和对比学习的方法分别学习的是什么特征，以及他们自己的区别和联系"><a href="#问题七：mae的方法和对比学习的方法分别学习的是什么特征，以及他们自己的区别和联系" class="headerlink" title="问题七：mae的方法和对比学习的方法分别学习的是什么特征，以及他们自己的区别和联系"></a>问题七：mae的方法和对比学习的方法分别学习的是什么特征，以及他们自己的区别和联系</h2><h3 id="MAE（Masked-Autoencoders）方法和对比学习（Contrastive-Learning）方法"><a href="#MAE（Masked-Autoencoders）方法和对比学习（Contrastive-Learning）方法" class="headerlink" title="MAE（Masked Autoencoders）方法和对比学习（Contrastive Learning）方法"></a>MAE（Masked Autoencoders）方法和对比学习（Contrastive Learning）方法</h3><h4 id="1-MAE（Masked-Autoencoders）方法"><a href="#1-MAE（Masked-Autoencoders）方法" class="headerlink" title="1. MAE（Masked Autoencoders）方法"></a>1. MAE（Masked Autoencoders）方法</h4><p><strong>定义：</strong> Masked Autoencoders (MAE) 是一种自监督学习方法，通过掩盖输入数据的一部分，然后训练模型重建被掩盖的部分。这个过程强迫模型理解和捕捉数据的全局结构和语义特征。</p><p><strong>学习特征：</strong></p><ul><li><strong>全局结构和上下文信息：</strong> MAE方法通过重建被掩盖的部分，学习到输入数据的全局结构和上下文信息。</li><li><strong>细节信息：</strong> 为了正确重建被掩盖部分，模型需要理解细节信息以及其与全局结构的关系。</li></ul><p><strong>工作流程：</strong></p><ol><li><strong>数据掩盖（Masking）：</strong> 随机掩盖输入数据的一部分（如图像的块或文字的单词）。</li><li><strong>编码器（Encoder）：</strong> 对部分可见的数据进行编码。</li><li><strong>解码器（Decoder）：</strong> 使用编码器输出和掩盖位置的信息重建被掩盖的部分。</li><li><strong>重建误差（Reconstruction Error）：</strong> 通过最小化重建误差来训练模型。</li></ol><p><strong>示例：</strong> 在图像处理中，掩盖部分图像块，然后使用MAE方法重建这些被掩盖的块。</p><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">import</span> torch<span class="token keyword">import</span> torch<span class="token punctuation">.</span>nn <span class="token keyword">as</span> nn<span class="token keyword">class</span> <span class="token class-name">MAE</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> encoder<span class="token punctuation">,</span> decoder<span class="token punctuation">)</span><span class="token punctuation">:</span>        <span class="token builtin">super</span><span class="token punctuation">(</span>MAE<span class="token punctuation">,</span> self<span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>        self<span class="token punctuation">.</span>encoder <span class="token operator">=</span> encoder        self<span class="token punctuation">.</span>decoder <span class="token operator">=</span> decoder        <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> x<span class="token punctuation">,</span> mask<span class="token punctuation">)</span><span class="token punctuation">:</span>        masked_x <span class="token operator">=</span> x <span class="token operator">*</span> mask        encoded <span class="token operator">=</span> self<span class="token punctuation">.</span>encoder<span class="token punctuation">(</span>masked_x<span class="token punctuation">)</span>        reconstructed <span class="token operator">=</span> self<span class="token punctuation">.</span>decoder<span class="token punctuation">(</span>encoded<span class="token punctuation">)</span>        <span class="token keyword">return</span> reconstructed<span class="token comment"># 示例使用</span>encoder <span class="token operator">=</span> nn<span class="token punctuation">.</span>Sequential<span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span><span class="token number">784</span><span class="token punctuation">,</span> <span class="token number">256</span><span class="token punctuation">)</span><span class="token punctuation">,</span> nn<span class="token punctuation">.</span>ReLU<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span>decoder <span class="token operator">=</span> nn<span class="token punctuation">.</span>Sequential<span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span><span class="token number">256</span><span class="token punctuation">,</span> <span class="token number">784</span><span class="token punctuation">)</span><span class="token punctuation">,</span> nn<span class="token punctuation">.</span>Sigmoid<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span>mae <span class="token operator">=</span> MAE<span class="token punctuation">(</span>encoder<span class="token punctuation">,</span> decoder<span class="token punctuation">)</span><span class="token comment"># 假设有一个DataLoader，加载数据</span><span class="token keyword">for</span> data <span class="token keyword">in</span> dataloader<span class="token punctuation">:</span>    inputs <span class="token operator">=</span> data<span class="token punctuation">.</span>view<span class="token punctuation">(</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">784</span><span class="token punctuation">)</span>    mask <span class="token operator">=</span> torch<span class="token punctuation">.</span>rand_like<span class="token punctuation">(</span>inputs<span class="token punctuation">)</span> <span class="token operator">></span> <span class="token number">0.5</span>  <span class="token comment"># 随机掩盖部分输入</span>    outputs <span class="token operator">=</span> mae<span class="token punctuation">(</span>inputs<span class="token punctuation">,</span> mask<span class="token punctuation">)</span>    loss <span class="token operator">=</span> nn<span class="token punctuation">.</span>MSELoss<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">(</span>outputs<span class="token punctuation">,</span> inputs<span class="token punctuation">)</span>    optimizer<span class="token punctuation">.</span>zero_grad<span class="token punctuation">(</span><span class="token punctuation">)</span>    loss<span class="token punctuation">.</span>backward<span class="token punctuation">(</span><span class="token punctuation">)</span>    optimizer<span class="token punctuation">.</span>step<span class="token punctuation">(</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h4 id="2-对比学习（Contrastive-Learning）方法"><a href="#2-对比学习（Contrastive-Learning）方法" class="headerlink" title="2. 对比学习（Contrastive Learning）方法"></a>2. 对比学习（Contrastive Learning）方法</h4><p><strong>定义：</strong> 对比学习是一种自监督学习方法，通过将样本映射到一个嵌入空间，使相似样本（正样本对）在该空间中的距离更近，不相似样本（负样本对）距离更远。</p><p><strong>学习特征：</strong></p><ul><li><strong>相对相似性和区分性：</strong> 对比学习主要关注样本之间的相对相似性，学习到的特征能够更好地区分不同类别的样本。</li><li><strong>全局和局部特征：</strong> 通过对比正负样本对，模型能够捕捉数据的全局特征和局部细节。</li></ul><p><strong>工作流程：</strong></p><ol><li><strong>数据增强（Data Augmentation）：</strong> 对输入数据进行数据增强，生成正样本对（如图像的不同视角）。</li><li><strong>编码器（Encoder）：</strong> 将正样本对编码为特征向量。</li><li><strong>相似性计算（Similarity Calculation）：</strong> 计算正样本对和负样本对的相似性。</li><li><strong>对比损失（Contrastive Loss）：</strong> 通过最小化对比损失，拉近正样本对的距离，拉远负样本对的距离。</li></ol><p><strong>示例：</strong> 使用SimCLR方法进行图像特征学习。</p><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">import</span> torch<span class="token keyword">import</span> torch<span class="token punctuation">.</span>nn <span class="token keyword">as</span> nn<span class="token keyword">import</span> torch<span class="token punctuation">.</span>nn<span class="token punctuation">.</span>functional <span class="token keyword">as</span> F<span class="token keyword">class</span> <span class="token class-name">SimCLR</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> encoder<span class="token punctuation">,</span> projection_dim<span class="token punctuation">)</span><span class="token punctuation">:</span>        <span class="token builtin">super</span><span class="token punctuation">(</span>SimCLR<span class="token punctuation">,</span> self<span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>        self<span class="token punctuation">.</span>encoder <span class="token operator">=</span> encoder        self<span class="token punctuation">.</span>projection_head <span class="token operator">=</span> nn<span class="token punctuation">.</span>Sequential<span class="token punctuation">(</span>            nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span>encoder<span class="token punctuation">.</span>out_dim<span class="token punctuation">,</span> <span class="token number">256</span><span class="token punctuation">)</span><span class="token punctuation">,</span>            nn<span class="token punctuation">.</span>ReLU<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span>            nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span><span class="token number">256</span><span class="token punctuation">,</span> projection_dim<span class="token punctuation">)</span>        <span class="token punctuation">)</span>        <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> x<span class="token punctuation">)</span><span class="token punctuation">:</span>        h <span class="token operator">=</span> self<span class="token punctuation">.</span>encoder<span class="token punctuation">(</span>x<span class="token punctuation">)</span>        z <span class="token operator">=</span> self<span class="token punctuation">.</span>projection_head<span class="token punctuation">(</span>h<span class="token punctuation">)</span>        <span class="token keyword">return</span> z<span class="token keyword">def</span> <span class="token function">nt_xent_loss</span><span class="token punctuation">(</span>z_i<span class="token punctuation">,</span> z_j<span class="token punctuation">,</span> temperature<span class="token operator">=</span><span class="token number">0.5</span><span class="token punctuation">)</span><span class="token punctuation">:</span>    batch_size <span class="token operator">=</span> z_i<span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span>    z <span class="token operator">=</span> torch<span class="token punctuation">.</span>cat<span class="token punctuation">(</span><span class="token punctuation">(</span>z_i<span class="token punctuation">,</span> z_j<span class="token punctuation">)</span><span class="token punctuation">,</span> dim<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">)</span>    sim_matrix <span class="token operator">=</span> F<span class="token punctuation">.</span>cosine_similarity<span class="token punctuation">(</span>z<span class="token punctuation">.</span>unsqueeze<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">,</span> z<span class="token punctuation">.</span>unsqueeze<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">)</span><span class="token punctuation">,</span> dim<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">)</span>    sim_matrix <span class="token operator">=</span> sim_matrix <span class="token operator">/</span> temperature        sim_i_j <span class="token operator">=</span> torch<span class="token punctuation">.</span>diag<span class="token punctuation">(</span>sim_matrix<span class="token punctuation">,</span> batch_size<span class="token punctuation">)</span>    sim_j_i <span class="token operator">=</span> torch<span class="token punctuation">.</span>diag<span class="token punctuation">(</span>sim_matrix<span class="token punctuation">,</span> <span class="token operator">-</span>batch_size<span class="token punctuation">)</span>        positives <span class="token operator">=</span> torch<span class="token punctuation">.</span>cat<span class="token punctuation">(</span><span class="token punctuation">(</span>sim_i_j<span class="token punctuation">,</span> sim_j_i<span class="token punctuation">)</span><span class="token punctuation">,</span> dim<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">)</span>    negatives <span class="token operator">=</span> sim_matrix<span class="token punctuation">[</span><span class="token operator">~</span>torch<span class="token punctuation">.</span>eye<span class="token punctuation">(</span><span class="token number">2</span> <span class="token operator">*</span> batch_size<span class="token punctuation">,</span> dtype<span class="token operator">=</span><span class="token builtin">bool</span><span class="token punctuation">)</span><span class="token punctuation">]</span><span class="token punctuation">.</span>view<span class="token punctuation">(</span><span class="token number">2</span> <span class="token operator">*</span> batch_size<span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span>        labels <span class="token operator">=</span> torch<span class="token punctuation">.</span>zeros<span class="token punctuation">(</span><span class="token number">2</span> <span class="token operator">*</span> batch_size<span class="token punctuation">)</span><span class="token punctuation">.</span>to<span class="token punctuation">(</span>z<span class="token punctuation">.</span>device<span class="token punctuation">)</span><span class="token punctuation">.</span><span class="token builtin">long</span><span class="token punctuation">(</span><span class="token punctuation">)</span>    logits <span class="token operator">=</span> torch<span class="token punctuation">.</span>cat<span class="token punctuation">(</span><span class="token punctuation">(</span>positives<span class="token punctuation">.</span>unsqueeze<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">,</span> negatives<span class="token punctuation">)</span><span class="token punctuation">,</span> dim<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span>        loss <span class="token operator">=</span> F<span class="token punctuation">.</span>cross_entropy<span class="token punctuation">(</span>logits<span class="token punctuation">,</span> labels<span class="token punctuation">)</span>    <span class="token keyword">return</span> loss<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h3 id="MAE-与-对比学习的区别与联系"><a href="#MAE-与-对比学习的区别与联系" class="headerlink" title="MAE 与 对比学习的区别与联系"></a>MAE 与 对比学习的区别与联系</h3><p><strong>区别：</strong></p><ol><li><p><strong>学习目标：</strong></p><ul><li><strong>MAE:</strong> 主要通过重建被掩盖的部分来学习数据的全局结构和细节信息。</li><li><strong>对比学习:</strong> 主要通过对比正负样本对来学习样本之间的相对相似性和区分性。</li></ul></li><li><p><strong>任务设计：</strong></p><ul><li><strong>MAE:</strong> 设计的是重建任务，需要预测被掩盖部分的内容。</li><li><strong>对比学习:</strong> 设计的是对比任务，需要拉近相似样本的距离，拉远不相似样本的距离。</li></ul></li><li><p><strong>损失函数：</strong></p><ul><li><strong>MAE:</strong> 使用重建误差（如MSE Loss）来衡量模型的性能。</li><li><strong>对比学习:</strong> 使用对比损失（如NT-Xent Loss）来衡量模型的性能。</li></ul></li></ol><p><strong>联系：</strong></p><ol><li><strong>自监督学习：</strong> MAE和对比学习都是自监督学习方法，都是通过数据自身生成的监督信号来进行训练。</li><li><strong>表示学习：</strong> 两者都旨在学习数据的有用表示，以便在下游任务中应用这些表示。</li><li><strong>无标签数据：</strong> 两者都可以利用大量无标签数据进行训练，从而提高模型的泛化能力。</li></ol><h3 id="总结-4"><a href="#总结-4" class="headerlink" title="总结"></a>总结</h3><ul><li><strong>MAE:</strong> 通过掩盖和重建输入数据的一部分，学习数据的全局结构和细节信息。</li><li><strong>对比学习:</strong> 通过对比正负样本对，学习样本之间的相对相似性和区分性。</li><li><strong>区别:</strong> 学习目标、任务设计和损失函数不同。</li><li><strong>联系:</strong> 都是自监督学习方法，旨在学习有用的表示，并且都可以利用无标签数据进行训练。</li></ul><p>理解MAE和对比学习的方法及其区别和联系，有助于选择合适的自监督学习方法来解决具体的任务和应用场景。</p><h2 id="问题八：目前主流的一些目标跟踪算法有哪些？"><a href="#问题八：目前主流的一些目标跟踪算法有哪些？" class="headerlink" title="问题八：目前主流的一些目标跟踪算法有哪些？"></a>问题八：目前主流的一些目标跟踪算法有哪些？</h2><p>目标跟踪是计算机视觉中的一个重要任务，旨在在视频序列中跟踪目标对象的运动轨迹。目前主流的目标跟踪算法可以根据使用的技术和方法分为以下几类：</p><h3 id="1-基于相关滤波器的目标跟踪算法"><a href="#1-基于相关滤波器的目标跟踪算法" class="headerlink" title="1. 基于相关滤波器的目标跟踪算法"></a>1. 基于相关滤波器的目标跟踪算法</h3><h4 id="1-1-KCF（Kernelized-Correlation-Filters）"><a href="#1-1-KCF（Kernelized-Correlation-Filters）" class="headerlink" title="1.1 KCF（Kernelized Correlation Filters）"></a><strong>1.1 KCF（Kernelized Correlation Filters）</strong></h4><ul><li><strong>特点：</strong> 使用线性核或高斯核相关滤波器进行跟踪，通过循环移位和快速傅里叶变换（FFT）实现高效计算。</li><li><strong>优势：</strong> 快速、实时、适用于低计算资源环境。</li><li><strong>缺点：</strong> 对于尺度变化和长时间遮挡的鲁棒性较差。</li><li><strong>代表论文：</strong> “High-Speed Tracking with Kernelized Correlation Filters” (CVPR 2015)</li></ul><h4 id="1-2-MOSSE（Minimum-Output-Sum-of-Squared-Error）"><a href="#1-2-MOSSE（Minimum-Output-Sum-of-Squared-Error）" class="headerlink" title="1.2 MOSSE（Minimum Output Sum of Squared Error）"></a><strong>1.2 MOSSE（Minimum Output Sum of Squared Error）</strong></h4><ul><li><strong>特点：</strong> 使用Mosse滤波器进行目标跟踪，通过最小化输出误差平方和进行滤波器的训练。</li><li><strong>优势：</strong> 快速、适用于低分辨率视频。</li><li><strong>缺点：</strong> 对复杂的目标形变和光照变化的鲁棒性较差。</li><li><strong>代表论文：</strong> “Visual Object Tracking using Adaptive Correlation Filters” (CVPR 2010)</li></ul><h3 id="2-基于深度学习的目标跟踪算法"><a href="#2-基于深度学习的目标跟踪算法" class="headerlink" title="2. 基于深度学习的目标跟踪算法"></a>2. 基于深度学习的目标跟踪算法</h3><h4 id="2-1-Siamese-Network-based-Trackers（孪生网络）"><a href="#2-1-Siamese-Network-based-Trackers（孪生网络）" class="headerlink" title="2.1 Siamese Network-based Trackers（孪生网络）"></a><strong>2.1 Siamese Network-based Trackers（孪生网络）</strong></h4><ul><li><p><strong>SiamFC（Fully-Convolutional Siamese Networks）：</strong></p><ul><li><strong>特点：</strong> 使用孪生网络架构，直接学习从目标模板到搜索区域的相似性映射。</li><li><strong>优势：</strong> 简单、高效、实时。</li><li><strong>缺点：</strong> 对于目标的尺度变化和旋转变化的处理较弱。</li><li><strong>代表论文：</strong> “Fully-Convolutional Siamese Networks for Object Tracking” (ECCV 2016)</li></ul></li><li><p><strong>SiamRPN（Region Proposal Network）：</strong></p><ul><li><strong>特点：</strong> 结合了区域提案网络（RPN）和孪生网络，通过引入区域建议框进行目标定位和尺度估计。</li><li><strong>优势：</strong> 在处理尺度变化和定位准确性上有显著提升。</li><li><strong>缺点：</strong> 模型复杂度和计算开销较大。</li><li><strong>代表论文：</strong> “High Performance Visual Tracking with Siamese Region Proposal Network” (CVPR 2018)</li></ul></li><li><p><strong>SiamMask：</strong></p><ul><li><strong>特点：</strong> 在SiamRPN的基础上增加了显式的目标分割功能，能够同时进行目标跟踪和分割。</li><li><strong>优势：</strong> 提高了跟踪精度和分割质量，适用于目标边界模糊的情况。</li><li><strong>缺点：</strong> 计算开销较大。</li><li><strong>代表论文：</strong> “SiamMask: Fast Online Object Tracking and Segmentation” (CVPR 2019)</li></ul></li></ul><h4 id="2-2-MDNet（Multi-Domain-Network）"><a href="#2-2-MDNet（Multi-Domain-Network）" class="headerlink" title="2.2 MDNet（Multi-Domain Network）"></a><strong>2.2 MDNet（Multi-Domain Network）</strong></h4><ul><li><strong>特点：</strong> 使用多域训练策略，通过共享前几层特征来提高不同目标之间的泛化能力，并在最后几层进行目标专用的微调。</li><li><strong>优势：</strong> 对于目标外观变化和背景干扰有较好的鲁棒性。</li><li><strong>缺点：</strong> 训练过程复杂，实时性较差。</li><li><strong>代表论文：</strong> “Learning Multi-Domain Convolutional Neural Networks for Visual Tracking” (CVPR 2016)</li></ul><h3 id="3-基于检测的目标跟踪算法"><a href="#3-基于检测的目标跟踪算法" class="headerlink" title="3. 基于检测的目标跟踪算法"></a>3. 基于检测的目标跟踪算法</h3><h4 id="3-1-SORT（Simple-Online-and-Realtime-Tracking）"><a href="#3-1-SORT（Simple-Online-and-Realtime-Tracking）" class="headerlink" title="3.1 SORT（Simple Online and Realtime Tracking）"></a><strong>3.1 SORT（Simple Online and Realtime Tracking）</strong></h4><ul><li><strong>特点：</strong> 基于卡尔曼滤波器和匈牙利算法的简单跟踪算法，通过检测结果和运动预测进行目标关联。</li><li><strong>优势：</strong> 简单、高效、适用于实时应用。</li><li><strong>缺点：</strong> 对于遮挡和目标外观变化的鲁棒性较差。</li><li><strong>代表论文：</strong> “Simple Online and Realtime Tracking with a Deep Association Metric” (ICCV 2017)</li></ul><h4 id="3-2-DeepSORT"><a href="#3-2-DeepSORT" class="headerlink" title="3.2 DeepSORT"></a><strong>3.2 DeepSORT</strong></h4><ul><li><strong>特点：</strong> 在SORT的基础上引入深度特征进行目标再识别，通过结合外观特征和运动信息提高目标关联的准确性。</li><li><strong>优势：</strong> 对于遮挡和外观变化有更好的鲁棒性。</li><li><strong>缺点：</strong> 计算开销较大。</li><li><strong>代表论文：</strong> “Simple Online and Realtime Tracking with a Deep Association Metric” (ICCV 2017)</li></ul><h3 id="4-基于强化学习的目标跟踪算法"><a href="#4-基于强化学习的目标跟踪算法" class="headerlink" title="4. 基于强化学习的目标跟踪算法"></a>4. 基于强化学习的目标跟踪算法</h3><h4 id="4-1-ATOM（Accurate-Tracking-by-Overlap-Maximization）"><a href="#4-1-ATOM（Accurate-Tracking-by-Overlap-Maximization）" class="headerlink" title="4.1 ATOM（Accurate Tracking by Overlap Maximization）"></a><strong>4.1 ATOM（Accurate Tracking by Overlap Maximization）</strong></h4><ul><li><strong>特点：</strong> 通过最大化目标重叠度进行跟踪，使用强化学习进行模型优化。</li><li><strong>优势：</strong> 在处理复杂背景和遮挡时有较好的表现。</li><li><strong>缺点：</strong> 训练过程复杂，实时性较差。</li><li><strong>代表论文：</strong> “ATOM: Accurate Tracking by Overlap Maximization” (CVPR 2019)</li></ul><h4 id="4-2-D3S（Discriminative-Single-Shot-Segmentation）"><a href="#4-2-D3S（Discriminative-Single-Shot-Segmentation）" class="headerlink" title="4.2 D3S（Discriminative Single Shot Segmentation）"></a><strong>4.2 D3S（Discriminative Single Shot Segmentation）</strong></h4><ul><li><strong>特点：</strong> 使用深度强化学习进行目标分割和跟踪，通过单阶段网络实现高效目标分割。</li><li><strong>优势：</strong> 处理目标边界模糊的情况效果较好。</li><li><strong>缺点：</strong> 计算开销较大，实时性较差。</li><li><strong>代表论文：</strong> “Discriminative Single Shot Segmentation Tracker” (CVPR 2020)</li></ul><h3 id="总结-5"><a href="#总结-5" class="headerlink" title="总结"></a>总结</h3><p>目标跟踪算法可以根据使用的技术和方法分为基于相关滤波器、基于深度学习、基于检测和基于强化学习的算法。每种方法都有其优势和缺点，具体选择哪种方法取决于应用场景和需求。</p><ul><li><strong>基于相关滤波器的方法：</strong> 适用于低计算资源环境，实时性好，但对复杂情况的鲁棒性较差。</li><li><strong>基于深度学习的方法：</strong> 学习能力强，适应性好，但计算开销较大。</li><li><strong>基于检测的方法：</strong> 通过结合检测结果进行跟踪，简单高效，但对目标外观变化的鲁棒性较差。</li><li><strong>基于强化学习的方法：</strong> 能处理复杂背景和遮挡，但训练过程复杂，实时性较差。</li></ul><p>在实际应用中，可以根据具体需求选择合适的目标跟踪算法，并结合不同方法的优势进行改进和优化。</p><h2 id="问题九：pytorch搭建一个简单的分类网络"><a href="#问题九：pytorch搭建一个简单的分类网络" class="headerlink" title="问题九：pytorch搭建一个简单的分类网络"></a>问题九：pytorch搭建一个简单的分类网络</h2><p>搭建一个简单的图像分类网络是使用PyTorch进行深度学习的基础任务之一。以下是一个使用PyTorch搭建简单卷积神经网络（CNN）用于图像分类的示例，包括数据加载、模型定义、训练和测试步骤。</p><h3 id="环境准备"><a href="#环境准备" class="headerlink" title="环境准备"></a>环境准备</h3><p>确保您已经安装了PyTorch和torchvision。可以使用以下命令安装：</p><pre class="line-numbers language-bash" data-language="bash"><code class="language-bash">pip <span class="token function">install</span> torch torchvision<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><h3 id="数据加载"><a href="#数据加载" class="headerlink" title="数据加载"></a>数据加载</h3><p>我们将使用torchvision中的CIFAR-10数据集作为示例数据集。CIFAR-10是一个包含10个类别、每个类别6000张32x32彩色图像的小型数据集。</p><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">import</span> torch<span class="token keyword">import</span> torchvision<span class="token keyword">import</span> torchvision<span class="token punctuation">.</span>transforms <span class="token keyword">as</span> transforms<span class="token comment"># 定义图像转换</span>transform <span class="token operator">=</span> transforms<span class="token punctuation">.</span>Compose<span class="token punctuation">(</span><span class="token punctuation">[</span>    transforms<span class="token punctuation">.</span>ToTensor<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span>    transforms<span class="token punctuation">.</span>Normalize<span class="token punctuation">(</span><span class="token punctuation">(</span><span class="token number">0.5</span><span class="token punctuation">,</span> <span class="token number">0.5</span><span class="token punctuation">,</span> <span class="token number">0.5</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token punctuation">(</span><span class="token number">0.5</span><span class="token punctuation">,</span> <span class="token number">0.5</span><span class="token punctuation">,</span> <span class="token number">0.5</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token comment"># 加载训练和测试数据集</span>trainset <span class="token operator">=</span> torchvision<span class="token punctuation">.</span>datasets<span class="token punctuation">.</span>CIFAR10<span class="token punctuation">(</span>root<span class="token operator">=</span><span class="token string">'./data'</span><span class="token punctuation">,</span> train<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span> download<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span> transform<span class="token operator">=</span>transform<span class="token punctuation">)</span>trainloader <span class="token operator">=</span> torch<span class="token punctuation">.</span>utils<span class="token punctuation">.</span>data<span class="token punctuation">.</span>DataLoader<span class="token punctuation">(</span>trainset<span class="token punctuation">,</span> batch_size<span class="token operator">=</span><span class="token number">32</span><span class="token punctuation">,</span> shuffle<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span> num_workers<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">)</span>testset <span class="token operator">=</span> torchvision<span class="token punctuation">.</span>datasets<span class="token punctuation">.</span>CIFAR10<span class="token punctuation">(</span>root<span class="token operator">=</span><span class="token string">'./data'</span><span class="token punctuation">,</span> train<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">,</span> download<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span> transform<span class="token operator">=</span>transform<span class="token punctuation">)</span>testloader <span class="token operator">=</span> torch<span class="token punctuation">.</span>utils<span class="token punctuation">.</span>data<span class="token punctuation">.</span>DataLoader<span class="token punctuation">(</span>testset<span class="token punctuation">,</span> batch_size<span class="token operator">=</span><span class="token number">32</span><span class="token punctuation">,</span> shuffle<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">,</span> num_workers<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">)</span><span class="token comment"># CIFAR-10类名</span>classes <span class="token operator">=</span> <span class="token punctuation">(</span><span class="token string">'plane'</span><span class="token punctuation">,</span> <span class="token string">'car'</span><span class="token punctuation">,</span> <span class="token string">'bird'</span><span class="token punctuation">,</span> <span class="token string">'cat'</span><span class="token punctuation">,</span> <span class="token string">'deer'</span><span class="token punctuation">,</span> <span class="token string">'dog'</span><span class="token punctuation">,</span> <span class="token string">'frog'</span><span class="token punctuation">,</span> <span class="token string">'horse'</span><span class="token punctuation">,</span> <span class="token string">'ship'</span><span class="token punctuation">,</span> <span class="token string">'truck'</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h3 id="模型定义"><a href="#模型定义" class="headerlink" title="模型定义"></a>模型定义</h3><p>我们将定义一个简单的卷积神经网络，包括两个卷积层和两个全连接层。</p><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">import</span> torch<span class="token punctuation">.</span>nn <span class="token keyword">as</span> nn<span class="token keyword">import</span> torch<span class="token punctuation">.</span>nn<span class="token punctuation">.</span>functional <span class="token keyword">as</span> F<span class="token keyword">class</span> <span class="token class-name">SimpleCNN</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span><span class="token punctuation">:</span>        <span class="token builtin">super</span><span class="token punctuation">(</span>SimpleCNN<span class="token punctuation">,</span> self<span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>        self<span class="token punctuation">.</span>conv1 <span class="token operator">=</span> nn<span class="token punctuation">.</span>Conv2d<span class="token punctuation">(</span><span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">16</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">,</span> padding<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span>  <span class="token comment"># 输入通道为3，输出通道为16，卷积核大小为3x3</span>        self<span class="token punctuation">.</span>pool <span class="token operator">=</span> nn<span class="token punctuation">.</span>MaxPool2d<span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">)</span>  <span class="token comment"># 最大池化层，核大小为2x2</span>        self<span class="token punctuation">.</span>conv2 <span class="token operator">=</span> nn<span class="token punctuation">.</span>Conv2d<span class="token punctuation">(</span><span class="token number">16</span><span class="token punctuation">,</span> <span class="token number">32</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">,</span> padding<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span>  <span class="token comment"># 输入通道为16，输出通道为32，卷积核大小为3x3</span>        self<span class="token punctuation">.</span>fc1 <span class="token operator">=</span> nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span><span class="token number">32</span> <span class="token operator">*</span> <span class="token number">8</span> <span class="token operator">*</span> <span class="token number">8</span><span class="token punctuation">,</span> <span class="token number">120</span><span class="token punctuation">)</span>  <span class="token comment"># 全连接层，输入大小为32*8*8，输出大小为120</span>        self<span class="token punctuation">.</span>fc2 <span class="token operator">=</span> nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span><span class="token number">120</span><span class="token punctuation">,</span> <span class="token number">10</span><span class="token punctuation">)</span>  <span class="token comment"># 全连接层，输入大小为120，输出大小为10（对应10个类别）</span>    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> x<span class="token punctuation">)</span><span class="token punctuation">:</span>        x <span class="token operator">=</span> self<span class="token punctuation">.</span>pool<span class="token punctuation">(</span>F<span class="token punctuation">.</span>relu<span class="token punctuation">(</span>self<span class="token punctuation">.</span>conv1<span class="token punctuation">(</span>x<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span>        x <span class="token operator">=</span> self<span class="token punctuation">.</span>pool<span class="token punctuation">(</span>F<span class="token punctuation">.</span>relu<span class="token punctuation">(</span>self<span class="token punctuation">.</span>conv2<span class="token punctuation">(</span>x<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span>        x <span class="token operator">=</span> x<span class="token punctuation">.</span>view<span class="token punctuation">(</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">32</span> <span class="token operator">*</span> <span class="token number">8</span> <span class="token operator">*</span> <span class="token number">8</span><span class="token punctuation">)</span>  <span class="token comment"># 展平特征图</span>        x <span class="token operator">=</span> F<span class="token punctuation">.</span>relu<span class="token punctuation">(</span>self<span class="token punctuation">.</span>fc1<span class="token punctuation">(</span>x<span class="token punctuation">)</span><span class="token punctuation">)</span>        x <span class="token operator">=</span> self<span class="token punctuation">.</span>fc2<span class="token punctuation">(</span>x<span class="token punctuation">)</span>        <span class="token keyword">return</span> x<span class="token comment"># 创建模型实例</span>net <span class="token operator">=</span> SimpleCNN<span class="token punctuation">(</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h3 id="定义损失函数和优化器"><a href="#定义损失函数和优化器" class="headerlink" title="定义损失函数和优化器"></a>定义损失函数和优化器</h3><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">import</span> torch<span class="token punctuation">.</span>optim <span class="token keyword">as</span> optimcriterion <span class="token operator">=</span> nn<span class="token punctuation">.</span>CrossEntropyLoss<span class="token punctuation">(</span><span class="token punctuation">)</span>  <span class="token comment"># 交叉熵损失</span>optimizer <span class="token operator">=</span> optim<span class="token punctuation">.</span>SGD<span class="token punctuation">(</span>net<span class="token punctuation">.</span>parameters<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span> lr<span class="token operator">=</span><span class="token number">0.001</span><span class="token punctuation">,</span> momentum<span class="token operator">=</span><span class="token number">0.9</span><span class="token punctuation">)</span>  <span class="token comment"># 随机梯度下降优化器</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span></span></code></pre><h3 id="训练模型"><a href="#训练模型" class="headerlink" title="训练模型"></a>训练模型</h3><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">for</span> epoch <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span><span class="token number">10</span><span class="token punctuation">)</span><span class="token punctuation">:</span>  <span class="token comment"># 训练10个epoch</span>    running_loss <span class="token operator">=</span> <span class="token number">0.0</span>    <span class="token keyword">for</span> i<span class="token punctuation">,</span> data <span class="token keyword">in</span> <span class="token builtin">enumerate</span><span class="token punctuation">(</span>trainloader<span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">)</span><span class="token punctuation">:</span>        inputs<span class="token punctuation">,</span> labels <span class="token operator">=</span> data                <span class="token comment"># 清零梯度</span>        optimizer<span class="token punctuation">.</span>zero_grad<span class="token punctuation">(</span><span class="token punctuation">)</span>                <span class="token comment"># 前向传播</span>        outputs <span class="token operator">=</span> net<span class="token punctuation">(</span>inputs<span class="token punctuation">)</span>                <span class="token comment"># 计算损失</span>        loss <span class="token operator">=</span> criterion<span class="token punctuation">(</span>outputs<span class="token punctuation">,</span> labels<span class="token punctuation">)</span>                <span class="token comment"># 反向传播和优化</span>        loss<span class="token punctuation">.</span>backward<span class="token punctuation">(</span><span class="token punctuation">)</span>        optimizer<span class="token punctuation">.</span>step<span class="token punctuation">(</span><span class="token punctuation">)</span>                <span class="token comment"># 打印统计信息</span>        running_loss <span class="token operator">+=</span> loss<span class="token punctuation">.</span>item<span class="token punctuation">(</span><span class="token punctuation">)</span>        <span class="token keyword">if</span> i <span class="token operator">%</span> <span class="token number">200</span> <span class="token operator">==</span> <span class="token number">199</span><span class="token punctuation">:</span>  <span class="token comment"># 每200个mini-batch打印一次</span>            <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string-interpolation"><span class="token string">f'Epoch </span><span class="token interpolation"><span class="token punctuation">&#123;</span>epoch <span class="token operator">+</span> <span class="token number">1</span><span class="token punctuation">&#125;</span></span><span class="token string">, Batch </span><span class="token interpolation"><span class="token punctuation">&#123;</span>i <span class="token operator">+</span> <span class="token number">1</span><span class="token punctuation">&#125;</span></span><span class="token string">, Loss: </span><span class="token interpolation"><span class="token punctuation">&#123;</span>running_loss <span class="token operator">/</span> <span class="token number">200</span><span class="token punctuation">:</span><span class="token format-spec">.3f</span><span class="token punctuation">&#125;</span></span><span class="token string">'</span></span><span class="token punctuation">)</span>            running_loss <span class="token operator">=</span> <span class="token number">0.0</span><span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">'Finished Training'</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h3 id="测试模型"><a href="#测试模型" class="headerlink" title="测试模型"></a>测试模型</h3><pre class="line-numbers language-python" data-language="python"><code class="language-python">correct <span class="token operator">=</span> <span class="token number">0</span>total <span class="token operator">=</span> <span class="token number">0</span><span class="token keyword">with</span> torch<span class="token punctuation">.</span>no_grad<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token keyword">for</span> data <span class="token keyword">in</span> testloader<span class="token punctuation">:</span>        images<span class="token punctuation">,</span> labels <span class="token operator">=</span> data        outputs <span class="token operator">=</span> net<span class="token punctuation">(</span>images<span class="token punctuation">)</span>        _<span class="token punctuation">,</span> predicted <span class="token operator">=</span> torch<span class="token punctuation">.</span><span class="token builtin">max</span><span class="token punctuation">(</span>outputs<span class="token punctuation">.</span>data<span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span>        total <span class="token operator">+=</span> labels<span class="token punctuation">.</span>size<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">)</span>        correct <span class="token operator">+=</span> <span class="token punctuation">(</span>predicted <span class="token operator">==</span> labels<span class="token punctuation">)</span><span class="token punctuation">.</span><span class="token builtin">sum</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>item<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string-interpolation"><span class="token string">f'Accuracy on the 10000 test images: </span><span class="token interpolation"><span class="token punctuation">&#123;</span><span class="token number">100</span> <span class="token operator">*</span> correct <span class="token operator">/</span> total<span class="token punctuation">:</span><span class="token format-spec">.2f</span><span class="token punctuation">&#125;</span></span><span class="token string">%'</span></span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h3 id="完整代码"><a href="#完整代码" class="headerlink" title="完整代码"></a>完整代码</h3><p>以下是上述步骤的完整代码：</p><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">import</span> torch<span class="token keyword">import</span> torchvision<span class="token keyword">import</span> torchvision<span class="token punctuation">.</span>transforms <span class="token keyword">as</span> transforms<span class="token keyword">import</span> torch<span class="token punctuation">.</span>nn <span class="token keyword">as</span> nn<span class="token keyword">import</span> torch<span class="token punctuation">.</span>nn<span class="token punctuation">.</span>functional <span class="token keyword">as</span> F<span class="token keyword">import</span> torch<span class="token punctuation">.</span>optim <span class="token keyword">as</span> optim<span class="token comment"># 数据加载和预处理</span>transform <span class="token operator">=</span> transforms<span class="token punctuation">.</span>Compose<span class="token punctuation">(</span><span class="token punctuation">[</span>    transforms<span class="token punctuation">.</span>ToTensor<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span>    transforms<span class="token punctuation">.</span>Normalize<span class="token punctuation">(</span><span class="token punctuation">(</span><span class="token number">0.5</span><span class="token punctuation">,</span> <span class="token number">0.5</span><span class="token punctuation">,</span> <span class="token number">0.5</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token punctuation">(</span><span class="token number">0.5</span><span class="token punctuation">,</span> <span class="token number">0.5</span><span class="token punctuation">,</span> <span class="token number">0.5</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">]</span><span class="token punctuation">)</span>trainset <span class="token operator">=</span> torchvision<span class="token punctuation">.</span>datasets<span class="token punctuation">.</span>CIFAR10<span class="token punctuation">(</span>root<span class="token operator">=</span><span class="token string">'./data'</span><span class="token punctuation">,</span> train<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span> download<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span> transform<span class="token operator">=</span>transform<span class="token punctuation">)</span>trainloader <span class="token operator">=</span> torch<span class="token punctuation">.</span>utils<span class="token punctuation">.</span>data<span class="token punctuation">.</span>DataLoader<span class="token punctuation">(</span>trainset<span class="token punctuation">,</span> batch_size<span class="token operator">=</span><span class="token number">32</span><span class="token punctuation">,</span> shuffle<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span> num_workers<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">)</span>testset <span class="token operator">=</span> torchvision<span class="token punctuation">.</span>datasets<span class="token punctuation">.</span>CIFAR10<span class="token punctuation">(</span>root<span class="token operator">=</span><span class="token string">'./data'</span><span class="token punctuation">,</span> train<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">,</span> download<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span> transform<span class="token operator">=</span>transform<span class="token punctuation">)</span>testloader <span class="token operator">=</span> torch<span class="token punctuation">.</span>utils<span class="token punctuation">.</span>data<span class="token punctuation">.</span>DataLoader<span class="token punctuation">(</span>testset<span class="token punctuation">,</span> batch_size<span class="token operator">=</span><span class="token number">32</span><span class="token punctuation">,</span> shuffle<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">,</span> num_workers<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">)</span>classes <span class="token operator">=</span> <span class="token punctuation">(</span><span class="token string">'plane'</span><span class="token punctuation">,</span> <span class="token string">'car'</span><span class="token punctuation">,</span> <span class="token string">'bird'</span><span class="token punctuation">,</span> <span class="token string">'cat'</span><span class="token punctuation">,</span> <span class="token string">'deer'</span><span class="token punctuation">,</span> <span class="token string">'dog'</span><span class="token punctuation">,</span> <span class="token string">'frog'</span><span class="token punctuation">,</span> <span class="token string">'horse'</span><span class="token punctuation">,</span> <span class="token string">'ship'</span><span class="token punctuation">,</span> <span class="token string">'truck'</span><span class="token punctuation">)</span><span class="token comment"># 定义模型</span><span class="token keyword">class</span> <span class="token class-name">SimpleCNN</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span><span class="token punctuation">:</span>        <span class="token builtin">super</span><span class="token punctuation">(</span>SimpleCNN<span class="token punctuation">,</span> self<span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>        self<span class="token punctuation">.</span>conv1 <span class="token operator">=</span> nn<span class="token punctuation">.</span>Conv2d<span class="token punctuation">(</span><span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">16</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">,</span> padding<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span>        self<span class="token punctuation">.</span>pool <span class="token operator">=</span> nn<span class="token punctuation">.</span>MaxPool2d<span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">)</span>        self<span class="token punctuation">.</span>conv2 <span class="token operator">=</span> nn<span class="token punctuation">.</span>Conv2d<span class="token punctuation">(</span><span class="token number">16</span><span class="token punctuation">,</span> <span class="token number">32</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">,</span> padding<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span>        self<span class="token punctuation">.</span>fc1 <span class="token operator">=</span> nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span><span class="token number">32</span> <span class="token operator">*</span> <span class="token number">8</span> <span class="token operator">*</span> <span class="token number">8</span><span class="token punctuation">,</span> <span class="token number">120</span><span class="token punctuation">)</span>        self<span class="token punctuation">.</span>fc2 <span class="token operator">=</span> nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span><span class="token number">120</span><span class="token punctuation">,</span> <span class="token number">10</span><span class="token punctuation">)</span>    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> x<span class="token punctuation">)</span><span class="token punctuation">:</span>        x <span class="token operator">=</span> self<span class="token punctuation">.</span>pool<span class="token punctuation">(</span>F<span class="token punctuation">.</span>relu<span class="token punctuation">(</span>self<span class="token punctuation">.</span>conv1<span class="token punctuation">(</span>x<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span>        x <span class="token operator">=</span> self<span class="token punctuation">.</span>pool<span class="token punctuation">(</span>F<span class="token punctuation">.</span>relu<span class="token punctuation">(</span>self<span class="token punctuation">.</span>conv2<span class="token punctuation">(</span>x<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span>        x <span class="token operator">=</span> x<span class="token punctuation">.</span>view<span class="token punctuation">(</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">32</span> <span class="token operator">*</span> <span class="token number">8</span> <span class="token operator">*</span> <span class="token number">8</span><span class="token punctuation">)</span>        x <span class="token operator">=</span> F<span class="token punctuation">.</span>relu<span class="token punctuation">(</span>self<span class="token punctuation">.</span>fc1<span class="token punctuation">(</span>x<span class="token punctuation">)</span><span class="token punctuation">)</span>        x <span class="token operator">=</span> self<span class="token punctuation">.</span>fc2<span class="token punctuation">(</span>x<span class="token punctuation">)</span>        <span class="token keyword">return</span> xnet <span class="token operator">=</span> SimpleCNN<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token comment"># 定义损失函数和优化器</span>criterion <span class="token operator">=</span> nn<span class="token punctuation">.</span>CrossEntropyLoss<span class="token punctuation">(</span><span class="token punctuation">)</span>optimizer <span class="token operator">=</span> optim<span class="token punctuation">.</span>SGD<span class="token punctuation">(</span>net<span class="token punctuation">.</span>parameters<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span> lr<span class="token operator">=</span><span class="token number">0.001</span><span class="token punctuation">,</span> momentum<span class="token operator">=</span><span class="token number">0.9</span><span class="token punctuation">)</span><span class="token comment"># 训练模型</span><span class="token keyword">for</span> epoch <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span><span class="token number">10</span><span class="token punctuation">)</span><span class="token punctuation">:</span>    running_loss <span class="token operator">=</span> <span class="token number">0.0</span>    <span class="token keyword">for</span> i<span class="token punctuation">,</span> data <span class="token keyword">in</span> <span class="token builtin">enumerate</span><span class="token punctuation">(</span>trainloader<span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">)</span><span class="token punctuation">:</span>        inputs<span class="token punctuation">,</span> labels <span class="token operator">=</span> data        optimizer<span class="token punctuation">.</span>zero_grad<span class="token punctuation">(</span><span class="token punctuation">)</span>        outputs <span class="token operator">=</span> net<span class="token punctuation">(</span>inputs<span class="token punctuation">)</span>        loss <span class="token operator">=</span> criterion<span class="token punctuation">(</span>outputs<span class="token punctuation">,</span> labels<span class="token punctuation">)</span>        loss<span class="token punctuation">.</span>backward<span class="token punctuation">(</span><span class="token punctuation">)</span>        optimizer<span class="token punctuation">.</span>step<span class="token punctuation">(</span><span class="token punctuation">)</span>        running_loss <span class="token operator">+=</span> loss<span class="token punctuation">.</span>item<span class="token punctuation">(</span><span class="token punctuation">)</span>        <span class="token keyword">if</span> i <span class="token operator">%</span> <span class="token number">200</span> <span class="token operator">==</span> <span class="token number">199</span><span class="token punctuation">:</span>            <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string-interpolation"><span class="token string">f'Epoch </span><span class="token interpolation"><span class="token punctuation">&#123;</span>epoch <span class="token operator">+</span> <span class="token number">1</span><span class="token punctuation">&#125;</span></span><span class="token string">, Batch </span><span class="token interpolation"><span class="token punctuation">&#123;</span>i <span class="token operator">+</span> <span class="token number">1</span><span class="token punctuation">&#125;</span></span><span class="token string">, Loss: </span><span class="token interpolation"><span class="token punctuation">&#123;</span>running_loss <span class="token operator">/</span> <span class="token number">200</span><span class="token punctuation">:</span><span class="token format-spec">.3f</span><span class="token punctuation">&#125;</span></span><span class="token string">'</span></span><span class="token punctuation">)</span>            running_loss <span class="token operator">=</span> <span class="token number">0.0</span><span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">'Finished Training'</span><span class="token punctuation">)</span><span class="token comment"># 测试模型</span>correct <span class="token operator">=</span> <span class="token number">0</span>total <span class="token operator">=</span> <span class="token number">0</span><span class="token keyword">with</span> torch<span class="token punctuation">.</span>no_grad<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token keyword">for</span> data <span class="token keyword">in</span> testloader<span class="token punctuation">:</span>        images<span class="token punctuation">,</span> labels <span class="token operator">=</span> data        outputs <span class="token operator">=</span> net<span class="token punctuation">(</span>images<span class="token punctuation">)</span>        _<span class="token punctuation">,</span> predicted <span class="token operator">=</span> torch<span class="token punctuation">.</span><span class="token builtin">max</span><span class="token punctuation">(</span>outputs<span class="token punctuation">.</span>data<span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span>        total <span class="token operator">+=</span> labels<span class="token punctuation">.</span>size<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">)</span>        correct <span class="token operator">+=</span> <span class="token punctuation">(</span>predicted <span class="token operator">==</span> labels<span class="token punctuation">)</span><span class="token punctuation">.</span><span class="token builtin">sum</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>item<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string-interpolation"><span class="token string">f'Accuracy on the 10000 test images: </span><span class="token interpolation"><span class="token punctuation">&#123;</span><span class="token number">100</span> <span class="token operator">*</span> correct <span class="token operator">/</span> total<span class="token punctuation">:</span><span class="token format-spec">.2f</span><span class="token punctuation">&#125;</span></span><span class="token string">%'</span></span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h3 id="总结-6"><a href="#总结-6" class="headerlink" title="总结"></a>总结</h3><p>以上代码展示了如何使用PyTorch构建一个简单的卷积神经网络（CNN）用于CIFAR-10图像分类任务。通过定义数据加载、模型、损失函数和优化器，并进行训练和测试，我们可以构建一个基本的分类模型。这是深度学习中非常常见的任务，对于理解和掌握深度学习模型的构建和训练过程非常有帮助。</p><h2 id="问题十：量化和通道剪枝怎么做的，一般的部署流程是怎样的"><a href="#问题十：量化和通道剪枝怎么做的，一般的部署流程是怎样的" class="headerlink" title="问题十：量化和通道剪枝怎么做的，一般的部署流程是怎样的"></a>问题十：量化和通道剪枝怎么做的，一般的部署流程是怎样的</h2><p>量化和通道剪枝是两种常见的模型压缩技术，用于减少深度学习模型的计算和存储需求，从而加快推理速度并减少内存占用。这些技术在模型部署到资源受限的设备（如移动设备、嵌入式系统）时尤为重要。</p><h3 id="1-量化（Quantization）"><a href="#1-量化（Quantization）" class="headerlink" title="1. 量化（Quantization）"></a>1. 量化（Quantization）</h3><p>量化通过将模型中的浮点数权重和激活值转换为低精度（如INT8）表示，从而减少模型大小和计算复杂度。</p><h4 id="PyTorch中的量化步骤"><a href="#PyTorch中的量化步骤" class="headerlink" title="PyTorch中的量化步骤"></a>PyTorch中的量化步骤</h4><ol><li><p><strong>准备训练好的模型：</strong> 假设已经有一个预训练的浮点模型。</p></li><li><p><strong>定义量化配置：</strong></p><ul><li>选择量化方案（如静态量化或动态量化）。</li><li>设置量化参数（如量化位宽）。</li></ul></li><li><p><strong>量化感知训练（QAT）：</strong> 模拟量化效果进行训练，微调模型以适应量化后的精度损失。</p></li><li><p><strong>模型转换和校准：</strong> 将模型转换为量化模型，并对校准数据进行推理，以确定量化参数。</p></li><li><p><strong>保存和加载量化模型：</strong></p></li></ol><p><strong>示例代码：</strong></p><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">import</span> torch<span class="token keyword">import</span> torch<span class="token punctuation">.</span>quantization <span class="token keyword">as</span> quantization<span class="token keyword">import</span> torchvision<span class="token punctuation">.</span>models <span class="token keyword">as</span> models<span class="token comment"># 加载预训练模型</span>model <span class="token operator">=</span> models<span class="token punctuation">.</span>resnet18<span class="token punctuation">(</span>pretrained<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span><span class="token comment"># 定义量化配置</span>model<span class="token punctuation">.</span>qconfig <span class="token operator">=</span> quantization<span class="token punctuation">.</span>get_default_qconfig<span class="token punctuation">(</span><span class="token string">'fbgemm'</span><span class="token punctuation">)</span><span class="token comment"># 准备量化模型</span>quantization<span class="token punctuation">.</span>prepare<span class="token punctuation">(</span>model<span class="token punctuation">,</span> inplace<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span><span class="token comment"># 模拟推理过程（可以使用少量训练数据）</span><span class="token comment"># 此步骤可以对模型进行校准</span>dummy_input <span class="token operator">=</span> torch<span class="token punctuation">.</span>randn<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">224</span><span class="token punctuation">,</span> <span class="token number">224</span><span class="token punctuation">)</span>model<span class="token punctuation">(</span>dummy_input<span class="token punctuation">)</span><span class="token comment"># 转换为量化模型</span>quantization<span class="token punctuation">.</span>convert<span class="token punctuation">(</span>model<span class="token punctuation">,</span> inplace<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span><span class="token comment"># 保存量化模型</span>torch<span class="token punctuation">.</span>save<span class="token punctuation">(</span>model<span class="token punctuation">.</span>state_dict<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token string">'quantized_model.pth'</span><span class="token punctuation">)</span><span class="token comment"># 加载量化模型</span>model<span class="token punctuation">.</span>load_state_dict<span class="token punctuation">(</span>torch<span class="token punctuation">.</span>load<span class="token punctuation">(</span><span class="token string">'quantized_model.pth'</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h3 id="2-通道剪枝（Channel-Pruning）"><a href="#2-通道剪枝（Channel-Pruning）" class="headerlink" title="2. 通道剪枝（Channel Pruning）"></a>2. 通道剪枝（Channel Pruning）</h3><p>通道剪枝通过移除卷积层中不重要的通道，减少模型的参数数量和计算量。</p><h4 id="PyTorch中的通道剪枝步骤"><a href="#PyTorch中的通道剪枝步骤" class="headerlink" title="PyTorch中的通道剪枝步骤"></a>PyTorch中的通道剪枝步骤</h4><ol><li><p><strong>准备训练好的模型：</strong> 假设已经有一个预训练的模型。</p></li><li><p><strong>定义剪枝方法：</strong></p><ul><li>选择剪枝策略（如基于L1范数、L2范数或稀疏性）。</li></ul></li><li><p><strong>计算剪枝掩码：</strong> 计算每个卷积层的通道重要性，并生成剪枝掩码。</p></li><li><p><strong>应用剪枝：</strong> 根据剪枝掩码移除不重要的通道，并微调模型。</p></li><li><p><strong>保存和加载剪枝模型：</strong></p></li></ol><p><strong>示例代码：</strong></p><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">import</span> torch<span class="token keyword">import</span> torch<span class="token punctuation">.</span>nn <span class="token keyword">as</span> nn<span class="token keyword">import</span> torch<span class="token punctuation">.</span>nn<span class="token punctuation">.</span>functional <span class="token keyword">as</span> F<span class="token keyword">import</span> torchvision<span class="token punctuation">.</span>models <span class="token keyword">as</span> models<span class="token keyword">class</span> <span class="token class-name">SimpleCNN</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span><span class="token punctuation">:</span>        <span class="token builtin">super</span><span class="token punctuation">(</span>SimpleCNN<span class="token punctuation">,</span> self<span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>        self<span class="token punctuation">.</span>conv1 <span class="token operator">=</span> nn<span class="token punctuation">.</span>Conv2d<span class="token punctuation">(</span><span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">16</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">,</span> padding<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span>        self<span class="token punctuation">.</span>pool <span class="token operator">=</span> nn<span class="token punctuation">.</span>MaxPool2d<span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">)</span>        self<span class="token punctuation">.</span>conv2 <span class="token operator">=</span> nn<span class="token punctuation">.</span>Conv2d<span class="token punctuation">(</span><span class="token number">16</span><span class="token punctuation">,</span> <span class="token number">32</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">,</span> padding<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span>        self<span class="token punctuation">.</span>fc1 <span class="token operator">=</span> nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span><span class="token number">32</span> <span class="token operator">*</span> <span class="token number">8</span> <span class="token operator">*</span> <span class="token number">8</span><span class="token punctuation">,</span> <span class="token number">120</span><span class="token punctuation">)</span>        self<span class="token punctuation">.</span>fc2 <span class="token operator">=</span> nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span><span class="token number">120</span><span class="token punctuation">,</span> <span class="token number">10</span><span class="token punctuation">)</span>    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> x<span class="token punctuation">)</span><span class="token punctuation">:</span>        x <span class="token operator">=</span> self<span class="token punctuation">.</span>pool<span class="token punctuation">(</span>F<span class="token punctuation">.</span>relu<span class="token punctuation">(</span>self<span class="token punctuation">.</span>conv1<span class="token punctuation">(</span>x<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span>        x <span class="token operator">=</span> self<span class="token punctuation">.</span>pool<span class="token punctuation">(</span>F<span class="token punctuation">.</span>relu<span class="token punctuation">(</span>self<span class="token punctuation">.</span>conv2<span class="token punctuation">(</span>x<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span>        x <span class="token operator">=</span> x<span class="token punctuation">.</span>view<span class="token punctuation">(</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">32</span> <span class="token operator">*</span> <span class="token number">8</span> <span class="token operator">*</span> <span class="token number">8</span><span class="token punctuation">)</span>        x <span class="token operator">=</span> F<span class="token punctuation">.</span>relu<span class="token punctuation">(</span>self<span class="token punctuation">.</span>fc1<span class="token punctuation">(</span>x<span class="token punctuation">)</span><span class="token punctuation">)</span>        x <span class="token operator">=</span> self<span class="token punctuation">.</span>fc2<span class="token punctuation">(</span>x<span class="token punctuation">)</span>        <span class="token keyword">return</span> x<span class="token comment"># 加载预训练模型</span>model <span class="token operator">=</span> SimpleCNN<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token comment"># 计算L1范数</span><span class="token keyword">def</span> <span class="token function">compute_l1_norm</span><span class="token punctuation">(</span>layer<span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token keyword">return</span> torch<span class="token punctuation">.</span><span class="token builtin">sum</span><span class="token punctuation">(</span>torch<span class="token punctuation">.</span><span class="token builtin">abs</span><span class="token punctuation">(</span>layer<span class="token punctuation">.</span>weight<span class="token punctuation">)</span><span class="token punctuation">,</span> dim<span class="token operator">=</span><span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token comment"># 剪枝比例</span>prune_ratio <span class="token operator">=</span> <span class="token number">0.5</span><span class="token comment"># 剪枝卷积层</span><span class="token keyword">for</span> name<span class="token punctuation">,</span> layer <span class="token keyword">in</span> model<span class="token punctuation">.</span>named_modules<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token keyword">if</span> <span class="token builtin">isinstance</span><span class="token punctuation">(</span>layer<span class="token punctuation">,</span> nn<span class="token punctuation">.</span>Conv2d<span class="token punctuation">)</span><span class="token punctuation">:</span>        l1_norm <span class="token operator">=</span> compute_l1_norm<span class="token punctuation">(</span>layer<span class="token punctuation">)</span>        threshold <span class="token operator">=</span> torch<span class="token punctuation">.</span>quantile<span class="token punctuation">(</span>l1_norm<span class="token punctuation">,</span> prune_ratio<span class="token punctuation">)</span>        mask <span class="token operator">=</span> l1_norm <span class="token operator">></span> threshold        layer<span class="token punctuation">.</span>weight<span class="token punctuation">.</span>data <span class="token operator">=</span> layer<span class="token punctuation">.</span>weight<span class="token punctuation">.</span>data<span class="token punctuation">[</span>mask<span class="token punctuation">]</span>        <span class="token keyword">if</span> layer<span class="token punctuation">.</span>bias <span class="token keyword">is</span> <span class="token keyword">not</span> <span class="token boolean">None</span><span class="token punctuation">:</span>            layer<span class="token punctuation">.</span>bias<span class="token punctuation">.</span>data <span class="token operator">=</span> layer<span class="token punctuation">.</span>bias<span class="token punctuation">.</span>data<span class="token punctuation">[</span>mask<span class="token punctuation">]</span><span class="token comment"># 微调模型</span><span class="token comment"># 省略微调代码</span><span class="token comment"># 保存剪枝模型</span>torch<span class="token punctuation">.</span>save<span class="token punctuation">(</span>model<span class="token punctuation">.</span>state_dict<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token string">'pruned_model.pth'</span><span class="token punctuation">)</span><span class="token comment"># 加载剪枝模型</span>model<span class="token punctuation">.</span>load_state_dict<span class="token punctuation">(</span>torch<span class="token punctuation">.</span>load<span class="token punctuation">(</span><span class="token string">'pruned_model.pth'</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h3 id="3-部署流程"><a href="#3-部署流程" class="headerlink" title="3. 部署流程"></a>3. 部署流程</h3><ol><li><p><strong>训练和验证：</strong></p><ul><li>训练和验证浮点模型，确保模型性能达到预期。</li></ul></li><li><p><strong>量化或剪枝：</strong></p><ul><li>根据需求选择量化或剪枝技术，进行模型压缩。</li><li>如果选择量化，可以进行量化感知训练（QAT）。</li><li>如果选择剪枝，可以进行剪枝后微调（Fine-Tuning）。</li></ul></li><li><p><strong>模型转换：</strong></p><ul><li>将压缩后的模型转换为适合部署的格式（如ONNX、TensorRT）。</li></ul></li><li><p><strong>测试和校准：</strong></p><ul><li>对转换后的模型进行测试，确保精度和性能满足要求。</li><li>对量化模型进行校准，确定量化参数。</li></ul></li><li><p><strong>部署：</strong></p><ul><li>将模型部署到目标设备（如移动设备、嵌入式系统或云服务器）。</li></ul></li><li><p><strong>监控和优化：</strong></p><ul><li>部署后监控模型的性能和行为，进行必要的优化和调整。</li></ul></li></ol><h3 id="总结-7"><a href="#总结-7" class="headerlink" title="总结"></a>总结</h3><ul><li><strong>量化：</strong> 通过将浮点数权重和激活值转换为低精度表示来减少计算和存储需求。</li><li><strong>通道剪枝：</strong> 通过移除不重要的通道减少模型参数数量和计算量。</li><li><strong>部署流程：</strong> 包括训练、模型压缩、转换、测试、校准、部署和优化。</li></ul><p>通过量化和通道剪枝，可以有效地减少模型的计算和存储开销，从而提高模型的部署效率和实际应用性能。</p><h2 id="问题十一：常见的匹配方法有哪些，比如匈牙利匹配等"><a href="#问题十一：常见的匹配方法有哪些，比如匈牙利匹配等" class="headerlink" title="问题十一：常见的匹配方法有哪些，比如匈牙利匹配等"></a>问题十一：常见的匹配方法有哪些，比如匈牙利匹配等</h2><p>在计算机科学和数学中，匹配算法用于解决各种配对和分配问题。以下是一些常见的匹配方法，包括匈牙利算法：</p><h3 id="1-匈牙利算法-Hungarian-Algorithm"><a href="#1-匈牙利算法-Hungarian-Algorithm" class="headerlink" title="1. 匈牙利算法 (Hungarian Algorithm)"></a>1. 匈牙利算法 (Hungarian Algorithm)</h3><ul><li><strong>用途</strong>：解决二分图中带权匹配问题，特别是寻找最小权匹配。</li><li><strong>描述</strong>：该算法通过构造最优零矩阵和调整权重，找到二分图中所有可能匹配的最小权值。</li><li><strong>应用</strong>：常用于任务分配问题，如分配工作给员工，使得总成本最小化。</li></ul><h3 id="2-最大匹配-Maximum-Matching"><a href="#2-最大匹配-Maximum-Matching" class="headerlink" title="2. 最大匹配 (Maximum Matching)"></a>2. 最大匹配 (Maximum Matching)</h3><ul><li><strong>用途</strong>：寻找二分图中匹配边数最多的匹配。</li><li><strong>描述</strong>：通过增广路径方法，逐步增加匹配边数，直到无法找到新的增广路径。</li><li><strong>应用</strong>：广泛用于网络配对、社交网络分析等领域。</li></ul><h3 id="3-最小完美匹配-Minimum-Perfect-Matching"><a href="#3-最小完美匹配-Minimum-Perfect-Matching" class="headerlink" title="3. 最小完美匹配 (Minimum Perfect Matching)"></a>3. 最小完美匹配 (Minimum Perfect Matching)</h3><ul><li><strong>用途</strong>：寻找加权二分图中总权重最小的完美匹配。</li><li><strong>描述</strong>：在图中找到每个顶点都匹配且总权重最小的匹配。</li><li><strong>应用</strong>：应用于优化资源分配问题，如物流配送。</li></ul><h3 id="4-Gale-Shapley算法-Gale-Shapley-Algorithm"><a href="#4-Gale-Shapley算法-Gale-Shapley-Algorithm" class="headerlink" title="4. Gale-Shapley算法 (Gale-Shapley Algorithm)"></a>4. Gale-Shapley算法 (Gale-Shapley Algorithm)</h3><ul><li><strong>用途</strong>：解决稳定婚姻问题，确保每个匹配都是稳定的。</li><li><strong>描述</strong>：通过提出和拒绝机制，确保所有配对中的每个人都得到最合适的配对，且不会有两个互相喜欢的个体愿意抛弃各自的配对。</li><li><strong>应用</strong>：应用于学校分配学生、医院分配实习医生等。</li></ul><h3 id="5-Hopcroft-Karp算法-Hopcroft-Karp-Algorithm"><a href="#5-Hopcroft-Karp算法-Hopcroft-Karp-Algorithm" class="headerlink" title="5. Hopcroft-Karp算法 (Hopcroft-Karp Algorithm)"></a>5. Hopcroft-Karp算法 (Hopcroft-Karp Algorithm)</h3><ul><li><strong>用途</strong>：用于寻找二分图的最大匹配。</li><li><strong>描述</strong>：通过广度优先搜索（BFS）和深度优先搜索（DFS）相结合的方式，快速找到最大匹配。</li><li><strong>应用</strong>：适用于网络流量优化、任务分配等。</li></ul><h3 id="6-Blossom算法-Edmonds’-Algorithm"><a href="#6-Blossom算法-Edmonds’-Algorithm" class="headerlink" title="6. Blossom算法 (Edmonds’ Algorithm)"></a>6. Blossom算法 (Edmonds’ Algorithm)</h3><ul><li><strong>用途</strong>：用于一般图的最大匹配。</li><li><strong>描述</strong>：通过处理图中的“花”结构，解决一般图中最大匹配问题。</li><li><strong>应用</strong>：适用于各种不局限于二分图的匹配问题，如图着色问题。</li></ul><h3 id="7-Kuhn-Munkres算法-Kuhn-Munkres-Algorithm"><a href="#7-Kuhn-Munkres算法-Kuhn-Munkres-Algorithm" class="headerlink" title="7. Kuhn-Munkres算法 (Kuhn-Munkres Algorithm)"></a>7. Kuhn-Munkres算法 (Kuhn-Munkres Algorithm)</h3><ul><li><strong>用途</strong>：解决二分图中带权匹配的最大权匹配问题。</li><li><strong>描述</strong>：通过逐步构建最优匹配，找到最大权匹配。</li><li><strong>应用</strong>：应用于分配问题，如任务分配、资源分配等。</li></ul><h3 id="8-贪心算法-Greedy-Algorithm"><a href="#8-贪心算法-Greedy-Algorithm" class="headerlink" title="8. 贪心算法 (Greedy Algorithm)"></a>8. 贪心算法 (Greedy Algorithm)</h3><ul><li><strong>用途</strong>：解决匹配问题的近似解。</li><li><strong>描述</strong>：通过每次选择当前最优解，逐步构建匹配。</li><li><strong>应用</strong>：适用于快速求解问题的近似解，如在线匹配问题。</li></ul><p>这些匹配方法各有其应用场景和优势，选择合适的算法取决于具体问题的需求和约束条件。</p><h2 id="问题十二：对ROS和Docker的了解多吗"><a href="#问题十二：对ROS和Docker的了解多吗" class="headerlink" title="问题十二：对ROS和Docker的了解多吗"></a>问题十二：对ROS和Docker的了解多吗</h2><h3 id="ROS-Robot-Operating-System"><a href="#ROS-Robot-Operating-System" class="headerlink" title="ROS (Robot Operating System)"></a>ROS (Robot Operating System)</h3><p><strong>简介</strong>：<br>ROS 是一个用于机器人软件开发的灵活框架，它提供了一系列工具、库和约定，旨在简化复杂机器人行为的创建。</p><p><strong>核心功能</strong>：</p><ol><li><strong>通信机制</strong>：ROS提供了节点间通信的多种机制，如主题（Topics）、服务（Services）和动作（Actions），支持分布式系统的开发。</li><li><strong>工具链</strong>：包括调试、可视化、日志记录和数据分析工具，如RViz（可视化工具）、rqt（图形化界面工具）和rosbag（日志记录工具）。</li><li><strong>包管理</strong>：ROS使用包（Packages）和工作空间（Workspaces）来组织代码和资源，便于模块化开发和分发。</li><li><strong>仿真</strong>：支持Gazebo等仿真器，方便开发者在虚拟环境中测试和验证算法。</li><li><strong>社区支持</strong>：拥有庞大的开源社区和丰富的资源，开发者可以利用现有的ROS包和库，快速搭建机器人系统。</li></ol><p><strong>应用场景</strong>：</p><ul><li>自主导航</li><li>机器人控制</li><li>传感器数据处理</li><li>多机器人系统协调</li></ul><h3 id="Docker"><a href="#Docker" class="headerlink" title="Docker"></a>Docker</h3><p><strong>简介</strong>：<br>Docker 是一个开源的容器化平台，允许开发者在隔离的环境中打包、分发和运行应用程序。Docker容器包含了应用程序运行所需的所有依赖项，使得应用程序在不同环境中具有一致的行为。</p><p><strong>核心功能</strong>：</p><ol><li><strong>容器化</strong>：Docker容器是轻量级的独立运行环境，与虚拟机不同，容器共享主机的操作系统内核，启动速度更快，资源占用更少。</li><li><strong>镜像</strong>：Docker镜像是只读模板，用于创建容器。镜像可以通过Docker Hub等仓库进行分发和共享。</li><li><strong>Dockerfile</strong>：用于定义镜像构建过程的文件，包含一系列指令，指定如何从基础镜像构建新的镜像。</li><li><strong>编排</strong>：通过Docker Compose、Kubernetes等工具，可以管理多个容器的部署、扩展和网络配置。</li><li><strong>隔离和安全</strong>：Docker通过命名空间和控制组（cgroups）提供进程隔离和资源限制，增强了应用程序的安全性。</li></ol><p><strong>应用场景</strong>：</p><ul><li>微服务架构</li><li>持续集成&#x2F;持续部署（CI&#x2F;CD）</li><li>开发和测试环境一致性</li><li>分布式应用程序的部署</li></ul><h3 id="结合使用"><a href="#结合使用" class="headerlink" title="结合使用"></a>结合使用</h3><p><strong>ROS和Docker结合的优势</strong>：</p><ol><li><strong>环境一致性</strong>：在不同开发、测试和生产环境中保持一致的ROS运行环境。</li><li><strong>依赖管理</strong>：容器化ROS系统，简化复杂依赖关系的管理，避免依赖冲突。</li><li><strong>快速部署</strong>：通过Docker镜像和容器快速部署ROS应用，便于分布式系统的开发和测试。</li><li><strong>资源隔离</strong>：Docker容器提供资源隔离，确保多个ROS节点或服务之间不会相互干扰。</li></ol><p><strong>实践案例</strong>：</p><ol><li><strong>ROS开发环境容器化</strong>：将ROS开发环境打包成Docker镜像，便于开发者快速搭建开发环境。</li><li><strong>仿真和测试</strong>：使用Docker容器运行ROS仿真器，如Gazebo，便于在CI&#x2F;CD流水线中进行自动化测试。</li><li><strong>分布式系统</strong>：利用Docker Compose或Kubernetes编排多个ROS节点，构建和管理复杂的分布式机器人系统。</li></ol><p>通过结合ROS和Docker，可以大大提升机器人系统的开发、测试和部署效率，减少环境配置和依赖管理的复杂性。</p><h2 id="问题十三：对linux系统用的多吗，介绍一些常用的指令"><a href="#问题十三：对linux系统用的多吗，介绍一些常用的指令" class="headerlink" title="问题十三：对linux系统用的多吗，介绍一些常用的指令"></a>问题十三：对linux系统用的多吗，介绍一些常用的指令</h2><p>是的，Linux系统在开发、运维和日常计算中非常常用。以下是一些常用的Linux命令及其简介：</p><h3 id="文件与目录管理"><a href="#文件与目录管理" class="headerlink" title="文件与目录管理"></a>文件与目录管理</h3><ol><li><p><strong>ls</strong>：列出目录内容</p><pre class="line-numbers language-bash" data-language="bash"><code class="language-bash"><span class="token function">ls</span>        <span class="token comment"># 列出当前目录内容</span><span class="token function">ls</span> <span class="token parameter variable">-l</span>     <span class="token comment"># 列出详细信息</span><span class="token function">ls</span> <span class="token parameter variable">-a</span>     <span class="token comment"># 列出所有文件，包括隐藏文件</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre></li><li><p><strong>cd</strong>：改变当前目录</p><pre class="line-numbers language-bash" data-language="bash"><code class="language-bash"><span class="token builtin class-name">cd</span> /path/to/directory   <span class="token comment"># 进入指定目录</span><span class="token builtin class-name">cd</span> <span class="token punctuation">..</span>                   <span class="token comment"># 返回上一级目录</span><span class="token builtin class-name">cd</span> ~                    <span class="token comment"># 进入用户主目录</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre></li><li><p><strong>pwd</strong>：显示当前工作目录</p><pre class="line-numbers language-bash" data-language="bash"><code class="language-bash"><span class="token builtin class-name">pwd</span>   <span class="token comment"># 显示当前路径</span><span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre></li><li><p><strong>mkdir</strong>：创建目录</p><pre class="line-numbers language-bash" data-language="bash"><code class="language-bash"><span class="token function">mkdir</span> mydir        <span class="token comment"># 创建名为mydir的目录</span><span class="token function">mkdir</span> <span class="token parameter variable">-p</span> mydir/subdir  <span class="token comment"># 递归创建目录</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre></li><li><p><strong>rmdir</strong>：删除空目录</p><pre class="line-numbers language-bash" data-language="bash"><code class="language-bash"><span class="token function">rmdir</span> mydir   <span class="token comment"># 删除名为mydir的空目录</span><span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre></li><li><p><strong>rm</strong>：删除文件或目录</p><pre class="line-numbers language-bash" data-language="bash"><code class="language-bash"><span class="token function">rm</span> file.txt        <span class="token comment"># 删除文件</span><span class="token function">rm</span> <span class="token parameter variable">-r</span> mydir        <span class="token comment"># 递归删除目录及其内容</span><span class="token function">rm</span> <span class="token parameter variable">-f</span> file.txt     <span class="token comment"># 强制删除文件</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre></li><li><p><strong>cp</strong>：复制文件或目录</p><pre class="line-numbers language-bash" data-language="bash"><code class="language-bash"><span class="token function">cp</span> source.txt destination.txt    <span class="token comment"># 复制文件</span><span class="token function">cp</span> <span class="token parameter variable">-r</span> sourcedir targetdir        <span class="token comment"># 递归复制目录</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre></li><li><p><strong>mv</strong>：移动或重命名文件或目录</p><pre class="line-numbers language-bash" data-language="bash"><code class="language-bash"><span class="token function">mv</span> oldname.txt newname.txt     <span class="token comment"># 重命名文件</span><span class="token function">mv</span> file.txt /path/to/directory <span class="token comment"># 移动文件到指定目录</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre></li></ol><h3 id="文件内容操作"><a href="#文件内容操作" class="headerlink" title="文件内容操作"></a>文件内容操作</h3><ol><li><p><strong>cat</strong>：连接并显示文件内容</p><pre class="line-numbers language-bash" data-language="bash"><code class="language-bash"><span class="token function">cat</span> file.txt   <span class="token comment"># 显示文件内容</span><span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre></li><li><p><strong>more</strong> 和 <strong>less</strong>：分页显示文件内容</p><pre class="line-numbers language-bash" data-language="bash"><code class="language-bash"><span class="token function">more</span> file.txt   <span class="token comment"># 分页显示文件内容（按空格键翻页）</span><span class="token function">less</span> file.txt   <span class="token comment"># 分页显示文件内容（支持前后翻页）</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre></li><li><p><strong>head</strong> 和 <strong>tail</strong>：显示文件的开头或结尾部分</p><pre class="line-numbers language-bash" data-language="bash"><code class="language-bash"><span class="token function">head</span> <span class="token parameter variable">-n</span> <span class="token number">10</span> file.txt   <span class="token comment"># 显示文件前10行</span><span class="token function">tail</span> <span class="token parameter variable">-n</span> <span class="token number">10</span> file.txt   <span class="token comment"># 显示文件后10行</span><span class="token function">tail</span> <span class="token parameter variable">-f</span> log.txt       <span class="token comment"># 实时显示文件新增内容</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre></li><li><p><strong>grep</strong>：搜索文本内容</p><pre class="line-numbers language-bash" data-language="bash"><code class="language-bash"><span class="token function">grep</span> <span class="token string">"search_term"</span> file.txt        <span class="token comment"># 在文件中搜索字符串</span><span class="token function">grep</span> <span class="token parameter variable">-r</span> <span class="token string">"search_term"</span> /path/to/dir <span class="token comment"># 递归搜索目录中的字符串</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre></li></ol><h3 id="系统管理"><a href="#系统管理" class="headerlink" title="系统管理"></a>系统管理</h3><ol><li><p><strong>ps</strong>：显示当前进程</p><pre class="line-numbers language-bash" data-language="bash"><code class="language-bash"><span class="token function">ps</span>          <span class="token comment"># 显示当前会话中的进程</span><span class="token function">ps</span> <span class="token parameter variable">-aux</span>     <span class="token comment"># 显示所有进程的详细信息</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre></li><li><p><strong>top</strong>：实时显示系统资源使用情况</p><pre class="line-numbers language-bash" data-language="bash"><code class="language-bash"><span class="token function">top</span>   <span class="token comment"># 显示系统的实时资源使用情况</span><span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre></li><li><p><strong>kill</strong>：终止进程</p><pre class="line-numbers language-bash" data-language="bash"><code class="language-bash"><span class="token function">kill</span> PID          <span class="token comment"># 终止指定PID的进程</span><span class="token function">kill</span> <span class="token parameter variable">-9</span> PID       <span class="token comment"># 强制终止指定PID的进程</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre></li><li><p><strong>df</strong>：显示文件系统的磁盘使用情况</p><pre class="line-numbers language-bash" data-language="bash"><code class="language-bash"><span class="token function">df</span> <span class="token parameter variable">-h</span>   <span class="token comment"># 显示人类可读格式的磁盘使用情况</span><span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre></li><li><p><strong>du</strong>：显示目录或文件的磁盘使用情况</p><pre class="line-numbers language-bash" data-language="bash"><code class="language-bash"><span class="token function">du</span> <span class="token parameter variable">-h</span> /path/to/dir   <span class="token comment"># 显示目录或文件的人类可读格式的磁盘使用情况</span><span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre></li></ol><h3 id="权限管理"><a href="#权限管理" class="headerlink" title="权限管理"></a>权限管理</h3><ol><li><p><strong>chmod</strong>：更改文件或目录的权限</p><pre class="line-numbers language-bash" data-language="bash"><code class="language-bash"><span class="token function">chmod</span> <span class="token number">755</span> file.txt        <span class="token comment"># 设置文件的权限为755（所有者读写执行，组和其他用户只读执行）</span><span class="token function">chmod</span> <span class="token parameter variable">-R</span> <span class="token number">755</span> /path/to/dir <span class="token comment"># 递归更改目录权限</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre></li><li><p><strong>chown</strong>：更改文件或目录的所有者</p><pre class="line-numbers language-bash" data-language="bash"><code class="language-bash"><span class="token function">chown</span> user:group file.txt        <span class="token comment"># 更改文件的所有者和组</span><span class="token function">chown</span> <span class="token parameter variable">-R</span> user:group /path/to/dir <span class="token comment"># 递归更改目录的所有者和组</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre></li></ol><h3 id="网络操作"><a href="#网络操作" class="headerlink" title="网络操作"></a>网络操作</h3><ol><li><p><strong>ping</strong>：测试网络连接</p><pre class="line-numbers language-bash" data-language="bash"><code class="language-bash"><span class="token function">ping</span> www.example.com   <span class="token comment"># 测试与主机的连通性</span><span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre></li><li><p><strong>ifconfig</strong>：显示或配置网络接口（较旧，推荐使用ip命令）</p><pre class="line-numbers language-bash" data-language="bash"><code class="language-bash"><span class="token function">ifconfig</span>          <span class="token comment"># 显示网络接口信息</span><span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre></li><li><p><strong>ip</strong>：显示或配置网络接口（较新）</p><pre class="line-numbers language-bash" data-language="bash"><code class="language-bash"><span class="token function">ip</span> addr show      <span class="token comment"># 显示网络接口信息</span><span class="token function">ip</span> <span class="token function">link</span> <span class="token builtin class-name">set</span> eth0 up   <span class="token comment"># 启用网络接口</span><span class="token function">ip</span> <span class="token function">link</span> <span class="token builtin class-name">set</span> eth0 down <span class="token comment"># 禁用网络接口</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre></li></ol><h3 id="包管理（以Debian系为例，如Ubuntu）"><a href="#包管理（以Debian系为例，如Ubuntu）" class="headerlink" title="包管理（以Debian系为例，如Ubuntu）"></a>包管理（以Debian系为例，如Ubuntu）</h3><ol><li><strong>apt-get</strong> 和 <strong>apt</strong>：包管理工具<pre class="line-numbers language-bash" data-language="bash"><code class="language-bash"><span class="token function">sudo</span> <span class="token function">apt-get</span> update        <span class="token comment"># 更新包列表</span><span class="token function">sudo</span> <span class="token function">apt-get</span> upgrade       <span class="token comment"># 升级所有已安装的软件包</span><span class="token function">sudo</span> <span class="token function">apt-get</span> <span class="token function">install</span> package_name   <span class="token comment"># 安装软件包</span><span class="token function">sudo</span> <span class="token function">apt-get</span> remove package_name    <span class="token comment"># 卸载软件包</span><span class="token function">sudo</span> <span class="token function">apt-get</span> autoremove    <span class="token comment"># 自动卸载不再需要的软件包</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span></span></code></pre></li></ol><p>这些命令只是Linux系统的冰山一角，熟练使用这些命令可以极大地提高操作效率和系统管理能力。</p><h2 id="问题十四：对linux网络编程了解的多吗，介绍一下线程和进程"><a href="#问题十四：对linux网络编程了解的多吗，介绍一下线程和进程" class="headerlink" title="问题十四：对linux网络编程了解的多吗，介绍一下线程和进程"></a>问题十四：对linux网络编程了解的多吗，介绍一下线程和进程</h2><h3 id="Linux-网络编程"><a href="#Linux-网络编程" class="headerlink" title="Linux 网络编程"></a>Linux 网络编程</h3><p>Linux网络编程涉及使用套接字（sockets）接口来实现网络通信。套接字是网络通信的基本机制，它允许程序在网络中发送和接收数据。</p><h4 id="套接字基础"><a href="#套接字基础" class="headerlink" title="套接字基础"></a>套接字基础</h4><ol><li><p><strong>套接字类型</strong>：</p><ul><li><strong>流套接字（SOCK_STREAM）</strong>：用于TCP协议的可靠、有序、基于连接的数据传输。</li><li><strong>数据报套接字（SOCK_DGRAM）</strong>：用于UDP协议的无连接、尽力而为的数据传输。</li></ul></li><li><p><strong>常用系统调用</strong>：</p><ul><li>**socket()**：创建一个新的套接字。<pre class="line-numbers language-c" data-language="c"><code class="language-c"><span class="token keyword">int</span> <span class="token function">socket</span><span class="token punctuation">(</span><span class="token keyword">int</span> domain<span class="token punctuation">,</span> <span class="token keyword">int</span> type<span class="token punctuation">,</span> <span class="token keyword">int</span> protocol<span class="token punctuation">)</span><span class="token punctuation">;</span><span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre></li><li>**bind()**：绑定套接字到一个地址（IP 地址和端口）。<pre class="line-numbers language-c" data-language="c"><code class="language-c"><span class="token keyword">int</span> <span class="token function">bind</span><span class="token punctuation">(</span><span class="token keyword">int</span> sockfd<span class="token punctuation">,</span> <span class="token keyword">const</span> <span class="token keyword">struct</span> <span class="token class-name">sockaddr</span> <span class="token operator">*</span>addr<span class="token punctuation">,</span> <span class="token class-name">socklen_t</span> addrlen<span class="token punctuation">)</span><span class="token punctuation">;</span><span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre></li><li>**listen()**：监听来自客户端的连接请求（仅用于TCP）。<pre class="line-numbers language-c" data-language="c"><code class="language-c"><span class="token keyword">int</span> <span class="token function">listen</span><span class="token punctuation">(</span><span class="token keyword">int</span> sockfd<span class="token punctuation">,</span> <span class="token keyword">int</span> backlog<span class="token punctuation">)</span><span class="token punctuation">;</span><span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre></li><li>**accept()**：接受一个客户端的连接请求（仅用于TCP）。<pre class="line-numbers language-c" data-language="c"><code class="language-c"><span class="token keyword">int</span> <span class="token function">accept</span><span class="token punctuation">(</span><span class="token keyword">int</span> sockfd<span class="token punctuation">,</span> <span class="token keyword">struct</span> <span class="token class-name">sockaddr</span> <span class="token operator">*</span>addr<span class="token punctuation">,</span> <span class="token class-name">socklen_t</span> <span class="token operator">*</span>addrlen<span class="token punctuation">)</span><span class="token punctuation">;</span><span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre></li><li>**connect()**：客户端用来连接服务器。<pre class="line-numbers language-c" data-language="c"><code class="language-c"><span class="token keyword">int</span> <span class="token function">connect</span><span class="token punctuation">(</span><span class="token keyword">int</span> sockfd<span class="token punctuation">,</span> <span class="token keyword">const</span> <span class="token keyword">struct</span> <span class="token class-name">sockaddr</span> <span class="token operator">*</span>addr<span class="token punctuation">,</span> <span class="token class-name">socklen_t</span> addrlen<span class="token punctuation">)</span><span class="token punctuation">;</span><span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre></li><li><strong>send()</strong> 和 **recv()**：发送和接收数据。<pre class="line-numbers language-c" data-language="c"><code class="language-c"><span class="token class-name">ssize_t</span> <span class="token function">send</span><span class="token punctuation">(</span><span class="token keyword">int</span> sockfd<span class="token punctuation">,</span> <span class="token keyword">const</span> <span class="token keyword">void</span> <span class="token operator">*</span>buf<span class="token punctuation">,</span> <span class="token class-name">size_t</span> len<span class="token punctuation">,</span> <span class="token keyword">int</span> flags<span class="token punctuation">)</span><span class="token punctuation">;</span><span class="token class-name">ssize_t</span> <span class="token function">recv</span><span class="token punctuation">(</span><span class="token keyword">int</span> sockfd<span class="token punctuation">,</span> <span class="token keyword">void</span> <span class="token operator">*</span>buf<span class="token punctuation">,</span> <span class="token class-name">size_t</span> len<span class="token punctuation">,</span> <span class="token keyword">int</span> flags<span class="token punctuation">)</span><span class="token punctuation">;</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre></li><li>**close()**：关闭套接字。<pre class="line-numbers language-c" data-language="c"><code class="language-c"><span class="token keyword">int</span> <span class="token function">close</span><span class="token punctuation">(</span><span class="token keyword">int</span> fd<span class="token punctuation">)</span><span class="token punctuation">;</span><span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre></li></ul></li></ol><h4 id="简单TCP服务器示例"><a href="#简单TCP服务器示例" class="headerlink" title="简单TCP服务器示例"></a>简单TCP服务器示例</h4><pre class="line-numbers language-c" data-language="c"><code class="language-c"><span class="token macro property"><span class="token directive-hash">#</span><span class="token directive keyword">include</span> <span class="token string">&lt;stdio.h></span></span><span class="token macro property"><span class="token directive-hash">#</span><span class="token directive keyword">include</span> <span class="token string">&lt;string.h></span></span><span class="token macro property"><span class="token directive-hash">#</span><span class="token directive keyword">include</span> <span class="token string">&lt;stdlib.h></span></span><span class="token macro property"><span class="token directive-hash">#</span><span class="token directive keyword">include</span> <span class="token string">&lt;unistd.h></span></span><span class="token macro property"><span class="token directive-hash">#</span><span class="token directive keyword">include</span> <span class="token string">&lt;arpa/inet.h></span></span><span class="token keyword">int</span> <span class="token function">main</span><span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token punctuation">&#123;</span>    <span class="token keyword">int</span> server_fd<span class="token punctuation">,</span> new_socket<span class="token punctuation">;</span>    <span class="token keyword">struct</span> <span class="token class-name">sockaddr_in</span> address<span class="token punctuation">;</span>    <span class="token keyword">int</span> addrlen <span class="token operator">=</span> <span class="token keyword">sizeof</span><span class="token punctuation">(</span>address<span class="token punctuation">)</span><span class="token punctuation">;</span>    <span class="token keyword">char</span> buffer<span class="token punctuation">[</span><span class="token number">1024</span><span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token punctuation">&#123;</span><span class="token number">0</span><span class="token punctuation">&#125;</span><span class="token punctuation">;</span>    <span class="token keyword">char</span> <span class="token operator">*</span>hello <span class="token operator">=</span> <span class="token string">"Hello from server"</span><span class="token punctuation">;</span>    <span class="token keyword">if</span> <span class="token punctuation">(</span><span class="token punctuation">(</span>server_fd <span class="token operator">=</span> <span class="token function">socket</span><span class="token punctuation">(</span>AF_INET<span class="token punctuation">,</span> SOCK_STREAM<span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">)</span><span class="token punctuation">)</span> <span class="token operator">==</span> <span class="token number">0</span><span class="token punctuation">)</span> <span class="token punctuation">&#123;</span>        <span class="token function">perror</span><span class="token punctuation">(</span><span class="token string">"socket failed"</span><span class="token punctuation">)</span><span class="token punctuation">;</span>        <span class="token function">exit</span><span class="token punctuation">(</span>EXIT_FAILURE<span class="token punctuation">)</span><span class="token punctuation">;</span>    <span class="token punctuation">&#125;</span>    address<span class="token punctuation">.</span>sin_family <span class="token operator">=</span> AF_INET<span class="token punctuation">;</span>    address<span class="token punctuation">.</span>sin_addr<span class="token punctuation">.</span>s_addr <span class="token operator">=</span> INADDR_ANY<span class="token punctuation">;</span>    address<span class="token punctuation">.</span>sin_port <span class="token operator">=</span> <span class="token function">htons</span><span class="token punctuation">(</span><span class="token number">8080</span><span class="token punctuation">)</span><span class="token punctuation">;</span>    <span class="token keyword">if</span> <span class="token punctuation">(</span><span class="token function">bind</span><span class="token punctuation">(</span>server_fd<span class="token punctuation">,</span> <span class="token punctuation">(</span><span class="token keyword">struct</span> <span class="token class-name">sockaddr</span> <span class="token operator">*</span><span class="token punctuation">)</span><span class="token operator">&amp;</span>address<span class="token punctuation">,</span> <span class="token keyword">sizeof</span><span class="token punctuation">(</span>address<span class="token punctuation">)</span><span class="token punctuation">)</span> <span class="token operator">&lt;</span> <span class="token number">0</span><span class="token punctuation">)</span> <span class="token punctuation">&#123;</span>        <span class="token function">perror</span><span class="token punctuation">(</span><span class="token string">"bind failed"</span><span class="token punctuation">)</span><span class="token punctuation">;</span>        <span class="token function">close</span><span class="token punctuation">(</span>server_fd<span class="token punctuation">)</span><span class="token punctuation">;</span>        <span class="token function">exit</span><span class="token punctuation">(</span>EXIT_FAILURE<span class="token punctuation">)</span><span class="token punctuation">;</span>    <span class="token punctuation">&#125;</span>    <span class="token keyword">if</span> <span class="token punctuation">(</span><span class="token function">listen</span><span class="token punctuation">(</span>server_fd<span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">)</span> <span class="token operator">&lt;</span> <span class="token number">0</span><span class="token punctuation">)</span> <span class="token punctuation">&#123;</span>        <span class="token function">perror</span><span class="token punctuation">(</span><span class="token string">"listen"</span><span class="token punctuation">)</span><span class="token punctuation">;</span>        <span class="token function">close</span><span class="token punctuation">(</span>server_fd<span class="token punctuation">)</span><span class="token punctuation">;</span>        <span class="token function">exit</span><span class="token punctuation">(</span>EXIT_FAILURE<span class="token punctuation">)</span><span class="token punctuation">;</span>    <span class="token punctuation">&#125;</span>    <span class="token keyword">if</span> <span class="token punctuation">(</span><span class="token punctuation">(</span>new_socket <span class="token operator">=</span> <span class="token function">accept</span><span class="token punctuation">(</span>server_fd<span class="token punctuation">,</span> <span class="token punctuation">(</span><span class="token keyword">struct</span> <span class="token class-name">sockaddr</span> <span class="token operator">*</span><span class="token punctuation">)</span><span class="token operator">&amp;</span>address<span class="token punctuation">,</span> <span class="token punctuation">(</span><span class="token class-name">socklen_t</span><span class="token operator">*</span><span class="token punctuation">)</span><span class="token operator">&amp;</span>addrlen<span class="token punctuation">)</span><span class="token punctuation">)</span> <span class="token operator">&lt;</span> <span class="token number">0</span><span class="token punctuation">)</span> <span class="token punctuation">&#123;</span>        <span class="token function">perror</span><span class="token punctuation">(</span><span class="token string">"accept"</span><span class="token punctuation">)</span><span class="token punctuation">;</span>        <span class="token function">close</span><span class="token punctuation">(</span>server_fd<span class="token punctuation">)</span><span class="token punctuation">;</span>        <span class="token function">exit</span><span class="token punctuation">(</span>EXIT_FAILURE<span class="token punctuation">)</span><span class="token punctuation">;</span>    <span class="token punctuation">&#125;</span>    <span class="token function">read</span><span class="token punctuation">(</span>new_socket<span class="token punctuation">,</span> buffer<span class="token punctuation">,</span> <span class="token number">1024</span><span class="token punctuation">)</span><span class="token punctuation">;</span>    <span class="token function">printf</span><span class="token punctuation">(</span><span class="token string">"Message from client: %s\n"</span><span class="token punctuation">,</span> buffer<span class="token punctuation">)</span><span class="token punctuation">;</span>    <span class="token function">send</span><span class="token punctuation">(</span>new_socket<span class="token punctuation">,</span> hello<span class="token punctuation">,</span> <span class="token function">strlen</span><span class="token punctuation">(</span>hello<span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">)</span><span class="token punctuation">;</span>    <span class="token function">printf</span><span class="token punctuation">(</span><span class="token string">"Hello message sent\n"</span><span class="token punctuation">)</span><span class="token punctuation">;</span>    <span class="token function">close</span><span class="token punctuation">(</span>new_socket<span class="token punctuation">)</span><span class="token punctuation">;</span>    <span class="token function">close</span><span class="token punctuation">(</span>server_fd<span class="token punctuation">)</span><span class="token punctuation">;</span>    <span class="token keyword">return</span> <span class="token number">0</span><span class="token punctuation">;</span><span class="token punctuation">&#125;</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h3 id="线程和进程"><a href="#线程和进程" class="headerlink" title="线程和进程"></a>线程和进程</h3><h4 id="进程（Process）"><a href="#进程（Process）" class="headerlink" title="进程（Process）"></a>进程（Process）</h4><ol><li><strong>定义</strong>：进程是操作系统中一个运行中的程序实例，拥有自己的地址空间、内存、数据等资源。</li><li><strong>特点</strong>：<ul><li><strong>独立性</strong>：进程之间相互独立，不能直接访问对方的内存。</li><li><strong>资源管理</strong>：操作系统为每个进程分配资源，如CPU时间、内存等。</li><li><strong>通信方式</strong>：进程间通信（IPC）通过管道、消息队列、共享内存、信号等机制进行。</li></ul></li><li><strong>系统调用</strong>：<ul><li>**fork()**：创建一个新的子进程。<pre class="line-numbers language-c" data-language="c"><code class="language-c"><span class="token class-name">pid_t</span> <span class="token function">fork</span><span class="token punctuation">(</span><span class="token keyword">void</span><span class="token punctuation">)</span><span class="token punctuation">;</span><span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre></li><li>**exec()**：在当前进程空间内执行一个新程序。<pre class="line-numbers language-c" data-language="c"><code class="language-c"><span class="token keyword">int</span> <span class="token function">execl</span><span class="token punctuation">(</span><span class="token keyword">const</span> <span class="token keyword">char</span> <span class="token operator">*</span>path<span class="token punctuation">,</span> <span class="token keyword">const</span> <span class="token keyword">char</span> <span class="token operator">*</span>arg<span class="token punctuation">,</span> <span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">)</span><span class="token punctuation">;</span><span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre></li><li>**wait()**：等待子进程结束。<pre class="line-numbers language-c" data-language="c"><code class="language-c"><span class="token class-name">pid_t</span> <span class="token function">wait</span><span class="token punctuation">(</span><span class="token keyword">int</span> <span class="token operator">*</span>status<span class="token punctuation">)</span><span class="token punctuation">;</span><span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre></li></ul></li></ol><h4 id="线程（Thread）"><a href="#线程（Thread）" class="headerlink" title="线程（Thread）"></a>线程（Thread）</h4><ol><li><strong>定义</strong>：线程是进程中的一个执行单元，一个进程可以包含多个线程，它们共享进程的地址空间和资源。</li><li><strong>特点</strong>：<ul><li><strong>共享性</strong>：线程共享进程的内存空间和资源，能直接访问共享数据。</li><li><strong>轻量级</strong>：创建和销毁线程的开销比进程小，切换速度快。</li><li><strong>同步与互斥</strong>：由于线程共享数据，需要使用同步机制（如互斥锁、条件变量等）避免竞态条件。</li></ul></li><li><strong>POSIX线程库（pthread）</strong>：<ul><li><strong>创建线程</strong>：<pre class="line-numbers language-c" data-language="c"><code class="language-c"><span class="token keyword">int</span> <span class="token function">pthread_create</span><span class="token punctuation">(</span><span class="token class-name">pthread_t</span> <span class="token operator">*</span>thread<span class="token punctuation">,</span> <span class="token keyword">const</span> <span class="token class-name">pthread_attr_t</span> <span class="token operator">*</span>attr<span class="token punctuation">,</span> <span class="token keyword">void</span> <span class="token operator">*</span><span class="token punctuation">(</span><span class="token operator">*</span>start_routine<span class="token punctuation">)</span><span class="token punctuation">(</span><span class="token keyword">void</span> <span class="token operator">*</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token keyword">void</span> <span class="token operator">*</span>arg<span class="token punctuation">)</span><span class="token punctuation">;</span><span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre></li><li><strong>线程同步</strong>：<ul><li><strong>互斥锁</strong>：<pre class="line-numbers language-c" data-language="c"><code class="language-c"><span class="token class-name">pthread_mutex_t</span> mutex <span class="token operator">=</span> PTHREAD_MUTEX_INITIALIZER<span class="token punctuation">;</span><span class="token function">pthread_mutex_lock</span><span class="token punctuation">(</span><span class="token operator">&amp;</span>mutex<span class="token punctuation">)</span><span class="token punctuation">;</span><span class="token function">pthread_mutex_unlock</span><span class="token punctuation">(</span><span class="token operator">&amp;</span>mutex<span class="token punctuation">)</span><span class="token punctuation">;</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre></li><li><strong>条件变量</strong>：<pre class="line-numbers language-c" data-language="c"><code class="language-c"><span class="token class-name">pthread_cond_t</span> cond <span class="token operator">=</span> PTHREAD_COND_INITIALIZER<span class="token punctuation">;</span><span class="token function">pthread_cond_wait</span><span class="token punctuation">(</span><span class="token operator">&amp;</span>cond<span class="token punctuation">,</span> <span class="token operator">&amp;</span>mutex<span class="token punctuation">)</span><span class="token punctuation">;</span><span class="token function">pthread_cond_signal</span><span class="token punctuation">(</span><span class="token operator">&amp;</span>cond<span class="token punctuation">)</span><span class="token punctuation">;</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre></li></ul></li><li><strong>线程终止</strong>：<pre class="line-numbers language-c" data-language="c"><code class="language-c"><span class="token keyword">void</span> <span class="token function">pthread_exit</span><span class="token punctuation">(</span><span class="token keyword">void</span> <span class="token operator">*</span>retval<span class="token punctuation">)</span><span class="token punctuation">;</span><span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre></li></ul></li></ol><h4 id="线程和进程的对比"><a href="#线程和进程的对比" class="headerlink" title="线程和进程的对比"></a>线程和进程的对比</h4><ul><li><strong>内存共享</strong>：线程共享进程的内存空间，而进程有独立的内存空间。</li><li><strong>创建开销</strong>：线程创建开销小于进程。</li><li><strong>通信效率</strong>：线程间通信效率高于进程间通信，因为线程共享内存空间。</li><li><strong>安全性</strong>：进程间相互隔离，安全性更高；线程共享内存，需额外处理同步问题。</li></ul><h3 id="例子：多线程TCP服务器"><a href="#例子：多线程TCP服务器" class="headerlink" title="例子：多线程TCP服务器"></a>例子：多线程TCP服务器</h3><pre class="line-numbers language-c" data-language="c"><code class="language-c"><span class="token macro property"><span class="token directive-hash">#</span><span class="token directive keyword">include</span> <span class="token string">&lt;stdio.h></span></span><span class="token macro property"><span class="token directive-hash">#</span><span class="token directive keyword">include</span> <span class="token string">&lt;stdlib.h></span></span><span class="token macro property"><span class="token directive-hash">#</span><span class="token directive keyword">include</span> <span class="token string">&lt;string.h></span></span><span class="token macro property"><span class="token directive-hash">#</span><span class="token directive keyword">include</span> <span class="token string">&lt;unistd.h></span></span><span class="token macro property"><span class="token directive-hash">#</span><span class="token directive keyword">include</span> <span class="token string">&lt;pthread.h></span></span><span class="token macro property"><span class="token directive-hash">#</span><span class="token directive keyword">include</span> <span class="token string">&lt;arpa/inet.h></span></span><span class="token keyword">void</span> <span class="token operator">*</span><span class="token function">handle_client</span><span class="token punctuation">(</span><span class="token keyword">void</span> <span class="token operator">*</span>arg<span class="token punctuation">)</span> <span class="token punctuation">&#123;</span>    <span class="token keyword">int</span> new_socket <span class="token operator">=</span> <span class="token operator">*</span><span class="token punctuation">(</span><span class="token punctuation">(</span><span class="token keyword">int</span> <span class="token operator">*</span><span class="token punctuation">)</span>arg<span class="token punctuation">)</span><span class="token punctuation">;</span>    <span class="token keyword">char</span> buffer<span class="token punctuation">[</span><span class="token number">1024</span><span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token punctuation">&#123;</span><span class="token number">0</span><span class="token punctuation">&#125;</span><span class="token punctuation">;</span>    <span class="token keyword">char</span> <span class="token operator">*</span>hello <span class="token operator">=</span> <span class="token string">"Hello from server"</span><span class="token punctuation">;</span>    <span class="token function">read</span><span class="token punctuation">(</span>new_socket<span class="token punctuation">,</span> buffer<span class="token punctuation">,</span> <span class="token number">1024</span><span class="token punctuation">)</span><span class="token punctuation">;</span>    <span class="token function">printf</span><span class="token punctuation">(</span><span class="token string">"Message from client: %s\n"</span><span class="token punctuation">,</span> buffer<span class="token punctuation">)</span><span class="token punctuation">;</span>    <span class="token function">send</span><span class="token punctuation">(</span>new_socket<span class="token punctuation">,</span> hello<span class="token punctuation">,</span> <span class="token function">strlen</span><span class="token punctuation">(</span>hello<span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">)</span><span class="token punctuation">;</span>    <span class="token function">printf</span><span class="token punctuation">(</span><span class="token string">"Hello message sent\n"</span><span class="token punctuation">)</span><span class="token punctuation">;</span>    <span class="token function">close</span><span class="token punctuation">(</span>new_socket<span class="token punctuation">)</span><span class="token punctuation">;</span>    <span class="token function">free</span><span class="token punctuation">(</span>arg<span class="token punctuation">)</span><span class="token punctuation">;</span>    <span class="token function">pthread_exit</span><span class="token punctuation">(</span><span class="token constant">NULL</span><span class="token punctuation">)</span><span class="token punctuation">;</span><span class="token punctuation">&#125;</span><span class="token keyword">int</span> <span class="token function">main</span><span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token punctuation">&#123;</span>    <span class="token keyword">int</span> server_fd<span class="token punctuation">,</span> new_socket<span class="token punctuation">;</span>    <span class="token keyword">struct</span> <span class="token class-name">sockaddr_in</span> address<span class="token punctuation">;</span>    <span class="token keyword">int</span> addrlen <span class="token operator">=</span> <span class="token keyword">sizeof</span><span class="token punctuation">(</span>address<span class="token punctuation">)</span><span class="token punctuation">;</span>    <span class="token keyword">if</span> <span class="token punctuation">(</span><span class="token punctuation">(</span>server_fd <span class="token operator">=</span> <span class="token function">socket</span><span class="token punctuation">(</span>AF_INET<span class="token punctuation">,</span> SOCK_STREAM<span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">)</span><span class="token punctuation">)</span> <span class="token operator">==</span> <span class="token number">0</span><span class="token punctuation">)</span> <span class="token punctuation">&#123;</span>        <span class="token function">perror</span><span class="token punctuation">(</span><span class="token string">"socket failed"</span><span class="token punctuation">)</span><span class="token punctuation">;</span>        <span class="token function">exit</span><span class="token punctuation">(</span>EXIT_FAILURE<span class="token punctuation">)</span><span class="token punctuation">;</span>    <span class="token punctuation">&#125;</span>    address<span class="token punctuation">.</span>sin_family <span class="token operator">=</span> AF_INET<span class="token punctuation">;</span>    address<span class="token punctuation">.</span>sin_addr<span class="token punctuation">.</span>s_addr <span class="token operator">=</span> INADDR_ANY<span class="token punctuation">;</span>    address<span class="token punctuation">.</span>sin_port <span class="token operator">=</span> <span class="token function">htons</span><span class="token punctuation">(</span><span class="token number">8080</span><span class="token punctuation">)</span><span class="token punctuation">;</span>    <span class="token keyword">if</span> <span class="token punctuation">(</span><span class="token function">bind</span><span class="token punctuation">(</span>server_fd<span class="token punctuation">,</span> <span class="token punctuation">(</span><span class="token keyword">struct</span> <span class="token class-name">sockaddr</span> <span class="token operator">*</span><span class="token punctuation">)</span><span class="token operator">&amp;</span>address<span class="token punctuation">,</span> <span class="token keyword">sizeof</span><span class="token punctuation">(</span>address<span class="token punctuation">)</span><span class="token punctuation">)</span> <span class="token operator">&lt;</span> <span class="token number">0</span><span class="token punctuation">)</span> <span class="token punctuation">&#123;</span>        <span class="token function">perror</span><span class="token punctuation">(</span><span class="token string">"bind failed"</span><span class="token punctuation">)</span><span class="token punctuation">;</span>        <span class="token function">close</span><span class="token punctuation">(</span>server_fd<span class="token punctuation">)</span><span class="token punctuation">;</span>        <span class="token function">exit</span><span class="token punctuation">(</span>EXIT_FAILURE<span class="token punctuation">)</span><span class="token punctuation">;</span>    <span class="token punctuation">&#125;</span>    <span class="token keyword">if</span> <span class="token punctuation">(</span><span class="token function">listen</span><span class="token punctuation">(</span>server_fd<span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">)</span> <span class="token operator">&lt;</span> <span class="token number">0</span><span class="token punctuation">)</span> <span class="token punctuation">&#123;</span>        <span class="token function">perror</span><span class="token punctuation">(</span><span class="token string">"listen"</span><span class="token punctuation">)</span><span class="token punctuation">;</span>        <span class="token function">close</span><span class="token punctuation">(</span>server_fd<span class="token punctuation">)</span><span class="token punctuation">;</span>        <span class="token function">exit</span><span class="token punctuation">(</span>EXIT_FAILURE<span class="token punctuation">)</span><span class="token punctuation">;</span>    <span class="token punctuation">&#125;</span>    <span class="token keyword">while</span> <span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">)</span> <span class="token punctuation">&#123;</span>        new_socket <span class="token operator">=</span> <span class="token function">accept</span><span class="token punctuation">(</span>server_fd<span class="token punctuation">,</span> <span class="token punctuation">(</span><span class="token keyword">struct</span> <span class="token class-name">sockaddr</span> <span class="token operator">*</span><span class="token punctuation">)</span><span class="token operator">&amp;</span>address<span class="token punctuation">,</span> <span class="token punctuation">(</span><span class="token class-name">socklen_t</span><span class="token operator">*</span><span class="token punctuation">)</span><span class="token operator">&amp;</span>addrlen<span class="token punctuation">)</span><span class="token punctuation">;</span>        <span class="token keyword">if</span> <span class="token punctuation">(</span>new_socket <span class="token operator">&lt;</span> <span class="token number">0</span><span class="token punctuation">)</span> <span class="token punctuation">&#123;</span>            <span class="token function">perror</span><span class="token punctuation">(</span><span class="token string">"accept"</span><span class="token punctuation">)</span><span class="token punctuation">;</span>            <span class="token function">close</span><span class="token punctuation">(</span>server_fd<span class="token punctuation">)</span><span class="token punctuation">;</span>            <span class="token function">exit</span><span class="token punctuation">(</span>EXIT_FAILURE<span class="token punctuation">)</span><span class="token punctuation">;</span>        <span class="token punctuation">&#125;</span>        <span class="token class-name">pthread_t</span> thread_id<span class="token punctuation">;</span>        <span class="token keyword">int</span> <span class="token operator">*</span>pclient <span class="token operator">=</span> <span class="token function">malloc</span><span class="token punctuation">(</span><span class="token keyword">sizeof</span><span class="token punctuation">(</span><span class="token keyword">int</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">;</span>        <span class="token operator">*</span>pclient <span class="token operator">=</span> new_socket<span class="token punctuation">;</span>        <span class="token function">pthread_create</span><span class="token punctuation">(</span><span class="token operator">&amp;</span>thread_id<span class="token punctuation">,</span> <span class="token constant">NULL</span><span class="token punctuation">,</span> handle_client<span class="token punctuation">,</span> pclient<span class="token punctuation">)</span><span class="token punctuation">;</span>        <span class="token function">pthread_detach</span><span class="token punctuation">(</span>thread_id<span class="token punctuation">)</span><span class="token punctuation">;</span>    <span class="token punctuation">&#125;</span>    <span class="token function">close</span><span class="token punctuation">(</span>server_fd<span class="token punctuation">)</span><span class="token punctuation">;</span>    <span class="token keyword">return</span> <span class="token number">0</span><span class="token punctuation">;</span><span class="token punctuation">&#125;</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>这个多线程TCP服务器示例展示了如何在Linux下使用多线程处理多个客户端连接。通过这种方式，可以提高服务器的并发处理能力。</p><h2 id="问题十五：怎么使用Python爬虫的，用bs4、xpath？"><a href="#问题十五：怎么使用Python爬虫的，用bs4、xpath？" class="headerlink" title="问题十五：怎么使用Python爬虫的，用bs4、xpath？"></a>问题十五：怎么使用Python爬虫的，用bs4、xpath？</h2><p>使用Python进行网页爬虫的基本步骤包括发送HTTP请求、解析网页内容、提取数据、处理数据并存储。下面将介绍如何使用<code>requests</code>库获取网页内容，以及使用<code>BeautifulSoup</code>（简称bs4）和<code>lxml</code>（用于XPath）进行网页解析和数据提取。</p><h3 id="1-安装必要的库"><a href="#1-安装必要的库" class="headerlink" title="1. 安装必要的库"></a>1. 安装必要的库</h3><p>首先，确保安装了<code>requests</code>、<code>beautifulsoup4</code>和<code>lxml</code>库：</p><pre class="line-numbers language-bash" data-language="bash"><code class="language-bash">pip <span class="token function">install</span> requests beautifulsoup4 lxml<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><h3 id="2-使用requests库获取网页内容"><a href="#2-使用requests库获取网页内容" class="headerlink" title="2. 使用requests库获取网页内容"></a>2. 使用<code>requests</code>库获取网页内容</h3><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">import</span> requestsurl <span class="token operator">=</span> <span class="token string">'https://example.com'</span>response <span class="token operator">=</span> requests<span class="token punctuation">.</span>get<span class="token punctuation">(</span>url<span class="token punctuation">)</span><span class="token keyword">if</span> response<span class="token punctuation">.</span>status_code <span class="token operator">==</span> <span class="token number">200</span><span class="token punctuation">:</span>    html_content <span class="token operator">=</span> response<span class="token punctuation">.</span>text<span class="token keyword">else</span><span class="token punctuation">:</span>    <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">"Failed to retrieve the webpage"</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h3 id="3-使用BeautifulSoup进行解析"><a href="#3-使用BeautifulSoup进行解析" class="headerlink" title="3. 使用BeautifulSoup进行解析"></a>3. 使用<code>BeautifulSoup</code>进行解析</h3><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">from</span> bs4 <span class="token keyword">import</span> BeautifulSoupsoup <span class="token operator">=</span> BeautifulSoup<span class="token punctuation">(</span>html_content<span class="token punctuation">,</span> <span class="token string">'html.parser'</span><span class="token punctuation">)</span><span class="token comment"># 示例：提取所有标题</span>titles <span class="token operator">=</span> soup<span class="token punctuation">.</span>find_all<span class="token punctuation">(</span><span class="token string">'h1'</span><span class="token punctuation">)</span><span class="token keyword">for</span> title <span class="token keyword">in</span> titles<span class="token punctuation">:</span>    <span class="token keyword">print</span><span class="token punctuation">(</span>title<span class="token punctuation">.</span>get_text<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h3 id="4-使用lxml和XPath进行解析"><a href="#4-使用lxml和XPath进行解析" class="headerlink" title="4. 使用lxml和XPath进行解析"></a>4. 使用<code>lxml</code>和XPath进行解析</h3><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">from</span> lxml <span class="token keyword">import</span> etree<span class="token comment"># 解析HTML</span>tree <span class="token operator">=</span> etree<span class="token punctuation">.</span>HTML<span class="token punctuation">(</span>html_content<span class="token punctuation">)</span><span class="token comment"># 示例：使用XPath提取所有标题</span>titles <span class="token operator">=</span> tree<span class="token punctuation">.</span>xpath<span class="token punctuation">(</span><span class="token string">'//h1/text()'</span><span class="token punctuation">)</span><span class="token keyword">for</span> title <span class="token keyword">in</span> titles<span class="token punctuation">:</span>    <span class="token keyword">print</span><span class="token punctuation">(</span>title<span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h3 id="示例项目：爬取一个简单的网页"><a href="#示例项目：爬取一个简单的网页" class="headerlink" title="示例项目：爬取一个简单的网页"></a>示例项目：爬取一个简单的网页</h3><p>假设我们要爬取一个包含文章标题和链接的网页，提取这些信息并保存到CSV文件中。</p><h4 id="步骤1：获取网页内容"><a href="#步骤1：获取网页内容" class="headerlink" title="步骤1：获取网页内容"></a>步骤1：获取网页内容</h4><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">import</span> requestsurl <span class="token operator">=</span> <span class="token string">'https://example.com/articles'</span>response <span class="token operator">=</span> requests<span class="token punctuation">.</span>get<span class="token punctuation">(</span>url<span class="token punctuation">)</span><span class="token keyword">if</span> response<span class="token punctuation">.</span>status_code <span class="token operator">==</span> <span class="token number">200</span><span class="token punctuation">:</span>    html_content <span class="token operator">=</span> response<span class="token punctuation">.</span>text<span class="token keyword">else</span><span class="token punctuation">:</span>    <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">"Failed to retrieve the webpage"</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h4 id="步骤2：使用BeautifulSoup解析并提取数据"><a href="#步骤2：使用BeautifulSoup解析并提取数据" class="headerlink" title="步骤2：使用BeautifulSoup解析并提取数据"></a>步骤2：使用<code>BeautifulSoup</code>解析并提取数据</h4><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">from</span> bs4 <span class="token keyword">import</span> BeautifulSoup<span class="token keyword">import</span> csvsoup <span class="token operator">=</span> BeautifulSoup<span class="token punctuation">(</span>html_content<span class="token punctuation">,</span> <span class="token string">'html.parser'</span><span class="token punctuation">)</span>articles <span class="token operator">=</span> soup<span class="token punctuation">.</span>find_all<span class="token punctuation">(</span><span class="token string">'article'</span><span class="token punctuation">)</span><span class="token comment"># 打开CSV文件用于写入</span><span class="token keyword">with</span> <span class="token builtin">open</span><span class="token punctuation">(</span><span class="token string">'articles.csv'</span><span class="token punctuation">,</span> <span class="token string">'w'</span><span class="token punctuation">,</span> newline<span class="token operator">=</span><span class="token string">''</span><span class="token punctuation">,</span> encoding<span class="token operator">=</span><span class="token string">'utf-8'</span><span class="token punctuation">)</span> <span class="token keyword">as</span> csvfile<span class="token punctuation">:</span>    fieldnames <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token string">'title'</span><span class="token punctuation">,</span> <span class="token string">'link'</span><span class="token punctuation">]</span>    writer <span class="token operator">=</span> csv<span class="token punctuation">.</span>DictWriter<span class="token punctuation">(</span>csvfile<span class="token punctuation">,</span> fieldnames<span class="token operator">=</span>fieldnames<span class="token punctuation">)</span>    writer<span class="token punctuation">.</span>writeheader<span class="token punctuation">(</span><span class="token punctuation">)</span>        <span class="token keyword">for</span> article <span class="token keyword">in</span> articles<span class="token punctuation">:</span>        title <span class="token operator">=</span> article<span class="token punctuation">.</span>find<span class="token punctuation">(</span><span class="token string">'h2'</span><span class="token punctuation">)</span><span class="token punctuation">.</span>get_text<span class="token punctuation">(</span><span class="token punctuation">)</span>        link <span class="token operator">=</span> article<span class="token punctuation">.</span>find<span class="token punctuation">(</span><span class="token string">'a'</span><span class="token punctuation">)</span><span class="token punctuation">[</span><span class="token string">'href'</span><span class="token punctuation">]</span>        writer<span class="token punctuation">.</span>writerow<span class="token punctuation">(</span><span class="token punctuation">&#123;</span><span class="token string">'title'</span><span class="token punctuation">:</span> title<span class="token punctuation">,</span> <span class="token string">'link'</span><span class="token punctuation">:</span> link<span class="token punctuation">&#125;</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h4 id="步骤3：使用lxml和XPath解析并提取数据"><a href="#步骤3：使用lxml和XPath解析并提取数据" class="headerlink" title="步骤3：使用lxml和XPath解析并提取数据"></a>步骤3：使用<code>lxml</code>和XPath解析并提取数据</h4><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">from</span> lxml <span class="token keyword">import</span> etree<span class="token keyword">import</span> csvtree <span class="token operator">=</span> etree<span class="token punctuation">.</span>HTML<span class="token punctuation">(</span>html_content<span class="token punctuation">)</span>titles <span class="token operator">=</span> tree<span class="token punctuation">.</span>xpath<span class="token punctuation">(</span><span class="token string">'//article/h2/text()'</span><span class="token punctuation">)</span>links <span class="token operator">=</span> tree<span class="token punctuation">.</span>xpath<span class="token punctuation">(</span><span class="token string">'//article/a/@href'</span><span class="token punctuation">)</span><span class="token comment"># 打开CSV文件用于写入</span><span class="token keyword">with</span> <span class="token builtin">open</span><span class="token punctuation">(</span><span class="token string">'articles.csv'</span><span class="token punctuation">,</span> <span class="token string">'w'</span><span class="token punctuation">,</span> newline<span class="token operator">=</span><span class="token string">''</span><span class="token punctuation">,</span> encoding<span class="token operator">=</span><span class="token string">'utf-8'</span><span class="token punctuation">)</span> <span class="token keyword">as</span> csvfile<span class="token punctuation">:</span>    fieldnames <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token string">'title'</span><span class="token punctuation">,</span> <span class="token string">'link'</span><span class="token punctuation">]</span>    writer <span class="token operator">=</span> csv<span class="token punctuation">.</span>DictWriter<span class="token punctuation">(</span>csvfile<span class="token punctuation">,</span> fieldnames<span class="token operator">=</span>fieldnames<span class="token punctuation">)</span>    writer<span class="token punctuation">.</span>writeheader<span class="token punctuation">(</span><span class="token punctuation">)</span>        <span class="token keyword">for</span> title<span class="token punctuation">,</span> link <span class="token keyword">in</span> <span class="token builtin">zip</span><span class="token punctuation">(</span>titles<span class="token punctuation">,</span> links<span class="token punctuation">)</span><span class="token punctuation">:</span>        writer<span class="token punctuation">.</span>writerow<span class="token punctuation">(</span><span class="token punctuation">&#123;</span><span class="token string">'title'</span><span class="token punctuation">:</span> title<span class="token punctuation">,</span> <span class="token string">'link'</span><span class="token punctuation">:</span> link<span class="token punctuation">&#125;</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h3 id="总结-8"><a href="#总结-8" class="headerlink" title="总结"></a>总结</h3><ul><li><strong>requests</strong>：用于发送HTTP请求，获取网页内容。</li><li><strong>BeautifulSoup</strong>：用于解析HTML和XML文档，提供简单易用的API来提取内容。</li><li><strong>lxml</strong>：提供强大的XML和HTML处理功能，支持XPath查询，适合需要复杂选择器的场景。</li></ul><p>通过以上方法，您可以构建简单而有效的Python爬虫来自动化数据收集任务。根据具体需求选择合适的解析工具，<code>BeautifulSoup</code>适合快速开发和简单解析，而<code>lxml</code>适合处理复杂的XML和HTML文档。</p><p>爬取图片的过程与爬取其他网页内容类似，只是需要特别关注图片的URL并下载图片文件。下面是一个使用Python的<code>requests</code>、<code>BeautifulSoup</code>和<code>os</code>库来爬取图片的完整示例。</p><h3 id="步骤1：安装必要的库"><a href="#步骤1：安装必要的库" class="headerlink" title="步骤1：安装必要的库"></a>步骤1：安装必要的库</h3><p>确保安装了<code>requests</code>和<code>beautifulsoup4</code>库：</p><pre class="line-numbers language-bash" data-language="bash"><code class="language-bash">pip <span class="token function">install</span> requests beautifulsoup4<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><h3 id="步骤2：获取网页内容"><a href="#步骤2：获取网页内容" class="headerlink" title="步骤2：获取网页内容"></a>步骤2：获取网页内容</h3><p>首先，发送HTTP请求获取网页内容：</p><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">import</span> requestsurl <span class="token operator">=</span> <span class="token string">'https://example.com'</span>  <span class="token comment"># 替换为目标网站URL</span>response <span class="token operator">=</span> requests<span class="token punctuation">.</span>get<span class="token punctuation">(</span>url<span class="token punctuation">)</span><span class="token keyword">if</span> response<span class="token punctuation">.</span>status_code <span class="token operator">==</span> <span class="token number">200</span><span class="token punctuation">:</span>    html_content <span class="token operator">=</span> response<span class="token punctuation">.</span>text<span class="token keyword">else</span><span class="token punctuation">:</span>    <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">"Failed to retrieve the webpage"</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h3 id="步骤3：解析网页并提取图片URL"><a href="#步骤3：解析网页并提取图片URL" class="headerlink" title="步骤3：解析网页并提取图片URL"></a>步骤3：解析网页并提取图片URL</h3><p>使用<code>BeautifulSoup</code>解析网页内容并提取所有图片的URL：</p><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">from</span> bs4 <span class="token keyword">import</span> BeautifulSoup<span class="token keyword">import</span> ossoup <span class="token operator">=</span> BeautifulSoup<span class="token punctuation">(</span>html_content<span class="token punctuation">,</span> <span class="token string">'html.parser'</span><span class="token punctuation">)</span>images <span class="token operator">=</span> soup<span class="token punctuation">.</span>find_all<span class="token punctuation">(</span><span class="token string">'img'</span><span class="token punctuation">)</span><span class="token comment"># 创建一个目录来存储下载的图片</span><span class="token keyword">if</span> <span class="token keyword">not</span> os<span class="token punctuation">.</span>path<span class="token punctuation">.</span>exists<span class="token punctuation">(</span><span class="token string">'images'</span><span class="token punctuation">)</span><span class="token punctuation">:</span>    os<span class="token punctuation">.</span>makedirs<span class="token punctuation">(</span><span class="token string">'images'</span><span class="token punctuation">)</span><span class="token comment"># 基础URL（如果图片URL是相对路径）</span>base_url <span class="token operator">=</span> <span class="token string">'https://example.com'</span><span class="token keyword">for</span> img <span class="token keyword">in</span> images<span class="token punctuation">:</span>    img_url <span class="token operator">=</span> img<span class="token punctuation">.</span>get<span class="token punctuation">(</span><span class="token string">'src'</span><span class="token punctuation">)</span>    <span class="token keyword">if</span> img_url<span class="token punctuation">:</span>        <span class="token comment"># 如果img_url是相对路径，拼接成完整的URL</span>        <span class="token keyword">if</span> <span class="token keyword">not</span> img_url<span class="token punctuation">.</span>startswith<span class="token punctuation">(</span><span class="token string">'http'</span><span class="token punctuation">)</span><span class="token punctuation">:</span>            img_url <span class="token operator">=</span> base_url <span class="token operator">+</span> img_url        <span class="token comment"># 下载并保存图片</span>        img_response <span class="token operator">=</span> requests<span class="token punctuation">.</span>get<span class="token punctuation">(</span>img_url<span class="token punctuation">)</span>        <span class="token keyword">if</span> img_response<span class="token punctuation">.</span>status_code <span class="token operator">==</span> <span class="token number">200</span><span class="token punctuation">:</span>            img_name <span class="token operator">=</span> os<span class="token punctuation">.</span>path<span class="token punctuation">.</span>join<span class="token punctuation">(</span><span class="token string">'images'</span><span class="token punctuation">,</span> os<span class="token punctuation">.</span>path<span class="token punctuation">.</span>basename<span class="token punctuation">(</span>img_url<span class="token punctuation">)</span><span class="token punctuation">)</span>            <span class="token keyword">with</span> <span class="token builtin">open</span><span class="token punctuation">(</span>img_name<span class="token punctuation">,</span> <span class="token string">'wb'</span><span class="token punctuation">)</span> <span class="token keyword">as</span> f<span class="token punctuation">:</span>                f<span class="token punctuation">.</span>write<span class="token punctuation">(</span>img_response<span class="token punctuation">.</span>content<span class="token punctuation">)</span>            <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string-interpolation"><span class="token string">f'Downloaded </span><span class="token interpolation"><span class="token punctuation">&#123;</span>img_url<span class="token punctuation">&#125;</span></span><span class="token string"> to </span><span class="token interpolation"><span class="token punctuation">&#123;</span>img_name<span class="token punctuation">&#125;</span></span><span class="token string">'</span></span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h3 id="完整示例：爬取图片并保存"><a href="#完整示例：爬取图片并保存" class="headerlink" title="完整示例：爬取图片并保存"></a>完整示例：爬取图片并保存</h3><p>以下是一个完整的示例程序，将上述步骤结合起来：</p><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">import</span> requests<span class="token keyword">from</span> bs4 <span class="token keyword">import</span> BeautifulSoup<span class="token keyword">import</span> os<span class="token keyword">def</span> <span class="token function">download_images</span><span class="token punctuation">(</span>url<span class="token punctuation">,</span> folder<span class="token operator">=</span><span class="token string">'images'</span><span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token comment"># 获取网页内容</span>    response <span class="token operator">=</span> requests<span class="token punctuation">.</span>get<span class="token punctuation">(</span>url<span class="token punctuation">)</span>    <span class="token keyword">if</span> response<span class="token punctuation">.</span>status_code <span class="token operator">!=</span> <span class="token number">200</span><span class="token punctuation">:</span>        <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">"Failed to retrieve the webpage"</span><span class="token punctuation">)</span>        <span class="token keyword">return</span>    <span class="token comment"># 解析网页内容</span>    html_content <span class="token operator">=</span> response<span class="token punctuation">.</span>text    soup <span class="token operator">=</span> BeautifulSoup<span class="token punctuation">(</span>html_content<span class="token punctuation">,</span> <span class="token string">'html.parser'</span><span class="token punctuation">)</span>    images <span class="token operator">=</span> soup<span class="token punctuation">.</span>find_all<span class="token punctuation">(</span><span class="token string">'img'</span><span class="token punctuation">)</span>    <span class="token comment"># 创建目录来存储下载的图片</span>    <span class="token keyword">if</span> <span class="token keyword">not</span> os<span class="token punctuation">.</span>path<span class="token punctuation">.</span>exists<span class="token punctuation">(</span>folder<span class="token punctuation">)</span><span class="token punctuation">:</span>        os<span class="token punctuation">.</span>makedirs<span class="token punctuation">(</span>folder<span class="token punctuation">)</span>    <span class="token comment"># 基础URL（如果图片URL是相对路径）</span>    base_url <span class="token operator">=</span> url<span class="token punctuation">.</span>rsplit<span class="token punctuation">(</span><span class="token string">'/'</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span>    <span class="token comment"># 下载并保存图片</span>    <span class="token keyword">for</span> img <span class="token keyword">in</span> images<span class="token punctuation">:</span>        img_url <span class="token operator">=</span> img<span class="token punctuation">.</span>get<span class="token punctuation">(</span><span class="token string">'src'</span><span class="token punctuation">)</span>        <span class="token keyword">if</span> img_url<span class="token punctuation">:</span>            <span class="token comment"># 如果img_url是相对路径，拼接成完整的URL</span>            <span class="token keyword">if</span> <span class="token keyword">not</span> img_url<span class="token punctuation">.</span>startswith<span class="token punctuation">(</span><span class="token string">'http'</span><span class="token punctuation">)</span><span class="token punctuation">:</span>                img_url <span class="token operator">=</span> base_url <span class="token operator">+</span> <span class="token string">'/'</span> <span class="token operator">+</span> img_url            <span class="token keyword">try</span><span class="token punctuation">:</span>                img_response <span class="token operator">=</span> requests<span class="token punctuation">.</span>get<span class="token punctuation">(</span>img_url<span class="token punctuation">)</span>                <span class="token keyword">if</span> img_response<span class="token punctuation">.</span>status_code <span class="token operator">==</span> <span class="token number">200</span><span class="token punctuation">:</span>                    img_name <span class="token operator">=</span> os<span class="token punctuation">.</span>path<span class="token punctuation">.</span>join<span class="token punctuation">(</span>folder<span class="token punctuation">,</span> os<span class="token punctuation">.</span>path<span class="token punctuation">.</span>basename<span class="token punctuation">(</span>img_url<span class="token punctuation">)</span><span class="token punctuation">)</span>                    <span class="token keyword">with</span> <span class="token builtin">open</span><span class="token punctuation">(</span>img_name<span class="token punctuation">,</span> <span class="token string">'wb'</span><span class="token punctuation">)</span> <span class="token keyword">as</span> f<span class="token punctuation">:</span>                        f<span class="token punctuation">.</span>write<span class="token punctuation">(</span>img_response<span class="token punctuation">.</span>content<span class="token punctuation">)</span>                    <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string-interpolation"><span class="token string">f'Downloaded </span><span class="token interpolation"><span class="token punctuation">&#123;</span>img_url<span class="token punctuation">&#125;</span></span><span class="token string"> to </span><span class="token interpolation"><span class="token punctuation">&#123;</span>img_name<span class="token punctuation">&#125;</span></span><span class="token string">'</span></span><span class="token punctuation">)</span>            <span class="token keyword">except</span> requests<span class="token punctuation">.</span>RequestException <span class="token keyword">as</span> e<span class="token punctuation">:</span>                <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string-interpolation"><span class="token string">f'Failed to download </span><span class="token interpolation"><span class="token punctuation">&#123;</span>img_url<span class="token punctuation">&#125;</span></span><span class="token string">: </span><span class="token interpolation"><span class="token punctuation">&#123;</span>e<span class="token punctuation">&#125;</span></span><span class="token string">'</span></span><span class="token punctuation">)</span><span class="token comment"># 示例：爬取示例网站的图片</span>download_images<span class="token punctuation">(</span><span class="token string">'https://example.com'</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h3 id="注意事项"><a href="#注意事项" class="headerlink" title="注意事项"></a>注意事项</h3><ol><li><strong>尊重网站的robots.txt文件</strong>：在爬取网页之前，检查网站的robots.txt文件，确保您遵守网站的爬虫规则。</li><li><strong>处理图片URL的相对路径</strong>：如果图片URL是相对路径，确保将其转换为完整的URL。</li><li><strong>异常处理</strong>：在下载过程中处理可能出现的异常，例如网络错误或文件IO错误。</li><li><strong>频率控制</strong>：避免频繁请求服务器，以免对目标网站造成过大的压力，可以使用<code>time.sleep()</code>函数进行适当的延迟。</li></ol><p>通过上述方法，您可以有效地爬取和下载网页上的图片。根据实际需求，您可以进一步优化和扩展代码功能，例如多线程下载、进度显示等。</p>]]></content>
      
      
      <categories>
          
          <category> 面试 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 面试 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Commonsense Prototype for Outdoor Unsupervised 3D Object Detection (CVPR 2024)</title>
      <link href="/2024/04/28/unsuper-det/"/>
      <url>/2024/04/28/unsuper-det/</url>
      
        <content type="html"><![CDATA[<h1 id="Commonsense-Prototype-for-Outdoor-Unsupervised-3D-Object-Detection-CVPR-2024"><a href="#Commonsense-Prototype-for-Outdoor-Unsupervised-3D-Object-Detection-CVPR-2024" class="headerlink" title="Commonsense Prototype for Outdoor Unsupervised 3D Object Detection (CVPR 2024)"></a>Commonsense Prototype for Outdoor Unsupervised 3D Object Detection (CVPR 2024)</h1><p><a href="https://arxiv.org/abs/2404.16493">论文链接</a></p><p><a href="https://github.com/hailanyi/cpd">代码链接</a></p><p><a href="https://github.com/hailanyi">第一作者主页</a></p><h2 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h2><p>无监督三维目标检测的主流方法是基于聚类的伪标签生成和迭代自训练过程。然而，由于激光雷达扫描的稀疏性，会导致尺寸和位置错误的伪标签，从而导致检测性能低于标准。为了解决这个问题，本文介绍了一种基于常识原型的检测器，称为CPD，用于无监督的3D物体检测。CPD首先基于常识性直觉，构建具有高质量边界框和密集点特征的常识性原型(CProto)。随后，CPD通过利用CProto先前的大小来细化低质量的伪标签。此外，CPD还利用CProto的几何知识提高了稀疏扫描目标的检测精度。在Waymo开放数据集(WOD)、PandaSet和KITTI数据集上，CPD的性能远远超过最先进的无监督3D探测器。此外，通过对CPD在WOD上的训练和在KITTI上的测试，CPD在简单和中等车型上的3D平均精度分别达到了90.85%和81.01%。这些成就使CPD接近于完全监督检测器，突出了我们方法的重要性。</p><h2 id="1-引言"><a href="#1-引言" class="headerlink" title="1. 引言"></a>1. 引言</h2><p>自动驾驶需要在城市场景中可靠地检测3D物体(例如车辆和骑自行车的人)，以便进行安全的路径规划和导航。由于神经网络的强大功能，许多研究已经通过完全监督的方法开发出高性能的3D探测器[4,15,30 - 33]。然而，这些模型在很大程度上依赖于来自不同场景的人工注释，以保证它们在不同场景中的有效性。这种数据标注过程通常既费力又耗时，限制了检测器在实践中的广泛部署[40]。</p><p>一些研究已经探索了通过弱监督学习减少标注需求的方法[3,26,46]，将标注成本降低了80%以上。值得注意的是，3D场景中的物体具有可区分的属性，可以通过一定的常识推理轻松识别(见图1)。例如，物体通常位于具有一定形状的地面上;对象大小在不同帧之间是固定的。这一见解促使我们开发了一种无监督的3D探测器，无需人工注释即可运行。</p><p><img src="/pic/unsup1.png" alt="图1所示。自动驾驶场景中无监督3D物体检测的常识性原型说明。"></p><p>近年来，传统方法利用地面去除[9]和聚类技术[42]进行无监督三维目标检测。然而，由于3D场景中物体的稀疏性和遮挡，这些方法往往难以达到令人满意的性能。先进的方法通过聚类从点云序列中创建初始伪标签，并通过迭代训练深度网络来引导一个好的检测器[41]。然而，激光雷达扫描的稀疏性和视域限制导致伪标签的大小和位置不准确，误导网络收敛，导致检测性能不佳。一个对象的子集，记作完全对象T，受益于至少对整个点云序列进行一次完整扫描，允许通过时间一致性对其伪标签进行细化[41]见图2 (a)。然而，大多数物体(如图2 (c)所示，WOD上的65%[25])被称为不完整物体J，缺乏完整的扫描覆盖(见图2 (b))，无法通过时间一致性恢复。</p><p><a href="https://ieeexplore.ieee.org/document/5548059">9:Fast segmentation of 3D point clouds for ground vehicles</a></p><p><a href="https://arxiv.org/abs/2311.02007">41:Towards unsupervised object detection from lidar point clouds</a></p><p><a href="https://ieeexplore.ieee.org/document/6630946">42:Unsupervised 3d category discovery and point labeling from a large urban environment</a></p><p><img src="/pic/unsup2.png" alt="图2。WOD[25]验证集上完整和不完整对象的说明和统计(大到足以展示一般问题)。(a)利用时间一致性改进完整对象T的伪标签。(b)不完整对象J的伪标签无法通过时间一致性进行细化。(c) 65%的对象缺乏完整的扫描覆盖，产生不准确的伪标签(Max IoU(交集超过联合)与GT(地面真值)&lt; 0.5)。(d)完整物体GTT和不完整物体GTJ的整车GT具有相似的尺寸分布。(e)完整对象PseT和不完整对象PseJ的伪标号大小分布不同。(f)(g)附近的静止物体在连续帧中具有高的完备性。"></p><p>为了解决这个问题，本文提出了一种基于常识原型的检测器，称为CPD，用于无监督的3D物体检测。CPD建立在两个关键见解之上:(1)类内对象的基本真理在不完整对象和完整对象之间保持相似的大小(长度、宽度和高度)分布(见图2 (d))。(2)附近的静止物体在连续帧中非常完整，可以通过常识直觉准确识别(见图2 (f)(g))。<strong>我们的想法是从完整的物体中构建一个表示精确几何形状和尺寸的常识原型(Commonsense Prototype, CProto)集，以改进不完整物体的伪标签，提高检测精度</strong>。为此，我们首先设计了一种无监督的多帧聚类(MFC)方法，该方法产生高召回率的初始伪标签。随后，我们引入了一个无监督的完整性和大小相似度(CSS)评分，该评分选择高质量的标签来构建CProto集。此外，我们设计了一种CProto约束盒正则化(CBR)方法，通过结合CProto的尺寸先验来改进伪标签。此外，我们开发了CProto约束自我训练(CST)，利用CProto的几何知识提高稀疏扫描目标的检测精度。</p><p>通过在广泛使用的WOD[25]、PandaSet[35]和KITTI数据集[6]上的实验验证了我们设计的有效性。此外，我们设计的各个组件也在WOD上进行了大量的实验验证[25]。这项工作的主要贡献包括:</p><ul><li>我们提出了一种基于常识原型的检测器(CPD)用于无监督的3D物体检测。CPD比最先进的无监督3D探测器要好得多。</li><li>提出了基于多帧聚类(MFC)和CProto约束盒正则化(CBR)的伪标签生成和细化方法，极大地提高了伪标签的查全率和查准率。</li><li>我们提出CProto约束盒自训练 (CST)用于无监督3D检测。提高了稀疏目标的识别和定位精度，显著提高了检测性能。</li></ul><h2 id="2-相关工作"><a href="#2-相关工作" class="headerlink" title="2. 相关工作"></a>2. 相关工作</h2><p><strong>完全&#x2F;弱监督3D对象检测</strong>。最近的全监督3D检测器构建了单阶段[8,10,27,39,48,49]、两阶段[4,20 - 22,31 - 33,37]或多阶段[2,30]的深度网络用于3D目标检测。然而，这些方法严重依赖于大量精确的注释。一些弱监督方法用低成本的点击标注代替方框标注[17]。其他方法通过只注释部分场景[3,26,45,46]或部分实例[34]来减少监督。与上述所有工作不同，我们的目标是设计一个不需要人类级别注释的3D检测器。</p><p><strong>无监督的3D物体检测</strong>。以前的无监督预训练方法通过屏蔽标签[36]或对比损失[14,38]来识别未标记数据中的潜在模式。但这些方法需要人工标记来进行微调。传统方法[1,19,24]采用地面去除和聚类方法进行三维目标检测，无需人工标记，但检测性能较差。一些基于深度学习的方法通过聚类生成伪标签，并使用伪标签迭代地训练3D检测器[40]。最近的OYSTER[41]通过时间一致性提高了伪标签质量。然而，大多数不完整对象的伪标签不能通过时间一致性来恢复。我们的CPD通过利用CProto的几何先验来改进伪标签并指导网络收敛，从而解决了这个问题。</p><p><strong>基于原型的方法</strong>。基于原型的方法被广泛应用于二维检测中[11,12,16,29,44]。受这些方法的启发，Prototypical VoteNet[47]构建了从基本类中学习的几何原型，用于少镜头3D物体检测。GPA-3D[13]和CL3D[18]从源域模型构建几何原型，用于域自适应三维检测。然而，从基本类学习和源域上的训练都需要高质量的注释。与此不同的是，我们使用常识性知识构建CProto，并在没有人类级别注释的情况下以zero-shot方式检测3D对象。</p><h2 id="3-CPD方法"><a href="#3-CPD方法" class="headerlink" title="3. CPD方法"></a>3. CPD方法</h2><p>本文介绍了基于常识原型的检测器(CPD)，这是一种用于无监督三维物体检测的新方法。如图3所示，CPD主要由三个部分组成:(1)初始标签生成;(2)标签细化;(3)自我训练。我们详细设计如下。<br><img src="/pic/unsup3.png" alt="图3。CPD框架。(a)初始伪标签由多帧聚类生成。(b)基于CSS评分的高质量伪标签构建常识性原型(CProto)。低质量的标签进一步完善的形状之前从CProto。(c)由CProto的密集点馈送的原型网络产生高质量的特征，引导检测网络收敛。"></p><h3 id="3-1-初始标签生成"><a href="#3-1-初始标签生成" class="headerlink" title="3.1. 初始标签生成"></a>3.1. 初始标签生成</h3><p>最近的无监督方法[40,41]以一种与类别无关的方式检测3D物体。如何在没有标注的情况下对物体(如车辆和行人)进行分类仍然是一个未解决的挑战。我们的观察表明，连续帧中的一些静止物体看起来更完整(见图2 (f))，并且可以通过预定义的大小进行分类。这促使我们设计了一种多帧聚类(MFC)方法来生成初始标签。MFC包括运动伪影去除、聚类和后处理。</p><p><a href="https://arxiv.org/abs/2203.15882">40:Learning to Detect Mobile Objects from LiDAR Scans Without Labels</a></p><p>**运动伪影去除(MAR)**。直接变换和连接2n+1个连续帧{x−n，…， xn}(即过去n，未来n和当前帧)到单个点云x * 0引入运动物体的运动伪影，导致随着n的增长标签误差增加(见图4(a))。为了缓解这个问题，我们首先将连续帧转换为全局系统，并计算连续帧的持续点分数(PPScore)[40]来识别运动中的点。我们保留x0中的所有点，并删除其他帧x - n中的移动点，…， x−1,x1，…在此移除之后，我们连接帧以获得密集点x * 0。</p><p><img src="/pic/unsup4.png" alt="图4。(a)不同帧的长度绝对误差。(b)多层占用评分。(c)初始标签的平均尺寸误差。"></p><p><strong>聚类和后处理</strong>。根据最近的研究[41]，我们在x∗0上应用ground removal[9]， DBSCAN[5]和bounding box fitting[43]来获得一组与类别无关的边界框(bounding box)b。我们观察到，同一类的对象在三维空间中通常具有相似的大小。因此，我们根据人类的常识预先定义了特定类别的尺寸阈值(例如，车辆的长度一般大于0.5m)，将b分类为不同的类别。然后，我们应用类别无关跟踪将小背景对象与前景轨迹关联起来，并通过使用时间相干性来增强对象大小的一致性[41]。此过程得到一组初始伪标签b &#x3D; {bj}j，其中bj &#x3D; [x, y, z, l, w, h， α， β， τ]分别代表位置、宽度、长度、高度、方位角、类恒等式和跟踪恒等式。</p><p><a href="https://dl.acm.org/doi/10.5555/3001460.3001507">5:A density-based algorithm for discovering clusters in large spatial databases with noise</a></p><p><a href="https://www.ri.cmu.edu/app/uploads/2017/07/Xiao-2017-Efficient-L-Shape-Fitting.pdf">43:Efficient l-shape fitting for vehicle detection using laser scanners</a></p><h3 id="3-2-标签细化的原型约束盒正则化"><a href="#3-2-标签细化的原型约束盒正则化" class="headerlink" title="3.2. 标签细化的原型约束盒正则化"></a>3.2. 标签细化的原型约束盒正则化</h3><p>如第1节所述，不完整对象的初始标签通常在大小和位置上不准确。为了解决这个问题，我们引入了CProto-constrained Box Regularization (CBR)方法。其核心思想是构建一个基于无监督评分的高质量CProto集，以完善不完整对象的伪标签。与OYSTER[41]只能细化至少一次完整扫描的目标的伪标签不同，我们的CBR可以细化所有目标的伪标签，显著降低了整体尺寸和位置误差。</p><p><strong>完整性和大小相似性(CSS)评分</strong>。现有的标签评分方法，如IoU评分[21]是为完全监督检测器设计的。相反，我们引入了一种无监督的完备性和大小相似性评分(CSS)方法。它旨在仅使用常识性知识来近似IoU分数(见图5)。</p><p><img src="/pic/unsup5.png" alt="图5。完整性和大小相似性评分。"></p><p><strong>距离得分</strong>。CSS首先根据距离评估对象的完整性，假设离自我车辆更近的标签可能更准确。对于初始标签bj，我们在[0,1]范围内归一化到自我车辆的距离，以计算距离得分为：</p><p><img src="/pic/unsup6.png"></p><p>其中N是归一化函数，cj是bj的位置。然而，这种基于距离的方法有其局限性。例如，靠近自我车辆的遮挡物体，本应获得较低的分数，但由于它们距离较近，无意中被分配了较高的分数。为了缓解这一问题，我们引入了多层次占用(MLO)评分，如图4 (b)所示。</p><p><strong>MLO分数</strong>。考虑到物体的大小不同，我们将初始标签的边界框划分为多个具有不同长度和宽度分辨率的网格。然后通过确定聚类点占用的网格比例来计算MLO分数：</p><p><img src="/pic/unsup7.png"></p><p>式中No为分辨率号，Ok为第k个分辨率下占用的网格数，rk为第k个分辨率下的网格号。</p><p><strong>尺寸相似度(SS)得分</strong>。虽然距离和MLO分数可以有效地评估定位和大小质量，但它们在评估分类质量方面存在不足。为了弥补这一差距，我们引入了SS分数。这个分数使用一个类特定的模板框a (Wikipedia中典型对象的平均大小)，并计算截断的KL散度[7]。请注意，这个分数是由比率差异决定的，而不是它们的具体值。l、w、h比率的简单常识(车辆2:1:1，行人1:1:2，骑自行车者2:1:2)也可以用在这里。</p><p><img src="/pic/unsup8.png"></p><p>式中qa σ∈{la, wa, ha}， qb σ∈{lb, wb, hb}为模板和标签的归一化长、宽、高。</p><p>我们将三个指标S(bj) &#x3D; pi ωiψi(bj)线性组合以产生最终得分，其中ωi是权重因子(在本研究中我们采用简单平均值，ωi &#x3D; 1&#x2F;3)。对于每个bj∈b，我们计算其CSS分数scss j &#x3D; S(bj)，得到分数S &#x3D; {scss j}j的集合。</p><p><strong>CProto集合构造</strong>。常规的基于原型的可学习方法需要标注[13,47]，这在无监督问题中是不可用的。我们构建了一个高质量的CProto集合P &#x3D; {Pk}k，表示基于无监督CSS分数的几何和大小中心。式中，Pk &#x3D; {xp k, bp k}，其中xp k表示内部点，bp k表示边界框。具体来说，我们首先根据初始标签b的跟踪身份τ将其分类为不同的组。在每一组中，我们选择符合高CSS评分阈值η(在验证集上确定，本研究中使用0.8)的高质量框和内部点。然后，我们将所有的点和盒子转换成一个局部坐标系，通过对高质量盒子进行平均得到xp k，通过将所有的点串联得到bp k。</p><p><strong>盒子正规化</strong>。接下来，我们根据CProto的大小对初始标签进行正则化。根据WOD验证集的统计[25]，我们观察到初始标签的高度相对于长度和宽度是相对正确的(见图4 (c))。直观地看，类内具有相同高度的3D对象具有相似的长度和宽度。因此，我们将初始标签bj与CProto Pk通过最小盒高差进行关联。初始的伪标签具有相同的Pk和相似的长度和宽度，自然会被归为同一组。然后，我们对每个组执行调整大小和重新定位，以改进伪标签。  </p><ul><li>(1)改变形状。我们直接用bp k∈Pk的长、宽、高来替换bj的大小。</li><li>(2)重新定位。由于点大多在物体的表面和边界上，我们将物体划分为不同的箱，并将箱的边界和方向与最密集部分的边界点对齐(见图6)。最后，我们得到改进的伪标签b∗&#x3D; {b∗j}j。</li></ul><p><img src="/pic/unsup9.png" alt="图6。将初始标签的大小替换为CProto框，并更正位置。"></p><p>3.3. CProto约束自我训练(CST)</p><p>最近的方法[40,41]利用伪标签来训练三维检测器。然而，即使经过改进，一些伪标签仍然不准确，降低了正确监督的有效性，并可能误导培训过程。为了解决这些问题，我们提出了两种设计:(1)css加权检测损失，它根据标签质量分配不同的训练权值，以抑制虚假监督信号。(2)几何对比度损失，将稀疏扫描点的预测与密集CProto对齐，从而提高特征一致性。</p><p><strong>网络体系结构</strong>。我们采用密集稀疏对齐架构(图3 (c))，由原型网络Fpro和检测网络Fdet组成，由两级CenterPoint构建[39]。在训练过程中，我们将每个b * j对应的点xp k从CProto Pk添加到场景中，得到密集的点云xpro。我们将xpro馈送到Fpro，以产生相对较好的特征和检测。然后我们将随机下采样点xdet作为稀疏样本馈送到Fdet。我们通过检测损失和对比度损失对两个分支的特征和检测进行对齐。在测试过程中，我们将没有下采样的点馈送到检测网络Fdet进行检测。</p><p><strong>CSS的权重</strong>。考虑到虚假的伪标签可能会误导网络收敛，我们首先根据不同的标签质量计算一个损失权值。形式上，我们将伪标签的CSS分数sssi转换为:</p><p><img src="/pic/unsup10.png"></p><p>其中SH和SL是高&#x2F;低质量阈值(我们根据经验分别设置0.7和0.4)。</p><p><strong>css加权检测损失</strong>。为了减少假标签的影响，我们制定了css加权检测损失来细化N个提案：</p><p><img src="/pic/unsup11.png"></p><p>式中Lpro i和Ldet i分别为Fpro和Fdet的检测损耗[4]。损失由伪标签b *和网络预测计算。</p><p>几何对比度损失。我们制定了两种对比度损失，以最小化原型和检测网络之间的特征和预测盒差。(1)特征对比度损失。对于检测网络中的前景RoI ri，我们通过体素集抽象[4]从原型网络中提取特征fp i，从检测网络中提取特征fd i。然后我们用余弦距离表示对比度损失:</p><p><img src="/pic/unsup12.png"></p><p>其中Nf为前景提案号。(2)箱体对比度损失。对于来自原型网络的盒状预测dp i和来自检测网络的盒状预测dd i。然后通过IoU、位置差、角度差来计算箱体对比度损失:</p><p><img src="/pic/unsup13.png"></p><p>表示IoU函数;CD I， α d1 I为dd I的位置和角度;Cp I， α p1为dp I的位置和角度。我们终于把所有的损失归结为训练探测器。</p><h2 id="4-实验"><a href="#4-实验" class="headerlink" title="4. 实验"></a>4. 实验</h2><h3 id="4-1-数据集"><a href="#4-1-数据集" class="headerlink" title="4.1. 数据集"></a>4.1. 数据集</h3><p>Waymo开放数据集(WOD)。由于场景的多样性，我们对其进行了大量的实验[25]。WOD包含798、202和150个序列，分别用于训练、验证和测试。我们采用了类似的指标(3D AP L1和L2)作为完全&#x2F;弱监督方法[31,34]。没有使用注释进行训练。</p><p>PandaSet数据集。为了与最近的无监督方法[41]进行比较，我们还在PandaSet上进行了实验[35]。像[41]一样，我们将数据集分成73个训练片段和30个验证片段，并使用与类别无关的BEV AP和召回指标，阈值分别为0.3、0.5和0.7 IoU。</p><p>KITTI数据集。由于KITTI检测数据集[6]没有提供连续帧，我们只在3769 val分割[4]上测试了我们的方法。我们使用了与完全&#x2F;弱监督方法类似的指标(具有0.5和0.7 IoU阈值的Car 3D AP R40)[32,34]。</p><h3 id="4-2-实现细节"><a href="#4-2-实现细节" class="headerlink" title="4.2. 实现细节"></a>4.2. 实现细节</h3><p><strong>网络的细节</strong>。原型网络和检测网络都采用了与CenterPoint[39]相同的三维主干，与Voxel-RCNN[4]相同的RoI细化网络。对于WOD和KITTI数据集，我们使用与CenterPoint相同的检测范围和体素大小[39]。对于Pandaset，我们使用与OYSTER相同的检测距离[41]。</p><p><strong>训练的细节</strong>。我们采用了广泛使用的全局缩放和旋转数据增强。我们使用ADAM优化器在8个Tesla V100 gpu上训练我们的网络。我们使用一个周期学习率策略的学习率为0.003。我们训练了CPD20个epoch。</p><h3 id="4-3-与无监督检测器的比较"><a href="#4-3-与无监督检测器的比较" class="headerlink" title="4.3. 与无监督检测器的比较"></a>4.3. 与无监督检测器的比较</h3><p><strong>WOD结果</strong>。WOD验证集和测试集的结果如表1和表2所示。所有方法都使用相同大小的阈值来定义对象类，并使用单遍历。我们的方法明显优于现有的无监督方法。值得注意的是，在IoU阈值为0.7、0.5和0.5的3D AP L2下，我们的CPD在车辆、行人和骑自行车者方面分别比OYSTER[41]高出18.03%、13.08%和4.55%。这些进步来自于我们的MFC, CBR和CST设计，这些设计产生了卓越的伪标签和提高的检测精度。CPD也超越了使用类特定原型的prototo -vanilla方法[23]。</p><p><img src="/pic/unsup14.png" alt="表1。WOD验证集上的无监督3D目标检测结果。我们重现了以前方法的结果。"></p><p><img src="/pic/unsup15.png" alt="表2。WOD测试集上的无监督三维检测结果。"></p><p><strong>PandaSet上的结果</strong>。在PandaSet上分类无关的结果如表3所示。在0.7 IoU阈值下，我们的方法优于OYSTER 6.5%的AP和9.3%的召回率。这一改进很大程度上是由于我们的CPD提高了标签质量。与OYSTER不同，OYSTER在训练过程中会受到虚假标签的误导影响，我们的CPD利用CProto的先验大小来显著改善这些标签。</p><p><img src="/pic/unsup16.png" alt="表3。在PandaSet数据集上的分类无关性比较结果，在0-80m检测范围上进行评估。"></p><p>4.4. 与完全&#x2F;弱监督检测器的比较</p><p><strong>KITTI数据集上的结果</strong>。为了进一步验证我们的方法，我们在WOD上预训练了我们的CPD，以及OYSTER[41]和MODEST[40]，并使用统计归一化(SN)在KITTI数据集上进行了测试[28]。汽车检测结果如表4所示。我们首先将我们的方法与稀疏监督方法(带有2%标签的弱监督方法)[34]进行比较，稀疏监督方法每帧注释一个实例进行训练。我们的无监督CPD在中等车型上的性能比这种稀疏监督方法高出23.52% 3D AP@ IoU0.7。此外，我们的方法在0.5 IoU阈值下实现了简单和中等汽车类别的90.85%和81.01%的3D AP。值得注意的是，这一性能与完全监督方法CenterPoint[39]相当，证明了我们方法的先进性。</p><p><img src="/pic/unsup17.png" alt="表4。KITTI val集上全监督/弱监督检测器的汽车检测比较。模型是在WOD上训练的。"></p><p><strong>WOD结果</strong>。我们还将我们的方法与WOD验证集上的完全&#x2F;弱监督方法进行了比较[25]。车辆检测结果如表5所示。我们的无监督CPD在3D AP L1和L2方面分别比稀疏监督方法(2%注释)高5.25%和4.16%。</p><p><img src="/pic/unsup18.png" alt="表5所示。WOD验证集上车辆检测与全监督/弱监督检测器的比较。"></p><p>4.5. 伪标签比较</p><p>为了验证我们的伪标签，我们在WOD验证集上分析了它们的3D召回率和精度。如表6所示，我们的方法以9.42%的召回率和5.29%的精度提高(在0.7 IoU阈值下)超过了之前表现最好的OYSTER。为了了解这种改进的来源，我们检查了伪标签和基础真值之间的IoU，并比较了图7 (a)(b)(c)中的IoU分布。我们还在图7 (d)(e)(f)中给出了不同伪标签之间的大小、位置和角度的平均绝对误差。该方法的IoU分布比其他方法更接近于1，并且在大小、位置和角度上的误差也更小。这些结果验证了我们的MFC和CBR显著减少了标签错误。</p><p><img src="/pic/unsup20.png" alt="表6所示。WOD验证集上的伪标签比较结果。"></p><p><img src="/pic/unsup19.png" alt="图7。(a-c)伪标签与基础真值之间的IoU分布。(d-f)不同方法生成的伪标签大小、位置、角度的平均绝对误差。"></p><h3 id="4-6-消融实验"><a href="#4-6-消融实验" class="headerlink" title="4.6. 消融实验"></a>4.6. 消融实验</h3><p><strong>CPD的成分分析</strong>。为了评估我们设计的单个贡献，我们逐步添加每个组件，并使用WOD验证集评估它们对车辆检测的影响。结果如表7所示。我们的MFC方法在AP中比单帧聚类(SFC)方法高出2.52%，这归因于与单帧相比，跨连续帧的对象的点表示更完整。CBR在AP中进一步提高了19.27%的性能，因为它减少了伪标签的大小和位置错误。CST对AP的贡献增加了8.09%，证明了CProto的几何特征在检测稀疏目标方面的有效性。</p><p><img src="/pic/unsup21.png" alt="表7所示。WOD验证集上的CPD成分分析结果。"></p><p><strong>mfc的帧数</strong>。为了检验帧数对初始伪标签质量的影响，我们在WOD验证集上实验了不同数量的过去和未来点云帧。图8 (a)(b)所示的BEV结果表明，在[- 5,5]帧(5个过去帧、5个未来帧和当前帧)下，性能最佳。额外的帧并没有显著提高记忆的回忆率和准确率。因此，我们在本研究中使用了11帧作为初始伪标签生成。</p><p><img src="/pic/unsup22.png" alt="图8。(a)(b)不同帧对初始伪标签的查全率和查准率。(c)不同分数对初始伪标签的召回精度曲线。"></p><p><strong>CSS评分的成分分析</strong>。为了评估评分系统的有效性，我们计算了不同分数的初始伪标签的BEV AP。表8中报告的标签数量平均绝对误差Precision Recall (%) Precision(%)评估显示，结合所有成分(距离、MLO和SS)产生最高的AP。图8 (c)中绘制的Recall - Precision曲线也支持这一发现。这表明了各成分在准确测量伪标签质量中的重要性。</p><p><img src="/pic/unsup23.png" alt="表8所示。WOD验证集上的CSS成分分析结果。"></p><p><strong>CBR的成分分析</strong>。为了评估调整尺寸和重新定位对CBR的影响，我们进行了实验并分析了伪标签的性能。如表9所示，在0.5 IoU和0.7 IoU阈值下，重新调整尺寸导致BEV召回率分别增加3.91%和3.4%;在这些阈值下，重新定位进一步提高了召回率12.68%和6.43%，同时也提高了准确率。这些结果表明了这两个成分的重要性，从而有效地改进了伪标签。</p><p><img src="/pic/unsup24.png" alt="表9所示。WOD验证集上的CBR成分分析结果。"></p><p><strong>CST的成分分析</strong>。为了评估CST中每个成分的有效性，我们仅使用cbr生成的伪标签建立了一个基线，用于训练两阶段CenterPoint检测器，然后逐渐添加我们的损失成分，并在WOD验证集上评估车辆检测性能。如表10所示，所有损耗成分都有助于提高性能。具体来说，我们的Lcss det使用CSS权重减轻了虚假伪标签的影响，并将IoU0.7的3D AP L2提高了4.79%。通过利用密集CProto的几何知识进行更有效的稀疏目标检测，我们的Lcss技术和Lcss盒分别将IoU0.7的3D AP L2提高了0.75%和2.55%。</p><p><img src="/pic/unsup25.png" alt="表10。WOD验证集上的CST成分分析结果。"></p><p>4.7. 可视化的比较</p><p>为了更直观地理解我们的方法是如何提高检测性能的，我们将我们的结果与MODEST[40]和OYSTER[41]进行了视觉比较，如图9所示。MODEST经常遗漏远处的、稀疏的物体(图9(1.1))，而OYSTER检测到它们，但不准确地报告它们的大小和位置(图9(2.1))。相比之下，CPD使用我们基于cproto的设计，不仅可以识别这些物体，还可以准确地预测它们的大小和位置(图9(3.1))。此外，由于我们的CST减少了虚假伪标签的影响，因此误报(图9(3.2))也比以前的方法(图9(1.2)(2.2))少得多。</p><p><img src="/pic/unsup26.png" alt="图9。WOD验证集上不同检测结果的可视化比较。"></p><h2 id="5-结论"><a href="#5-结论" class="headerlink" title="5. 结论"></a>5. 结论</h2><p>本文提出了一种新的无监督三维目标精确检测方法——CPD框架。首先，我们开发了一种MFC方法来生成初始伪标签。然后，使用CSS评分构造CProto集。接下来，我们介绍了一种CBR方法来改进这些伪标签。最后，设计了一种CST来提高稀疏目标的检测精度。大量的实验验证了我们设计的有效性。值得注意的是，我们的无监督CPD方法首次超过了一些弱监督方法，证明了我们方法的先进性。</p><p><strong>局限性</strong>。我们工作的一个值得注意的限制是，与车辆等更普遍的类别相比，少数类别(如骑自行车的人)的平均精度(AP)明显较低(表1)。这种差异很大程度上是由于数据集中这些少数类的实例很少。未来收集这些物体的努力可能是解决这一问题的一个有希望的途径。</p><h1 id="补充材料"><a href="#补充材料" class="headerlink" title="补充材料"></a>补充材料</h1><h2 id="6-方法详情"><a href="#6-方法详情" class="headerlink" title="6. 方法详情"></a>6. 方法详情</h2><p><img src="/pic/unsup27.png" alt="图10。MFC包括运动伪影去除、聚类(地面去除、点聚类和盒拟合)和后处理(跟踪、平滑和分类)。"></p><p>更多关于mfc的细节。在我们的主要论文3.1节中，我们介绍了用于初始标签生成的多帧聚类(MFC)。为了更直观的理解，我们在图10中提供了一个框架说明。这里我们介绍更多的后处理细节。正如我们在主要论文中提到的，我们基于人类常识预先定义了一组特定于类的大小阈值，将伪标签分类到不同的类别中。以WOD为例，我们预先定义了五个类别:‘Discard Small’, ‘Pedestrian’, ‘Cyclist’, ‘Vehicle’, and ‘Discard Large’。形式上，对于聚类盒bj，我们通过从阈值顺序匹配来确定类恒等式β:</p><p><img src="/pic/unsup28.png"></p><p>式中l、w、h分别为bj的长、宽、高。“废弃大”盒子大多是树木和建筑物被直接移除。‘Discard Large’框包含潜在的前景对象和背景对象。然后，我们应用类别不可知跟踪将小背景对象与前景轨迹关联起来，并利用时间相干性增强对象大小的一致性。</p><p><strong>CSS评分的更多细节</strong>。在我们的主要论文3.2节中，我们介绍了CSS评分。为了更好地理解CSS评分如何接近IoU评分，我们在图11中给出了IoU评分曲线，其中我们展示了三种方法:密度评分(sden)，距离评分(sdis)和CSS评分(scss)。直观上，好的评分应该与IoU评分保持一致。换句话说，随着分数的增加，所选择的伪标签与基础真值的借据应该更大。我们发现，随着IoU的增加，我们的CSS评分保持了最一致的增长。这里我们还提供了主论文Eq. 3中计算Size Similarity的模板盒的长度、宽度和高度:</p><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token punctuation">&#123;</span><span class="token string">'Vehicle'</span><span class="token punctuation">:</span> <span class="token punctuation">[</span><span class="token number">5.06</span><span class="token punctuation">,</span> <span class="token number">1.86</span><span class="token punctuation">,</span> <span class="token number">1.49</span><span class="token punctuation">]</span><span class="token punctuation">,</span><span class="token string">'Pedestrian'</span><span class="token punctuation">:</span> <span class="token punctuation">[</span><span class="token number">1.0</span><span class="token punctuation">,</span> <span class="token number">1.0</span><span class="token punctuation">,</span> <span class="token number">2.0</span><span class="token punctuation">]</span><span class="token punctuation">,</span><span class="token string">'Cyclist'</span><span class="token punctuation">:</span> <span class="token punctuation">[</span><span class="token number">1.9</span><span class="token punctuation">,</span> <span class="token number">0.85</span><span class="token punctuation">,</span> <span class="token number">1.8</span><span class="token punctuation">]</span><span class="token punctuation">&#125;</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span></span></code></pre><p><img src="/pic/unsup29.png" alt="图11。不同评分方法的比较。"></p><h2 id="7-更多实验结果"><a href="#7-更多实验结果" class="headerlink" title="7. 更多实验结果"></a>7. 更多实验结果</h2><p><strong>更多可视化结果</strong>。为了更好地理解我们的方法是如何提高检测结果的，这里我们展示了更多的可视化结果。从图12中可以看出，由于基于cproto的设计，我们的方法(3.1-3.4)的识别和定位性能都比之前的方法(1.1-1.4,2.1-2.4)要好得多。</p><p><img src="/pic/unsup30.png" alt="图12。不同的无监督检测器预测的可视化结果。"></p><p><strong>WOD验证集上的BEV AP和3D APH结果</strong>。一些完全监督的方法也报道了BEV AP L2和3D APH的性能。在这里，我们分别在表11和表12中给出了结果。我们的CPD在BEV AP L2和APH L2上都大大优于之前的MODEST和OYSTER，进一步证明了我们方法的有效性。</p><p><img src="/pic/unsup31.png" alt="表11所示。WOD验证集上的3D AP L2和BEV AP L2结果。"></p><p><img src="/pic/unsup32.png" alt="表12。WOD验证集上的3D APH结果。"></p>]]></content>
      
      
      <categories>
          
          <category> DL </category>
          
      </categories>
      
      
        <tags>
            
            <tag> CV </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>弱监督下的三维目标检测(激光点云篇)</title>
      <link href="/2024/04/25/weakly-3d-det2/"/>
      <url>/2024/04/25/weakly-3d-det2/</url>
      
        <content type="html"><![CDATA[<h1 id="弱监督下的三维目标检测-激光点云篇，接单目篇"><a href="#弱监督下的三维目标检测-激光点云篇，接单目篇" class="headerlink" title="弱监督下的三维目标检测(激光点云篇，接单目篇)"></a>弱监督下的三维目标检测(激光点云篇，接单目篇)</h1><h2 id="二、基于激光点云的三维目标检测"><a href="#二、基于激光点云的三维目标检测" class="headerlink" title="二、基于激光点云的三维目标检测"></a>二、基于激光点云的三维目标检测</h2><h3 id="第一篇：VS3D"><a href="#第一篇：VS3D" class="headerlink" title="第一篇：VS3D"></a>第一篇：VS3D</h3><p>Weakly Supervised 3D Object Detection from Point Clouds（2020）</p><p><a href="https://arxiv.org/abs/2007.13970">论文链接</a></p><p><a href="https://github.com/Zengyi-Qin/Weakly-Supervised-3D-Object-Detection">代码链接</a></p><p><img src="/pic/weak3d27.png"></p><h3 id="第二篇：WS3D"><a href="#第二篇：WS3D" class="headerlink" title="第二篇：WS3D"></a>第二篇：WS3D</h3><p>Weakly Supervised 3D Object Detection from Lidar Point Cloud（2020）</p><p>Towards a Weakly Supervised Framework for 3D Point Cloud Object Detection and Annotation（2021）</p><p><strong>上面两篇文章为同一作者所写，后者为前者的延伸</strong></p><p><a href="https://arxiv.org/abs/2007.11901">论文链接1</a><br><a href="https://ieeexplore.ieee.org/document/9369074/">论文链接2</a></p><p><a href="https://github.com/hlesmqh/WS3D">代码链接</a></p><p><img src="/pic/weak3d28.png"></p><h3 id="第三篇：SS3D"><a href="#第三篇：SS3D" class="headerlink" title="第三篇：SS3D"></a>第三篇：SS3D</h3><p>Sparsely-Supervised 3D Object Detection from Point Cloud（2022）</p><p><a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Liu_SS3D_Sparsely-Supervised_3D_Object_Detection_From_Point_Cloud_CVPR_2022_paper.pdf">论文链接</a></p><p><a href="https://github.com/gaocq/SS3D2">代码暂未开源</a></p><h4 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h4><p>传统的基于深度学习的 3D 目标检测方法需要大量的 3D 边界框注释进行训练，这在实践中获取成本很高。稀疏注释的对象检测可以很大程度上减少注释，这是非常具有挑战性的，因为丢失的注释实例在训练过程中将被视为背景。在本文中，我们提出了一种稀疏监督的 3D 对象检测方法，称为 SS3D。为了消除缺失注释造成的负面监督，我们设计了一个缺失注释实例挖掘模块，采用严格的过滤策略来挖掘正例。同时，我们设计了可靠的背景挖掘模块和点云填充数据增强策略，以生成置信数据，以便在可靠的监督下进行迭代学习。所提出的 SS3D 是一个通用框架，可用于学习任何现代 3D 对象检测器。对 KITTI 数据集的大量实验表明，在不同的 3D 检测器上，所提出的 SS3D 框架只需 20% 的注释即可实现与完全监督方法相当的性能。与 KITTI 上最先进的半监督 3D 对象检测相比，我们的 SS3D 在相同注释工作负载下显着提高了基准。此外，我们的 SS3D 还显着优于最先进的弱监督方法，凸显了其有效性。</p><h4 id="引言"><a href="#引言" class="headerlink" title="引言"></a>引言</h4><p>三维（3D）物体检测旨在从 3D 传感器数据（例如 LiDAR 点云）中对物体进行定位和分类，由于其在自动驾驶、增强&#x2F;虚拟现实和室内机器人领域的多样化应用而受到越来越多的关注。最近，人们提出了许多基于体素或点特征的方法[1,17,18,34,35]，并在大规模基准数据集上实现了高性能[2,21]。然而，大多数提出的 3D 对象检测器都需要完全监督学习，这意味着模型学习需要完全注释的数据集。与 2D 图像对象相比，注释 3D 点云对象更加耗费人力：注释者必须仔细切换视点或放大和缩小整个 3D 场景，以标记每个 3D 对象。因此，开发具有同等检测性能且仅需要轻量级对象注释的 3D 检测器是实际应用中需要解决的一个有意义的问题。</p><p>最近，很少有工作[10,15,24,26,33]被提出来解决这个问题。在[10]中，采用了弱监督学习策略。具体来说，点注释方案用于减轻注释边界框的负担。然而，点标注提供的监督信息较弱，因此必须额外提供一定量的全标注，才能达到最佳性能。在[24, 33]中，使用了半监督学习策略，其中仅部分数据集被注释，其余部分未标记。利用师生框架将信息从标记数据传输到未标记数据。然而，当标记数据和未标记数据之间的差距很大时，信息传输往往是无效的。此外，虽然只标注了数据集的一部分，但标记单个场景仍然需要付出不可忽视的劳动，特别是对于具有许多 3D 对象的拥挤场景，如图 1 所示。</p><p><img src="/pic/weak3d38.png" alt="图 1. 完全监督方法和我们的方法所需注释的演示。左边的案例显示了 PV-RCNN [16] 的训练阶段，PV-RCNN 是一种高性能检测器，具有完整的注释作为输入，而我们的模型只为每个场景注释一个实例。右侧案例显示了 PV-RCNN 和我们的模型的预测结果，表明我们的模型实现了与全监督方法相当的性能。"></p><p>在本文中，我们采用稀疏标注策略，仅标注场景中的一个 3D 对象，如图 1 左侧所示。这样，我们就能够获得每个场景的一个 3D 对象的完整监督信息。 。直观上，这有助于学习未标记对象的信息，因为场景内信息传输比跨场景知识传输容易得多。然而，稀疏注释的对象检测也提出了新的挑战：缺失注释的实例将带来不正确的监督信号（即作为负样本）来干扰网络的训练。在训练过程中，由于缺失注释的实例以及这些实例附近的区域可能被错误地标记为背景，因此当梯度反向传播时，网络的权重更新将被显着误导。通过利用 2D 对象之间的重叠或层次关系信息，2D 稀疏对象检测方法 [11, 27] 已经解决了这一挑战。然而，此类信息在 3D 数据集中可能不存在，例如 KITTI [2]，这阻碍了此类方法直接应用于 3D 应用。</p><p>为了应对这一挑战，我们提出了一种新颖有效的稀疏注释 3D 对象检测方法，即 SS3D，它可以应用于任何先进的 3D 检测器。我们的 SS3D 的主要思想是以高置信度迭代挖掘正实例和背景，并进一步使用这些生成的数据来训练 3D 检测器。我们设计了两个有效的模块，即<strong>缺失标注实例挖掘模块和可靠背景挖掘模块</strong>，分别挖掘可靠缺失正实例和背景。这确保了 3D 检测器能够使用可靠的监督数据进行训练。通过这种设计，与使用完全注释数据集训练的 3D 检测器相比，我们的 SS3D 可以实现相当的性能，其中稀疏注释数据集只需要 20% 的注释。</p><p>总而言之，我们的贡献如下： </p><ul><li>我们提出了一种从点云进行稀疏注释的 3D 对象检测的新方法，该方法可以用作训练任何现有 3D 全监督检测器的通用框架。据我们所知，这是第一个探索 3D 对象检测任务的稀疏注释策略的工作。</li><li>我们设计了两个有效的模块来分别挖掘可靠的缺失正实例和背景，这确保了3D检测器能够使用可靠的监督数据进行训练。 </li><li>实验结果表明，我们的稀疏注释方法可以实现与完全监督方法相当的性能，并且远远优于最先进的半监督和弱监督3D 对象检测方法。</li></ul><h4 id="相关工作"><a href="#相关工作" class="headerlink" title="相关工作"></a>相关工作</h4><p><strong>完全监督的 3D 物体检测</strong></p><p>现有的3D检测方法可大致分为两类：基于体素的方法[4,5,28,34,35]和基于点的方法[12,17,19,29,30,32]。</p><p><strong>对于基于体素的方法</strong>，体素化是不规则点云应用传统 2D 或 3D 卷积的常用措施。在voxelNet [36]中，采用体素特征编码层从点云中提取统一的特征表示。 SECOND [28] 通过修改稀疏卷积算法 [3, 7] 有效地从 3D 体素中提取特征。 TANet [8] 利用堆叠注意力模块来利用多级特征关系。 Part-A² [18]提出了一个两阶段网络，通过对对象内部分特征进行分组来探索空间关系。 SE-SSD[35]采用了一对教师和学生检测器来提高性能，而无需在推理中引入额外的计算。 Voxel R-CNN [1] 设计了一个体素 RoI 池化来直接聚合来自 3D 体素特征卷的空间上下文。</p><p><strong>基于点的方法</strong>直接将原始不规则点作为输入来提取局部和全局特征[13, 14]。 PointRCNN [17] 融合了提取的特征和以自下而上的方式生成的 3D 提案中的原始点以进行细化。 STD [30]提出了一种新颖的球形锚点来减少锚点的数量，并利用稀疏到密集的思想来提高性能。 3DSSD[29]提出了一种基于特征距离的融合采样策略，以保留丰富的信息。 PV-RCNN [16]利用体素到关键点场景编码和关键点到网格特征聚合来提高性能。</p><p>尽管之前的工作已经取得了显着的进展并表现出了令人印象深刻的性能，但这样的结果很大程度上依赖于大规模的手动注释，这非常耗时且费力。我们提出的方法采用稀疏注释策略，仅为每个场景注释一个对象，同时实现与这些完全监督方法相当的性能。此外，无论是基于体素的探测器还是基于点的探测器，我们的 SS3D 都可以直接应用。</p><p><strong>弱&#x2F;半监督 3D 物体检测</strong></p><p>为了减少 3D 对象的注释，WS3D [24] 采用了弱监督学习策略，这是通过基于点击注释方案的两阶段架构来实现的。 WS3D [10] 在第 1 阶段通过单击注释场景生成圆柱形对象建议，并在第 2 阶段使用轻微标记良好的实例细化建议以获取长方体。然而，弱监督点标注提供的监督信息较弱，因此必须额外提供一定量的全标注。同时，基于VoteNet [12]，SESS [33]首先提出了一种半监督3D对象检测，利用互助师生[22]框架来强制执行三种一致性损失。在SESS之后，3DIoUMatch [24]被提出来估计3D IoU作为定位度量，并设置一个自我调整的阈值来过滤伪标签。</p><p>与这些方法不同，我们提出的方法对每个场景中存在的对象进行精确的监督信息，这使我们能够在场景内传输可靠的监督信息。直观上，这优于跨场景传输监督信息，特别是对于变化较大的场景。</p><p><strong>稀疏监督的二维物体检测</strong></p><p>稀疏注释的对象检测是减少网络对仅注释部分对象的数据注释的依赖的另一种方法。由于部分实例缺少注释，当梯度反向传播时，网络的权重更新可能会受到很大的误导。为了解决这个问题，现有的先进方法采用了对 ROI（感兴趣区域）损失进行重新加权或重新校准的策略，以消除未标记实例的影响。软采样 [27] 利用 RoIs 和带注释的实例之间的重叠来重新加权损失。基于焦点损失[6]的背景重新校准损失[31]将未标记的实例视为硬负样本并重新校准其损失，仅适用于单级检测器。特别是，部分感知采样[11]通过使用人类直觉来判断标记和未标记实例之间的层次关系，从而忽略了部分类别的分类损失。联合挖矿[25]提出了一个联合生成模块，将未标记的实例转换为正向监督。</p><p>上述稀疏标注的目标检测方法都是针对二维图像目标的。由于 2D 图像和 3D 点云之间的模态差异，这些方法不能应用于我们的 3D 对象检测任务。例如，在KITTI [2]中，3D对象是自然分离的，这意味着对象之间的重叠为零，并且对象之间不存在层次关系。与重新加权和重新校准方法相比，本文提出了一种稀疏注释的 3D 对象检测的新方法，该方法利用缺失注释的实例挖掘模块和简单但有效的背景挖掘模块来挖掘置信的正实例和背景，这是训练高性能检测器的关键。</p><h4 id="方法"><a href="#方法" class="headerlink" title="方法"></a>方法</h4><p><strong>总体框架</strong></p><p>作为一个通用框架，所提出的 SS3D 旨在促进 3D 检测器的学习，以便在基于稀疏注释数据集从头开始训练时获得最佳检测性能。如图2所示，所提出的SS3D主要由缺失注释实例挖掘模块、可靠背景挖掘模块、点云填充数据增强和实例库组成。给定 3D 检测器，最初，我们在稀疏注释的数据集上从头开始训练检测器。然后，我们使用检测器通过具有严格过滤策略的缺失注释实例挖掘模块，从训练数据中的点云中挖掘可靠的缺失注释实例。我们将挖掘的实例（橙色）和原始带注释的实例（红色）存储到实例库中。依托实例库，我们进一步利用检测器通过可靠背景挖掘模块来挖掘可靠背景。基于这两个模块的结果，我们利用所提出的点云填充数据增强来构建置信数据集，该数据集可进一步用于重新训练检测器。通过这种迭代学习方式，我们最终可以获得高性能的3D检测器。下面详细介绍。</p><p><img src="/pic/weak3d39.png" alt="图 2.我们的 SS3D 管道。缺失标注实例挖掘模块搜索缺失标注实例并将其存储在实例库中。可靠背景挖掘模块利用实例库进一步获取具有可靠背景的破碎场景。然后使用点云填充数据增强策略生成置信数据以迭代学习检测器。"></p><p><strong>探测器架构</strong></p><p>我们的方法是使用稀疏注释数据集训练 3D 目标检测器的通用框架，可以直接应用于各种检测器。在本文中，我们使用 PointRCNN [17]、Part-A2 [18]、PV-RCNN [16] 和 VoxelRCNN [1] 最先进的 3D 检测器验证我们的 SS3D。我们以PV-RCNN为例，简单回顾一下该方法。 PV-RCNN 是一种高性能、高效的两级点云检测器，通过新颖的方法将多尺度 3D 体素卷积神经网络 (CNN) 特征和基于 PointNet++ 的集合抽象特征深度集成到一小组关键点中。体素集抽象模块。</p><p><strong>缺失注释实例挖掘模块</strong></p><p>如图3所示，我们设计了一个缺失注释的实例挖掘模块，该模块结合了IoU引导的抑制和基于分数的过滤方案，作为将未标记的正实例挖掘为高质量伪实例的强化措施。然后，将选定的伪实例存储在实例库中，以进一步指导可靠后台挖掘模块。</p><p><img src="/pic/weak3d40.png" alt="图 3.我们提出的缺失注释实例挖掘模块的图示。训练数据和相应的增强数据是检测器的两个不同输入。然后，我们利用基于分数的过滤来删除原始训练数据的增强预测和具有低置信度分数的增强数据的预测。此外，提出了 IoU 引导抑制来过滤掉低质量的预测。最后，我们将剩余的预测作为伪实例存储在实例库中。"></p><p><strong>基于分数的过滤</strong> 如图 3 所示，首先，原始输入点云 x 通过顶部检测器生成预测 pt。然后，我们执行一组全局增强，其中包括对 x 进行随机旋转、翻转和缩放以生成增强点云 ˆx，与 pt 同步生成增强预测 ˆpt，底部检测器基于 ˆx 生成预测 pb。最后，我们设置分类置信度阈值τcls来过滤掉可能包含错误类别的pb和^pt的预测，然后获得过滤后的预测。</p><p><strong>IoU引导抑制</strong>请注意，仅基于分数的过滤策略无法获得可靠的预测。受 FixMatch [20] 的启发，我们进一步提出了一种有效的 IoUguided 抑制策略。得到过滤后的预测后，我们计算 ^pt 和 pb 中每对边界框之间的 IoU 矩阵，旨在匹配来自不规则点云的两个预测的框。然后，我们过滤掉 IoU 小于阈值 τIoU 的不匹配的配对边界框，从而进一步提高伪实例的质量。</p><p>最后一步实例库处理结合基于分数的过滤和IoU引导的抑制，可以有效避免低质量的伪实例生成，最终获得一组边界框{br}N n&#x3D;1，其中N和r是训练数量场景和边界框分别保留在场景中。然后，我们计算索引 n 的同一场景的框 br n 和 bB n （来自实例库 B 的边界框）之间的 IoU，并选择不与 bB n 重叠的 br n 。最后，所选的边界框（橙色）以及相应的预测类标签和点云存储在实例库中，该实例库还包含所有稀疏注释的立场（红色）。通过这样的设计，随着网络的迭代，我们的实例库可以存储越来越多的正实例来指导可靠背景挖掘模块挖掘更可靠的背景。</p><p><strong>可靠的背景挖掘模块</strong></p><p>依靠更新的实例库，我们利用所提出的可靠背景挖掘模块来挖掘背景点，并进一步消除由于缺失注释实例而导致的负面监督信息。与现有的针对不正确监督的重新缩放策略[11, 31]相比，我们的方法更加简单有效。</p><p>如图4所示，为了获得可靠的背景点云，我们采取尽可能寻找潜在前景点的策略。具体来说，我们使用具有低置信度得分阈值τl的检测器来获得对象检测结果。同时，我们从检测器中删除了非极大值抑制（NMS）操作。通过这种方式，我们确保结果尽可能包含潜在的前景点，这意味着原始点云的其余部分往往是可靠的背景点云。为了生成新的训练数据，我们删除了检测到的对象的 3D 边界框中不与实例库中的实例重叠的点数据。</p><p><img src="/pic/weak3d41.png" alt="图 4.我们提出的可靠后台挖掘模块的图示。首先，我们将原始点云提供给没有 NMS 的检测器以产生近乎重复的预测，并利用实例库中存储的实例来过滤掉不可靠的对象点。这将导致场景破碎，并通过点云填充策略进一步处理。"></p><p><strong>点云填充数据增强</strong></p><p>经过可靠的背景选择处理后，点云场景被打破。同时，场景中的实例可能非常稀疏。这些问题将显着降低网络的性能。受到[28]提出的地面实况（GT）采样增强的启发，我们进一步提出了点云填充数据增强策略来解决这些问题。对于每个剩余的边界框，我们从实例库中随机选择一个边界框，并将相应的点云放置在所选边界框内的剩余边界框的中心，如果所选边界框不与实例库中现有的边界框重叠。破碎的场景。然后我们利用 GT 采样增强 [28] 进一步增强当前场景。最后，我们获得具有置信的正例和可靠背景的合并点云。通过这种设计，我们可以修复之前删除点所带来的密度不均匀问题，同时更多的groundtruth box也减少了每个场景中只有少量实例稀疏标注时对网络的负面影响。</p><p>通过前面的处理，可能对网络造成负面影响的歧义点大部分被去除，包括那些缺失标注的实例并且没有被我们的缺失标注实例挖掘模块挖掘到。此外，生成的置信数据为以迭代方式重新训练检测器提供了重要的监督信息。算法 1 总结了我们的 SS3D。</p><p><img src="/pic/weak3d42.png"></p><h4 id="实验"><a href="#实验" class="headerlink" title="实验"></a>实验</h4><p>遵循最先进的方法 [4,8,17,18,34,35]，我们在 KITTI 3D 和 BEV 对象检测基准 [2] 上评估我们的 SS3D。这是一个广泛用于性能评估的流行数据集，包含 3D 对象检测的完整注释。有 7,481 个样本用于训练，7,518 个样本用于测试，我们进一步将训练样本分为 3,712 个样本的训练分割和 3,769 个样本的验证分割作为常见做法[16]。另外，由于对象的遮挡和截断程度，KITTI基准在评估上分为三个难度级别：简单、中等和困难。在[31]中生成稀疏注释数据集之后，我们在训练分割中的每个 3D 场景中随机保留一个注释对象，以生成极其稀疏的分割。与 KITTI 上所有对象的完整注释相比，极其稀疏的分割只需要注释 20% 的对象。为了公平比较，我们报告了 40 个和 11 个召回位置的 mAP，对于汽车、行人和骑自行车这三个类别，3D 重叠阈值分别为 0.7、0.5、0.5。</p><p><strong>实施细节</strong></p><p>首先，我们按照 PCDet [23] 以有监督的方式训练我们的检测器，并使用极其稀疏的分割，并保持与所使用的检测器相同的监督损失。在训练阶段，我们采用 ADAM 优化器和余弦退火学习率 [9]，批量大小为 8，持续 6 个 epoch。在可靠的背景选择中，我们将低分阈值τl设置为0.01。对于基于分数的过滤和 IoU 引导的抑制，我们将置信度分数阈值 τcls 和 IoU 阈值 τIoU 设置为 0.9。注意，我们设置迭代学习的次数M&#x3D;10。在我们的全局增强中，我们以 0.5 的概率沿 X 轴和 Y 轴随机翻转每个场景，然后使用 [0.8, 1.2] 中的均匀采样因子对其进行缩放。最后，我们以从 [ −π &#x2F;4 , π&#x2F; 4 ] 采样的随机角度绕 Z 轴旋转点云。</p><p><strong>与最先进方法的比较</strong></p><p>与完全监督方法的比较我们将所提出的方法与四种最先进的完全监督方法进行比较：PointRCNN [17]、Part-A2 [18]、PVRCNN [16]、Voxel-RCNN [1]，分别是带注释的训练分割和极稀疏训练分割，其中这些在极稀疏分割上训练的检测器用作我们方法的初始检测器。不同方法的结果如表1所示。 </p><p><img src="/pic/weak3d43.png" alt="表 1. 在 KITTI val split 上与使用完整注释和极其稀疏的分割（完整注释的 20% 实例）训练的不同检测器进行比较。 3D 物体检测和鸟瞰图检测通过 11 个召回位置的平均精度进行评估。"></p><p>从表中可以看出，由于缺失标注实例的负面影响，在极度稀疏分割上训练的四个检测器的性能平均下降了10%以上。我们的方法显着提高了这些检测器的性能，使它们接近完全监督的性能，这表明我们的方法在挖掘缺失注释实例和可靠背景方面具有良好的效果。</p><p>我们的 SS3D 预测结果的可视化如图 5 所示。为了更好地查看结果，我们将 3D 点云的预测投影到相应的彩色图像上。从图中我们可以看出，所提出的方法具有高质量的预测结果。</p><p><img src="/pic/weak3d44.png" alt="图 5.我们的 SS3D（基于 PV-RCNN）在 KITTI val 数据集上的定性结果。汽车、骑自行车者和行人的地面实况 3D 边界框分别用绿色、黄色和青色绘制。我们将预测的边界框设置为红色，并将点云中的框投影回彩色图像上以进行可视化。"></p><p><strong>与半监督方法的比较</strong>我们将所提出的方法与基于先进检测器PV-RCNN [16]的半监督方法3DIoUMatch [24]进行比较。为了进行公平的比较，我们还采用 PV-RCNN 作为检测器，并保持所有方法具有相同数量的注释对象进行训练。在 KITTI train split 中，有 3,712 个场景，这些场景总共包含 17,289 个汽车、行人和骑自行车的物体。对于半监督方法，1％的标记数据意味着37（3712×1％）个场景，其中平均包括172（17289×1％）个用于训练的标记对象。因此，对于我们极其稀疏的分割中的 1% 标记数据，我们随机选择 172 个场景（包括 172 个标记对象）进行训练。我们还测试了两种方法的 2% 标记训练数据的情况。不同比例标记数据的结果见表 1。如图 2 所示，这说明我们的 SS3D 在具有所有三个难度级别的三个类别中显着优于当前最先进的 3DIoUMatch。与3DIoUMatch相比，我们的网络更大的优势是训练时只使用了172个场景。我们放弃剩余的场景，而 3DIoUMatch 使用列车分割中的所有 3712 个场景进行信息传输。</p><p><img src="/pic/weak3d45.png" alt="表 2. 在 1% 或 2% 标记数据下 KITTI val split 与 3DIoUMatch 的比较。我们的 SS3D 和 3DIoUMatch 都是基于 PV-RCNN 的。我们报告了 40 个召回位置的 mAP，汽车、行人和骑自行车者的 IoU 阈值分别为 0.5、0.25、0.25。"></p><p><strong>与弱监督方法的比较</strong>在弱监督方法中，WS3D [10]，使用 500 个带有中心单击标签的场景和 534 个精确注释的实例来训练网络。由于标准检测器不适用于中心单击标签，因此我们仅使用相同的 534 个精确注释实例来训练我们提出的 SS3D。标签。图3表示比较结果。显然，我们的 SS3D 具有不同的 3D 检测器，在所有难度级别上都取得了最高的结果，在标记工作量较少的情况下大幅优于 WS3D。</p><p><img src="/pic/weak3d46.png" alt="表 3. KITTI val 分割与 WS3D 的比较。我们报告了 11 个召回位置的 mAP。 “*”表示具有中心单击的场景，“#”表示精确注释的实例。"></p><h3 id="第四篇：A-Simple-Vision-Transformer-for-Weakly-Semi-supervised-3D-Object-Detection（2023）"><a href="#第四篇：A-Simple-Vision-Transformer-for-Weakly-Semi-supervised-3D-Object-Detection（2023）" class="headerlink" title="第四篇：A Simple Vision Transformer for Weakly Semi-supervised 3D Object Detection（2023）"></a>第四篇：A Simple Vision Transformer for Weakly Semi-supervised 3D Object Detection（2023）</h3><p><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Zhang_A_Simple_Vision_Transformer_for_Weakly_Semi-supervised_3D_Object_Detection_ICCV_2023_paper.pdf">论文链接</a></p><p>[代码暂未开源]</p><h4 id="摘要-1"><a href="#摘要-1" class="headerlink" title="摘要"></a>摘要</h4><p>先进的 3D 物体检测方法通常依赖于大规模、精心标记的数据集来实现良好的性能。然而，标记 3D 对象的边界框既困难又昂贵。虽然半监督（SS3D）和弱监督3D对象检测（WS3D）方法可以有效降低标注成本，但它们存在两个局限性：</p><ul><li>1）它们的性能远远不如全监督的同行； </li><li>2）它们很难适应不同的探测器或场景（例如室内或室外）。</li></ul><p>在本文中，我们研究了带有点注释的弱半监督 3D 对象检测（WSS3D），其中数据集包含少量完全标记和大量弱标记数据，并为每个 3D 对象注释了一个点。为了充分利用点注释，我们采用简单且非分层的视觉转换器来形成点到框转换器，称为 ViTWSS3D。通过对 LiDAR 点和相应弱标签之间的全局交互进行建模，我们的 ViT-WSS3D 可以生成高质量的伪边界框，然后将其用于训练任何 3D 检测器，而无需进行详尽的调整。对室内和室外数据集（SUN RGBD 和 KITTI）的大量实验证明了我们方法的有效性。特别是，当仅使用 10% 完全标记数据，其余作为点标记数据时，我们的 ViT-WSS3D 可以使大多数检测器使用 100% 完全标记数据实现与预言机模型类似的性能。</p><h4 id="引言-1"><a href="#引言-1" class="headerlink" title="引言"></a>引言</h4><p>3D 物体检测是计算机视觉的基本任务之一，在自动驾驶和导航等现实世界中具有广泛的应用。它的目的是回归给定场景的 3D 边界框和对象的相应类别标签。由于LiDAR传感器的固有限制，点云通常是无序且稀疏的，使得3D物体检测成为一项具有挑战性的任务。</p><p>为了准确定位目标，现有方法不可避免地需要对大规模数据进行精心注释，而标记 3D 边界框既繁琐又耗时。最近，一些方法[16,35,27,1]被提出来降低昂贵的标记成本。有两种典型的设置：半监督 3D 对象检测 (SS3D) [35, 47]，其中只有少量精确注释的场景可用；弱监督 3D 对象检测 (WS3D) [27, 37]，其中使用粗略注释（例如，标记对象的点）来训练 3D 检测器，而不是精确注释的 3D 边界框。</p><p><img src="/pic/weak3d29.png" alt="图 1.(a) 3D 对象检测的不同类型的监督。 (b) 各种注释格式。点级标注的成本明显低于框级标注。 (c) 使用完全监督和弱半监督设置的比较性能。"></p><p>虽然SS3D和WS3D方法可以有效降低标注成本，但它们仍然具有明显的局限性。一方面，他们的表现仍然远远不如完全监督的同行。具体来说，SS3D 方法 [35, 47] 通常在师生框架中将知识从标记数据迁移到未标记数据。然而，当标记数据和未标记数据之间的领域差距很大时（例如，标记数据和未标记数据分别属于晴天和雨天），知识转移可能无效。对于 WS3D 方法[27, 37]，弱标注提供的监督信息很难反映 3D 对象的特征（例如几何结构），导致性能较差。另一方面，当前的SS3D和WS3D方法通常是针对特定框架或场景（例如室内或室外）设计的，很难转换为其他框架或场景。例如，最初为 PVRCNN [29] 设计的代表性半监督方法 3DIoUMatch [35]，与 2% 全标签设置下的监督对应方法相比，可以带来 4.6% 的改进。然而，我们凭经验发现它在 PointRCNN [31] 中效果不佳，仅实现了 1.5% 的改进。</p><p>考虑到这些问题，通过通用范式以相当低的注释成本训练 3D 对象检测器，同时实现与完全监督的对应物相当的性能是值得探索的。为了实现这一目标，需要一种廉价但有效的注释格式。在各种弱格式（例如，点级[37]、场景级[27]）中，点级注释注释简单，易于存储和使用，并且具有本地化感知能力，可以提供更强的对象位置先验。 。根据[37]中的方法，一个框标注需要110秒1，而一个点标注只需要5秒，如图1（b）所示。</p><p>然而，仅采用点级注释是不够的。在检测性能和注释成本之间实现良好权衡的一种自然方法是结合少量完全注释的数据，我们将这种设置视为弱半监督范例。最近，一些方法[3,44,8]已经证明了弱半监督范式在二维目标检测中的潜力。这些方法帮助学生获得良好的成绩并节省大量的资源消耗。对于 3D 对象检测，毫无疑问，用 3D 点云中的点标签替换 3D 边界框是必要的，因为注释 3D 对象比 2D 对象更耗时、更费力。然而，如何将点弱半监督学习应用于3D场景，特别是点云，尚未被探索。</p><p>在本文中，我们的目标是探索带有点的弱半监督3D对象检测（WSS3D），如图1（a）所示。为了充分利用有限的框级注释和丰富的点，我们提出了一种简单而有效的 WSS3D Pipeline：</p><ul><li>1）使用少量完全标记的数据训练点到框转换器。 </li><li>2）经过训练的转换器将大量点注释转换为伪边界框。 </li><li>3）最后，在完全监督的环境中使用完全标记和伪标记场景训练任何 3D 对象检测器。</li></ul><p>这种Pipeline的核心是构建一个强大的点对盒转换器。最近，视觉转换器 [4, 38] 在特征交互方面表现出了巨大的潜力。受到直接将图像标记编码为对象检测序列的 YOLOS [7] 的启发，我们提出了一种简单的基于视觉转换器的 WSS3D 转换器，称为 ViTWSS3D。具体来说，ViT-WSS3D采用简单且非分层的ViT [5]从点云和点注释中提取特征。尽管设计简单，ViT-WSS3D 可以通过点注释生成高质量的伪框。</p><p>ViT-WSS3D的好处来自三个方面：</p><ul><li>1）由于视觉变压器强大的特征表示能力，我们的ViT-WSS3D可以非常简单，它具有简单且非分层的编码器结构，无需特定的领域知识进行设计。 </li><li>2）简单紧凑的ViT式架构使得可以轻松扩展模型并利用2D视觉进步中提出的预训练技术（例如MAE [11]）。 </li><li>3）我们的方法是开箱即用的，可以适用于任何 3D 物体检测器，无需进行彻底的调整和修改。</li></ul><p>为了证明我们方法的有效性，我们在室外 KITTI [9] 和室内 SUN RGB-D [32] 数据集上进行了广泛的实验。特别是，两个数据集上只有 10% 完全注释的场景，我们的 ViTWSS3D 可以帮助现有检测器与 100% 完全监督的对应物相比表现得更加接近。</p><h4 id="相关工作-1"><a href="#相关工作-1" class="headerlink" title="相关工作"></a>相关工作</h4><p><strong>完全监督的 3D 物体检测</strong></p><p>现有的3D物体检测器可以通过特征表示大致分为三个分支：基于体素&#x2F;柱的[49,39,15,41,14,10]，基于点的[31,25,40,45,6]和混合的风格 [29, 30]。</p><p>对于基于体素和柱的方法，VoxelNet [49]将点云划分为等距的 3D 体素，并使用卷积来提取特征。由于3D卷积的开销较高，SECOND [39]和PointPillars [15]分别引入稀疏卷积和支柱表示来提高速度。 CenterPoint [41] 将表示形式展平为俯视图视图，并使用基于图像的关键点检测器。为了更好地利用体素特征，提出了密度感知的 RoI 网格池 [14] 和基于体素的集合注意力 [10]。对于基于点的方法，PointRCNN [31] 在精炼之前通过分割生成提案，而 VoteNet [25] 通过深度霍夫投票处理点云的稀疏性质。为了减少下采样带来的信息损失，引入了各种采样方法[40, 45]，并且一些检测器[6]摆脱了下采样。对于混合式方法，PVRCNN [29, 30] 系列集成了 3D CNN 和基于点的集合抽象来学习更多判别性特征。</p><p>尽管这些方法取得了显着的性能，但它们的成功是建立在大规模、精心标记的数据集之上的，而满足这些要求是乏味且耗时的。</p><p><strong>半&#x2F;弱监督 3D 物体检测</strong></p><p>为了减轻标签的沉重负担，人们提出了两个方法分支：半监督[47,35,16,13]和弱监督[22,27,26,37]方法。</p><p><strong>半监督方法</strong>通常利用师生学习框架。具体来说，SESS [47]设计了彻底的扰动方案和一致性损失，以增强预测提案之间的一致性。 3DIoUMatch [35]引入了一种基于 3D IoU 的过滤机制来过滤噪声伪标签。 DDS3D [16]提出了一种动态阈值策略，用于选择高质量的伪标签。与传统的半监督环境不同，Liu 等人。 [19]提出了第一个探索 3D 对象检测任务的稀疏注释策略的工作，该策略只需要为每个场景注释一些实例。</p><p><strong>弱监督方法</strong>试图通过各种手段来恢复弱标签带来的信息丢失。具体来说，秦等人。 [26]提出了一种跨模式知识蒸馏策略来帮助学生预测结果。任等人。 [27]提出在训练时无法访问空间标签的情况下的自我和跨任务一致性损失。徐等人。 [37]利用合成的 3D 形状来补充和完善真实的标签。孟等人。 [22]建议在弱监督下生成圆柱形对象建议，并使用一些标记良好的实例对其进行细化。</p><p>尽管这些方法减轻了标签的沉重负担，但它们的性能不如完全监督的方法，而且它们通常是针对特定框架或场景（例如室外和室内）而设计的。与它们不同的是，我们的方法可以生成更精确的伪标签来有效地指导学生，而无需对学生和场景做出假设，这很容易迁移。</p><p><strong>Vision transformer</strong></p><p>Transformer [34] 在许多计算机视觉任务中占据主导地位 [2,48,38,20,17]，这归因于其强大的特征提取能力。 3D 点云是无序的数据和集合，这使得利用 Transformer [34] 处理点云变得可行。最近，许多基于 Transformer 的网络被提出用于 3D 对象分类 [46]、点云预训练 [42, 24] 和 3D 对象检测 [23, 21, 36, 43]。与它们不同的是，我们采用普通视觉变换器来解决弱半监督 3D 物体检测。</p><h4 id="方法-1"><a href="#方法-1" class="headerlink" title="方法"></a>方法</h4><p><strong>问题定义</strong>。在这项工作中，我们探索弱半监督 3D 物体检测，其中数据集由一小组完全标记的激光雷达场景 Φf &#x3D; {(Ii, phii f)}Nf i&#x3D;1 和大量弱注释（即点注释）激光雷达场景 Φp &#x3D; {(Ii, phii p)}Np i&#x3D;1。具体来说，Nf和Np表示全标记场景和点标记场景的数量。 Ii 表示全标记或点标记场景的点云。全标记场景的注释 phii f 表示 3D 边界框（中心、尺寸和方向）和相应的类标签，而 phii p 表示带有类标签的点注释（即 [px i , py i , pz i , ci]) 的点标记场景。请注意，对于点标记的激光雷达场景，由于原始数据集不提供点级注释，我们将随机干扰 R 添加到 3D 边界框的重力中以构造 phip。</p><p><strong>网络架构</strong></p><p>我们的方法的概述如图 2 所示。整个过程包含三个阶段：</p><ul><li>在少量完全注释的边界框 ψf 上训练点到框转换器作为教师模型。在这个阶段，我们使用具有随机扰动R的中心点作为模拟点注释，迫使转换器从噪声点中恢复框，如图2（a）所示。 </li><li>使用训练有素的教师从大量点注释 phip 中推理出伪 3D 边界框 phi′ p，如图 2 (b) 所示。请注意，此阶段没有可用的 3D 边界框，受过培训的教师必须重建完整的 3D 边界框，只能访问点注释。 </li><li>以完全监督的方式在完全注释的边界框 ψf 和伪框 ψ′ p 上训练任何学生检测器（图 2（c））。请注意，由于整个范式没有对学生做出任何假设，因此教师完全独立于学生，并且他们是单独训练的。</li></ul><p><img src="/pic/weak3d30.png" alt="图 2.我们方法的总体框架。 (a) 我们首先训练一个点到框转换器，它对点注释进行编码并进行全局交互。 (b) 然后，我们通过经过训练的转换器从弱点注释中推理出伪框。 (c) 最后，我们将完全标记的框和伪框结合起来，以完全监督的方式训练任何学生，无论学生的架构和表示类型如何。"></p><p>为了更好地利用点注释，我们方法的一个基本思想是直接利用前向传递中的点注释，并通过简单且非分层的视觉转换器与点云特征交互[5]。简单紧凑的转换器包含场景编码器、注释编码器、变压器编码器和简单的检测头。在下面的部分中，我们将详细介绍每个模块的设计以及我们方法的灵活性。</p><p><strong>点标记化</strong></p><p>标记化的目的是将给定场景的点云和点注释嵌入到有意义的标记序列中。输入是场景点 I ∈ RS×(3+C) 和注释点 phip ∈ RM×3，其中 I 通过场景编码器转换为场景标记 Zs ∈ RN×D，而 phip 嵌入到注释标记 Za ∈ RM× 中D 通过注释编码器，然后进行简单积分，形成 token 序列 Z0 ∈ R(N+M)×D。</p><p><strong>场景编码器</strong>用于将无序场景点 I 嵌入到名为场景标记 Z 的信息标记中，其中包含场景的综合特征。我们将场景点分组为 N 个局部补丁并将它们映射到特征空间。具体来说，我们首先使用最远点采样（FPS）算法从原始场景点I中选择N个关键点，然后使用kNN算法为每个关键点选择k个最近邻点以形成N个补丁。为了聚合局部信息，通过减去补丁的关键点以获得相对坐标来对每个局部补丁内的点进行归一化。我们最终使用 mini-PointNet 将无偏局部补丁映射到特征空间，获得场景标记 Zs ∈ RN×D。</p><p><strong>注释编码器</strong>的目的是将点注释 phip 编码为有用的标记，称为注释标记 Za，其携带点注释的重要信息。由于点注释包含有关对象位置的丰富先验信息，因此我们希望尽可能多地利用信息而不干扰它们。因此，我们不会对它们进行分组或使它们标准化。相反，我们利用朴素的 mini-PointNet 作为编码器将点注释嵌入到注释标记 Za ∈ RM×D 中。请注意，我们将点注释用零填充到相同的 M 以进行批处理，因为对象的数量因场景而异。</p><p>在场景和注释编码之后，我们需要将输出整合成有意义的标记序列，以供变压器的后续处理。</p><p><img src="/pic/weak3d31.png"></p><p>正如方程。如图 1 所示，我们首先堆叠注释标记和场景标记，然后通过在每个标记的中心点应用多层感知器（MLP）来获得位置嵌入 Ep，并将它们相加得到标记序列 Z0 ∈ R(N+M)×D 。这个简单的操作保留了场景和注释标记中的所有信息，这有助于特征提取。</p><p><strong>变压器编码器</strong></p><p>由于我们希望注释标记 Za 与场景标记 Zs 无障碍交互并平等对待它们，因此我们使用普通且非分层的 ViT [5] 从输入标记序列 Z0 中提取特征。每个 Transformer 编码器层包含多头注意力 (MSA)、MLP 和两层归一化 (LN)，并在 MSA 和 MLP 之后插入剩余旁路，正式写为：</p><p><img src="/pic/weak3d32.png"></p><p>其中 Zl 是第 l 个编码器层的输出标记。在编码器的最后一层之后，我们只输出最初来自注释标记的标记，其状态作为对象的特征表示，正式描述为：</p><p><img src="/pic/weak3d33.png"></p><p>其中 L 是 Transformer 编码器的深度，Zi L 是最后一个编码器层输出的第 i 个标记，Zd 是用于预测最终结果的检测标记。</p><p>普通和非分层变压器编码器平等地对待场景标记和注释标记，无需额外组件（例如交叉注意）即可实现它们之间的直接交互，并通过自交互辅助隐式上下文（例如场景布局）表示学习在每种类型的令牌中。此外，这样的设计使得模型更容易放大。人们可以通过简单地改变变压器编码器的深度或修改特征维度来调整模型的复杂性。此外，我们的方法还可以使用流行的 2D ViT 预训练范例来提高性能，而无需额外成本。简而言之，我们的模型可以随着 2D 视觉变换器的进步而发展。</p><p><strong>检测头</strong></p><p>由于我们简单的设计，检测头不需要复杂的网络架构和手工制作的标签分配。由于点注释和 Transformer 编码器带来的效率，在我们的方法中使用 MLP 作为检测头就足够了。更具体地说，我们将 3D 框分为三个部分：中心、尺寸和方向，并使用 Zd 上的 MLP 来预测每个部分。对于标签分配，由于 3D 对象之间几乎没有重叠，因此我们直接为与其中心对应的预测分配一个真实值，以摆脱二分匹配，这是优雅且实用的。我们采用广泛使用的用于回归的平滑 L1 损失和用于分类的焦点损失 [18] 来训练教师。</p><p><img src="/pic/weak3d34.png" alt="表1.模型架构的详细设置。"></p><h4 id="实验-1"><a href="#实验-1" class="headerlink" title="实验"></a>实验</h4><p><strong>KITTI</strong>是最流行的以自动驾驶相关任务为中心的户外数据集之一，包含7481个训练样本和7518个测试样本。我们将训练样本分为训练分割（3712 个样本）和验证分割（3769 个样本）。对于汽车、行人和骑车人类别，我们使用具有 40 个召回点的 mAP 和 3D IoU 阈值分别为 0.7(<a href="mailto:&#109;&#x41;&#80;&#52;&#48;&#x40;&#48;&#46;&#x37;">&#109;&#x41;&#80;&#52;&#48;&#x40;&#48;&#46;&#x37;</a>)、0.5(<a href="mailto:&#109;&#65;&#80;&#52;&#x30;&#x40;&#48;&#46;&#x35;">&#109;&#65;&#80;&#52;&#x30;&#x40;&#48;&#46;&#x35;</a>) 和 0.5(<a href="mailto:&#x6d;&#65;&#x50;&#52;&#48;&#64;&#48;&#46;&#53;">&#x6d;&#65;&#x50;&#52;&#48;&#64;&#48;&#46;&#53;</a>)。 </p><p><strong>SUN RGB-D</strong>是一个室内数据集，旨在推进所有主要场景理解任务的最新技术，该数据集由四个不同的传感器捕获，包含 10,335 个 RGB-D 图像。整个数据集分为 5285 个用于训练的样本和 5050 个用于验证的样本。我们使用 3D IoU 阈值 0.25 (<a href="mailto:&#x6d;&#65;&#x50;&#64;&#x30;&#x2e;&#50;&#53;">&#x6d;&#65;&#x50;&#64;&#x30;&#x2e;&#50;&#53;</a>) 的 mAP 作为 3D 检测器的指标。</p><p>我们采用ViT-Small（ViT-S）和ViT-Base（ViT-B [5]）作为默认的transformer编码器，模型架构的详细设置如表1所示。 1. 我们的方法中 ViT-S 和 ViT-B 之间存在细微差别。我们只需要更改参数即可获得更强大的模型。这种便利得益于我们简单的设计。</p><p>当完全标记的场景很少时（KITTI 为 2%、5%，SUN RGB-D 为 5%），我们使用 ViT-S，否则使用 ViT-B，因为更大的 Transformer 编码器更强大，更容易过度拟合。对于场景编码器，我们默认设置最近邻居的数量 k &#x3D; 32 和场景标记的数量 N &#x3D; 2048。对于注释编码器，我们为 KITTI 设置 M &#x3D; 100，为 SUN RGB-D 设置 M &#x3D; 300。我们分别为 KITTI 和 SUN RGB-D 设置随机扰动 R &#x3D; 0.1 和 R &#x3D; 0.0。在训练教师时，我们在 KITTI 上使用水平随机翻转、全局旋转尺度变换、点洗牌和 GT-Sampling [39] 数据增强。请注意，我们为每个训练集重新生成 GT-Sampling 数据库，以避免标记数据泄漏风险。在 SUN RGB-D 上，我们仅使用水平随机翻转和全局旋转比例变换增强。我们使用 2 个 NVIDIA GeForce 3090 GPU 来培训教师，在 KITTI 上需要大约 8 小时，在 SUN RGB-D 上需要 4 小时来培训教师。</p><p>为了执行半弱监督学习，我们首先使用固定步长从原始数据集中均匀采样全标记场景，然后准备弱注释场景</p><p><strong>定量结果</strong></p><p><strong>KITTI</strong>。为了衡量我们方法的有效性，我们选择了三个完全标记场景的比例水平（10％、5％和2％）来评估我们的方法给学生带来的表现增益。我们在 KITTI 上选择了四种典型的 3D 检测器，它们利用不同的表示形式：基于柱的 PointPillars [15]、基于体素的 SECOND [39]、基于点的 PointRCNN [31] 和混合式 PVRCNN [29]。</p><p>如表2所示 、与仅在少量完全标记场景上进行训练的检测器竞争，我们的方法生成的伪标签显着提高了学生的表现。例如，在 2% 的完整数据设置和中等难度下，我们的方法帮助 PointPillars、SECOND、PointRCNN 和 PVRCNN 的整体性能分别获得了 23.9%、6.0%、7.6% 和 8.1% 的 mAP 改进。此外，我们的方法极大地缩小了学生与其 100% 完整数据对应者之间的差距。请注意，在 90% 弱设置下，低于 10% 完整的学生可以达到与 100% 完整基线相当的性能，这证明了我们方法的伪标签的卓越质量。</p><p><img src="/pic/weak3d35.png" alt="表 2. 有伪标签和无伪标签的学生在不同数据设置下在 KITTI val split 上的比较结果。我们分别报告汽车、行人 (Ped.) 和骑自行车者 (Cyc.) 类别的 mAP40@0.7、mAP40@0.5 和 mAP40@0.5。"></p><p>对于每个类别，我们发现行人和骑行者对标记数据的比例比汽车更敏感，并且我们的方法在这两个类别上效果更好。在行人和骑自行车者方面，我们的方法使所有四名学生受益匪浅。请注意，在 10% 完整数据设置下，我们的方法可以帮助所有这些检测器实现与在行人 100% 完全标记数据上训练的基线相当甚至更好的性能。即使在灵敏度较低的汽车上，我们的方法仍然可以将所有这些检测器的性能提升显着，特别是对于整洁的 PointPillars。</p><p><img src="/pic/weak3d36.png" alt="表 3. 3DIoUMatch 和我们的方法在 KITTI val split 上的比较结果。我们以PVRCNN为学生，在中等难度下报告mAP40。"></p><p><strong>定性结果</strong></p><p>我们将不同方法的伪标签可视化以进行直观比较，如图 3 所示。在户外 KITTI 上，PVRCNN† 输出许多误报（用气泡圈出），影响 3DIoUMatch 的伪标签质量，因为后者是基于 PVRCNN 构建的。它在室内 SUN RGB-D 上显示了类似的问题，其中 VoteNet† 遗漏了一些对象，使得 3DIoUMatch 的伪标签与 GT 显着不同（用箭头标记）。可视化展示了我们的方法的伪标签的更好质量。</p><p><img src="/pic/weak3d37.png" alt="图 3.不同方法的伪标签的可视化。 (a) KITTI 的 2% 完整数据。 (b) SUN RGB-D 的 5% 完整数据。 † 表示在完全标记的数据上训练检测器，然后使用它们来推断伪框。定性结果表明我们的方法的伪标签质量更好。"></p><h3 id="第五篇：VG-W3D"><a href="#第五篇：VG-W3D" class="headerlink" title="第五篇：VG-W3D"></a>第五篇：VG-W3D</h3><p>Weakly Supervised 3D Object Detection via Multi-Level Visual Guidance(2023)</p><p><a href="https://arxiv.org/abs/2312.07530">论文链接</a></p><p><a href="https://github.com/kuanchihhuang/VG-W3D">代码暂未开源</a></p><h4 id="摘要-2"><a href="#摘要-2" class="headerlink" title="摘要"></a>摘要</h4><p>弱监督 3D 对象检测旨在学习具有较低注释成本的 3D 检测器，例如 2D 标签。与仍然依赖于少量准确 3D 注释的先前工作不同，我们提出了一个框架来研究如何在不需要任何 3D 标签的情况下利用 2D 和 3D 域之间的约束。具体来说，我们从三个角度利用视觉数据来建立 2D 和 3D 域之间的联系。首先，我们设计一个特征级约束，根据对象感知区域来对齐 LiDAR 和图像特征。其次，开发输出级约束以强制 2D 和投影 3D 框估计之间的重叠。最后，通过生成与视觉数据对齐的准确且一致的 3D 伪标签来利用训练级别约束。我们在 KITTI 数据集上进行了广泛的实验，以验证所提出的三个约束的有效性。在不使用任何 3D 标签的情况下，我们的方法比最先进的方法取得了良好的性能，并且与使用 500 帧 3D 注释的方法具有竞争力。</p><h4 id="引言-2"><a href="#引言-2" class="headerlink" title="引言"></a>引言</h4><p>自主系统的一个关键特征是能够在周围环境中准确感知和定位 3D 对象 [4, 8–10, 26]，使代理（例如车辆）能够做出明智的决策并安全导航。然而，由于其成本和复杂性，获取带注释的 3D 标签来训练 3D 对象检测模型会带来挑战，特别是与在视觉数据上标记 2D 框的速度相比（例如，慢 3-16 倍，如 [30] 所示） 。因此，3D 对象检测的弱监督学习已成为解决注释瓶颈的实用方法。</p><p>最近，人们提出了多种方法来训练 3D 对象检测器并减少注释要求。 FGR [30]提出以非学习的方式从具有平截头体几何关系的相应 2D 框生成 3D 候选框。 WS3D [20, 21] 将 BEV 中对象的中心注释为弱标签，并使用一些 3D 标签进行具有圆柱约束的模型训练。几种方法 [15,16] 结合图像和 LiDAR 信息来共同学习探测器。然而，现有方法要么仍然需要精确的 3D 框标签 [15,16,20,21]，要么由于有限的学习监督而只能实现次优性能 [30]。值得注意的是，这些方法都没有探索在训练期间整合 2D 和 3D 各种视觉引导的潜力。</p><p>在这项工作中，我们探索将视觉数据集成到 3D 物体检测器的训练过程中，仅利用 2D 注释进行弱监督 3D 物体检测，这与上述方法相比是独特的（比较见表 1）。如图1所示，我们从三个角度研究视觉引导的学习过程：特征层面的客观性学习、输出层面的响应学习和训练层面的伪标签学习，并有以下三个观察结果。</p><p><img src="/pic/weak3d48.png" alt="表 1：VG-W3D 与相关工作的比较。我们的方法以多种视觉线索为指导，包括 2D 框输出、图像特征和训练分数，无需 3D 注释。"></p><p>观察 1：特征级指导。对于校准良好的图像和激光雷达，从图像获得的物体预测应与激光雷达数据中的相应区域对齐。例如，当 3D 检测器将点云识别为前景对象时，其在图像平面上的相应投影像素应与 2D 检测器做出的类似预测对齐，反之亦然。因此，我们在特征层面上利用这个想法，将图像特征映射到点特征并仅考虑物体部分来增强 3D 物体检测器的特征学习过程。</p><p>观察 2：输出水平指导。我们注意到图像平面上的 2D 和投影 3D 边界框之间存在大量重叠。基于这一见解，我们建立了独特的 2D-3D 约束来指导 3D 提案的监督。通过此约束，模型可确保估计的 3D 框准确定位在对象图像区域的截锥体内，以生成更高质量的建议。</p><p><img src="/pic/weak3d47.png" alt="图 1：弱监督 3D 物体检测的多级视觉引导。我们提出了一个框架，使用三个不同的视角，包括特征、输出和训练级别约束，从弱标签（例如图像平面上的 2D 边界框）学习 3D 对象检测器。特征级为点特征学习提供对象感知信号。输出级结合 2D-3D 框约束来强制模型生成合理的框预测。训练级指导将 2D 框的置信度融入伪标签技术中，以确保 2D 和 3D 域之间的分数一致性。"></p><p>观察 3：训练级别指导。我们发现，由于点云数据的稀疏性，通过非学习启发式 [30] 使用初始 3D 标签可能会产生噪声并且部分丢失对象。因此，迭代地完善这些标签以获得更高的准确性至关重要。另一个挑战是减少生成的伪标签的误报，因为在自训练过程中，模型可能很容易产生具有高置信度分数的意外估计。因此，我们提出了一种解决方案，将视觉域中的 2D 框的预测分数集成到伪标签技术中，以确保 2D 和 3D 域内任何对象的分数一致性。</p><p>基于这些观察，我们提出了一种用于弱监督 3D 对象检测的多级视觉引导方法，名为 VG-W3D。我们利用视觉线索仅使用 2D 注释从三个角度训练强大的 3D 对象检测器：特征、输出和训练级别。我们的方法可以进一步与从不同领域训练的现成图像对象检测器获得的 2D 注释集成，使我们的方法适用于更具可扩展性和成本效益的 3D 对象检测。</p><p>综合实验结果验证了所提出方法的有效性。具体来说，与具有相似注释成本的方法相比，我们的方法在 KITTI [6] 数据集上的 AP3D 中表现出至少 5.8% 的实质性改进。此外，我们的方法展示了与需要 500 帧 3D 注释的最先进的弱监督 3D 检测方法相当的性能。</p><h4 id="相关工作-2"><a href="#相关工作-2" class="headerlink" title="相关工作"></a>相关工作</h4><p><strong>基于 LiDAR 的 3D 物体检测</strong>。基于 LiDAR 的 3D 物体检测器 [7,12,26] 最近引起了广泛关注。根据不同的点聚合技术，它们可以大致分为体素方法和基于点的方法。基于体素的方法 [5,19,31,33] 将点云体素化为体素网格表示，然后进行 2D 和 3D 卷积运算以进行对象检测。在[33]中，VoxelNet 对原始点云中的体素特征进行编码，然后使用密集区域提议网络进行 3D 对象检测。 CIA-SSD [31] 利用鸟瞰 (BEV) 网格上的轻量级网络来学习空间和语义特征，并结合 IoU 感知置信度细化模块来实现稳健检测。此外，Voxel-RCNN [19] 应用体素感兴趣区域（RoI）池化来提取提议中的体素特征以进行后续细化，而 VoxelNeXt [4] 引入了完全稀疏的架构，可以直接根据稀疏体素特征而不是密集表示来预测对象。</p><p>许多方法[11、18、26、27]直接利用原始点云作为输入来提取点级特征。 PointRCNN [26]提出了一个两阶段框架，该框架对前景点进行分割，生成对象建议，然后细化规范坐标。另一方面，3DSSD[18]引入了使用特征距离的融合采样技术来确保全面的信息保存。在[27]中，Point-GNN 采用图神经网络来创建点云数据的更简洁的表示。最近，Pointformer [23] 使用了一个直接在点云上运行的用于局部和全局注意力的转换器模块。</p><p>虽然这些现有的 3D 对象检测器已表现出理想的性能，但它们依赖 3D 注释进行监督学习，效率较低。相比之下，我们的方法强调开发强大的 3D 探测器，而不会产生劳动密集型成本。</p><p><strong>使用弱标签的 3D 对象检测</strong>。在点云上注释 3D 边界框非常耗时。因此，几种方法专注于如何以较低的注释成本训练 3D 检测器，以减少劳动密集型工作 [15,16,20,21,24,30]。 VS3D [49] 通过以无监督的方式分析点云密度来生成 3D 提案。然后，框内的点被投影到图像平面上，由使用类标签训练的 2D 模型引导，以确定提案是否包含对象。 WS3D [20,21] 利用中心点击来注释 BEV 中对象的粗略位置，以生成初始的圆柱形对象建议，然后在第二阶段进行细化过程，几乎没有精确的 3D 标签。在[30]中，FGR引入了一种非学习技术，利用2D边界框来识别平截头体子点云，然后使用启发式方法基于分割的点云计算最精确的3D边界框。 MAP-Gen [16] 和 MTrans [15] 利用丰富的图像数据来解决 3D 点云固有的稀疏性挑战。然而，上述大多数方法仍然需要部分质心或准确的 3D 注释来开发其方法。在这项工作中，我们提出了一种不需要任何 3D 注释的方法，同时仍能实现与现有弱监督方法相比的竞争性能。</p><h4 id="方法-2"><a href="#方法-2" class="headerlink" title="方法"></a>方法</h4><p><strong>框架概述</strong></p><p>给定 LiDAR 点云 P 及其相应的相机图像 I，我们的目标是开发一个 3D 对象检测器，而不使用任何 3D 对象注释。在本文中，我们介绍了 VG-W3D，一种用于弱监督 3D 对象检测的多级视觉引导方法，旨在使用密集视觉信号和来自图像域的带注释的 2D 框来学习精确的 3D 边界框。</p><p><img src="/pic/weak3d49.png" alt="图 2：所提出的 VG-W3D 的总体框架。我们利用非学习方法 [30] 来识别对象的视锥体点云，然后采用启发式算法来估计初始噪声边界框（右上）。在图像分支中，我们基于 2D 注释训练对象检测器来预测图像特征 FI 和 2D 边界框 BI 及其置信度 σI，作为训练 3D 检测器的视觉指导。然后，采用基于 PointNet 的 3D 对象检测器来提取点特征 FP 并输出 3D 边界框 BP 以及置信度分数 σP。我们的方法结合了 3D 训练的三个级别的视觉指导，即特征级别（第 3.2 节）、输出级别（第 3.3 节）和训练级别（第 3.4 节）。请注意，图像分支在训练阶段被冻结，并在推理阶段被丢弃。"></p><p>如图 2 所示，VG-W3D 利用特征级、输出级和训练级线索来指导 3D 对象检测。为了获得用于训练的初始 3D 框，我们采用类似于 FGR [30] 的非学习方法来识别对象的截锥体点云，然后采用启发式算法来估计临时 3D 标签（更多详细信息请参阅 [30]）。</p><p>在初始阶段，我们使用提供的 2D 框注释 ^BI 训练 2D 检测器 [32]，以提取视觉特征 FI 并预测 2D 边界框 BI 及其相应的置信度分数 σI。随后，我们采用基于 3D PointNet 的检测器 [26] 提取点云特征 FP 并生成 3D 边界框 BP 及其检测分数 σP。</p><p>下面，我们介绍三个 2D 视觉线索来指导 3D 学习。首先，特征级线索限制了对图像和点云中前景物体的注意力，确保两种模式之间的预测一致（第 3.2 节）。其次，输出级指导通过在图像平面上强制投影 3D 框和 2D 框之间的大量重叠来确保预测 3D 提案的逻辑定位（第 3.3 节）。最后，为了生成用于重新训练 3D 对象检测器的高质量伪标签，我们将图像框和分数合并到自训练技术中，确保伪标签的改进（第 3.4 节）。</p><p><strong>特征级视觉指导</strong></p><p>对于弱监督 3D 物体检测，各个点通常缺乏明确的指导。即使初始 3D 标签可用 [30]，学习良好的 3D 表示仍然具有挑战性，因为这些 3D 标签有时不完整且有噪声。因此，我们利用密集的视觉提示来指导已知像素点映射的特征学习。</p><p><img src="/pic/weak3d50.png" alt="图 3：特征级视觉引导。一旦我们获得了投影点特征 FP&#39; 和图像特征 FI，我们就利用对象前景图 S 来监督目标，其中应用预训练的无监督实例分割模块 MS 来提取每个带注释的 2D 边界框的对象前景图以生成S. 此外，应用图像引导的 KL 散度损失来从图像特征中学习分布。"></p><p><strong>特征映射</strong>。如图 3 所示，我们考虑图像特征 FI ∈ RH×W×C（H × W 和 C 表示特征大小和特征通道数）和点云特征 FP ∈ RP×C，其中 P 为特征通道数。点。最初，我们使用相机标定参数将点云特征投影到图像平面上，产生投影点特征：FP′ &#x3D; Proj(FP) ε RH×W×C。因此，我们获得了与图像平面上的像素点匹配的投影点云特征，以实现更好的特征学习。功能指导。一种简单的方法是强制点云特征来模仿图像表示 [13]，从具有 L2 损失的 LiDAR 模态中学习图像模态的特征：</p><p><img src="/pic/weak3d51.png"></p><p>其中A是图像上与投影点匹配的有效像素区域，∥·∥2是L2范数距离。然而，这可能会损害点云特征学习的过程，因为图像特征不能提供比点云更多的几何信息。相反，我们建议强制执行图像和点云特征 FI 和 FP’ 的预测对象概率，以确保 3D 检测器可以通过特征级指导识别前景点。</p><p>我们使用分割图进行对象性监督，以仅允许在对象区域中进行指导。在不产生额外注释成本的情况下，我们利用自监督分割方法 [22, 29] 来生成没有注释的前景图。具体来说，在每个真实的 2D 边界框中，对于 DINO [2] 获得的每个对象，都会提取对象及其前景图。然后将这些单独的图合并以形成分割地面实况图 S ∈ RH×W。然后，我们利用分类器 MP′ 将点云特征映射到预测投影点云的对象性的二元概率： CP′ &#x3D; MP′ (FP′ ) 通过焦点损失 FL：</p><p><img src="/pic/weak3d52.png"></p><p>另一方面，对于图像域，我们在带有线性分类器 MI 的 2D 图像检测器上添加另一个分支，以学习图像上像素的对象性：CI &#x3D;MI(FI)，使用与 (2 ）：</p><p><img src="/pic/weak3d53.png"></p><p>为此，我们可以利用 KL 散度损失来增强点云模态的客观性，并学习与图像模态相似的分布，而不会丢失点云的几何信息：</p><p><img src="/pic/weak3d54.png"></p><p><strong>输出级视觉引导</strong></p><p>值得一提的是，使用 2D 和 3D 边界框检测到的任何对象都应该表现出高度重叠，如图 4 所示。这意味着在 3D 边界框未知的弱监督学习场景中，我们可以利用地面实况2D 框用于监督 3D 检测器预测的 3D 框。</p><p>首先，给定预测的 3D 边界框 BP，我们获得其 3D 坐标中的八个角，表示为 C3(BP) ∈ R8×3。然后，我们利用已知的相机标定参数获得二维投影角点C ∈ R8×2。接下来，C的边界框可以通过以下方式确定：</p><p><img src="/pic/weak3d55.png"></p><p>其中 (xa, ya) 和 (xb, yb) 是框 Bproj 的左上角和右下角坐标。因此，我们利用相应的2D框预测BI来约束这两个框之间的差异：</p><p><img src="/pic/weak3d56.png"></p><p>其中 GIoU 是联合 [25] 上的广义交集，用于指导框学习。与正常的 IoU 损失相比，GIoU 损失可以更好地缓解投影 3D 框和地面真实 2D 框之间非重叠情况 [25] 的梯度消失问题。</p><p>此外，^σI &#x3D; σI&#x2F; PN i σIi 是同一场景中所有 N 个对象的每个预测 2D 框的归一化分数。我们观察到，2D 检测器置信度较低的对象表明预测框的不确定性，这可能不是高质量的框。因此，不宜对每个盒子一视同仁。因此，我们引入预测分数 ^σI 作为（6）中每个框的损失权重。为此，所提出的输出级指导可以确保投影边界框与其 2D 对应物之间的精确对齐。</p><p><strong>训练级视觉引导</strong></p><p>提供直接监督信号的一种常见方法是使用伪标签。我们最初考虑其他方法，如 [30]，无需学习即可生成 3D 伪标签。然而，它们容易受到噪音影响，并且可能会错过许多物体。例如，[30]生成的训练数据集（3712帧）中只有大约2700帧包含伪标签。此外，伪标签可能会引入额外的误报，从而对自我训练产生负面影响。为了解决这些问题，我们引入了一种图像引导方法来生成高质量的伪标签，如算法 1 中所述。该方法从 [30] 中的初始 3D 伪标签 Bˆ 0、2D 框注释 BI 和预训练的2D 物体探测器 θI。我们利用 2D 检测器来预测每个 2D 注释的置信度分数 σI。在实践中，我们从图像检测器预测的热图中提取每个地面真实对象的中心索引的置信度。</p><p><img src="/pic/weak3d57.png"></p><p>每次迭代都包含三个主要步骤。首先，我们在第 t 轮基于伪标签 ^Bt 训练 3D 对象检测器，并分别使用第 3.2 节和第 3.3 节中提到的特征和输出级别指导。其次，我们为下一轮生成初始伪标签 ^Bt+1 以及相应的置信度 σP。最后，我们根据以下标准过滤伪标签以确保质量：1）利用匈牙利算法，我们匹配 2D 和投影 3D 边界框，保留 IoU 分数大于阈值 α0 的那些。此外，2D 和 3D 框的平均置信度得分应大于 α1。生成的伪标签表示为 Boverlap。 2）对于剩余的投影3D框Bunmatch &#x3D; Bt+1\Boverlap，我们应用非极大值抑制（NMS）来消除冗余框，仅保留那些具有高置信度α2的框，称为Bscore。最终选择的3D伪标签是两个集合：^Bt+1 &#x3D; Boverlap +Bscore。</p><blockquote><p>具体实验结果看原论文结果</p></blockquote><h3 id="第六篇：MIXSUP"><a href="#第六篇：MIXSUP" class="headerlink" title="第六篇：MIXSUP"></a>第六篇：MIXSUP</h3><p>MIXED-GRAINED SUPERVISION FOR LABELEFFICIENT LIDAR-BASED 3D OBJECT DETECTION(MIXSUP：基于标签高效激光雷达的 3D 物体检测的混合粒度监督2024)</p><p><a href="https://arxiv.org/abs/2401.16305">论文链接</a><br><a href="https://github.com/BraveGroup/PointSAM-for-MixSup">代码链接</a></p><h4 id="摘要-3"><a href="#摘要-3" class="headerlink" title="摘要"></a>摘要</h4><p>基于标签高效 LiDAR 的 3D 物体检测目前以弱&#x2F;半监督方法为主。我们提出了 MixSup，而不是仅仅遵循其中一个，这是一种更实用的范例，同时利用大量廉价的粗标签和有限数量的精确标签进行混合粒度监督。我们首先观察到点云通常是无纹理的，这使得学习语义变得困难。然而，点云具有丰富的几何形状，并且对于距传感器的距离具有尺度不变性，使得学习物体的几何形状（例如姿势和形状）相对容易。因此，MixSup 利用大量粗略的集群级标签来学习语义，并利用一些昂贵的框级标签来学习准确的姿势和形状。我们重新设计了主流检测器中的标签分配，使它们能够无缝集成到 MixSup 中，从而实现实用性和通用性。我们使用各种检测器在 nuScenes、Waymo 开放数据集和 KITTI 中验证其有效性。 MixSup 使用廉价的集群注释和仅 10% 的框注释，实现了高达 97.31% 的完全监督性能。此外，我们提出基于Segment Anything Model的PointSAM用于自动粗标记，进一步减轻注释负担。</p><h4 id="引言-3"><a href="#引言-3" class="headerlink" title="引言"></a>引言</h4><p>基于激光雷达的3D感知是自动驾驶不可或缺的功能。然而，费力的标签程序阻碍了其在学术界和工业界的发展。因此，针对基于激光雷达的3D目标检测出现了许多标签高效的学习方法，例如半监督学习（Zhao et al., 2020; Wang et al., 2021; Yin et al., 2022a; Liu et al., 2022a）。 ，2023a）和弱监督学习（Qin 等，2020；Meng 等，2020；2021；Zhang 等，2023b；Xia 等，2023）。</p><p>在本文中，我们提出了一种更实用的标签高效学习范例，用于基于 LiDAR 的 3D 物体检测。特别是，我们利用大量廉价的粗标签和有限数量的精确标签进行混合粒度监督（MixSup），而不是完全遵循以前的标签高效学习范例之一。 MixSup 源于我们对点云的以下观察。 </p><ul><li>(1) 纹理缺失：3D点云缺乏独特的纹理和外观。 </li><li>(2) 尺度不变性：3D物理世界中的点云对于距传感器的距离是尺度不变的，因为没有像2D成像那样的透视投影。 </li><li>(3) 几何丰富性：3D点云由原始欧氏坐标组成，自然包含丰富的几何信息。</li></ul><p>我们在图 1 中总结了这些不同的属性。这些属性是双向的。一方面，纹理和外观的缺乏使得学习点云的类别和识别物体所在的大致区域（统称为语义）变得具有挑战性。另一方面，尺度不变性和几何丰富性可能使估计对象的几何属性相对容易，例如准确的姿势和形状。</p><p><img src="/pic/weak3d58.png" alt="图 1：点云与图像相比的不同属性的图示。它们使得从点进行语义学习变得困难，但简化了几何估计，这是 MixSup 的最初动机。"></p><p>因此，我们得出了 MixSup 的动机：一个好的检测器需要大量的语义标签来进行困难的语义学习，但只需要一些准确的标签来进行几何估计。幸运的是，对象语义标签可以很粗糙，并且比几何标签便宜得多，因为前者不需要准确的姿势和形状。因此，我们特别选择语义点簇作为粗标签，并提出 MixSup，旨在同时利用廉价的簇级标签和准确的框级标签。从技术上讲，我们重新设计了流行检测器中基于中心和基于框的分配，以确保与集群级标签的兼容性。这样，几乎任何检测器都可以集成到 MixSup 中。为了进一步降低注释成本，我们利用新兴的分段任意模型（Kirillov et al., 2023）并提出用于粗聚类标签生成的 PointSAM，享受图像识别进步带来的“免费赠品”。我们的贡献如下：</p><ul><li>1.基于对点云属性的观察，我们提出并验证了一个发现，即一个好的检测器需要大量粗略的语义标签来进行困难的语义学习，但只需要一些精确的几何标签来进行几何估计。 </li><li>2.我们建议采用语义点簇作为粗标签，并构建一个实用且通用的范式 MixSup，以利用大量廉价的簇标签和一些准确的框标签来进行标签高效的基于 LiDAR 的 3D 对象检测。 </li><li>3.我们利用Segment Anything Model，开发PointSAM进行实例分割，实现自动化粗标注，进一步降低集群标签的成本。 </li><li>4.在三个基准测试和各种检测器上进行的大量实验表明，MixSup 在具有 10% 框注释和廉价集群注释的情况下，可达到完全监督对应物的 97.31% 性能。</li></ul><h4 id="相关工作-3"><a href="#相关工作-3" class="headerlink" title="相关工作"></a>相关工作</h4><p><strong>基于LiDAR的3D物体检测</strong>：主流的基于LiDAR的3D检测大致可以分为基于点的方法和基于体素的方法。基于点的检测器(Shi et al., 2019; Yang et al., 2020; Shi et al., 2020b; Li et al., 2021)通常采用PointNet系列(Qi et al., 2017a;b)作为点特征提取器，遵循不同的架构来预测 3D 边界框。基于体素的方法 (Zhou &amp; Tuzel, 2018; Yan et al., 2018; Yin et al., 2021; Fan et al., 2022a;b; Chen et al., 2023b; Wang et al., 2023a;b; Liu 等人，2023d）将原始点变换为 3D 体素，这有利于 3D 稀疏卷积或变换器机制。此外，还采用混合方法（Yang et al., 2020; Shi et al., 2020a; 2023）来利用双方的利益。</p><p><strong>3D 中的半监督学习</strong>：半监督学习旨在通过使用少量标记数据和大量未标记数据训练模型来减轻注释负担。受 2D 领域成就的启发，半监督学习已扩展到 3D 领域。 SESS（Zhao et al., 2020）继承了Mean Teacher（Tarvainen &amp; Valpola, 2017）范式，并鼓励教师模型和学生模型之间达成共识。 3DIoUMatch（Wang et al., 2021）专注于通过一系列手工设计来提高伪标签的质量。与 3DIoUMatch 不同，Proficient Teacher (Yin et al., 2022a) 利用时空集成模块和基于聚类的框投票模块来增强教师模型并获得准确的伪标签，从而消除了故意选择的阈值。考虑到师生框架中的增强能力较弱，HSSDA（Liu et al., 2023a）提出了shuffle数据增强来加强学生模型的训练。</p><p><strong>弱监督学习</strong>：弱监督学习采用廉价的弱标签来减轻注释成本的负担。特别是对于室外场景，出现的方法主要利用弱注释，包括点击级（Meng et al., 2020; 2021; Liu et al., 2022; 2023b; Zhuang et al., 2023b）、涂鸦级（Unal et al., 2023b） ., 2022）和图像级（Qin et al., 2020）。尽管这些工作取得了可喜的性能，但它们不可避免地涉及复杂的训练制度或复杂的网络架构。在本文中，我们发现利用一些准确的标签可以估计良好的几何形状。因此，引入一些准确的标签而不是遵循纯粹的弱监督设置可能更实际。</p><h4 id="试点研究：对于标签效率真正重要的是什么"><a href="#试点研究：对于标签效率真正重要的是什么" class="headerlink" title="试点研究：对于标签效率真正重要的是什么"></a>试点研究：对于标签效率真正重要的是什么</h4><p>我们认为一个好的检测器需要大量的粗标签来进行语义学习，但只需要一些准确的标签来进行几何估计。在这里，我们进行了一项试点研究来证实我们的主张的有效性。</p><p>我们利用预先训练的检测器（Fan 等人，2022b）的预测来裁剪点云区域。因此，这些区域被很好地分类，我们只需要关注裁剪区域中对象的几何估计。在裁剪之前，我们在提案中引入强噪声，以避免几何信息泄漏。特别是，我们将提案在所有三个维度上扩展了 2 米，随机移动它们 0.2 ∼ 0.5 米，并将它们旋转 −45° ∼ 45°。通过这种方式，我们构建了一个分类良好的数据集，其中包含裁剪后的噪声区域。最后，我们使用分类良好的数据集的不同部分训练基于稀疏卷积的检测器。试点研究的示意图如图 2 所示。</p><p><img src="/pic/weak3d59.png" alt="图 2：试点研究说明。我们开发了一个分类良好的数据集来分解分类，并且只关注不同数据量对几何估计的影响。"></p><p>表 1 的结果表明，数据量从 5% 到 100% 的性能非常相似。这种现象表明，基于激光雷达的探测器确实只需要非常有限数量的精确标签来进行几何估计。此外，我们在附录 A.2 中探讨了不同数据量对 3D 检测器语义学习的影响，支持我们的主张，即海量数据只是语义学习所必需的。幸运的是，语义注释相对便宜并且不需要精确的几何形状。因此，在本文的其余部分，我们深入研究利用大量廉价的粗标签进行语义学习和利用有限的精确标签进行几何估计。</p><p><img src="/pic/weak3d60.png" alt="表 1：在分类良好的数据集上不同数据量的性能。"></p><h4 id="方法-3"><a href="#方法-3" class="headerlink" title="方法"></a>方法</h4><p>在本节中，我们首先提出利用簇级标签并将它们与先前的粗略中心级标签进行比较（第 4.1 节），以及如何将粗略标签集成到 MixSup 中以供一般使用（第 4.2 节）。然后，我们详细介绍如何使用 PointSAM 获取粗标签以进一步释放注释负担（第 4.3 节）。</p><p><img src="/pic/weak3d61.png" alt="图 3：MixSup 概述。大量的簇级标签用于语义学习，一些框标签用于学习几何属性。我们重新设计标签分配，将各种检测器集成到 MixSup 中。"></p><p><strong>簇级粗标签</strong></p><p>获得精确的 3D 边界框是一项艰巨且耗时的任务，需要进行细致的微调才能满足高精度的需求。一系列工作旨在获取更便宜的粗标签，例如中心级标签（Meng 等人，2020；2021）。他们单击鸟瞰视图上对象的中心以获得中心级别的标签。虽然简单，但单个中心点提供的有关物体的信息非常有限，并且不方便采用各种类型的检测器。此外，对于注释者来说，进行准确的中心单击也很重要。</p><p>从今以后，我们引入集群作为更好的粗标签。聚类标签的获取非常简单。基本上，注释者可以遵循以下协议：在鸟瞰视图中围绕对象进行三次粗略单击。然后这三个点击点形成一个平行四边形，作为平行四边形的三个角。平行四边形内的点形成一个粗簇。我们强调集群的标记非常有效，因为它只需要在对象角周围进行三次粗略单击，而不是在精确的对象中心进行精确单击。在秒。 5.3，我们凭经验发现集群的平均标记成本仅为准确框的 14% 左右。我们在附录 D.1 中提供了标签协议的简单说明。</p><p><strong>粗略标签分配</strong></p><p>在本小节中，我们将演示如何将粗集群级标签和框标签集成到不同类型的检测器中以进行混合粒度监督，如图 3 所示。检测器中与标签最相关的部分是标签分配模块，负责为检测器正确分配标签以提供分类和回归监督。因此，MixSup只需要重新设计簇级标签的标签分配即可保证通用性。我们将这些分配分为两种类型：<strong>基于中心的分配和基于框的分配</strong>。</p><p><strong>基于中心的分配</strong>和不一致消除 基于中心的分配在众多探测器中广泛采用。对于它们，我们用簇中心 ¯c 替换原始对象中心，簇中心在等式 1 中定义。 1. 替换不可避免地导致真实对象中心（精确框的）与聚类中心不一致。为了解决不一致的问题，对于框标签，我们还使用其内部聚类中心作为分类监督。至于回归监督，只能从几个框标签中获得。</p><p><img src="/pic/weak3d62.png"></p><p>其中 x、y、z 表示簇中点的坐标集。</p><p><strong>基于框的分配</strong> 基于框的分配是将标签分配给预定义的锚点或提案的过程。例如，基于锚点的方法将与框标签具有高交集（IoU）的锚点视为正。类似地，两阶段方法选择具有适当 IoU 和框标签的提案，以进行细化和置信度学习。下面我们只关注为提案分配集群级标签，因为锚点的设计是相同的。</p><p>为了实现基于框的分配，我们首先定义框簇 IoU，它被定义为提案中的点簇与簇级标签之间的点级 IoU。如图 4 所示，灰色点代表框中的点簇，而绿色轮廓的点表示簇级标签。盒簇 IoU 计算为带有绿色轮廓的灰色点与图中所有点的比率。通过盒簇 IoU，我们可以将簇级标签分配给提案，以训练任何基于锚的检测器和两级检测器。</p><p><img src="/pic/weak3d63.png" alt="图 4：盒簇 IoU 图示。"></p><p><strong>基于盒子的分配的模糊性</strong> 值得注意的是，盒子簇 IoU 本质上是模糊的。特别是，边界框的轻微扰动可能会导致普通框 IoU 的显着变化。然而，对边界框的轻微扰动通常不会改变内部簇，因此框簇 IoU 可能保持不变。幸运的是，我们只依靠盒簇 IoU 进行语义分配，而不是几何标签分配，并且前者不需要精确的 IoU。在秒。 5.5，我们定量地证明了歧义的不利影响可以忽略不计。</p><p><strong>用于粗略标签生成的 POINTSAM</strong></p><p>集群级标签的使用大大减少了人工注释的需求。为了进一步减少粗标签的注释负担，我们建议使用 PointSAM 进行自动粗标签，利用强大的 SAM（Kirillov 等人，2023）来生成粗集群级别标签。 PointSAM如图5所示，它包括两个模块：（1）基于SAM的3D实例分割：我们使用SAM来推断过度分割的掩模并将它们映射到3D点云。 （2）SeparabilityAware Refinement：由于SAM的过度分割和点像素投影不精确，我们提出SAR来缓解这些问题以提高分割质量。</p><p><img src="/pic/weak3d64.png" alt="图 5：PointSAM 的总体情况。"></p><p><strong>SAM 辅助的 3D 实例分割</strong>：我们首先利用预先训练的语义分割模型来生成 2D 语义掩模。然后我们将 3D 点投影到 2D 语义掩模中。映射到 2D 前景语义掩码的点可以提示 SAM 生成 2D 过分割掩码，从而显着提高推理速度。对于 SAM 生成的每个掩码，语义标签是根据掩码内像素数最高的类别分配的。通过 3D-2D 投影，我们获得初始 3D 实例掩模。</p><p>**可分离性感知细化 (SAR)**： 尽管如此，SAM 的过度分割和投影误差导致分割质量平庸。例如，可能有一些属于相同对象的点被分配了不同的掩码ID，或者同一方向上两个相距较远的簇可能被分配了相同的掩码ID。幸运的是，这些问题可以通过利用点云固有的空间可分离性来缓解。具体来说，我们在前景点上采用连通分量标记（CCL）。执行CCL后，我们获得多个组件。我们将跨多个组件的蒙版分开，然后合并属于单个组件的蒙版。附录 C.2 给出了 SAR 的简单说明。我们在附录 C.3 中探讨了 SAR 对不准确校准的抵抗力。附录 C.4 中介绍了 PointSAM 与其他基于 SAM 的 3D 任务方法之间的比较。</p><p><strong>训练损失</strong></p><p>在训练阶段，粗聚类标签仅有助于分类（或置信度）Lcls，而准确的框标签仅有助于回归Lreg。基于标签分配，我们将分配有精确标签的正样本表示为Sa，将分配有粗标签的正样本表示为Sc，将负样本表示为Sn。 MixSup 的损失函数可以表示为等式 2：</p><p><img src="/pic/weak3d65.png"></p><p><strong>讨论：将 mixsup 与其他标签高效方法进行比较</strong></p><p>MixSup 和其他标签高效学习设置（例如半&#x2F;弱&#x2F;自监督框架）具有提高标签效率的相同目的。然而，它们在设计理念上却截然不同。例如，弱监督方法关注如何利用某种类型的弱标签。流行的半监督方法设计自我训练等训练方案来生成高质量的伪标签。 MixSup 遵循更实用的理念来利用不同类型的监督，并尝试将它们集成到流行的检测器中以实现通用性。由于这些本质区别，MixSup 可以与其他设置无缝协作以获得更好的性能。为了展示潜力，在第二节中。 5.5，我们建立了一个简单的基线来利用半监督学习带来的自我训练技术。我们将在未来的工作中追求 MixSup 和其他标签高效方法的更有效结合。</p><h4 id="实验-2"><a href="#实验-2" class="headerlink" title="实验"></a>实验</h4><blockquote><p>具体实验结果请查看原文，这里展示部分结果</p></blockquote><p><strong>POINTSAM 分析</strong></p><p><strong>定量分析</strong> 我们在 nuScenes 上执行 PointSAM 进行自动粗标记，并将标签与基于 LiDAR 的全景分割基准上的现有技术进行比较（Fong 等人，2022）。由于 PointSAM 不考虑背景，我们仅报告前景事物类别的性能，如表 10 所示。得益于强大的 SAM，PointSAM 与最近完全监督的全景分割模型不相上下，无需任何 3D 注释。</p><p><img src="/pic/weak3d66.png" alt="表 10：nuScenes 验证拆分中事物类的全景分割性能。"></p><p><strong>人工纠正</strong>：虽然 SAM 通常会生成高质量的聚类，但由于 nuScenes 中 3D-2D 投影的错误，不可避免地会出现假阳性聚类和假阴性。由于传感器校准不精确，这些误差无法完全修复。我们在附录 C.1 中提供了这些不良案例的分析。因此，我们根据第 2 节中的标签协议手动纠正假阳性标签和假阴性。 4.1.人工纠正产生了表 11 中的显着结果，但代价是所有粗标签的注释负担增加了 50%。</p><p><strong>附录</strong></p><blockquote><p>具体补充请查看原文，这里展示PointSAM的补充</p></blockquote><p><em><strong>C PointSAM</strong></em></p><p><em><strong>C.1 定性分析</strong></em></p><p>我们在图 7 中列出了生成的粗略标签的可视化。如子图 (a) 和 (b) 所示，PointSAM 展示了生成高质量集群级标签的能力。然而，由于极端情况下的低质量分割或3D-2D投影误差，不可避免地会出现假阳性簇和假阴性，如图(c)和(d)所示。特别是，子图 (c) 展示了夜间驾驶的极端情况，其中 SAM 无法提供有效的分割掩模，导致不可信的集群标签。子图（d）举例说明了不正确投影的情况，其中背景点被错误地投影到前景标签。</p><p><img src="/pic/weak3d67.png" alt="图 7：通过 PointSAM 生成的标签的可视化。子图（a）和（b）描绘了准确生成的样本，而子图（c）和（d）说明了包含红色圆圈中的假阳性簇和黄色圆圈中的假阴性簇的样本。"></p><p><em><strong>C.2 可分离性感知细化（SAR）</strong></em></p><p>图 8 显示了 SAR 的简单说明。对于跨多个组件的掩模，我们首先分析每个组件中每个掩模的数量，并保留计数最高的那个。它确保每个掩码仅与一个组件关联。随后，我们合并属于单个组件的掩模并输出最终的分割掩模。</p><p><img src="/pic/weak3d68.png" alt="图 8：可分离性感知细化 (SAR) 的图示。在特区，！表示保留掩模，%表示将这些点视为背景。"></p><p><em><strong>C.3 SAR 对不准确校准的抵抗力</strong></em></p><p>在整个实验过程中，我们发现 nuScenes 校准中固有的不准确性，导致前景物体的像素点投影存在差异。因此，我们引入 SAR 模块来减轻投影误差引起的分割质量下降。为了进一步研究投影差异对 PointSAM 的影响，我们向每个摄像机的位置引入了随机噪声。表 14 总结了 nuScenes val split 全景分割上前景对象的性能。结果表明，SAR 派生的粗实例掩模表现出一定程度的对校准误差的抵抗力，这归因于 SAR 模块引入的细化。</p><p><img src="/pic/weak3d68.png" alt="表 14：噪声校准的性能。"></p><p><em><strong>C.4 基于 SAM 的方法的讨论</strong></em></p><p>据我们所知，PointSAM 是第一个利用 SAM 在户外场景中进行实例分割的举措。值得注意的是，PointSAM 的性能与最近的完全监督全景分割模型相当，无需任何 3D 注释，如表 10 所示。此外，我们的 PointSAM 创新性地利用点云固有的空间可分离性来细化实例分割，从而减轻投影误差。</p><p>就其他基于 SAM 的工作而言，采用 SAM3D 进行检测（Zhang et al., 2023a）、SAM3D 进行实例分割（Yang et al., 2023b）、Seal（Liu et al., 2023c）和 Label-free Scene以理解（Chen et al., 2023a）为例，用于检测的 SAM3D（Yang et al., 2023a）是一项早期探索性工作，创新地将 SAM 应用于基于 LiDAR 的 3D 物体检测。然而，它有一些局限性，例如仅限于检测车辆和不切实际的性能。 SAM3D 实例分割（Yang et al., 2023b）侧重于室内场景中的多视图合并，而 PointSAM 则注重室外场景中基于空间可分离性的细化。此外，我们集成语义来丰富信息，例如掩码。后两项工作（Liu et al., 2023c; Chen et al., 2023a）都专注于通过像素点投影进行语义分割的预训练，并取得了显着的成果。但是，它们无法处理实例分割。需要强调的是，PointSAM 的目的是为 MixSup 提供粗略标签。因此，我们很高兴看到这些杰出的作品发挥着类似的作用，并有可能与 MixSup 集成。</p><blockquote><p>这里将这几个应用于3D的SAM模型进行汇总</p></blockquote><p>SAM3D: Zero-Shot 3D Object Detection via Segment Anything Model</p><p><a href="https://arxiv.org/abs/2306.02245">论文链接</a><br><a href="https://github.com/DYZhang09/SAM3D">代码链接</a></p><p>SAM3D: Segment Anything in 3D Scenes</p><p><a href="https://arxiv.org/abs/2306.03908">论文链接</a><br><a href="https://github.com/Pointcept/SegmentAnything3D">代码链接</a></p><p>Segment Any Point Cloud Sequences by Distilling Vision Foundation Models</p><p><a href="https://arxiv.org/abs/2306.09347">论文链接</a><br><a href="https://github.com/youquanl/Segment-Any-Point-Cloud">代码链接</a></p><p>Towards Label-free Scene Understanding by Vision Foundation Models</p><p><a href="https://arxiv.org/abs/2306.03899">论文链接</a><br><a href="https://github.com/runnanchen/Label-Free-Scene-Understanding">代码链接</a></p>]]></content>
      
      
      <categories>
          
          <category> DL </category>
          
      </categories>
      
      
        <tags>
            
            <tag> CV </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>弱监督下的三维目标检测(单目篇)</title>
      <link href="/2024/04/24/weakly-3d-det/"/>
      <url>/2024/04/24/weakly-3d-det/</url>
      
        <content type="html"><![CDATA[<h1 id="弱监督下的三维目标检测-单目篇"><a href="#弱监督下的三维目标检测-单目篇" class="headerlink" title="弱监督下的三维目标检测(单目篇)"></a>弱监督下的三维目标检测(单目篇)</h1><h2 id="一、基于单目图像的三维目标检测"><a href="#一、基于单目图像的三维目标检测" class="headerlink" title="一、基于单目图像的三维目标检测"></a>一、基于单目图像的三维目标检测</h2><p><a href="">3D Object Detection from Images for<br>Autonomous Driving: A Survey</a></p><h3 id="第一篇：WeakM3D"><a href="#第一篇：WeakM3D" class="headerlink" title="第一篇：WeakM3D"></a>第一篇：WeakM3D</h3><p>Towards Weakly Supervised Monocular 3D Object Detection（2022）</p><p><a href="https://browse.arxiv.org/pdf/2203.08332v1">论文链接</a></p><p><a href="https://github.com/SPengLiang/WeakM3D">代码链接</a></p><h4 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h4><p>单目 3D 物体检测是 3D 场景理解中最具挑战性的任务之一。由于单目图像的不适定性质，现有的单目 3D 检测方法高度依赖于 LiDAR 点云上手动注释的 3D 框标签的训练。这个注释过程非常费力且昂贵。为了摆脱对 3D 框标签的依赖，在本文中，我们探索了弱监督的单目 3D 检测。具体来说，我们首先检测图像上的 2D 框。然后，我们采用生成的2D框来选择相应的RoI LiDAR点作为弱监督。最终，我们采用网络来预测 3D 框，它可以与相关的 RoI LiDAR 点紧密对齐。该网络是通过最小化我们新提出的 3D 框估计和相应的 RoI LiDAR 点之间的 3D 对齐损失来学习的。我们将说明上述学习问题的潜在挑战，并通过在我们的方法中引入几种有效的设计来解决这些挑战。</p><p><img src="/pic/weak3d1.png" alt="图 1：我们使用 LiDAR 点云作为训练中的弱监督。 (a)：调整过程； (b)：对齐模糊问题； (c)：LiDAR 点分布不均匀。"></p><p>贡献如下：</p><ul><li>首先，我们探索了一种弱监督单目 3D 检测的新方法 (WeakM3D)，消除了对 3D 框标签的依赖。</li><li>其次，我们提出了 WeakM3D 中的主要挑战，并相应地介绍了四种有效的策略来解决这些问题，包括几何对齐损失、光线追踪损失、损失平衡和学习解纠缠。</li><li>第三，根据 KITTI 基准进行评估，我们的方法为弱监督单目 3D 检测构建了强大的基线，甚至优于一些现有的使用大量 3D 框标签的完全监督方法。</li></ul><h4 id="相关工作"><a href="#相关工作" class="headerlink" title="相关工作"></a>相关工作</h4><p><strong>基于单目的三维目标检测方法</strong><br>近年来，单目 3D 物体检测取得了显着的进步。 巴西和刘（2019）；马等人（2021）。先前的作品，例如 Mono3D Chen 等人(20Penet) 和 Deep3DBox Mousavian 等人(2017)主要利用几何约束和辅助信息。最近，Mondle Ma 等人 (2021) 通过三种定制策略来减少单目 3D 检测中的定位误差。此外，随着深度估计的发展，其他一些单目方法尝试使用由现成的深度估计器生成的显式深度信息。伪LiDARWang等人（2019）； Weng &amp; Kitani (2019) 转换纯图像表示，以模仿真实的 LiDAR 信号，以利用现有的基于 LiDAR 的 3D 探测器。 PatchNet Ma 等人 (2020)重新思考伪激光雷达的底层机制，指出其有效性来自于3D坐标变换。尽管最近的单目 3D 物体检测方法取得了令人兴奋的结果，但它们严重依赖于大量手动标记的 3D 框。</p><p><strong>基于弱监督的三维目标检测方法</strong></p><p>为了减轻繁重的注释成本，提出了一些弱监督方法。 WS3D 孟等人(2020) 引入了一种用于基于 LiDAR 的 3D 对象检测的弱监督方法，该方法仍然需要一小组弱注释的场景和一些精确标记的对象实例。他们使用两阶段架构，第一阶段学习在鸟瞰视图中单击注释的水平中心下生成圆柱形对象建议，第二阶段学习细化圆柱形建议以获得 3D 框和置信度分数。这种弱监督设计并没有完全摆脱对 3D 框标签的依赖，仅适用于 LiDAR 点云输入。另一种弱监督3D检测方法Qin等人（2020，VS3D）也将点云作为输入。它提出了一个 3D 提案模块，并利用现成的 2D 分类网络来识别从点云生成的 3D 框提案。这两种方法都不能直接应用于拟合单个 RGB 图像输入。此外，扎哈罗夫等人(2020，Autolabeling) 提出了一个自动标记管道。具体来说，他们将一种新颖的可微分形状渲染器应用于有符号距离场（SDF），并与归一化对象坐标空间（NOCS）一起使用。他们的自动标记流程由六个步骤组成，并具有课程学习策略。与 WeakM3D 相比，他们的方法不是端到端的，而且相当复杂。</p><h4 id="方法"><a href="#方法" class="headerlink" title="方法"></a>方法</h4><p><img src="/pic/weak3d2.png" alt="图 2：网络架构。在推理阶段，我们只需要单个 RGB 图像作为输入并输出相应的 3D 框。在训练阶段，我们使用 LiDAR 点云和从​​预训练模型估计的 2D 框来获取对象 LiDAR 点，这些点用于通过 3D 框预测建立损失来训练网络。最好以彩色形式观看。"></p><ul><li>首先，通过 RANSAC Fischler &amp; Bolles (1981) 从原始 LiDAR 点云估计地平面，用于删除原始点云中的地面点。</li><li>选择在 2D 框内的图像上投影的剩余 LiDAR 点作为初始目标点云，其中包含一些不感兴趣的点，例如背景点。或者，我们还可以使用预训练网络中的 2D 实例掩模来选择初始对象点云。</li><li>然后，我们通过 Ester 等人的无监督密度聚类算法过滤初始点云来获得对象 LiDAR 点。 （1996），附录中有详细说明。</li></ul><p><img src="/pic/weak3d4.png" alt="图 4：损耗设计。我们为每个目标 LiDAR 点和相应的 3D 框预测引入了几何对准损失和光线追踪损失以及损失平衡。几何对齐损失侧重于将 3D 框预测与对象 LiDAR 点紧密对齐。光线追踪损失进一步考虑了相机成像/LiDAR 扫描的遮挡约束，以消除场景中的对齐模糊性。"></p><p><strong>几何对齐的3D框预测</strong></p><p>不失一般性，3D 对象框应包含对象LiDAR 点并沿框边缘与它们对齐。为了促进物体 3D 位置的学习，我们在网络的 3D 框预测和物体 LiDAR 点之间施加了位置约束。简单的解决方案：</p><ul><li>最小化从物体中心到每个激光雷达点的欧几里德距离。</li><li>然而，这种微不足道的损失由于其定位不准确而误导了网络。这个中心距离损失 Lcenter 会将预测的对象中心推到尽可能靠近点云的位置。不幸的是，物体激光雷达点通常是从物体的可见表面捕获的，这意味着仅使用 Lcenter 的预测中心往往接近真实框边缘，而不是真实框中心。</li></ul><p>基于上述分析，我们提出了预测的 3D 框和关联的目标 LiDAR 点之间的几何对齐损失，其中主要障碍在于正确测量点到 3D 框的距离。为此，我们创建一条从 3D 框中心 P3d 到每个物体 LiDAR 点 P 的射线，其中该射线与框预测的边缘在 PI 处相交。因此，物体-LiDAR点之间每个点的几何对准损失如下：</p><p><img src="/pic/weak3d3.png"></p><p><strong>光线追踪消除对准模糊性</strong></p><p>尽管几何对齐限制了 3D 框预测和物体 LiDAR 点，但当物体 LiDAR 点无法表示物体的充分 3D 轮廓时，就会出现歧义，例如，物体 LiDAR 点仅从物体的一个表面捕获目的。具体来说，模糊度是指在对齐过程中如何在语义上决定每个目标激光雷达点与框预测的边缘之间的对应关系。我们称这个问题为对齐模糊性。</p><p>一般来说，我们通过考虑遮挡约束来解决对齐模糊性。与相机成像的过程类似，在扫描场景时，如果遇到障碍物，激光雷达设备的信号就会被反射。考虑到相机FOV（视场）内反射的LiDAR信号，我们建议实现从相机光心Pcam到每个物体LiDAR点的光线追踪，最小化每个物体LiDAR点到物体上交点的距离盒子。如图4所示，激光雷达点为P，交点为PR（使用Z缓冲，即选择较近的交点），光线追踪损失如下：</p><p><img src="/pic/weak3d5.png"></p><p>{P1, P2} &#x3D; Intersect(RayPcam→P, b3d)，如果 P1 更靠近相机，则 PR &#x3D; P1，否则为 P2。请注意，如果光线不与预测的 3D 框相交，则光线追踪损失为零，这意味着此处的损失不会对反向传播中的梯度下降产生影响。通过这种方式，我们消除了对齐模糊性，鼓励 3D 框预测遵循遮挡约束，同时与对象 LiDAR 点进行几何对齐。</p><p><strong>逐点损耗平衡</strong></p><p>目标激光雷达点的不同空间分布也是一个障碍，即点密度有些高，有些低。诸如几何对准损失和光线追踪损失等逐点损失都受到不均匀分布的影响。具体来说，密集区域产生的损失可以主导总损失，忽略其他相对稀疏但重要的点所产生的损失。为了平衡影响，我们在计算损失时对目标激光雷达点的密度进行归一化。让我们计算 Pi 点附近的 LiDAR 点 Ei 的数量，如下所示：</p><p><img src="/pic/weak3d6.png"></p><p>其他具体参考原论文</p><h3 id="第二篇：Weakmono3d"><a href="#第二篇：Weakmono3d" class="headerlink" title="第二篇：Weakmono3d"></a>第二篇：Weakmono3d</h3><p>Weakly Supervised Monocular 3D Object Detection using Multi-View Projection and Direction Consistency（2023）</p><p><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Tao_Weakly_Supervised_Monocular_3D_Object_Detection_Using_Multi-View_Projection_and_CVPR_2023_paper.pdf">论文链接</a></p><p><a href="https://github.com/weakmono3d/weakmono3d">代码暂未开源</a></p><h4 id="摘要-1"><a href="#摘要-1" class="headerlink" title="摘要"></a>摘要</h4><p>单目3D物体检测因其易于应用而成为自动驾驶的主流方法。一个突出的优点是在推理过程中不需要LiDAR点云。然而，大多数当前方法仍然依赖 3D 点云数据来标记训练阶段使用的地面事实。训练和推理之间的这种不一致使得大规模反馈数据难以利用并增加了数据收集费用。为了弥补这一差距，我们提出了一种新的弱监督单目 3D 物体检测方法，该方法可以仅使用图像上标记的 2D 标签来训练模型。具体来说，我们在该任务中探索了三种一致性，即投影一致性、多视图一致性和方向一致性，并基于这些一致性设计了弱监督架构。此外，我们在此任务中提出了一种新的2D方向标记方法来指导模型进行准确的旋转方向预测。实验表明，我们的弱监督方法取得了与一些完全监督方法相当的性能。当用作预训练方法时，我们的模型可以显着优于仅具有 1&#x2F;3 3D 标签的相应完全监督基线。</p><p>主要贡献:</p><ul><li>在这项工作中，我们提出了一种新颖的单目 3D 物体检测弱监督方法，该方法仅利用 2D 标签作为地面实况，而不依赖 3D 点云进行标记，使我们成为第一个这样做的人。</li><li>我们的方法结合了投影一致性和多视图一致性，用于设计两种一致性损失，指导准确的 3D 边界框的预测。</li><li>此外，我们引入了一种称为 2D 方向标签的新标签方法，取代了点云数据中的 3D 旋转标签以及基于新标签的方向一致性损失。</li><li>我们的实验表明，我们提出的弱监督方法实现了与一些完全监督方法相当的性能，即使只有 1&#x2F;3 的真实标签，我们的方法也优于相应的完全监督基线，展示了其基于反馈生产数据改进模型的潜力。</li></ul><p><img src="/pic/weak3d12.png" alt="图 1.所提出方法的架构。左列显示，在训练阶段，来自不同视点的图像对被发送到检测模型中，并在预测和 2D 地面实况之间计算 4 个损失。右栏显示了投影一致性和多视图一致性的详细信息。为了计算投影一致性损失，我们将预测框投影到2D图像中并将其转换为2D框，最后计算2D框和2D框标签之间的差异。为了计算一致性损失，我们首先将视点 1 的预测 3D 框转换为视点 2 的坐标系，然后计算转换后的框与视点 2 的预测框之间的差异。"></p><p><img src="/pic/weak3d13.png" alt="图 2. WeakMono3D 结果的可视化。图像中的实心面和 BEV 框中的短条都表示物体的方向。"></p><h3 id="第三篇：SKD-WM3D"><a href="#第三篇：SKD-WM3D" class="headerlink" title="第三篇：SKD-WM3D"></a>第三篇：SKD-WM3D</h3><p>Weakly Supervised Monocular 3D Detection with a Single-View Image（2024）<br><a href="https://arxiv.org/abs/2402.19144">论文链接</a></p><p>[代码暂未公布]</p><h4 id="摘要-2"><a href="#摘要-2" class="headerlink" title="摘要"></a>摘要</h4><p>单目 3D 检测 (M3D) 旨在从单视图图像中精确定位 3D 对象，这通常涉及 3D 检测框的劳动密集型注释。最近研究了弱监督 M3D，通过利用许多现有的 2D 注释来消除 3D 注释过程，但它通常需要额外的训练数据，例如 LiDAR 点云或多视图图像，这大大降低了其在各种应用中的适用性和可用性。我们提出了 SKD-WM3D，这是一种弱监督的单目 3D 检测框架，它利用深度信息仅通过单视图图像实现 M3D，无需任何 3D 注释或其他训练数据。 SKD-WM3D 的一个关键设计是自知识蒸馏框架，它通过融合深度信息将图像特征转换为 3D 表示，并有效减轻单目场景中固有的深度模糊性，而推理中的计算开销很小。此外，我们设计了一种不确定性感知蒸馏损失和一种梯度目标转移调制策略，分别促进知识获取和知识转移。大量实验表明 SKD-WM3D 明显超越了最先进的方法，甚至与许多完全监督的方法相当。</p><h4 id="介绍"><a href="#介绍" class="headerlink" title="介绍"></a>介绍</h4><p>弱监督 M3D (WM3D) [32] 最近被探索用于学习没有 3D 框注释的有效 3D 检测器，旨在利用 2D 注释来弥补 3D 信息的缺失。例如，WeakM3D [32] 利用 LiDAR 点云来推断 3D 信息，如图 1(a) 所示。然而，它需要昂贵且复杂的激光雷达传感器来收集点云，这极大地限制了其适用性和可用性。 WeakMono3D [40] 仅通过利用来自多个摄像机的图像的多视图双目或从连续视频帧构建伪多视图透视来使用 2D 信息，如图 1(b) 所示。然而，收集多视角图像很复杂，并且采用伪多视角视角会明显降低检测性能。随着单视图深度估计的进步，具有单视图图像深度的 WM3D 为补偿 3D 注释的缺失提供了一种潜在的解决方案。另一方面，将这种深度直接集成到现有框架中通常需要复杂的网络架构，这会进一步产生大量的计算成本。这就提出了一个相关问题：当​​不使用额外的激光雷达点云或多视图图像对时，是否可以利用现成的深度估计器的深度，而不会在推理中引入大量计算开销？</p><p>我们设计了 SKD-WM3D，这是一种新颖的弱监督单目 3D 物体检测方法，专门基于单视图图像。 SKDWM3D 的一个关键设计是自我知识蒸馏框架，由深度引导自教学网络 (DSN) 和单目 3D 检测网络 (MDN) 组成。如图 1（c）所示，</p><ul><li>SKD-WM3D 利用从现成的深度估计器 [Penet] 获得的深度信息来增强 DSN 的 3D 定位能力，并通过自知识蒸馏将这种能力转移到 MDN。这种自蒸馏设计使得MDN能够独立地从单视图图像中挖掘内在的深度信息，绕过预训练深度估计网络等附加模块，实现精确高效的 3D 定位，推理期间的计算开销很小。</li><li>在 DSN 和 MDN 之上，我们设计了一种不确定性感知蒸馏损失，通过加权更确定的知识同时降低不太确定的知识来优化传输的 3D 定位知识的利用率。</li><li>此外，我们设计了一种梯度目标转移调制策略，在学习3D定位知识的过程中同步DSN和MDN的学习进度，通过在MDN落后于DSN的初始阶段优先考虑MDN学习并使其能够提供更多反馈当 MDN 在后期得到更好的训练时，转向 DSN。</li></ul><p><img src="/pic/weak3d7.png" alt="图 1. 弱监督单目 3D 检测的不同范例。 (c) 中的方法利用单视图图像中的伪深度标签来实现弱监督的单目 3D 检测，不需要额外的训练数据，如 (a) 和 (b) 中的 LiDAR 点云或多视图图像。它大大提高了可用性和适用性。伪深度标签是使用现成的深度估计器[Penet]获得的，无需额外的训练和地面实况深度标签。红色数据表示网络训练中的额外数据。"></p><p>主要贡献：</p><ul><li>首先，我们设计了一个新颖的框架，通过在深度引导自学网络和单目 3D 检测网络之间提取知识来实现​​弱监督单目 3D 检测。无需任何额外的训练数据（例如 LiDAR 点云或多视图图像），该框架仅利用单个图像的深度，推理时的计算开销很小。</li><li>其次，我们设计了一种不确定性感知蒸馏损失和一种梯度目标转移调制策略，分别促进知识获取和知识转移。</li><li>第三，所提出的方法明显优于弱监督单目 3D 检测的最新技术，其性能甚至与几种完全监督方法相当。</li></ul><h4 id="相关工作-1"><a href="#相关工作-1" class="headerlink" title="相关工作"></a>相关工作</h4><p><strong>基于单目的三维目标检测方法</strong></p><p>单目 3D 对象检测旨在从单视图图像预测 3D 对象定位。标准单目探测器 [1, 6, 13, 51, 54] 仅对单个图像进行操作，无需使用额外的数据。然而，与双目检测相比，单目检测固有的深度模糊性严重阻碍了其性能。为了解决这一限制，各种方法在额外数据的帮助下寻求解决方案，例如 LiDAR 点云 [4,7,20,25]、视频序列 [2]、3D CAD 模型 [5,23,29] 和深度估计[9,33,43,46]。具体来说，MonoRUNn [4] 采用不确定性感知区域重建网络，以 LiDAR 点云作为额外监督来回归像素关联的 3D 对象坐标。 MonoDistill [7] 引入了一种有效的基于蒸馏的方法，该方法将 LiDAR 信号的空间信息合并到单目 3D 检测中。此外，基于伪 LiDAR 的方法 [43, 46] 转换估计的深度图以模拟真实的 LiDAR 点云，以利用精心设计的基于 LiDAR 的 3D 探测器。在推理过程中，与使用深度估计的方法相比，我们的方法不需要伪深度标签和复杂的网络架构，计算开销很小。此外，现有的完全监督方法需要大规模的 3D 框地面实况，这对于收集和注释来说是劳动密集型的。</p><p><strong>弱监督 3D 物体检测</strong></p><p>由于在 3D 对象检测任务中注释 3D 框的成本很高，因此提出了各种弱监督方法。例如，WS3D [27] 提出了一种用于 3D LiDAR 物体检测的弱监督方法，该方法仅需要有限数量的带有中心注释 BEV 地图的弱注释场景。 VS3D[34]引入了一种跨模型知识蒸馏策略，将知识从RGB域转移到点云域，使用LiDAR点云作为弱监督。最近关于弱监督 3D 对象检测的研究已转向探索单目设置。例如，WeakM3D[32]生成2D框来选择RoI激光点云作为弱监督，然后预测与所选 RoI LiDAR 点云紧密对齐的 3D 框。最近，WeakMono3D [40] 消除了对 LiDAR 的需求，提供多视图和单视图多帧版本。前者从多个摄像机获取立体图像输入，而后者使用多个视频帧构建伪多视图透视。与多视图方法相比，多帧版本由于其较小的帧间视差而表现出较差的 3D 场景理解能力，从而导致性能下降。我们不需要 LiDAR 点云或多视图图像等额外的训练数据，而是通过专门利用单视图图像来解决弱监督单目 3D 检测的挑战。</p><p><strong>自认识蒸馏</strong></p><p>知识蒸馏[8,10,15,17,22,30,36,41,52]旨在将知识从预先训练的教师网络转移到学生网络以提高其性能。自我知识蒸馏[28,39,45]与传统的知识蒸馏不同，它利用学生网络内的信息来促进其学习，而无需预先训练的教师网络。具体来说，数据增强方法[14,44,48]通过相同训练数据的不同扭曲来传递知识。然而，它们很容易受到不适当的增强的影响，例如不适当的实例旋转或扭曲，可能会引入阻碍网络学习的噪音。另一种典型的方法利用辅助网络 [50, 55]。例如，DKS [38]引入了辅助监督分支和成对知识对齐，而FRSKD [18]添加了由原始特征监督的新分支，并利用了软标签和特征图蒸馏。我们的工作是第一个引入自知识蒸馏和辅助网络的弱监督单目 3D 检测的工作。它有效地利用单视图图像的深度信息，在推理过程中几乎不需要计算开销。</p><h4 id="方法-1"><a href="#方法-1" class="headerlink" title="方法"></a>方法</h4><p><img src="/pic/weak3d8.png" alt="图 2. 所提出的自知识蒸馏网络的框架。该框架由深度引导自学网络和单目 3D 检测网络组成。深度引导自学网络利用深度信息获取全面的 3D 定位知识，并通过软标签蒸馏将其学到的专业知识转移到单目 3D 检测网络，以提高其性能。我们设计了不确定性感知蒸馏损失和梯度目标转移调制策略，以有效促进两个网络之间的知识转移。在推理过程中，单目 3D 检测网络独立地从单视图图像中提取内在深度信息，计算开销很小。"></p><p><strong>问题定义和概述</strong></p><p>弱监督单目 3D 检测以 RGB 图像和相应的 2D 边界框作为监督，旨在在 3D 空间中对对象进行分类并确定其边界框，而无需在训练中涉及任何 3D 注释。每个对象的预测由对象类别C、2D边界框B2D和3D边界框B3D组成。具体来说，3D框B3D可以进一步分解为对象3D位置（x3D，y3D，z3D），具有高度、宽度和长度的对象尺寸（h3D，w3D，l3D）以及方向θ。</p><p>我们设计了一个自我知识蒸馏框架来应对单视图图像弱监督单目 3D 检测的挑战。如图 2 所示，该框架由两个子网络组成，包括深度引导自教学网络和单目 3D 检测网络。在深度引导自学习网络中，主干提取的全局特征FG被输入深度头以获得深度特征。接下来，将全局特征 FG 和提取的深度特征输入融合层以获得类 3D 特征 F3D。然后通过 RoIAlign 获得每个对象的 3D 特征，并进一步馈送到深度感知 3D 头以预测 3D 框 Bb 3D p 和不确定性 bU。在单目 3D 检测网络中，我们首先使用 RoIAlign 从全局特征 FG 生成对象级特征，然后将它们提供给 2D-to-3D Head 来预测 3D 框 bB2D p 和不确定性 U。此外，通过以下方式预测的 3D 框两个网络都进一步投影到二维盒子中。此外，我们设计了一种不确定性感知蒸馏损失 Lud 来获得低不确定性知识，并设计了一种梯度目标转移调制策略，通过控制 Lud 的梯度 bG 和 G 来同步两个网络之间的学习速度。</p><p><strong>自我知识蒸馏框架</strong></p><p>自知识蒸馏框架利用来自现成深度估计器的深度信息增强深度引导自教学网络的 3D 定位能力，然后通过自知识蒸馏将该能力转移到单目 3D 检测网络。</p><p><strong>深度引导自学网络</strong>为了使自学网络具备 3D 定位能力，我们建议从现成的深度估计器中学习全局特征 FG 和深度信息，以获得全面的 3D 知识。深度信息通过两种主要设计来利用。首先，我们引入一个提取深度特征FD的深度头D，利用深度特征 FD 来生成深度图 Dp，其中深度图的生成由深度图 Dgt 的伪地面实况监督，深度图 Dgt 是由现成的深度估计器通过使用焦点损失 [21] 来预测的深度损失 Ldep。因此，深度引导自学习网络可以有效地获取深度特征。</p><p>备注 1. 我们使用带有冻结权重的现成深度估计器 [16] 生成深度伪标签，从而无需额外的训练和地面实况深度标签。与之前需要 LiDAR 点云 [32] 或多视图图像 [40] 的研究相比，采用现成的深度估计器所产生的成本可以忽略不计。</p><p>其次，我们通过集成提供沿深度维度信息的深度特征 FD 以及捕获有关 2D 图像平面的知识的全局特征 FG 来获得类 3D 特征 FG3D。具体来说，我们设计了一个融合层，将深度特征 FD 与全局特征 FG 融合，得到 FG3D，如下所示：</p><p>其中FFN是前馈网络，CA、SA分别表示CrossAttention、SelfAttention。 CrossAttention 和 SelfAttention 的结构采用标准的 Transformer 架构 [42]。获得的 3D 理解提高了网络精确定位对象的能力，有效减轻了单视图图像输入引起的深度模糊。</p><p><strong>单目 3D 检测网络</strong> 单目3D检测网络从深度引导自学习网络获取3D定位知识。通过提取深度引导自学习网络生成的软标签，单目 3D 检测网络可以在推理过程中独立地从图像中提取内在深度信息。这消除了对额外复杂模块（例如预训练深度估计网络或深度融合模块）的需求，从而以很少的计算开销促进推理。</p><p><strong>不确定性蒸馏损失</strong></p><p>在知识蒸馏过程中，如果平等对待所有转移的知识，不确定的知识可能会对网络训练产生负面影响。为了更多地从某些知识中受益并削弱不确定知识的影响，我们在自知识蒸馏框架中的两个网络预测的 3D 框之间设计了一个不确定性感知的蒸馏损失。不确定性蒸馏损失利用预测不确定性来调节蒸馏损失大小</p><p><strong>传输调制策略</strong></p><p>深度引导自学网络利用深度信息来预测 3D 框，将其学到的 3D 知识传输到单目 3D 检测网络。两个网络的异步学习速度对有效的 3D 知识迁移提出了潜在的挑战。我们设计了一种梯度目标转移调制策略来同步深度引导自学网络和单目 3D 检测网络的学习速度。我们通过控制不确定性蒸馏损失 Lud 的梯度来动态调节知识转移。具体来说，我们根据每个网络的 2D 投影性能来调整梯度，为性能良好的网络分配较小的后向梯度，为性能差的网络分配较高的后向梯度。梯度目标转移</p><p>总体目标由三个损失组成，包括 Lud、Ldep 和 Lbase。 Lud 是定义的不确定性蒸馏损失。 Ldep是用于监督预测深度图的深度损失。 Lbase 包括用于监督 2D 头和 3D 框预测的 2D 框预测的损失，这已在先前的 CenterNet [53] 和 WeakMono3D [40] 中采用。我们将每个损失项的权重设置为1.0</p><p><strong>评估协议</strong></p><p>对于 KITTI 3D 数据集，按照[37]，我们采用评估指标 AP|R40，它是 40 个召回点的 AP 的平均值。我们将鸟瞰图和 3D 物体检测的平均精度报告为 APBEV|R40 和 AP3D|R40 。此外，由于大多数弱监督 3D 对象检测方法对测试集应用 0.7 的 IoU 阈值，对验证集应用 0.5 的 IoU 阈值，因此我们采用相同的阈值进行公平基准测试。我们采用四个指标对nuScenes数据集进行评估，即AP（平均精度）、ATE（平均翻译误差）、ASE（平均尺度误差）和AAE（平均属性误差）。根据[32]，由于弱监督方法中缺乏对速度和运动方向的监督，因此未报告 AVE（平均速度误差）和 AOE（平均方向误差）。</p><p>表 1. Car 类别在 KITTI 测试集上的性能比较。对于所有结果，我们使用 IoU 阈值等于 0.7 的 AP|R40 指标。最好的结果以粗体显示。<br><img src="/pic/weak3d9.png"></p><p>表 2. Car 类别在 KITTI val 集上的性能比较。对于所有结果，我们使用 AP|R40 指标，IoU 阈值等于 0.5。 * 表示此性能是从官方代码复制的。弱监督 3D 对象检测方法的最佳结果以粗体显示。<br><img src="/pic/weak3d10.png"></p><p>图 3. KITTI val 集的定性说明。红色框表示真实注释，绿色框表示我们的预测。 LiDAR 点云的地面实况仅用于可视化目的。放大观看效果最佳。<br><img src="/pic/weak3d11.png"></p><h3 id="第四篇：Ba2det"><a href="#第四篇：Ba2det" class="headerlink" title="第四篇：Ba2det"></a>第四篇：Ba2det</h3><p>Weakly Supervised 3D Object Detection with Multi-Stage Generalization（2023）</p><p><a href="https://arxiv.org/abs/2306.05418">论文链接</a></p><p><a href="https://ba2det.site/">项目页面</a></p><h4 id="摘要-3"><a href="#摘要-3" class="headerlink" title="摘要"></a>摘要</h4><p>随着大型模型的快速发展，对数据的需求变得越来越重要。特别是在 3D 对象检测中，昂贵的手动注释阻碍了进一步的进步。为了减轻注释的负担，我们研究了仅基于 2D 注释实现 3D 对象检测的问题。得益于先进的 3D 重建技术，现在可以重建整个静态 3D 场景。然而，从整个场景中提取精确的对象级注释并将这些有限的注释推广到整个场景仍然是挑战。在本文中，我们介绍了一种称为 BA2-Det 的新颖范式，包括伪标签生成和多阶段泛化。我们设计了 DoubleClustering 算法，从重建的场景级点中获取对象簇，并通过发展从完整到部分、从静态到动态、从近到远的三个泛化阶段，进一步增强模型的检测能力。在大规模 Waymo 开放数据集上进行的实验表明，BA2-Det 的性能与使用 10% 注释的完全监督方法相当。此外，使用大型原始视频进行预训练，BA2-Det 可以在 KITTI 数据集上实现 20% 的相对改进。该方法在复杂场景中检测开放集 3D 对象方面也具有巨大潜力。</p><h4 id="相关内容"><a href="#相关内容" class="headerlink" title="相关内容"></a>相关内容</h4><p>由于需要大量且昂贵的手动注释，完全监督的 3D 对象检测方法的进一步发展可能受到限制。为了克服这一限制，之前的一些工作探索了弱监督算法（Zakharov et al., 2020; Peng et al., 2022c）和额外的 LiDAR 数据来释放未标记图像的潜力。然而，对激光雷达传感器的依赖限制了这些方法在更一般场景中的实用性。随着二维基础模型的进步（Kirillov et al., 2023），二维注释不再是瓶颈。在本文中，我们旨在研究仅基于 2D 注释实现 3D 对象检测的可行性，这是一个尚未探索的问题。</p><p>该问题的核心挑战在于从 2D 图像中获取 3D 信息。从 3D 重建技术（Schonberger &amp; Frahm，2016）中汲取灵感，我们可以获得整体静态 3D 场景结构。因此，核心挑战已经转移到从全局场景中提取对象级伪标签，并将有限的对象伪标签推广到更多对象。</p><p>主要贡献：</p><ul><li>我们提出了一种仅使用 2D 标签进行弱监督单目 3D 物体检测的新范例。利用 3D 重建和神经网络的泛化能力，我们首次提出了该问题的实用解决方案。 </li><li>我们提出的方法名为 BA2-Det，解决了学习 3D 对象检测器时的三个基本技术挑战。我们概括了三个阶段：从全面到局部、从静态到动态、从近到远。 </li><li>我们在各种数据集上进行了实验，包括 KITTI 数据集和大规模 Waymo 开放数据集 (WOD)，证明了我们的方法在生成高质量 3D 标签和利用大规模数据进行预训练方面的有效性。 BA2-Det 的性能与仅用 10% 的视频训练的完全监督 BADet 相当，甚至优于一些领先的完全监督方法。作为一种预训练方法，BA2-Det 在 KITTI 数据集上可以实现 20% 的相对提升。 </li><li>我们进一步研究了我们的方法的潜在影响，包括复杂场景中开放集3D 对象的检测以及3D 对象跟踪的下游应用。</li></ul><h4 id="方法-2"><a href="#方法-2" class="headerlink" title="方法"></a>方法</h4><p><img src="/pic/weak3d14.png" alt="图 1：BA2-Det 的Pipeline。上图：基于重建的伪标签生成过程。我们对重建场景中的对象点云进行聚类，并拟合紧密边界框作为伪标签。底部：网络泛化的三个阶段。红色圆角矩形内的神经网络也用于推理。"></p><p>算法概述。我们简要介绍我们的框架 BA2Det 并解释模块设计。如图1所示，BA2-Det包括两个主要部分，高质量伪标签生成和伪标签的多阶段泛化。</p><ul><li>在伪标签生成阶段，我们首先利用移动摄像机的场景级重建来获取全局点云。为了从场景重建中提取 3D 对象簇，我们设计了 DoubleClustering 算法。对象簇进一步与长方体拟合以形成 3D 边界框。</li><li>在随后的多阶段泛化阶段（第 3.2 节），<ul><li>（1）为了从完整泛化到部分，我们开发了一个神经网络来从良好重构的对象中学习部分对象的 3D 对象边界框；</li><li>(2) 为了从静态推广到动态，我们通过精心设计的学习策略和迭代细化来训练 3D 目标检测器；</li><li>(3) 为了从近到远进行概括，我们遵循具有几何特征聚合的学习时间对象检测器。</li></ul></li></ul><p><strong>场景级重建的伪标签生成</strong></p><p>使用运动结构 (SfM) 技术，可以根据自我运动重建 3D 场景。然后，从重建场景中，借助每帧中的 2D 边界框，可以通过对重建场景中的前景点进行聚类来获得 3D 对象簇。因此，在本节中，我们介绍一种称为 DoubleClustering（算法 1）的算法，用于从 3D 重建场景中提取 3D 对象簇。</p><p><img src="/pic/weak3d16.png"></p><p><strong>场景重建</strong>。首先，让我们回顾一下使用 SfM 进行场景重建。我们将视频序列表示为 V &#x3D; {It|t &#x3D; 1, 2 · · · , T}，图像 It 中的关键点表示为 pi t &#x3D; [ui, vi]⊤, (i &#x3D; 1, 2, · · · , n ）和每个关键点上的局部特征为 Ft &#x3D; {fi t }。在本文中，我们使用关键点提取器和局部特征匹配网络 SuperPoint (DeTone et al., 2018) 和 SuperGlue (Sarlin et al., 2020)。给定时间 t 内摄像机的内在参数 K 和外在参数 Tt &#x3D; [Rt|tt]，全局帧中的 3D 关键点 Pi 可以通过求解束调整 (BA) 进行优化，其中投影误差在任意两个之间的相应关键点上计算图像为:<br><img src="/pic/weak3d17.png"></p><p>其中 Π(·) 是将世界坐标系中的 3D 点投影到图像的函数。此外，K和Tt也可以在BA过程中进行优化。请注意，当自我缓慢移动时，两帧之间的差异很小，观察噪声会影响重建。因此，当摄像机缓慢移动时，我们忽略视频序列，不重建场景。速度阈值定义为ω。</p><p><strong>对象点云聚类</strong>。场景重建后，我们引入了一种两步对象聚类算法，称为 DoubleClustering，以分离和聚类重建场景中的对象点云。首先，在每一帧中，我们选择可以投影在2D边界框中的3D点，并执行局部点聚类（LPC）来为每个对象Bt j选择最大的聚类：<br><img src="/pic/weak3d18.png"></p><p>其中我们将 bt j 表示为 2D 框 Bt j 中的图像区域。聚类算法基于连通分量(CC)算法，CC算法中的距离阈值为δ1。</p><p>其次，我们收集每一帧的聚类，并在整个场景中进行全局聚类。聚类算法也是基于距离阈值δ2的CC，称为全局点聚类（GPC）：<br><img src="/pic/weak3d19.png"></p><p>其中 n′ d 是对象簇总数。我们忽略点数低于阈值 θ 的簇，因为它们可能代表噪声点。最后，我们选择其中投影点数量最多的对象簇作为 2D 边界框 Bt j 的对应簇 P*j。</p><p><strong>3D 边界框拟合</strong>。现在我们获得了对象簇，然后我们需要根据每个簇中的对象点生成初始 3D 伪框。给定对象簇 P* j ，我们根据对象点拟合一个紧密的 3D 边界框。这些紧密的盒子充当初始 3D 标签。利用重建点主要位于物体表面的假设，并受到Zhang等人的启发。 (2017)，我们通过最小化点与其最近边缘之间的总距离来优化方向 ry。随后，我们调整鸟瞰图边界框的宽度和长度以实现最小面积：<br><img src="/pic/weak3d20.png"></p><p>其中 Pi 是对象簇中的 3D 点，R(ry)Bl bev 是鸟瞰图 (BEV) 中旋转边界框的边缘，角度为 ry，我们使用 l2 距离作为距离函数 d(·)。我们使用沿 z 轴的点来计算盒子的高度。我们通过测量点云沿 z 轴的最高点和最低点之间的距离来计算盒子的高度，最终生成对象 j 的 3D 盒子 B* j 。</p><p><strong>多阶段泛化</strong></p><p>某些对象可能会被异常点遮挡或影响。这些部分对象没有得到很好的重建，导致标签不准确，特别是对于大小和方向估计。因此我们设计了第一阶段的完整到部分泛化。在传统的SfM系统中，重建静态物体上的点很容易，但重建运动物体却很困难，因为它们的运动与自运动不同，并且受到对极几何约束的过滤。只有静态对象位于初始伪标签中。因此我们提出了第二阶段的静态到动态的泛化。此外，远处的物体具有脆弱的视觉特征。因此我们提出了第三阶段的近远时间泛化。</p><p><img src="/pic/weak3d15.png" alt="图 2：拟合边界框和广义框的图示。一些被遮挡的对象重建得很差，导致伪标签不准确。 Gθ 可以从增强的完整对象推广到部分对象。"></p><p><strong>从完整对象到部分对象的概括</strong>。泛化的第一阶段是训练模型以根据遮挡&#x2F;部分对象点云预测完整的 3D 边界框。因此，我们设计了一个神经网络 Gθ 来从重建良好的完整对象中学习和细化初始 3D 边界框。一般来说，我们期望首先找到重建良好&#x2F;完整的对象作为训练数据，即可以从对象点云正确拟合其3D边界框的对象。然后从这些对象中学习，网络可以预测所有对象簇的完整 3D 框。图 2 显示了 Gθ 的工作原理。然后，我们为每个对象簇优化并生成紧密的 3D 边界框作为初始伪标签。</p><p>该网络以对象簇 P<em>j ∈ Rmj×3 作为输入，并使用初始 3D 伪框 B</em>j 的中心标准化 3D 点的坐标。它由 PointNet 主干和预测 7DoF 3D 边界框 Bb j &#x3D; [cx, cy, cz, w, h, l, ry] 的头组成。我们使用平滑的 L1 边界框定位损失 Lreg。请注意，我们仅将 [σ0, σ1] 之间的长度视为重构良好的对象，并采用这些 3D 伪框来监督 Gθ。为了模拟遮挡引起的部分对象点，我们通过从重建良好的对象簇中随机切除区域来执行数据增强。训练 Gθ 的更多细节在第 2 节中。附录A.3.5。</p><p><strong>从静态对象到移动对象的概括</strong>。我们还需要解决伪标签主要来自静态对象的问题。我们观察到，在单个单眼图像中，静态和移动的物体具有相似的外观。因此，网络可以将从静态标签学习到的 3D 对象预测推广到其他移动对象。我们将单目 3D 物体检测器表示为 Nθ。网络架构基于CenterNet（Zhou et al., 2019；张等人，2021）。在我们的弱监督设置中，许多对象都有 2D 标签 Bj，但由于它们的运动，没有相应的 3D 伪标签 bBj。此外，伪标签bBj可能具有不准确的方向。所以，不同于全监督的目标检测，我们设计了一个新的标签分配策略和方向损失。</p><p>对于未标记的移动物体的问题，使用传统的 3D 标签分配来训练网络会导致许多漏报。我们使用 2D 地面实况 (GT) 标签分配标签，如果没有 3D 伪标签，则忽略它们的 3D 损失：</p><p><img src="/pic/weak3d21.png"></p><p>其中 (u, v) 是图像上的像素，C &#x3D; {(cu, cv)|[(cu, cv) 是 Bj 的中心] ∧ [ bBj ̸&#x3D; ∅]}。如图 3 所示，所提出的 2D 分配确保未标记的对象不被视为负样本。</p><p><img src="/pic/weak3d22.png" alt="图 3：2D 和 3D 标签分配之间的比较。对于完全监督设置中使用的 3D 标签分配，移动对象是负样本。然而，我们的 2D 标签分配将它们保留为正样本。"></p><p>弱监督的另一个困难是区分物体是朝前还是朝后。 3D 伪标签的方向可能与真实航向存在 180° 的偏差。我们修改了原始的 MultiBin 方向损失（Mousavian et al., 2017）来缓解这个问题。新的方向损失是原始损失和 180° 反转损失中最小的：</p><p><img src="/pic/weak3d23.png"></p><p>其中 LM 是 MultiBin Loss，^ry 是偏航预测，ry 是伪地面实况。</p><p>为了进一步细化广义对象框，我们用预测作为更新的伪标签迭代地重新训练检测器。我们采用重新训练策略，使用 3D 伪标签进行初始训练，并使用上次迭代的预测更新标签:</p><p><img src="/pic/weak3d24.png"></p><p>其中eD（X，Y）是具有生成的伪标签的数据集，D（l）（X，Y）是具有预测标签的数据集，（l）表示第l次自训练迭代。请注意，我们不会保留每次自重新训练迭代的最后网络参数，并为相同的 κ epochs训练网络 Nθ。</p><p><strong>从近处到远处的物体的概括</strong>。单目 3D 物体检测器面临的一个自然挑战是远处物体的像素较少，因此很难估计其 3D 位置，尤其是深度估计。受 BA-Det (He et al., 2023) 的启发，我们利用几何时间聚合将近距离物体推广到远处物体。具体来说，我们使用以对象为中心的时间对应学习（OTCL）模块来学习对象的以对象为中心的特征对应关系，并解决跟踪对象预测{eB1 j，eB2 j，……，eBT之间的对象中心束调整（OBA） j } 在推理过程中由 Nθ 得出。</p><h4 id="结果"><a href="#结果" class="headerlink" title="结果"></a>结果</h4><p>表 1：WOD val 集的主要结果。 “3D Sup.”是指带有 3D 标签的视频序列的比例。 “Tem.”是指使用时间对象检测器。 †：使用 1&#x2F;3 帧进行训练。<br><img src="/pic/weak3d25.png"></p><p>表2：KITTI检测基准测试集的结果。 “Raw”是指使用 KITTI Raw 集中的图像。 “Mod.”是指中等难度。<br><img src="/pic/weak3d26.png"></p>]]></content>
      
      
      <categories>
          
          <category> DL </category>
          
      </categories>
      
      
        <tags>
            
            <tag> CV </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>第三届中国三维视觉大会China3DV 2024（三维检测Poster）</title>
      <link href="/2024/04/21/3dv-object-det/"/>
      <url>/2024/04/21/3dv-object-det/</url>
      
        <content type="html"><![CDATA[<h1 id="第三届中国三维视觉大会China3DV-2024（三维检测Poster）"><a href="#第三届中国三维视觉大会China3DV-2024（三维检测Poster）" class="headerlink" title="第三届中国三维视觉大会China3DV 2024（三维检测Poster）"></a>第三届中国三维视觉大会China3DV 2024（三维检测Poster）</h1><blockquote><p>第三届中国三维视觉大会(China3DV 2024)由中国图象图形学学会(CSIG)主办，CSIG三维视觉专委会承办，哈尔滨工业大学（深圳）协办，于2024年4月19-21日在深圳线下召开</p></blockquote><p>这里主要记录几个大会展出的关于三维检测相关的工作（侵删）</p><p><img src="/pic/3DV1.jpg"></p><p><img src="/pic/3DV2.jpg"></p><p><img src="/pic/3DV3.jpg"></p><p><img src="/pic/3DV4.jpg"></p><p>具体文章可以自行搜索获取</p>]]></content>
      
      
      <categories>
          
          <category> 3DV </category>
          
      </categories>
      
      
        <tags>
            
            <tag> CV </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>自动驾驶数据集调查：数据统计、标注与展望</title>
      <link href="/2024/04/10/ad-datasets/"/>
      <url>/2024/04/10/ad-datasets/</url>
      
        <content type="html"><![CDATA[<h1 id="A-Survey-on-Autonomous-Driving-Datasets-Data-Statistic-Annotation-and-Outlook-自动驾驶数据集调查：数据统计、标注与展望"><a href="#A-Survey-on-Autonomous-Driving-Datasets-Data-Statistic-Annotation-and-Outlook-自动驾驶数据集调查：数据统计、标注与展望" class="headerlink" title="A Survey on Autonomous Driving Datasets: Data Statistic, Annotation, and Outlook(自动驾驶数据集调查：数据统计、标注与展望)"></a>A Survey on Autonomous Driving Datasets: Data Statistic, Annotation, and Outlook(自动驾驶数据集调查：数据统计、标注与展望)</h1><p><a href="https://arxiv.org/abs/2401.01454">论文链接</a><br><a href="https://www.semanticscholar.org/paper/A-Survey-on-Autonomous-Driving-Datasets%3A-Data-and-Liu-Yurtsever/0902d431c6aa364f1387ad013f4a2fb492f38fb9">系列论文链接</a></p><p><a href="https://github.com/MingyuLiu1/autonomous_driving_datasets">代码链接</a></p><h2 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h2><p>由于硬件和深度学习技术的最新进展，自动驾驶得到了迅速发展并显示出令人鼓舞的性能。高质量的数据集是开发可靠的自动驾驶算法的基础。以前的数据集调查要么集中在有限的数量上，要么缺乏对数据集特征的详细调查。为此，我们从多个角度对 265 个自动驾驶数据集进行了详尽的研究，包括传感器模式、数据大小、任务和上下文条件。我们引入了一种新颖的指标来评估数据集的影响，这也可以作为创建新数据集的指南。此外，我们还分析了标注流程、现有标注工具以及数据集的标注质量，表明建立标准标注流程的重要性。另一方面，我们深入分析了地理和敌对环境条件对自动驾驶系统性能的影响。此外，我们还展示了几个重要数据集的数据分布，并相应地讨论了它们的优缺点。最后，我们讨论了当前自动驾驶数据集的挑战和未来的发展趋势。</p><h2 id="介绍"><a href="#介绍" class="headerlink" title="介绍"></a>介绍</h2><p>自主驾驶（AD）旨在通过创造能够准确感知环境、做出智能决策并在无需人工干预的情况下安全驾驶的车辆来彻底改变交通系统。由于令人兴奋的技术发展，各种自动驾驶产品已在多个领域实现，例如robotaxis [1]。自动驾驶的这些快速进步在很大程度上依赖于广泛的数据集，这有助于自动驾驶系统在复杂的驾驶环境中保持稳健和可靠。</p><p>近年来，自动驾驶数据集的质量和种类显着增加。数据集开发中第一个明显的现象是各种数据收集策略，包括由模拟器生成并从现实世界记录的合成数据集 [2]-[12] [13]-[29]，仅举几例。其次，数据集的组成各不相同，包括但不限于相机图像和激光雷达点云等多种传感器模式、各种任务的不同注释类型以及数据分布。图 1 描绘了 6 个著名的现实世界数据集（Argoverse 2 [28]、KITTI [13]、nuScenes [22]、ONCE [30]、Waymo [23] 和 ZOD [ 31]）在鸟瞰图（BEV）下，突出显示每个数据集的独特注释特征。传感器安装位置还反映了数据集的各种传感领域，包括机载、车辆到万物 (V2X) 或无人机领域。数据集的几何多样性和不同的天气条件也增强了自动驾驶数据集的通用性。</p><p><img src="/pic/ADD1.png" alt="图 1. 数据集鸟瞰对象分布。每个热图代表一个数据集，并使用 X 和 Y 坐标绘制。 Y 是自我车辆的行驶方向。每个数据集的独特注释特征反映在分布范围、密度和边界框的数量上。"></p><p><strong>A. 研究差距和动机</strong></p><p>我们在图2中展示了每年发布的感知数据集数量，以说明自动驾驶数据集的增长趋势。鉴于公开数据集数量庞大且不断增长，对这些资源的全面调查对于推进自动驾驶的学术和工业研究非常有价值。在之前的工作中，Yin 等人。 [32] 总结了 27 个公开可用的数据集，其中包含在公共道路上收集的数据。作为[32]的后续工作，[33]扩展了数据集的数量。郭等人。 [34] 和贾奈等人。 [35]从应用角度对现有数据集进行了系统介绍。除了描述现有数据集之外，Liu 等人。 [36]讨论了合成数据和真实数据之间的领域适应以及自动标记方法。李等人。 [37]总结了现有数据集并对下一代数据集的特征进行了详尽的分析。但这些调查仅总结了少量数据集，范围不广。 ADDataset[38]收集了大量数据集，但缺乏对这些数据集属性的详细分析。与通用数据集的研究相比，一些研究人员对特定类型的自动驾驶数据集进行了调查，例如异常检测[39]、合成数据集[40]、3D语义分割[41]或决策[42]。此外，一些特定任务的调查[43]、[44]也组织了相关的AD数据集。</p><p><img src="/pic/ADD2.png" alt="我们从以下角度将我们的调查论文与其他自动驾驶数据集调查进行比较：收集的数据集数量（#DATASET）、相关任务、传感域（S. DOMAIN）、传感器模态（S. MODA.）、几何条件（GEO.）、环境条件（ENV.），分析数据分布，介绍注释质量和过程。在环境条件方面，我们指的是天气条件和照明的变化。几何条件包括情景类型和地理范围。我们粗粒度地描述任务类型，包括感知（PERC.）、预测（PRED.）、规划（PL.）、控制（C.）、0和端到端（E2E）。"></p><p>在这项工作中，我们对自动驾驶中的大量数据集进行了全面、系统的调查。我们将我们的调查与表一中的其他调查进行比较。我们的调查涵盖了从感知到控制的所有任务，考虑了现实世界和合成数据，并提供了对几个关键数据集的数据模式和质量的见解。</p><p><strong>B. 主要贡献</strong></p><p>本文的主要贡献可概括如下： </p><ul><li>我们概述了迄今为止记录的最详尽的自动驾驶数据集调查。我们尽可能全面地展示公开可用的数据集，记录其基本特征，例如发布年份、数据大小、传感器模式、传感域、几何和环境条件以及支持任务。</li><li>我们系统地说明了用于收集AD 数据的传感器和传感域。此外，我们描述了自动驾驶的主要任务，包括任务目标、所需的数据模式和评估指标。 </li><li>我们根据数据集的传感领域和相关任务对数据集进行分类，使研究人员能够有效地识别和编译目标数据集的信息。这种方法有利于更加集中和富有成效的研究和开发工作。 </li><li>此外，我们引入了影响力评分指标来评估已发布感知数据集的影响力。该指标还可以作为开发未来数据集的指南。我们深入分析具有高影响力分数的数据集，突出它们的优势和实用性。</li><li>我们研究数据集的注释质量以及各种自动驾驶任务的现有标记程序。</li><li>我们详细的数据统计从不同角度展示了各种数据集的数据分布，展示了其固有的局限性和合适的用例。</li><li>我们分析最新技术趋势和下一代数据集的发展方向，例如将语言集成到AD数据中、使用视觉语言模型生成AD数据、标准化数据创建以及促进开放数据生态系统。</li></ul><p><img src="/pic/ADD3.png" alt="图 3。我们介绍了传感器模式，以便直观地了解每个传感器的特性。 (a) 来自 nuScenes [22]，(b) 来自 KITTI [13]，(c) 来自 [46]，(d) 来自 [47]，(e) 来自 [48]。所有数据均来自数据集的开源数据。"></p><p><img src="/pic/ADD4.png" alt="图 4. 自动驾驶管线概览。自动驾驶系统可分为两种类型：模块化和端到端。这两种类型都依赖于安装在车辆或基础设施上的各种传感器收集的数据。这些系统在驾驶场景中与周围环境交互并做出响应。"></p><p><img src="/pic/ADD5.png" alt="图 5. 各种自动驾驶任务的示例。 (a) 来自 KITTI [13]，(b) 来自 Cityscapes [14]，(c) 来自 V2X-Seq [56]，(d) 来自 BDD100K [24]，(e) 来自 Refer-KITTI [57]，(f) 来自 KITTI-360 [58]，(g) 来自 Dr(eye)ve [59]，(h) 来自 TUMTraf [60]。所有数据均来自数据集的开源数据或托管数据集的网站。"></p><p><img src="/pic/ADD6.png" alt="图 6. 时间概览。我们显示了按影响分数排名的前 50 个数据集。该图涵盖了 2009 年至 2024 年 3 月 17 日发布的数据集。"></p><p><img src="/pic/ADD7.png" alt="图 7.高影响力的感知数据集。为了进行更全面的演示，我们展示了来自不同传感领域的 50 个感知数据集，而不是分数最高的数据集。由于这一点的快速变化，表中未显示每个数据集的引文数。"></p><p><img src="/pic/ADD7.png" alt="图 8.预测、规划和控制数据集。我们展示了与预测、规划和控制相关的几个关键数据集。 BP：行为预测、DAP：驾驶员注意力预测、IP：意图预测、MP：运动预测、TP：轨迹预测、MPLAN：运动规划、DM：决策、OT：目标跟踪、QA：问答、DBP：驾驶员行为识别"></p>]]></content>
      
      
      <categories>
          
          <category> AD </category>
          
      </categories>
      
      
        <tags>
            
            <tag> ad </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>3D稀疏卷积理解与使用</title>
      <link href="/2024/03/17/spconv/"/>
      <url>/2024/03/17/spconv/</url>
      
        <content type="html"><![CDATA[<h1 id="3D稀疏卷积理解与使用"><a href="#3D稀疏卷积理解与使用" class="headerlink" title="3D稀疏卷积理解与使用"></a>3D稀疏卷积理解与使用</h1><p><a href="https://zhuanlan.zhihu.com/p/382365889">通俗易懂的解释Sparse Convolution过程</a><br><a href="https://blog.csdn.net/zxyOVO/article/details/130029360">sparse conv稀疏卷积</a><br><a href="https://blog.csdn.net/weixin_42905141/article/details/127842465?ops_request_misc=&request_id=&biz_id=102&utm_term=spconv&utm_medium=distribute.pc_search_result.none-task-blog-2~all~sobaiduweb~default-1-127842465.nonecase&spm=1018.2226.3001.4187">3d稀疏卷积——spconv源码剖析系列（1-6）</a><br><a href="https://zhuanlan.zhihu.com/p/438209175">3D稀疏卷积粗略理解</a><br><a href="https://github.com/traveller59/spconv/blob/master/docs/USAGE.md">spconv官方使用技巧描述</a><br><a href="https://github.com/traveller59/spconv/tree/master">spconv官方源代码</a></p><h2 id="常用的稀疏卷积分类"><a href="#常用的稀疏卷积分类" class="headerlink" title="常用的稀疏卷积分类"></a>常用的稀疏卷积分类</h2><table><thead><tr><th>Layer APIs</th><th align="center">Common Usage</th><th align="right">Dense Version</th><th align="right">Note</th></tr></thead><tbody><tr><td><code>spconv.SparseConv3d</code></td><td align="center">Downsample</td><td align="right"><code>nn.Conv3d</code></td><td align="right">Use <code>indice_key</code> to save data for inverse</td></tr><tr><td><code>spconv.SubMConv3d</code></td><td align="center">Convolution</td><td align="right">N&#x2F;A</td><td align="right">Use <code>indice_key</code> to save data for reuse</td></tr><tr><td><code>spconv.SparseInverseConv3d</code></td><td align="center">Upsample</td><td align="right">N&#x2F;A</td><td align="right">Use pre-saved <code>indice_key</code> to upsample</td></tr><tr><td><code>spconv.SparseConvTranspose3d</code></td><td align="center">Upsample (for generative model)</td><td align="right"><code>nn.ConvTranspose3d</code></td><td align="right">VERY SLOW and CAN’T RECOVER ORIGIN POINT CLOUD</td></tr><tr><td><code>spconv.SparseMaxPool3d</code></td><td align="center">Downsample</td><td align="right"><code>nn.MaxPool3d</code></td><td align="right">Use <code>indice_key</code> to save data for inverse</td></tr><tr><td><code>spconv.SparseSequential</code></td><td align="center">Container</td><td align="right"><code>nn.Sequential</code></td><td align="right">support layers above and <code>nn.ReLU, nn.BatchNorm, ...</code></td></tr><tr><td><code>spconv.SparseGlobalMaxPool</code></td><td align="center">global pool</td><td align="right">N&#x2F;A</td><td align="right">return dense tensor instead of SparseConvTensor</td></tr><tr><td><code>spconv.SparseGlobalAvgPool</code></td><td align="center">global pool</td><td align="right">N&#x2F;A</td><td align="right">return dense tensor instead of SparseConvTensor</td></tr></tbody></table><h2 id="如何使用"><a href="#如何使用" class="headerlink" title="如何使用"></a>如何使用</h2><h3 id="初始化一个SparseConvTensor"><a href="#初始化一个SparseConvTensor" class="headerlink" title="初始化一个SparseConvTensor"></a>初始化一个SparseConvTensor</h3><ul><li>特征：[N, num_channels]</li><li>索引：[N, (batch_idx + x + y + z)]带有批处理轴的坐标张量，坐标 xyz 顺序必须匹配空间形状和卷积参数</li></ul><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">import</span> spconv<span class="token punctuation">.</span>pytorch <span class="token keyword">as</span> spconvfeatures <span class="token operator">=</span> <span class="token comment"># 储存密集的feature [N, num_channels]</span>indices <span class="token operator">=</span> <span class="token comment"># 储存每个feature对应的voxel坐标系下的坐标 [N, ndim + 1], batch 索引必须放在indices[:, 0]</span>spatial_shape <span class="token operator">=</span> <span class="token comment"># 稀疏张量的空间形状, spatial_shape[i] 是indices[:, 1 + i]的形状.如spatial_shape=[60,1000,1000]代表z,y,x的空间形状，则indices[:,1:4]代表z,y,x坐标信息</span>batch_size <span class="token operator">=</span> <span class="token comment"># 稀疏张量的batch_size.</span>x <span class="token operator">=</span> spconv<span class="token punctuation">.</span>SparseConvTensor<span class="token punctuation">(</span>features<span class="token punctuation">,</span> indices<span class="token punctuation">,</span> spatial_shape<span class="token punctuation">,</span> batch_size<span class="token punctuation">)</span>x_dense_NCHW <span class="token operator">=</span> x<span class="token punctuation">.</span>dense<span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token comment"># 转换稀疏张量成dense NCHW tensor.</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><blockquote><p>features, indices, spatial_shape, batch_size传入之后都是可以在外部进行调用的，比如有SparseConvTensor对象sinput，使用sinput.dense(),sinput.features,sinput.indices等即可取出数据。</p></blockquote><h3 id="使用两种3D稀疏卷积"><a href="#使用两种3D稀疏卷积" class="headerlink" title="使用两种3D稀疏卷积"></a>使用两种3D稀疏卷积</h3><p>常用的两种3D稀疏卷积SparseConv3d(稀疏卷积)和SubMConv3d(子流形卷积)：</p><ul><li>普通的卷积一样，只要kernel 覆盖一个 active input site，就可以计算出output site，对应论文<a href="https://pdfs.semanticscholar.org/5125/a16039cabc6320c908a4764f32596e018ad3.pdf">SECOND: Sparsely Embedded Convolutional Detection</a></li><li>只有当kernel的中心覆盖一个 active input site时，卷积输出才会被计算。对应论文<a href="https://arxiv.org/pdf/1711.10275.pdf">3D Semantic Segmentation with Submanifold Sparse Convolutional Networks</a></li></ul><p>SubMConv3d输入与输出feature map上不为空的位置相同，保持了稀疏性(sparity&#x3D;不为空的位置&#x2F;所有位置和），也就保持了计算量。而SparseConv3d会增加稀疏性，从而也就增加了计算量。但是如果只用SubMConv3d，卷积核的感受野会限制在一定范围内，所以要结合stride&#x3D;2的SparseConv3d一起使用，在尽量保持稀疏性的同时增大感受野。</p><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">class</span> <span class="token class-name">SparseConv3d</span><span class="token punctuation">(</span>SparseConvolution<span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span>                 in_channels<span class="token punctuation">,</span>                  out_channels<span class="token punctuation">,</span>                  kernel_size<span class="token punctuation">,</span>                  stride<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">,</span>                  padding<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">,</span>                  dilation<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">,</span>                  groups<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">,</span>                  bias<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span>                  indice_key<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">,</span>                  use_hash<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">,</span>                  algo<span class="token operator">=</span>ops<span class="token punctuation">.</span>ConvAlgo<span class="token punctuation">.</span>Native<span class="token punctuation">)</span><span class="token punctuation">:</span><span class="token keyword">class</span> <span class="token class-name">SubMConv3d</span><span class="token punctuation">(</span>SparseConvolution<span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span>                 in_channels<span class="token punctuation">,</span>                 out_channels<span class="token punctuation">,</span>                 kernel_size<span class="token punctuation">,</span>                 stride<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">,</span>                 padding<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">,</span>                 dilation<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">,</span>                 groups<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">,</span>                 bias<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span>                 indice_key<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">,</span>                 algo<span class="token punctuation">:</span> Optional<span class="token punctuation">[</span>ConvAlgo<span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token boolean">None</span><span class="token punctuation">,</span>                 fp32_accum<span class="token punctuation">:</span> Optional<span class="token punctuation">[</span><span class="token builtin">bool</span><span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token boolean">None</span><span class="token punctuation">,</span>                 name<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">)</span><span class="token punctuation">:</span><span class="token keyword">class</span> <span class="token class-name">SparseSequential</span><span class="token punctuation">(</span>SparseModule<span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> <span class="token operator">*</span>args<span class="token punctuation">,</span> <span class="token operator">**</span>kwargs<span class="token punctuation">)</span><span class="token punctuation">:</span>        <span class="token builtin">super</span><span class="token punctuation">(</span>SparseSequential<span class="token punctuation">,</span> self<span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>        <span class="token keyword">if</span> <span class="token builtin">len</span><span class="token punctuation">(</span>args<span class="token punctuation">)</span> <span class="token operator">==</span> <span class="token number">1</span> <span class="token keyword">and</span> <span class="token builtin">isinstance</span><span class="token punctuation">(</span>args<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">,</span> OrderedDict<span class="token punctuation">)</span><span class="token punctuation">:</span>            <span class="token keyword">for</span> key<span class="token punctuation">,</span> module <span class="token keyword">in</span> args<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">.</span>items<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>                self<span class="token punctuation">.</span>add_module<span class="token punctuation">(</span>key<span class="token punctuation">,</span> module<span class="token punctuation">)</span>        <span class="token keyword">else</span><span class="token punctuation">:</span>            <span class="token keyword">for</span> idx<span class="token punctuation">,</span> module <span class="token keyword">in</span> <span class="token builtin">enumerate</span><span class="token punctuation">(</span>args<span class="token punctuation">)</span><span class="token punctuation">:</span>                self<span class="token punctuation">.</span>add_module<span class="token punctuation">(</span><span class="token builtin">str</span><span class="token punctuation">(</span>idx<span class="token punctuation">)</span><span class="token punctuation">,</span> module<span class="token punctuation">)</span>        <span class="token keyword">for</span> name<span class="token punctuation">,</span> module <span class="token keyword">in</span> kwargs<span class="token punctuation">.</span>items<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>            <span class="token keyword">if</span> sys<span class="token punctuation">.</span>version_info <span class="token operator">&lt;</span> <span class="token punctuation">(</span><span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">6</span><span class="token punctuation">)</span><span class="token punctuation">:</span>                <span class="token keyword">raise</span> ValueError<span class="token punctuation">(</span><span class="token string">"kwargs only supported in py36+"</span><span class="token punctuation">)</span>            <span class="token keyword">if</span> name <span class="token keyword">in</span> self<span class="token punctuation">.</span>_modules<span class="token punctuation">:</span>                <span class="token keyword">raise</span> ValueError<span class="token punctuation">(</span><span class="token string">"name exists."</span><span class="token punctuation">)</span>            self<span class="token punctuation">.</span>add_module<span class="token punctuation">(</span>name<span class="token punctuation">,</span> module<span class="token punctuation">)</span>        self<span class="token punctuation">.</span>_sparity_dict <span class="token operator">=</span> <span class="token punctuation">&#123;</span><span class="token punctuation">&#125;</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h3 id="使用spconv-SparseSequential构建3D稀疏卷积网络更便捷"><a href="#使用spconv-SparseSequential构建3D稀疏卷积网络更便捷" class="headerlink" title="使用spconv.SparseSequential构建3D稀疏卷积网络更便捷"></a>使用spconv.SparseSequential构建3D稀疏卷积网络更便捷</h3><p>类似于<a href="https://blog.csdn.net/zxyOVO/article/details/128486116?csdn_share_tail=%7B%22type%22:%22blog%22,%22rType%22:%22article%22,%22rId%22:%22128486116%22,%22source%22:%22zxyOVO%22%7D">nn.Sequential</a>,都是将多个模块连接起来，将上一个模块的输出作为输入传入下一个模块。且SparseSequential是可以传入torch.nn中的模块的，内部做了封装，包括取出feature，feature赋值replace_feature等等，直接将SparseConvTensor给传入torch.nn的模块是不行的<br>，用于实现不同的操作，使用方法如下面三种:</p><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token comment"># Example of using Sequential</span>model <span class="token operator">=</span> SparseSequential<span class="token punctuation">(</span>            SparseConv2d<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span><span class="token number">20</span><span class="token punctuation">,</span><span class="token number">5</span><span class="token punctuation">)</span><span class="token punctuation">,</span>            nn<span class="token punctuation">.</span>ReLU<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span>            SparseConv2d<span class="token punctuation">(</span><span class="token number">20</span><span class="token punctuation">,</span><span class="token number">64</span><span class="token punctuation">,</span><span class="token number">5</span><span class="token punctuation">)</span><span class="token punctuation">,</span>            nn<span class="token punctuation">.</span>ReLU<span class="token punctuation">(</span><span class="token punctuation">)</span>        <span class="token punctuation">)</span><span class="token comment"># Example of using Sequential with OrderedDict</span>model <span class="token operator">=</span> SparseSequential<span class="token punctuation">(</span>OrderedDict<span class="token punctuation">(</span><span class="token punctuation">[</span>            <span class="token punctuation">(</span><span class="token string">'conv1'</span><span class="token punctuation">,</span> SparseConv2d<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span><span class="token number">20</span><span class="token punctuation">,</span><span class="token number">5</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">,</span>            <span class="token punctuation">(</span><span class="token string">'relu1'</span><span class="token punctuation">,</span> nn<span class="token punctuation">.</span>ReLU<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">,</span>            <span class="token punctuation">(</span><span class="token string">'conv2'</span><span class="token punctuation">,</span> SparseConv2d<span class="token punctuation">(</span><span class="token number">20</span><span class="token punctuation">,</span><span class="token number">64</span><span class="token punctuation">,</span><span class="token number">5</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">,</span>            <span class="token punctuation">(</span><span class="token string">'relu2'</span><span class="token punctuation">,</span> nn<span class="token punctuation">.</span>ReLU<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span>        <span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token comment"># Example of using Sequential with kwargs(python 3.6+)</span>model <span class="token operator">=</span> SparseSequential<span class="token punctuation">(</span>            conv1<span class="token operator">=</span>SparseConv2d<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span><span class="token number">20</span><span class="token punctuation">,</span><span class="token number">5</span><span class="token punctuation">)</span><span class="token punctuation">,</span>            relu1<span class="token operator">=</span>nn<span class="token punctuation">.</span>ReLU<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span>            conv2<span class="token operator">=</span>SparseConv2d<span class="token punctuation">(</span><span class="token number">20</span><span class="token punctuation">,</span><span class="token number">64</span><span class="token punctuation">,</span><span class="token number">5</span><span class="token punctuation">)</span><span class="token punctuation">,</span>            relu2<span class="token operator">=</span>nn<span class="token punctuation">.</span>ReLU<span class="token punctuation">(</span><span class="token punctuation">)</span>        <span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h3 id="replace-feature方法"><a href="#replace-feature方法" class="headerlink" title="replace_feature方法"></a>replace_feature方法</h3><p>实际上就是将一个tensor读入，替换原来的feature，比如</p><p>对于F.relu，我们要传入features，之前要这样写</p><pre class="line-numbers language-python" data-language="python"><code class="language-python">x<span class="token punctuation">.</span>features <span class="token operator">=</span> F<span class="token punctuation">.</span>relu<span class="token punctuation">(</span>x<span class="token punctuation">.</span>features<span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>但这样写会导致torch.fx出问题，replace_feature就是为了解决这个问题而出现的</p><pre class="line-numbers language-python" data-language="python"><code class="language-python">x <span class="token operator">=</span> x<span class="token punctuation">.</span>replace_feature<span class="token punctuation">(</span>F<span class="token punctuation">.</span>relu<span class="token punctuation">(</span>x<span class="token punctuation">.</span>features<span class="token punctuation">)</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>注意该函数并非在x上进行更改了，而是创建了一个全新的SparseConvTensor，也就是说要用x去接收返回值</p><p>下面给一个示例：</p><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> x<span class="token punctuation">)</span><span class="token punctuation">:</span>    identity <span class="token operator">=</span> self<span class="token punctuation">.</span>layers_in<span class="token punctuation">(</span>x<span class="token punctuation">)</span>    output <span class="token operator">=</span> self<span class="token punctuation">.</span>layers<span class="token punctuation">(</span>x<span class="token punctuation">)</span>    <span class="token keyword">return</span> output<span class="token punctuation">.</span>replace_feature<span class="token punctuation">(</span>F<span class="token punctuation">.</span>leaky_relu<span class="token punctuation">(</span>output<span class="token punctuation">.</span>features <span class="token operator">+</span> identity<span class="token punctuation">.</span>features<span class="token punctuation">,</span> <span class="token number">0.1</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span></span></code></pre>]]></content>
      
      
      <categories>
          
          <category> 稀疏卷积 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> spconv </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>室外场景的点云对比学习方法(CO^3，结合代码)</title>
      <link href="/2024/03/15/point-contrast/"/>
      <url>/2024/03/15/point-contrast/</url>
      
        <content type="html"><![CDATA[<h1 id="室外场景的点云对比学习方法-CO-3，结合代码"><a href="#室外场景的点云对比学习方法-CO-3，结合代码" class="headerlink" title="室外场景的点云对比学习方法(CO^3，结合代码)"></a>室外场景的点云对比学习方法(CO^3，结合代码)</h1><p><a href="https://zhuanlan.zhihu.com/p/346686467">对比学习（Contrastive Learning）方法综述</a><br><a href="https://mp.weixin.qq.com/s/sUAoNXGvwWa6lecq73pyAg">对比学习（Contrastive Learning），必知必会</a><br><a href="https://arxiv.org/abs/2206.04028">论文地址：CO^3: Cooperative Unsupervised 3D Representation Learning for Autonomous Driving</a><br><a href="https://ar5iv.labs.arxiv.org/html/2206.04028#S1.F2">论文主页</a><br><a href="https://github.com/Runjian-Chen/CO3">代码地址</a></p><blockquote><p>作者信息：<a href="https://www.rjchen.site/">陈润健博士</a>、<a href="http://luoping.me/">罗平教授</a></p></blockquote><h2 id="《COˆ3-Cooperative-Unsupervised-3D-Representation-Learning-for-Autonomous-Driving》室外场景的无监督对比学习方法（使用MMDetection3D和OpenPCDet实现）"><a href="#《COˆ3-Cooperative-Unsupervised-3D-Representation-Learning-for-Autonomous-Driving》室外场景的无监督对比学习方法（使用MMDetection3D和OpenPCDet实现）" class="headerlink" title="《COˆ3: Cooperative Unsupervised 3D Representation Learning for Autonomous Driving》室外场景的无监督对比学习方法（使用MMDetection3D和OpenPCDet实现）"></a>《COˆ3: Cooperative Unsupervised 3D Representation Learning for Autonomous Driving》室外场景的无监督对比学习方法（使用MMDetection3D和OpenPCDet实现）</h2><h3 id="动机"><a href="#动机" class="headerlink" title="动机"></a>动机</h3><ul><li>室内场景点云的无监督对比学习取得了巨大的成功。然而，室外场景点云的无监督表示学习仍然具有挑战性</li><li>到目前为止，随机初始化和直接从头开始对详细注释数据进行训练仍然在室外场景点云任务中占据主导地位</li></ul><h3 id="挑战"><a href="#挑战" class="headerlink" title="挑战"></a>挑战</h3><ul><li>如何找到更好的视图来学习室外场景激光雷达点云的表示？</li><li>之前的方法：<ul><li>室内场景：<ul><li><a href="https://github.com/facebookresearch/PointContrast?tab=readme-ov-file">PointContrast</a> 建议重建整个室内场景，收集两个不同姿势的部分点云，并将它们用作对比学习中的两个视图，以学习密集（点级或体素级）表示</li><li>然而，室外场景是动态的、大规模的，无法重建建筑景观的整个场景</li></ul></li><li>室外场景：<ul><li>将数据增强应用于单帧点云，并将原始版本和增强版本视为不同的视图，然而，点云的所有增强，包括随机下降、旋转和缩放，都可以通过线性变换来实现，并且以这种方式构建的视图没有足够的差异</li><li>将不同时间戳的点云视为不同的视图，然而，移动的物体会很难找到正确的对应关系来进行对比学习，由于这些限制，预训练的 3D 编码器在传输到不同 LiDAR 传感器收集的数据集时无法取得明显的改进。</li></ul></li></ul></li></ul><p><img src="/pic/CO1.png" alt="对比学习中不同方法构建的示例视图。(A)、(B)和(C)使用的采样图像和视点(原始图像的不同放大)。(D)、(E)和(F)显示了在PointContrast中构建的两个视图的示例，这两个视图在重建的点云(D)中以不同的姿势捕获。以这种方式构建的视图差别很大，但同时仍然维护了足够的公共语义信息，包括相同的沙发和桌子。(G)和(H)是室外场景点云的视图。(G)是点云的原始框架，作者将点云增强应用于(G)for(H)，这可以通过简单的线性变换来实现。(G)和(I)所示使用不同时间戳的点云作为对比学习的视图。当其他汽车和行人四处走动时，自动驾驶汽车在十字路口等待，这使得很难找到准确的对应关系来进行对比学习。(J)是采样真实场景的图解，(K)，(L)是从车辆和基础设施方面的视图，COˆ3使用这些视图使视图更适合对比学习。"></p><p>提出了两个模块，分别是协同对比学习方法(Cooperative Contrastive)以及上下文形状预测(Contextual Shape Prediction):</p><ul><li>协同对比学习方法使用的是<a href="https://thudair.baai.ac.cn/index">DAIR-V2X数据集</a>，从基础设施LiDAR和车辆LiDAR的点云来构建用于对比表示学习的两个视图(不同位置相同时间戳获取足够不同并共享足够语义信息)；</li><li>上下文形状预测使用密集表示来重建相邻点的局部分布，使用的是形状上下文来描述每个点邻域的局部分布，预训练的任务是利用提取的点级或体素级表示来预测每个点或体素的局部分布，论文说这种精细化的重建预训练任务引入了更多与任务相关的信息，并有助于学习更好的表征</li><li>最后将COˆ3在DAIR-V2X数据集上训练的预训练模型中的稀疏卷积的3D编码器主干网络应用到不同数据集（ONCE，KITTI，NuScenes，Waymo）下游任务三维目标检测（SECOND，PV-RCNN，CenterPoint）中，发现效果都有提升。</li></ul><p><img src="/pic/CO2.png" alt="3D目标检测器大致流程。原始输入LiDAR点云首先由3D主干处理，并生成逐点/体素表示，然后将其进一步转换为Bird-EyeView(BEV)地图。最后，应用2D主干和检测头生成检测结果。"></p><p><img src="/pic/CO3.png" alt="CO^3的Pipeline，对基础设施点云进行转换，并预先与车载点云融合。以车载点云和融合点云为输入，首先用3D主干对它们进行处理，生成点/体素级别的表示。在这些密集的表征下，提出了两个训练前的目标：(A)协同对比损失，这为对比学习引入了足够的视角。(B)上下文形状预测损失，以引入有利于下游任务的与任务相关的信息。"></p><h3 id="协同对比损失"><a href="#协同对比损失" class="headerlink" title="协同对比损失"></a>协同对比损失</h3><p>损失的定义如下：</p><p> $$L_{CO_2}&#x3D;\frac1N_1\sum_{n&#x3D;1}^{N_1}-\log(\frac{\exp(z_{veh}^n\cdot z_{fusion}^n&#x2F;\tau)}{\sum_{i &#x3D;1}^{N_1}\exp(z_{veh}^i \cdot z_{fusion}^i &#x2F;\tau)}) \tag{1} $$</p><p> $$ { z_{v&#x2F;f}^n }<em>{n&#x3D;1}^{N_1} \stackrel{sample}{\sim} Z</em>{v&#x2F;f} $$</p><p> $$  Z_{v&#x2F;f}&#x3D;normalize(MLP_1(\textbf{P}_{v&#x2F;f}^{feat^{enc}})) $$ </p><p>其中，车辆和融合点云的嵌入特征，$\textbf{P}<em>{veh} ^{feat^{enc}}$和$\textbf{P}</em>{fusion} ^{feat^{enc}}$，</p><ul><li>首先由多层感知器$\rm MLP_1$投影到公共特征空间，</li><li>然后进行归一化。$Z_{v&#x2F;f}\in\mathbb R^{N_{v&#x2F;f}^{p^{enc}}\times d_1}$是车辆点云和融合点云的投影特征，其中$d_1$表示公共特征空间的维度，$N_{v&#x2F;f}^{p^{enc}}$分别是编码车辆和融合点云的点&#x2F;体素数。</li><li>然后，从$Z_{v&#x2F;f}$中采样$N_1$对特征($z$)进行对比学习。根据的经验观察，基点对对比学习有很大的负面影响。因此，将那些高度值低于阈值$z_{thd}$的点标记为地面点，并在采样时将其过滤掉。</li><li>经过滤波后，<strong>从车辆点云中随机采样N1个点，并在融合点云中找到它们对应的点（或体素），形成N1对点（或体素）。</strong>将对应点（或体素）视为正对，否则视为负对进行对比学习，最终的损失函数如公式（1）所示，其中τ是温度参数。</li></ul><blockquote><p>补充：公式（1）为对比学习损失（常用的是<a href="https://zhuanlan.zhihu.com/p/506544456">InfoNCE loss</a>，在何凯明大佬的论文<a href="https://zhuanlan.zhihu.com/p/555390520">MoCo</a>中也有讲解，跟cross entropy loss相似，唯一的区别是，在cross entropy loss里，$N_1-1$指代的是数据集里类别的数量，而在对比学习InfoNCE loss里，这个$N_1-1$指的是负样本的数量。注意上式分母中的sum是在1个正样本和$N_1-1$个负样本上做的，从1到$N_1$，所以共$N_1$个样本）</p></blockquote><h4 id="参考代码"><a href="#参考代码" class="headerlink" title="参考代码"></a>参考代码</h4><p>整个网络的代码<a href="https://github.com/Runjian-Chen/CO3/blob/main/configs/co3_unsupervised_representation_learning/co3.py">配置文件</a>，模型名为LIDAR_3D_ENCODER，由5部分组成，分别是车载点云和融合点云栅格化范围尺寸的配置、栅格编码(简单平均每个Voxel中的点云特征作为Volex的特征)、中间层编码(稀疏卷积)以及训练的配置文件(主要是自监督的两个损失函数的设计)：</p><ul><li>将基础设施点云转化为车载点云坐标下的<a href="https://github.com/Runjian-Chen/CO3/blob/main/tools/create_data.py#L191-L241">代码</a></li><li>转换后的基础设施点云融合车载点云的融合点云<a href="https://github.com/Runjian-Chen/CO3/blob/main/mmdet3d/datasets/pipelines/transforms_3d.py#L1309-L1321">代码</a>，在第一维上直接进行拼接</li><li>根据车载点云和融合点云栅格化范围尺寸进行体素化<a href="https://github.com/Runjian-Chen/CO3/blob/main/mmdet3d/ops/voxel/voxelize.py">代码</a>，返回 voxels: [M, max_points, ndim]、coordinates: [M, 3]、num_points_per_voxel: [M]、voxels_coors：[points.size(0), 3]，表示每个点云属于对应体素的坐标</li><li>通过HardSimpleVFE获取Voxel的特征<a href="https://github.com/Runjian-Chen/CO3/blob/main/mmdet3d/models/voxel_encoders/voxel_encoder.py#L14-L45">代码</a></li><li>使用稀疏卷积(SparseConv3d和SubMConv3d)返回<a href="https://github.com/Runjian-Chen/CO3/blob/main/mmdet3d/models/middle_encoders/sparse_encoder.py">spatial_features, voxel_features, coors</a></li><li>损失函数设计，分别是协同对比损失(cooperative_contrastive_loss)和下面要讲到的上下文形状预测损失(contextual_shape_prediction_loss)，这里先讲<a href="https://github.com/Runjian-Chen/CO3/blob/main/mmdet3d/models/detectors/lidar_3d_encoder.py#L296-L330">前者</a>：<ul><li>从encode_features[-1].dense()到permute(0,2,3,4,1)：N, D, H, W, C到view(-1, C)到一个简单的全连接层(pts_transform_spatial_feature，最后一维是256)到view(N, D, H, W ,256)，即恢复成dense的情况</li><li>最后将车载点云特征和融合点云特征以、对应的Voxel坐标、打乱的融合点云的id以及车载点云(用于获取z轴值来过滤地面点)送进损失函数中<ul><li><a href="https://github.com/Runjian-Chen/CO3/blob/main/mmdet3d/models/losses/cooperative_contrastive_loss.py">损失函数</a>的参数主要为温度系数，采样数量，是否过滤地面点等，这里进行简略的介绍</li></ul></li></ul></li></ul><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">if</span> self<span class="token punctuation">.</span>fileter_ground_points<span class="token punctuation">:</span>    none_ground_idx <span class="token operator">=</span> torch<span class="token punctuation">.</span>where<span class="token punctuation">(</span>vehicle_point<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span><span class="token number">2</span><span class="token punctuation">]</span> <span class="token operator">></span> <span class="token operator">-</span><span class="token number">1.6</span><span class="token punctuation">)</span><span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span>    sample_from_none_ground <span class="token operator">=</span> torch<span class="token punctuation">.</span>randperm<span class="token punctuation">(</span>none_ground_idx<span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">[</span><span class="token punctuation">:</span>self<span class="token punctuation">.</span>sample_num<span class="token punctuation">]</span><span class="token punctuation">.</span>to<span class="token punctuation">(</span>vehicle_voxels_coor<span class="token punctuation">.</span>device<span class="token punctuation">)</span>    sampled_idx <span class="token operator">=</span> none_ground_idx<span class="token punctuation">[</span>sample_from_none_ground<span class="token punctuation">]</span><span class="token keyword">else</span><span class="token punctuation">:</span>    sampled_idx <span class="token operator">=</span> torch<span class="token punctuation">.</span>randperm<span class="token punctuation">(</span>vehicle_voxels_coor<span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">[</span><span class="token punctuation">:</span>self<span class="token punctuation">.</span>sample_num<span class="token punctuation">]</span><span class="token punctuation">.</span>to<span class="token punctuation">(</span>vehicle_voxels_coor<span class="token punctuation">.</span>device<span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><ul><li>这部分代码根据一个标志决定是否过滤地面点,并从剩余点中随机采样一定数量的点。</li></ul><pre class="line-numbers language-python" data-language="python"><code class="language-python">sampled_vehicle_voxels_coor <span class="token operator">=</span> vehicle_voxels_coor<span class="token punctuation">[</span>sampled_idx<span class="token punctuation">]</span>sampled_vehicle_D<span class="token punctuation">,</span> sampled_vehicle_H<span class="token punctuation">,</span> sampled_vehicle_W <span class="token operator">=</span> sampled_vehicle_voxels_coor<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">,</span> sampled_vehicle_voxels_coor<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">,</span> sampled_vehicle_voxels_coor<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span><span class="token number">2</span><span class="token punctuation">]</span>sampled_vehicle_voxel_feat <span class="token operator">=</span> dense_vehicle_voxel_feat<span class="token punctuation">[</span>sampled_vehicle_D<span class="token punctuation">,</span> sampled_vehicle_H<span class="token punctuation">,</span> sampled_vehicle_W<span class="token punctuation">,</span> <span class="token punctuation">:</span><span class="token punctuation">]</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre><ul><li>这几行根据采样的点索引提取出对应的体素坐标和特征。</li></ul><pre class="line-numbers language-python" data-language="python"><code class="language-python">sampled_fusion_points_shuffled_id <span class="token operator">=</span> torch<span class="token punctuation">.</span>where<span class="token punctuation">(</span><span class="token punctuation">(</span>fusion_points_shuffled_id<span class="token punctuation">.</span>unsqueeze<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">)</span> <span class="token operator">-</span> sampled_idx<span class="token punctuation">.</span>unsqueeze<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token operator">==</span><span class="token number">0</span><span class="token punctuation">)</span><span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span>sampled_fusion_voxels_coor <span class="token operator">=</span> fusion_voxels_coor<span class="token punctuation">[</span>sampled_fusion_points_shuffled_id<span class="token punctuation">]</span>sampled_fusion_D<span class="token punctuation">,</span> sampled_fusion_H<span class="token punctuation">,</span> sampled_fusion_W <span class="token operator">=</span> sampled_fusion_voxels_coor<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">,</span> sampled_fusion_voxels_coor<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">,</span> sampled_fusion_voxels_coor<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">]</span>sampled_fusion_voxel_feat <span class="token operator">=</span> dense_fusion_voxel_feat<span class="token punctuation">[</span>sampled_fusion_D<span class="token punctuation">,</span> sampled_fusion_H<span class="token punctuation">,</span> sampled_fusion_W<span class="token punctuation">,</span> <span class="token punctuation">:</span><span class="token punctuation">]</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span></span></code></pre><ul><li>这部分代码根据采样的点索引提取出对应的融合体素坐标和特征。</li></ul><pre class="line-numbers language-python" data-language="python"><code class="language-python">sampled_vehicle_voxel_feat <span class="token operator">=</span> torch<span class="token punctuation">.</span>nn<span class="token punctuation">.</span>functional<span class="token punctuation">.</span>normalize<span class="token punctuation">(</span>sampled_vehicle_voxel_feat<span class="token punctuation">,</span> dim<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span>sampled_fusion_voxel_feat <span class="token operator">=</span> torch<span class="token punctuation">.</span>nn<span class="token punctuation">.</span>functional<span class="token punctuation">.</span>normalize<span class="token punctuation">(</span>sampled_fusion_voxel_feat<span class="token punctuation">,</span> dim<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><ul><li>这两行对采样的特征进行归一化处理。</li></ul><pre class="line-numbers language-python" data-language="python"><code class="language-python">s <span class="token operator">=</span> torch<span class="token punctuation">.</span>matmul<span class="token punctuation">(</span>sampled_vehicle_voxel_feat<span class="token punctuation">,</span> sampled_fusion_voxel_feat<span class="token punctuation">.</span>permute<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">)</span><span class="token punctuation">.</span>contiguous<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><ul><li>这行计算采样的车辆体素特征和融合体素特征之间的相似度矩阵。</li></ul><pre class="line-numbers language-python" data-language="python"><code class="language-python">s_ <span class="token operator">=</span> s<span class="token punctuation">.</span>clone<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>detach<span class="token punctuation">(</span><span class="token punctuation">)</span>s_pos_out <span class="token operator">=</span> torch<span class="token punctuation">.</span>diag<span class="token punctuation">(</span>s_<span class="token punctuation">)</span>s_sum_out <span class="token operator">=</span> torch<span class="token punctuation">.</span><span class="token builtin">sum</span><span class="token punctuation">(</span>s_<span class="token punctuation">,</span> dim<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre><ul><li>这几行从相似度矩阵中提取出正样本相似度和负样本相似度的分量。</li></ul><pre class="line-numbers language-python" data-language="python"><code class="language-python">s <span class="token operator">=</span> s <span class="token operator">/</span> self<span class="token punctuation">.</span>temperatures <span class="token operator">=</span> torch<span class="token punctuation">.</span>exp<span class="token punctuation">(</span>s<span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><ul><li>这两行对相似度矩阵进行温度缩放和指数运算。</li></ul><pre class="line-numbers language-python" data-language="python"><code class="language-python">s_pos <span class="token operator">=</span> torch<span class="token punctuation">.</span>diag<span class="token punctuation">(</span>s<span class="token punctuation">)</span>s_sum <span class="token operator">=</span> torch<span class="token punctuation">.</span><span class="token builtin">sum</span><span class="token punctuation">(</span>s<span class="token punctuation">,</span> dim<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><ul><li>这两行从缩放后的相似度矩阵中提取出正样本相似度和负样本相似度。</li></ul><pre class="line-numbers language-python" data-language="python"><code class="language-python">cooperative_contrastive_loss <span class="token operator">+=</span> torch<span class="token punctuation">.</span>mean<span class="token punctuation">(</span><span class="token operator">-</span><span class="token number">1.0</span> <span class="token operator">*</span> torch<span class="token punctuation">.</span>log<span class="token punctuation">(</span>s_pos <span class="token operator">/</span> s_sum<span class="token punctuation">)</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><ul><li>这行计算当前样本的协作对比损失,并累加到总的损失中。</li></ul><pre class="line-numbers language-python" data-language="python"><code class="language-python">pos_sim<span class="token punctuation">.</span>append<span class="token punctuation">(</span>torch<span class="token punctuation">.</span>mean<span class="token punctuation">(</span>s_pos_out<span class="token punctuation">)</span><span class="token punctuation">.</span>unsqueeze<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">)</span><span class="token punctuation">)</span>neg_sim<span class="token punctuation">.</span>append<span class="token punctuation">(</span>torch<span class="token punctuation">.</span>mean<span class="token punctuation">(</span>s_sum_out <span class="token operator">-</span> s_pos_out<span class="token punctuation">)</span><span class="token punctuation">.</span>unsqueeze<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">)</span> <span class="token operator">/</span> <span class="token punctuation">(</span>self<span class="token punctuation">.</span>sample_num <span class="token operator">-</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><ul><li>这两行分别将当前样本的正样本相似度和负样本相似度存储到对应的列表中。</li></ul><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">if</span> <span class="token builtin">len</span><span class="token punctuation">(</span>pos_sim<span class="token punctuation">)</span> <span class="token operator">></span> <span class="token number">0</span><span class="token punctuation">:</span>    pos_sim <span class="token operator">=</span> torch<span class="token punctuation">.</span>mean<span class="token punctuation">(</span>torch<span class="token punctuation">.</span>cat<span class="token punctuation">(</span>pos_sim<span class="token punctuation">)</span><span class="token punctuation">)</span>    neg_sim <span class="token operator">=</span> torch<span class="token punctuation">.</span>mean<span class="token punctuation">(</span>torch<span class="token punctuation">.</span>cat<span class="token punctuation">(</span>neg_sim<span class="token punctuation">)</span><span class="token punctuation">)</span>    self<span class="token punctuation">.</span>log_var<span class="token punctuation">[</span><span class="token string">'pos_sim'</span><span class="token punctuation">]</span> <span class="token operator">=</span> pos_sim    self<span class="token punctuation">.</span>log_var<span class="token punctuation">[</span><span class="token string">'neg_sim'</span><span class="token punctuation">]</span> <span class="token operator">=</span> neg_sim    <span class="token keyword">del</span> pos_sim    <span class="token keyword">del</span> neg_sim<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><ul><li>这部分代码在遍历完所有样本后,计算平均正样本相似度和平均负样本相似度,并将它们记录到 <code>log_var</code> 字典中,最后释放这两个列表的内存。</li></ul><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">return</span> cooperative_contrastive_loss<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><ul><li>最后,该方法返回计算得到的协作对比损失。</li></ul><h3 id="上下文形状预测损失"><a href="#上下文形状预测损失" class="headerlink" title="上下文形状预测损失"></a>上下文形状预测损失</h3><p>CO^3 旨在学习适用于各种下游数据集的表示。但是公式（1）中的对比损失很难编码任务相关信息，如这个<a href="https://arxiv.org/abs/2203.07004">论文</a>中讲到的所示，作者诉诸于图像上的额外重建目标来引入任务相关信息。然而，对于室外场景点云，用点&#x2F;体素级表示重建整个场景极其困难。为了缓解这个问题，建议用其表示重建每个点&#x2F;体素的邻域。具体来说，上下文形状预测损失设计如下。描述了车辆特征的过程，类似的过程也适用于融合特征，损失函数如下：</p><p> $$ L_{CSP} &#x3D;\frac1N_2\sum_{n&#x3D;1}^{N_2}\sum_{m&#x3D;1}^{N_{bin}}p_{n,m}\log\frac{p_{n,m}}{q_{n,m}} \tag{2}$$</p><p> $$ {p_{n,*}}_{n&#x3D;1}^{N_2} \stackrel{sample}{\sim}P $$</p><p> $$ {q_{n,*}}<em>{n&#x3D;1}^{N_2} \stackrel{sample}{\sim}Q；P&#x3D; softmax(MLP_2(\textbf{P}</em>{veh}^{feat^{enc}})) $$</p><p>其中车辆点云的编码特征$\textbf{P}_{veh}^{feat^{enc}}$ :</p><ul><li>首先通过另一个多层感知器 $MLP_2$，并对投影特征应用 softmax 操作以获得每个点&#x2F;体素的预测局部分布，$N_{veh}^{p^{enc}}$ 是由 3D 编码器嵌入后的车辆侧点&#x2F;体素的数量，$N_{bin}$ 是划分每个点&#x2F;体素的局部邻域的 bin 数。</li><li>在本文中使用 $N_{bin} &#x3D; 32$ 并预先计算“groundtruth”局部形状上下文 （$N_{veh}^p$是原始输入车辆点&#x2F;体素的数量），这将在稍后讨论。</li><li>对于 P 和 Q，从 P 和 Q 中抽取 $N_2$ 个采样点&#x2F;体素。有 $ p_{n,<em>}  \in \mathbb R^{N_{bin}}$和 $ q_{n,</em>}  \in \mathbb R^{N_{bin}}$ 。请注意，这些采样的预测上下文形状分布是成对的。</li><li>最后，如等式（2）中的第一行所示，$L_{CSP} $是应用于 $p_{n,<em>}$和 $q_{n,</em>}$的 KL 散度损失，其中 KL 散度描述了两个概率分布（$p_{n,<em>}$和 $q_{n,</em>}$）之间的距离。</li></ul><blockquote><p>为了计算第 i 个点&#x2F;体素的“ground Truth”形状上下文，首先将点&#x2F;体素的邻域沿 x-y 平面划分为 $N_{bin}$ 个 bin，其中 R1 &#x3D; 0.5m，R2 &#x3D; 4m。然后计算每个 bin 中的点&#x2F;体素的数量，这导致 $N_{bin} &#x3D; 32$ 个数字 $ Q_{i,<em>} ^{raw} \in \mathbb R^{N_{bin}}$ 。接下来，$Qi,</em>$ 最终确定如下，其中 $SF_{CSP}$ 是使分布合理的比例因子。</p></blockquote><p> $$ Q_{i,<em>}&#x3D;softmax(normalize(Q_{i,</em>} ^{raw})\times SF_{CSP})\tag{3} $$</p><blockquote><p>补充：K-L散度是一种量化两种概率分布P和Q之间差异的方式，又叫相对熵，散度越小，说明概率 Q 与概率 P 之间越接近，那么估计的概率分布与真实的概率分布也就越接近。<a href="https://www.jianshu.com/p/43318a3dc715">参考链接1</a>，<a href="https://blog.csdn.net/Poyunji/article/details/123771660">参考链接2</a></p></blockquote><h4 id="参考代码-1"><a href="#参考代码-1" class="headerlink" title="参考代码"></a>参考代码</h4><p>损失函数名为contextual_shape_prediction_loss,<a href="https://github.com/Runjian-Chen/CO3/blob/main/mmdet3d/models/detectors/lidar_3d_encoder.py#L332-L340">代码</a>，输入为vehicle_points(车载点云)、vehicle_voxels_coors（每个车载点云对应的栅格的坐标）、vehicle_spatial_features（车载点云的dense特征,维度是N, D, H, W, C）、vehicle_spatial_feature_voxels_coors（车载栅格特征所对应的点云的栅格坐标，与vehicle_voxels_coors相同）、fusion_spatial_features（融合点云的dense特征,维度是N, D, H, W, C）、fusion_spatial_feature_voxels_coors（融合栅格特征所对应的点云的栅格坐标）、fusion_points（融合点云），<a href="https://github.com/Runjian-Chen/CO3/blob/main/mmdet3d/models/losses/contextual_shape_prediction_loss.py#L202-L314">前向传播代码</a>，主要作用是计算两个方面的 contextual shape prediction loss，分别是车辆端和融合端：</p><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> <span class="token operator">**</span>kwargs<span class="token punctuation">)</span><span class="token punctuation">:</span>    vehicle_points <span class="token operator">=</span> kwargs<span class="token punctuation">[</span><span class="token string">'vehicle_points'</span><span class="token punctuation">]</span>    fusion_points <span class="token operator">=</span> kwargs<span class="token punctuation">[</span><span class="token string">'fusion_points'</span><span class="token punctuation">]</span>    vehicle_voxels_coors <span class="token operator">=</span> kwargs<span class="token punctuation">[</span><span class="token string">'vehicle_voxels_coors'</span><span class="token punctuation">]</span>    vehicle_spatial_features <span class="token operator">=</span> kwargs<span class="token punctuation">[</span><span class="token string">'vehicle_spatial_features'</span><span class="token punctuation">]</span>    vehicle_spatial_feature_voxels_coors <span class="token operator">=</span> kwargs<span class="token punctuation">[</span><span class="token string">'vehicle_spatial_feature_voxels_coors'</span><span class="token punctuation">]</span>    fusion_spatial_features <span class="token operator">=</span> kwargs<span class="token punctuation">[</span><span class="token string">'fusion_spatial_features'</span><span class="token punctuation">]</span>    fusion_spatial_feature_voxels_coors <span class="token operator">=</span> kwargs<span class="token punctuation">[</span><span class="token string">'fusion_spatial_feature_voxels_coors'</span><span class="token punctuation">]</span>    batch_size <span class="token operator">=</span> <span class="token builtin">len</span><span class="token punctuation">(</span>vehicle_points<span class="token punctuation">)</span>    contextual_shape_prediction_loss <span class="token operator">=</span> torch<span class="token punctuation">.</span>zeros<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">.</span>to<span class="token punctuation">(</span>vehicle_points<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">.</span>device<span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><ul><li>获取传入的关键字参数，这些参数包含了车辆端和融合端的相关数据，如车辆点云、融合点云、车辆体素坐标等。</li><li>初始化 contextual shape prediction loss 为零。</li></ul><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">for</span> b <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span>batch_size<span class="token punctuation">)</span><span class="token punctuation">:</span>    vehicle_point <span class="token operator">=</span> vehicle_points<span class="token punctuation">[</span>b<span class="token punctuation">]</span>    vehicle_voxels_coor <span class="token operator">=</span> vehicle_voxels_coors<span class="token punctuation">[</span>b<span class="token punctuation">]</span>    vehicle_spatial_feature <span class="token operator">=</span> vehicle_spatial_features<span class="token punctuation">[</span>b<span class="token punctuation">]</span>    vehicle_spatial_feature_voxels_coor <span class="token operator">=</span> vehicle_spatial_feature_voxels_coors<span class="token punctuation">[</span>b<span class="token punctuation">]</span><span class="token punctuation">.</span><span class="token builtin">long</span><span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token operator">//</span> <span class="token number">8</span>    vehicle_spatial_feature_voxels_coor <span class="token operator">=</span> limit_coor_range<span class="token punctuation">(</span>vehicle_spatial_feature_voxels_coor<span class="token punctuation">,</span> vehicle_spatial_feature<span class="token punctuation">)</span>    fusion_spatial_feature <span class="token operator">=</span> fusion_spatial_features<span class="token punctuation">[</span>b<span class="token punctuation">]</span>    fusion_spatial_feature_voxels_coor <span class="token operator">=</span> fusion_spatial_feature_voxels_coors<span class="token punctuation">[</span>b<span class="token punctuation">]</span><span class="token punctuation">.</span><span class="token builtin">long</span><span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token operator">//</span> <span class="token number">8</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><ul><li>遍历每个批次中的样本。</li><li>获取当前批次中的车辆端和融合端数据，包括车辆点云、车辆体素坐标、车辆特征、融合特征以及相应的体素坐标。</li><li>将车辆空间特征的体素坐标限制在一定范围内。</li></ul><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">if</span> self<span class="token punctuation">.</span>fileter_ground_points<span class="token punctuation">:</span>    none_ground_idx <span class="token operator">=</span> torch<span class="token punctuation">.</span>where<span class="token punctuation">(</span>vehicle_point<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span><span class="token number">2</span><span class="token punctuation">]</span> <span class="token operator">></span> <span class="token operator">-</span><span class="token number">1.6</span><span class="token punctuation">)</span><span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span>    sample_from_none_ground <span class="token operator">=</span> torch<span class="token punctuation">.</span>randperm<span class="token punctuation">(</span>none_ground_idx<span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">[</span><span class="token punctuation">:</span>self<span class="token punctuation">.</span>sample_num<span class="token punctuation">]</span><span class="token punctuation">.</span>to<span class="token punctuation">(</span>vehicle_point<span class="token punctuation">.</span>device<span class="token punctuation">)</span>    sampled_idx <span class="token operator">=</span> none_ground_idx<span class="token punctuation">[</span>sample_from_none_ground<span class="token punctuation">]</span><span class="token keyword">else</span><span class="token punctuation">:</span>    sampled_idx <span class="token operator">=</span> torch<span class="token punctuation">.</span>randperm<span class="token punctuation">(</span>vehicle_point<span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">[</span><span class="token punctuation">:</span>self<span class="token punctuation">.</span>sample_num<span class="token punctuation">]</span><span class="token punctuation">.</span>to<span class="token punctuation">(</span>vehicle_point<span class="token punctuation">.</span>device<span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><ul><li>根据是否过滤地面点云，选择采样的方式。如果需要过滤地面点云，则从车辆点云中去除地面点云，然后随机采样一定数量的点云数据。否则，直接随机采样车辆点云。</li></ul><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">with</span> torch<span class="token punctuation">.</span>no_grad<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>    sampled_vehicle_points <span class="token operator">=</span> vehicle_point<span class="token punctuation">[</span>sampled_idx<span class="token punctuation">]</span>    fusion_point <span class="token operator">=</span> fusion_points<span class="token punctuation">[</span>b<span class="token punctuation">]</span>    distance_vehicle_to_fusion <span class="token operator">=</span> torch<span class="token punctuation">.</span><span class="token builtin">sum</span><span class="token punctuation">(</span>        <span class="token punctuation">(</span>sampled_vehicle_points<span class="token punctuation">.</span>unsqueeze<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">)</span> <span class="token operator">-</span> fusion_point<span class="token punctuation">.</span>unsqueeze<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">)</span><span class="token punctuation">)</span> <span class="token operator">**</span> <span class="token number">2</span><span class="token punctuation">,</span>        dim<span class="token operator">=</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span>    <span class="token builtin">sorted</span><span class="token punctuation">,</span> indices <span class="token operator">=</span> torch<span class="token punctuation">.</span>sort<span class="token punctuation">(</span>distance_vehicle_to_fusion<span class="token punctuation">,</span> dim<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span>    fix_point_number_query_indices <span class="token operator">=</span> indices<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span> <span class="token punctuation">:</span>self<span class="token punctuation">.</span>topk_nearest<span class="token punctuation">]</span>    queried_points <span class="token operator">=</span> torch<span class="token punctuation">.</span>gather<span class="token punctuation">(</span>fusion_point<span class="token punctuation">.</span>unsqueeze<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">)</span><span class="token punctuation">.</span>repeat<span class="token punctuation">(</span>fix_point_number_query_indices<span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span><span class="token punctuation">:</span><span class="token punctuation">,</span><span class="token punctuation">:</span><span class="token number">3</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span>                                  fix_point_number_query_indices<span class="token punctuation">.</span>unsqueeze<span class="token punctuation">(</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">.</span>repeat<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">)</span><span class="token punctuation">)</span>    source_partition <span class="token operator">=</span> self<span class="token punctuation">.</span>local_feature_extractor<span class="token punctuation">.</span>compute_partitions_batch<span class="token punctuation">(</span>queried_points<span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><ul><li>使用无梯度计算环境（<code>torch.no_grad()</code>），计算采样车辆点云与融合点云之间的欧式距离，并选取距离最近的前 k 个点。</li><li>通过局部特征提取器计算查询点所属的分区。</li><li>计算每个分区内的点数。</li></ul><pre class="line-numbers language-python" data-language="python"><code class="language-python">sampled_vehicle_voxels_coor_low_res <span class="token operator">=</span> vehicle_spatial_feature_voxels_coor<span class="token punctuation">[</span>sampled_idx<span class="token punctuation">]</span>sampled_vehicle_D<span class="token punctuation">,</span> sampled_vehicle_H<span class="token punctuation">,</span> sampled_vehicle_W <span class="token operator">=</span> sampled_vehicle_voxels_coor_low_res<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span>                                                          <span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">,</span> sampled_vehicle_voxels_coor_low_res<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span>                                                              <span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">,</span> sampled_vehicle_voxels_coor_low_res<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">]</span>sampled_vehicle_voxel_feat_low_res <span class="token operator">=</span> vehicle_spatial_feature<span class="token punctuation">[</span>sampled_vehicle_D<span class="token punctuation">,</span> sampled_vehicle_H<span class="token punctuation">,</span>                                     sampled_vehicle_W<span class="token punctuation">,</span> <span class="token punctuation">:</span><span class="token punctuation">]</span>projected_feat_low_res <span class="token operator">=</span> self<span class="token punctuation">.</span>projector_low_res<span class="token punctuation">(</span>sampled_vehicle_voxel_feat_low_res<span class="token punctuation">)</span>predicted_dist <span class="token operator">=</span> torch<span class="token punctuation">.</span>softmax<span class="token punctuation">(</span>self<span class="token punctuation">.</span>projector_final<span class="token punctuation">(</span>projected_feat_low_res<span class="token punctuation">)</span><span class="token punctuation">,</span> dim<span class="token operator">=</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span>contextual_shape_prediction_loss <span class="token operator">+=</span> <span class="token number">10.0</span> <span class="token operator">*</span> self<span class="token punctuation">.</span>kld<span class="token punctuation">(</span>predicted_dist<span class="token punctuation">.</span>log<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span> gt_dist<span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><ul><li>从车辆端空间特征中提取采样点对应的低分辨率体素特征。</li><li>将低分辨率体素特征通过投影器转换为高分辨率特征。</li><li>通过 softmax 函数计算预测的分布。</li><li>计算 KL 散度，并加到 contextual shape prediction loss 中。</li></ul><pre class="line-numbers language-python" data-language="python"><code class="language-python">sampled_fusion_voxels_coor_low_res <span class="token operator">=</span> fusion_spatial_feature_voxels_coor<span class="token punctuation">[</span>sampled_idx<span class="token punctuation">]</span>sampled_fusion_D<span class="token punctuation">,</span> sampled_fusion_H<span class="token punctuation">,</span> sampled_fusion_W <span class="token operator">=</span> sampled_fusion_voxels_coor_low_res<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span>                                                      <span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">,</span> sampled_fusion_voxels_coor_low_res<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span>                                                          <span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">,</span> sampled_fusion_voxels_coor_low_res<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">]</span>sampled_fusion_voxel_feat_low_res <span class="token operator">=</span> fusion_spatial_feature<span class="token punctuation">[</span>sampled_fusion_D<span class="token punctuation">,</span> sampled_fusion_H<span class="token punctuation">,</span>                                     sampled_fusion_W<span class="token punctuation">,</span> <span class="token punctuation">:</span><span class="token punctuation">]</span>projected_feat_low_res <span class="token operator">=</span> self<span class="token punctuation">.</span>projector_low_res_fusion<span class="token punctuation">(</span>sampled_fusion_voxel_feat_low_res<span class="token punctuation">)</span>predicted_dist <span class="token operator">=</span> torch<span class="token punctuation">.</span>softmax<span class="token punctuation">(</span>self<span class="token punctuation">.</span>projector_final<span class="token punctuation">(</span>projected_feat_low_res<span class="token punctuation">)</span><span class="token punctuation">,</span> dim<span class="token operator">=</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span>contextual_shape_prediction_loss <span class="token operator">+=</span> <span class="token number">10.0</span> <span class="token operator">*</span> self<span class="token punctuation">.</span>kld<span class="token punctuation">(</span>predicted_dist<span class="token punctuation">.</span>log<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span> gt_dist<span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><ul><li>从融合端空间特征中提取采样点对应的低分辨率体素特征。</li><li>将低分辨率体素特征通过融合端的投影器转换为高分辨率特征。</li><li>通过 softmax 函数计算预测的分布。</li><li>计算 KL 散度，并加到 contextual shape prediction loss 中。</li></ul><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">return</span> contextual_shape_prediction_loss<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><ul><li>返回计算得到的 contextual shape prediction loss。</li></ul><blockquote><p>用于计算形状上下文的Python风格代码算法流程图</p></blockquote><p><img src="/pic/CO4.png"></p><h3 id="实验结果"><a href="#实验结果" class="headerlink" title="实验结果"></a>实验结果</h3><p>表 1：Once 数据集上的 3D 对象检测结果。在 3 种不同的检测器上进行了实验：Second（简称 Sec.）、PV-RCNN（简称 PV）和 CenterPoint （简称 Cen.）以及 8 种不同的初始化方法，包括随机（简称如 Rand，即从头开始训练）、Swav 、Deep Cluster（简称为 D. Cl.）、BYOL、Point Contrast（简称为 P.C.）、GCC-3D 和STRL 。结果为 mAP（以 % 表示）。 “0-30m”、“30-50m”和“&gt;50m”分别表示0至30米、30至50米和50米至无穷远物体的结果。最后一栏的“mAP”是总体评估和比较的主要指标。为了更好地理解，对每个范围内每个类别中的前 3 个 mAP 使用粗体字体。</p><p><img src="/pic/CO5.png"></p><p>表 2：KITTI 数据集上的 3D 物体检测结果 。结果为 mAP（以 % 表示）。 “Easy”、“Moderate”和“Hard”分别表示 KITTI 数据集中定义的难度级别。每个类别的结果都处于中等水平。最后一列中的“总体”结果是比较的主要指标。为了更好地理解，对每个难度级别的每个类别中的前 3 个 mAP 使用粗体字体。</p><p><img src="/pic/CO6.png"></p>]]></content>
      
      
      <categories>
          
          <category> DL </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 对比学习 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>SAM(segment-anything)与自动驾驶(Autonomous driving)</title>
      <link href="/2024/03/10/sam-ad/"/>
      <url>/2024/03/10/sam-ad/</url>
      
        <content type="html"><![CDATA[<h1 id="SAM-segment-anything-与自动驾驶-Autonomous-driving"><a href="#SAM-segment-anything-与自动驾驶-Autonomous-driving" class="headerlink" title="SAM(segment-anything)与自动驾驶(Autonomous driving)"></a>SAM(segment-anything)与自动驾驶(Autonomous driving)</h1><p><a href="https://zhuanlan.zhihu.com/p/644601380">SAM（Segment Anything）讲解及自动驾驶应用思考</a><br><a href="https://www.sohu.com/a/723363300_121771907">2023机器人行业深度研究报告：12大模型迭代智能驾驶机器人算法进化</a><br><a href="https://viso.ai/deep-learning/segment-anything-model-sam-explained/">Segment Anything Model (SAM) – The Complete 2024 Guide</a><br><a href="https://blog.roboflow.com/how-to-use-segment-anything-model-sam/">How to Use the Segment Anything Model (SAM)</a><br><a href="https://zhuanlan.zhihu.com/p/676532784">Segment Anything(sam)项目整理汇总</a><br><a href="https://github.com/zhanghm1995/forge_vfm4ad">Forging Vision Foundation Models for Autonomous Driving: Challenges, Methodologies, and Opportunities</a><br><a href="https://arxiv.org/pdf/2306.02245.pdf">SAM3D: Zero-Shot 3D Object Detection via Segment Anything Model</a><br><a href="https://github.com/DYZhang09/SAM3D">SAM3D代码</a><br><a href="https://zhuanlan.zhihu.com/p/677633539">超越BEVFusion | RoboFusion：通过SAM实现稳健的多模态3D检测</a><br><a href="https://arxiv.org/abs/2401.03907">RoboFusion: Towards Robust Multi-Modal 3D obiect Detection via SAM</a><br><a href="https://github.com/dvlab-research/3D-Box-Segment-Anything">3D-Box-Segment-Anything</a><br><a href="https://zhuanlan.zhihu.com/p/663321688">[工具分享][AnyLabeling][SAM+Labelme]</a></p><h2 id="《SAM3D-Zero-Shot-3D-Object-Detection-via-Segment-Anything-Model》"><a href="#《SAM3D-Zero-Shot-3D-Object-Detection-via-Segment-Anything-Model》" class="headerlink" title="《SAM3D: Zero-Shot 3D Object Detection via Segment Anything Model》"></a>《SAM3D: Zero-Shot 3D Object Detection via Segment Anything Model》</h2><p>室外场景的应用SAM进行零样本 3D 对象检测（目前是单目标，缺乏语义标签，使用MMDetection3D和Segment-Anything实现）</p><ul><li>将点云投影成BEV图像：使用投影方程决定每个点在图像平面的坐标，并预定义一组反射强度到RGB的映射，以得到BEV图像像素的RGB值</li><li>由于转化后的BEV图像比较稀疏，使用最大池化减小间隙</li><li>然后使用SAM语义分割，SAM支持使用各种提示，如点、框和掩膜，本文使用网格提示覆盖整张图像。具体来说，创建32 × 32的在图像平面均匀分布的网格，将它们视作SAM的点提示。分割处尽可能多的前景目标</li><li>SAM输出的掩膜是有噪声的，使用汽车是有大概的面积和长宽比这些先验知识，用于滤除不符合规则的掩膜</li><li>直接从2D掩膜估计3D边界框的水平属性，高度属性使用的是落在2D掩膜的点云高度进行计算的</li></ul><p><img src="/pic/SAM3D.png" alt="图 1：(a) 总体框架。"></p><p>(b) 使用不同版本的 SAM 的 SAM3D 结果。 (c) 使用不同柱尺寸的 SAM3D 结果。在 Waymo 验证集上报告 [0,30)范围内的 VEHICLE 指标。</p><h2 id=""><a href="#" class="headerlink" title=""></a></h2><h2 id="《RoboFusion-Towards-Robust-Multi-Modal-3D-obiect-Detection-via-SAM》"><a href="#《RoboFusion-Towards-Robust-Multi-Modal-3D-obiect-Detection-via-SAM》" class="headerlink" title="《RoboFusion: Towards Robust Multi-Modal 3D obiect Detection via SAM》"></a>《RoboFusion: Towards Robust Multi-Modal 3D obiect Detection via SAM》</h2><p>动机：现实环境的复杂性和恶劣条件，提高自动驾驶中多模态 3D 物体检测的鲁棒性和泛化性带来了机遇和挑战，利用 SAM 等 VFM 来解决分布外 (OOD) 噪声场景</p><p>基本思路：引入了 AD-FPN 对 SAM 提取的图像特征进行上采样。采用小波分解对深度引导图像进行去噪，以进一步降低噪声和天气干扰。最后，采用自注意力机制来自适应地重新加权融合特征，增强信息特征，同时抑制多余的噪声</p><p>改进策略：</p><ul><li>1）利用从 SAM 中提取的特征而不是推理分割结果。</li><li>2）提出SAM-AD，它是针对AD场景的​​预训练SAM。 </li><li>3）引入了一种新颖的 AD-FPN 来解决特征上采样问题，以将 VFM 与多模态 3D 物体检测器对齐。 </li><li>4）为了进一步减少噪声干扰并保留基本信号特征，设计了深度引导小波注意（DGWA）模块，可以有效衰减高频和低频噪声。 </li><li>5）融合点云特征和图像特征后，提出自适应融合，通过自注意力自适应地重新加权融合特征，进一步增强特征鲁棒性和抗噪性。</li><li>在 KITTI-C 和 nuScenes-C 数据集中验证了 RoboFusion 针对 OOD 噪声场景的鲁棒性 [Dong et al., 2023]，在噪声中实现了 SOTA 性能。</li></ul><p><img src="/pic/Rob.png" alt="图 1：(a) 总体框架。"></p><p>LiDAR 分支几乎遵循基线 [<a href="https://github.com/dvlab-research/FocalsConv">FocalsConv</a>; <a href="https://github.com/XuyangBai/TransFusion">Transfusion</a>] 生成 LiDAR 特征。在相机分支中，首先，使用高度优化的 SAM-AD 提取鲁棒的图像特征，并使用 AD-FPN 获取多尺度特征。其次，由原始点生成稀疏深度图S，并将其输入深度编码器以获得深度特征，并与多尺度图像特征融合以获得深度引导图像特征。然后使用小波注意力来消除突变噪声。最后，自适应融合通过自注意力机制将点云特征与鲁棒图像特征和深度信息相结合。</p><blockquote><p>SAM-AD模型的构成：</p></blockquote><p>SAM-AD。为了进一步使SAM适应AD（自动驾驶）场景，对SAM进行预训练以获得SAM-AD。具体来说，从成熟的数据集（即 KITTI [Geiger et al., 2012] 和 nuScenes [Caesar et al., 2020]）中收集了大量图像样本，形成了基础 AD 数据集。遵循 DMAE [Wu et al., 2023]，对 SAM 进行预训练，以获得 AD 场景中的 SAM-AD，如下图所示。</p><ul><li>将 x 表示为来自 AD 数据集的干净图像（即 KITTI [Geiger et al., 2012] 和 nuScenes [Caesar et al., 2020]） 和 η 作为 [Dong et al., 2023] 基于 x 生成的噪声图像。噪声类型和严重程度分别从四种天气（即雨、雪、雾和阳光）和从1到5的五个严重程度中随机选择。</li><li>采用 SAM <a href="https://github.com/facebookresearch/segment-anything">Kirillov et al., 2023</a>和 MobileSAM <a href="https://github.com/ChaoningZhang/MobileSAM">Zhang et al., 2023a</a> 的图像编码器作为编码器，而解码器和重建损失与 DMAE <a href="https://github.com/quanlin-wu/dmae">Wu et al., 2023</a> 相同。</li><li>对于 FastSAM <a href="https://github.com/CASIA-IVA-Lab/FastSAM">Zhao et al., 2023</a>，采用 YOLOv8 在 AD 数据集上预训练 FastSAM。为了避免过度拟合，使用随机调整大小和裁剪作为数据增强。还将掩码比率设置为 0.75，并在 8 个 NVIDIA A100 GPU 上训练了 400 个 epoch。</li></ul><p><img src="/pic/Rob2.png" alt="图 2：预训练框架图示。"></p><p>用包含多个天气噪声的 η 来破坏干净的图像 x，然后随机屏蔽噪声图像 x+η 上的几个补丁，以获得屏蔽的噪声图像Mask(x+η)。 SAM-AD 和 DMAE 解码器经过训练，可以根据 Mask(x + η) 重建干净的图像 ^x。</p><blockquote><p>AD-FPN模块<br>采用 SAM 的图像编码器来提取鲁棒的图像特征。然而，SAM 使用 ViT 系列 <a href="https://github.com/facebookresearch/detectron2/tree/main/projects/ViTDet">Dosovitskiy et al., 2020</a> 作为其图像编码器，它排除了多尺度特征，仅提供高维低分辨率特征。为了生成目标检测所需的多尺度特征，受 ViTDet[Li et al., 2022a] 的启发，我们设计了一个 AD-FPN，它提供基于 ViT 的多尺度特征。具体来说，利用 SAM 提供的步幅为 16（尺度&#x3D;1&#x2F;16）的高度维图像嵌入，我们生成了一系列步幅为 {32,16,8,4} 的多尺度特征 Fms。接下来，我们通过类似于 FPN 的自下而上的方式整合 Fms 来获得多尺度特征</p></blockquote><blockquote><p>深度引导小波注意（DGWA）模块:</p></blockquote><ul><li>1）设计了深度引导网络，通过结合图像特征和点云的深度特征，在图像特征之前添加几何形状。</li><li>2）使用Haar小波变换将图像的特征分解为四个小波子带[Liu et al., 2020a]，然后注意力机制允许对子带中的信息特征进行去噪。</li></ul><blockquote><p>Adaptive Fusion模块</p></blockquote><ul><li>采用自注意力机制来自适应地重新加权融合特征，如图3所示。模态特异性的损坏程度是动态的，自注意力机制允许自适应重新加权特征可以增强信息特征并抑制冗余噪声。</li></ul><p><img src="/pic/Rob3.png" alt="图 3：自适应融合的架构图示。"></p><p>实验结果具体看<a href="https://arxiv.org/abs/2401.03907">论文</a></p><h2 id="3D-Box-Segment-Anything"><a href="#3D-Box-Segment-Anything" class="headerlink" title="3D-Box-Segment-Anything"></a>3D-Box-Segment-Anything</h2><p>这是一个项目，将将<a href="https://github.com/facebookresearch/segment-anything">Segment Anything</a>与<a href="https://github.com/dvlab-research/VoxelNeXt">VoxelNeXt</a>相结合，将其扩展到 3D 感知</p><p><img src="/pic/sam-voxelnext.png"></p><blockquote><p>核心思想</p></blockquote><p>VoxelNeXt是一个完全稀疏的 3D 检测器。它根据每个稀疏体素预测 3D 对象。我们将 3D 稀疏体素投影到 2D 图像上。然后可以为 SAM 掩模中的体素生成 3D 框。</p><blockquote><p>一些生成图的样例：</p></blockquote><p><img src="/pic/sam-voxelnext2.png"></p><blockquote><p>演示的<a href="https://github.com/dvlab-research/3D-Box-Segment-Anything">demo代码</a></p></blockquote>]]></content>
      
      
      <categories>
          
          <category> SAM </category>
          
      </categories>
      
      
        <tags>
            
            <tag> sam </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>室外大规模点云自监督学习代码篇(Occupancy-MAE)</title>
      <link href="/2024/01/03/occupancy-mae-code/"/>
      <url>/2024/01/03/occupancy-mae-code/</url>
      
        <content type="html"><![CDATA[<h1 id="室外大规模点云自监督学习代码篇-Occupancy-MAE"><a href="#室外大规模点云自监督学习代码篇-Occupancy-MAE" class="headerlink" title="室外大规模点云自监督学习代码篇(Occupancy-MAE)"></a>室外大规模点云自监督学习代码篇(Occupancy-MAE)</h1><p><a href="https://arxiv.org/pdf/2206.09900">Occupancy-MAE,原版名为Voxel-MAE(v1~v7)</a></p><p><a href="https://github.com/chaytonmin/Occupancy-MAE">代码链接</a></p><h2 id="Occupancy-MAE-Self-supervised-Pre-training-Large-scale-LiDAR-Point-Clouds-with-Masked-Occupancy-Autoencoders-代码"><a href="#Occupancy-MAE-Self-supervised-Pre-training-Large-scale-LiDAR-Point-Clouds-with-Masked-Occupancy-Autoencoders-代码" class="headerlink" title="Occupancy-MAE: Self-supervised Pre-training Large-scale LiDAR Point Clouds with Masked Occupancy Autoencoders(代码)"></a>Occupancy-MAE: Self-supervised Pre-training Large-scale LiDAR Point Clouds with Masked Occupancy Autoencoders(代码)</h2><p>本工作的主要贡献如下:</p><ul><li>提出了一种新的自监督掩码占用自动编码框架，称为Occupancy-MAE，用于预训练大规模户外LiDAR点云，减少了对昂贵的注释3D数据的需求。</li><li>提出了一个3D占用预测pretext task，该任务利用大规模LiDAR点云逐渐稀疏的占用结构。通过从少量可见体素中恢复3D周围世界的掩模占用分布，迫使网络提取高级语义信息。</li><li>引入距离感知随机掩码策略，利用大规模LiDAR点云密度变化的优势，提高预训练性能。</li><li>提出的Occupancy-MAE在各种下游任务上显著优于从头开始的训练，包括3D目标检测、语义分割、多目标跟踪和无监督域自适应。<br><img src="/pic/mae3d10.png" alt="核心流程图"></li></ul><p><img src="/pic/mae3d7.png" alt="整体架构。首先将大规模不规则LiDAR点云转换为体积表示，根据体素与LiDAR传感器的距离随机掩码(即距离感知掩码策略)，然后使用非对称自编码器网络重建一般3D世界的几何占用结构。采用以位置编码为编码骨干的3D空间稀疏卷积[6]。使用二元占用分类作为pretext task来区分体素是否包含点。预训练后，丢弃轻量级解码器，使用编码器对下游任务的主干进行预热。"></p><h3 id="代码介绍"><a href="#代码介绍" class="headerlink" title="代码介绍"></a>代码介绍</h3><p>网络依托<a href="https://github.com/open-mmlab/OpenPCDet">OpenPCDet(v0.5)</a>进行构建</p><blockquote><p>OpenPCDet 是一个清晰、简单、独立的开源项目，用于基于 LiDAR 的 3D 物体检测。同时也是[PointRCNN]、[Part-A2-Net]、[PV-RCNN]、[Voxel R-CNN]、[PV-RCNN++]和[MPPNet]的官方代码发布。</p></blockquote><p><strong>配置文件保存在tools&#x2F;cfgs目录下，不同的数据集设置了不同的配置(主要看携带mae的配置文件)：</strong></p><pre class="line-numbers language-none"><code class="language-none">tools&#x2F;cfgs├── dataset_configs│   ├── kitti_dataset.yaml│   ├── lyft_dataset.yaml│   ├── nuscenes_dataset.yaml│   ├── pandaset_dataset.yaml│   └── waymo_dataset.yaml├── kitti_models│   ├── CaDDN.yaml│   ├── PartA2_free.yaml│   ├── PartA2.yaml│   ├── pointpillar_newaugs.yaml│   ├── pointpillar_pyramid_aug.yaml│   ├── pointpillar.yaml│   ├── pointrcnn_iou.yaml│   ├── pointrcnn.yaml│   ├── pv_rcnn.yaml│   ├── second_iou.yaml│   ├── second_multihead.yaml│   ├── second.yaml│   ├── voxel_mae_kitti.yaml│   └── voxel_rcnn_car.yaml├── lyft_models│   ├── cbgs_second_multihead.yaml│   └── cbgs_second-nores_multihead.yaml├── nuscenes_models│   ├── cbgs_dyn_pp_centerpoint.yaml│   ├── cbgs_pp_multihead.yaml│   ├── cbgs_second_multihead.yaml│   ├── cbgs_voxel0075_res3d_centerpoint.yaml│   ├── cbgs_voxel01_res3d_centerpoint.yaml│   └── voxel_mae_res_nuscenes.yaml└── waymo_models    ├── centerpoint_dyn_pillar_1x.yaml    ├── centerpoint_pillar_1x.yaml    ├── centerpoint_without_resnet.yaml    ├── centerpoint.yaml    ├── PartA2.yaml    ├── pointpillar_1x.yaml    ├── pv_rcnn_plusplus_resnet.yaml    ├── pv_rcnn_plusplus.yaml    ├── pv_rcnn_with_centerhead_rpn.yaml    ├── pv_rcnn.yaml    ├── second.yaml    ├── voxel_mae_waymo.yaml    └── voxel_rcnn_with_centerhead_dyn_voxel.yaml<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h4 id="下面将以kitti数据集上的配置代码撸一遍流程"><a href="#下面将以kitti数据集上的配置代码撸一遍流程" class="headerlink" title="下面将以kitti数据集上的配置代码撸一遍流程"></a>下面将以kitti数据集上的配置代码撸一遍流程</h4><p>**以kitti数据集为例,它的配置文件是<a href="https://github.com/open-mmlab/OpenPCDet/blob/master/tools/cfgs/dataset_configs/kitti_dataset.yaml">cfgs&#x2F;dataset_configs&#x2F;kitti_dataset.yaml</a>**：<br>这是一个用于配置KITTI数据集处理的配置文件。以下是主要部分的解释：</p><ol><li><p><strong>DATASET</strong>: 数据集的名称，指定为’KittiDataset’。</p></li><li><p><strong>DATA_PATH</strong>: 数据集的路径，指定为’..&#x2F;data&#x2F;kitti’。</p></li><li><p><strong>POINT_CLOUD_RANGE</strong>: 点云的范围，指定为<code>[0, -40, -3, 70.4, 40, 1]</code>。</p></li><li><p><strong>DATA_SPLIT</strong>: 数据集的划分，包含’train’和’test’两个子集。</p></li><li><p><strong>INFO_PATH</strong>: 数据集信息的路径，分别指定了’train’和’test’子集的信息文件。</p></li><li><p><strong>GET_ITEM_LIST</strong>: 获取数据项的列表，包括”points”。</p></li><li><p><strong>FOV_POINTS_ONLY</strong>: 是否仅使用视场内的点，指定为True。</p></li><li><p><strong>DATA_AUGMENTOR</strong>: 数据增强的配置，包括禁用的增强、增强配置列表等。</p><ul><li><p><strong>DISABLE_AUG_LIST</strong>: 禁用的数据增强，包括’placeholder’。</p></li><li><p><strong>AUG_CONFIG_LIST</strong>: 数据增强的配置列表，包括gt_sampling、random_world_flip、random_world_rotation和random_world_scaling。</p></li></ul></li><li><p><strong>POINT_FEATURE_ENCODING</strong>: 点特征的编码配置，采用绝对坐标编码，包括使用的特征列表等。</p></li><li><p><strong>DATA_PROCESSOR</strong>: 数据处理的配置，包括mask_points_and_boxes_outside_range、shuffle_points和transform_points_to_voxels。</p><ul><li><p><strong>mask_points_and_boxes_outside_range</strong>: 在范围之外屏蔽点和框。</p></li><li><p><strong>shuffle_points</strong>: 随机打乱点的顺序，指定在训练和测试阶段是否启用。</p></li><li><p><strong>transform_points_to_voxels</strong>: 将点转换为体素的配置，包括体素大小、最大点数等。</p></li></ul></li></ol><p>这个配置文件定义了KITTI数据集的加载、处理和增强流程，以及训练和测试阶段的一些设置。配置文件中的每个部分都用于指导数据流程的一个方面。</p><p><strong>配置文件以<a href="https://github.com/chaytonmin/Occupancy-MAE/blob/main/tools/cfgs/kitti_models/voxel_mae_kitti.yaml">voxel_mae_kitti.yaml</a>为例：</strong></p><pre class="line-numbers language-none"><code class="language-none">CLASS_NAMES: [&#39;Car&#39;, &#39;Pedestrian&#39;, &#39;Cyclist&#39;]DATA_CONFIG:     _BASE_CONFIG_: cfgs&#x2F;dataset_configs&#x2F;kitti_dataset.yamlMODEL:    NAME: Voxel_MAE    VFE:        NAME: MeanVFE    BACKBONE_3D:        NAME: Voxel_MAE        MASKED_RATIO: 0.5  # masked_ratio for Voxel_MAEOPTIMIZATION:    BATCH_SIZE_PER_GPU: 4    NUM_EPOCHS: 30    OPTIMIZER: adam_onecycle    LR: 0.003    WEIGHT_DECAY: 0.01    MOMENTUM: 0.9    MOMS: [0.95, 0.85]    PCT_START: 0.4    DIV_FACTOR: 10    DECAY_STEP_LIST: [35, 45]    LR_DECAY: 0.1    LR_CLIP: 0.0000001    LR_WARMUP: False    WARMUP_EPOCH: 1    GRAD_NORM_CLIP: 10<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>这是一个用于配置模型、优化器和训练过程的配置文件。以下是主要部分的解释：</p><ol><li><p><strong>CLASS_NAMES</strong>: 数据集中的类别名称，包括’Car’, ‘Pedestrian’, ‘Cyclist’。</p></li><li><p><strong>DATA_CONFIG</strong>: 数据集的配置，包括基础配置文件的引用。</p><ul><li><strong><em>BASE_CONFIG</em></strong>: 数据集配置的基础配置文件，指定为’cfgs&#x2F;dataset_configs&#x2F;kitti_dataset.yaml’。</li></ul></li><li><p><strong>MODEL</strong>: 模型的配置。</p><ul><li><p><strong>NAME</strong>: 模型的名称，指定为’Voxel_MAE’。</p></li><li><p><strong>VFE</strong>: 体素特征编码器的配置，包括名称为’MeanVFE’的体素特征编码器。</p></li><li><p><strong>BACKBONE_3D</strong>: 3D主干网络结构的配置，包括名称为’Voxel_MAE’的结构和’MASKED_RATIO’的设置。</p></li></ul></li><li><p><strong>OPTIMIZATION</strong>: 优化器和训练过程的配置。</p><ul><li><p><strong>BATCH_SIZE_PER_GPU</strong>: 每个GPU的批量大小，设置为4。</p></li><li><p><strong>NUM_EPOCHS</strong>: 训练的总时期数，设置为30。</p></li><li><p><strong>OPTIMIZER</strong>: 优化器的名称，设置为’adam_onecycle’。</p></li><li><p><strong>LR</strong>: 学习率，设置为0.003。</p></li><li><p><strong>WEIGHT_DECAY</strong>: 权重衰减，设置为0.01。</p></li><li><p><strong>MOMENTUM</strong>: 动量，设置为0.9。</p></li><li><p><strong>MOMS</strong>: 动量的配置，包括[0.95, 0.85]。</p></li><li><p><strong>PCT_START</strong>: OneCycleLR学习率策略的初始学习率占总时期数的百分比，设置为0.4。</p></li><li><p><strong>DIV_FACTOR</strong>: OneCycleLR学习率策略的学习率最大值和最小值的比例，设置为10。</p></li><li><p><strong>DECAY_STEP_LIST</strong>: 学习率衰减的时期列表，设置为[35, 45]。</p></li><li><p><strong>LR_DECAY</strong>: 学习率衰减因子，设置为0.1。</p></li><li><p><strong>LR_CLIP</strong>: 学习率的下限，设置为0.0000001。</p></li><li><p><strong>LR_WARMUP</strong>: 是否进行学习率预热，设置为False。</p></li><li><p><strong>WARMUP_EPOCH</strong>: 学习率预热的时期数，设置为1。</p></li><li><p><strong>GRAD_NORM_CLIP</strong>: 梯度范数的裁剪值，设置为10。</p></li></ul></li></ol><p>这个配置文件定义了模型的结构、优化器的选择以及训练的各种参数。每个部分都用于指导模型训练的一个方面。</p><p>主干网络包含如下这些，其中我们主要使用<a href="https://github.com/chaytonmin/Occupancy-MAE/blob/main/pcdet/models/detectors/voxel_mae_net.py">Voxel_MAE</a>：</p><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">from</span> <span class="token punctuation">.</span>detector3d_template <span class="token keyword">import</span> Detector3DTemplate<span class="token keyword">from</span> <span class="token punctuation">.</span>PartA2_net <span class="token keyword">import</span> PartA2Net<span class="token keyword">from</span> <span class="token punctuation">.</span>point_rcnn <span class="token keyword">import</span> PointRCNN<span class="token keyword">from</span> <span class="token punctuation">.</span>pointpillar <span class="token keyword">import</span> PointPillar<span class="token keyword">from</span> <span class="token punctuation">.</span>pv_rcnn <span class="token keyword">import</span> PVRCNN<span class="token keyword">from</span> <span class="token punctuation">.</span>second_net <span class="token keyword">import</span> SECONDNet<span class="token keyword">from</span> <span class="token punctuation">.</span>second_net_iou <span class="token keyword">import</span> SECONDNetIoU<span class="token keyword">from</span> <span class="token punctuation">.</span>caddn <span class="token keyword">import</span> CaDDN<span class="token keyword">from</span> <span class="token punctuation">.</span>voxel_rcnn <span class="token keyword">import</span> VoxelRCNN<span class="token keyword">from</span> <span class="token punctuation">.</span>centerpoint <span class="token keyword">import</span> CenterPoint<span class="token keyword">from</span> <span class="token punctuation">.</span>pv_rcnn_plusplus <span class="token keyword">import</span> PVRCNNPlusPlus<span class="token keyword">from</span> <span class="token punctuation">.</span>detector3d_template_voxel_mae <span class="token keyword">import</span> Detector3DTemplate_voxel_mae<span class="token keyword">from</span> <span class="token punctuation">.</span>voxel_mae_net <span class="token keyword">import</span> Voxel_MAE__all__ <span class="token operator">=</span> <span class="token punctuation">&#123;</span>    <span class="token string">'Detector3DTemplate'</span><span class="token punctuation">:</span> Detector3DTemplate<span class="token punctuation">,</span>    <span class="token string">'SECONDNet'</span><span class="token punctuation">:</span> SECONDNet<span class="token punctuation">,</span>    <span class="token string">'PartA2Net'</span><span class="token punctuation">:</span> PartA2Net<span class="token punctuation">,</span>    <span class="token string">'PVRCNN'</span><span class="token punctuation">:</span> PVRCNN<span class="token punctuation">,</span>    <span class="token string">'PointPillar'</span><span class="token punctuation">:</span> PointPillar<span class="token punctuation">,</span>    <span class="token string">'PointRCNN'</span><span class="token punctuation">:</span> PointRCNN<span class="token punctuation">,</span>    <span class="token string">'SECONDNetIoU'</span><span class="token punctuation">:</span> SECONDNetIoU<span class="token punctuation">,</span>    <span class="token string">'CaDDN'</span><span class="token punctuation">:</span> CaDDN<span class="token punctuation">,</span>    <span class="token string">'VoxelRCNN'</span><span class="token punctuation">:</span> VoxelRCNN<span class="token punctuation">,</span>    <span class="token string">'CenterPoint'</span><span class="token punctuation">:</span> CenterPoint<span class="token punctuation">,</span>    <span class="token string">'PVRCNNPlusPlus'</span><span class="token punctuation">:</span> PVRCNNPlusPlus<span class="token punctuation">,</span>    <span class="token string">'Detector3DTemplate_voxel_mae'</span><span class="token punctuation">:</span> Detector3DTemplate_voxel_mae<span class="token punctuation">,</span>    <span class="token string">'Voxel_MAE'</span><span class="token punctuation">:</span> Voxel_MAE<span class="token punctuation">&#125;</span><span class="token keyword">def</span> <span class="token function">build_detector</span><span class="token punctuation">(</span>model_cfg<span class="token punctuation">,</span> num_class<span class="token punctuation">,</span> dataset<span class="token punctuation">)</span><span class="token punctuation">:</span>    model <span class="token operator">=</span> __all__<span class="token punctuation">[</span>model_cfg<span class="token punctuation">.</span>NAME<span class="token punctuation">]</span><span class="token punctuation">(</span>        model_cfg<span class="token operator">=</span>model_cfg<span class="token punctuation">,</span> num_class<span class="token operator">=</span>num_class<span class="token punctuation">,</span> dataset<span class="token operator">=</span>dataset    <span class="token punctuation">)</span>    <span class="token keyword">return</span> model<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>voxel_mae_net.py代码如下，继承<a href="https://github.com/chaytonmin/Occupancy-MAE/blob/main/pcdet/models/detectors/detector3d_template_voxel_mae.py">Detector3DTemplate_voxel_mae</a>,这里主要讲了如何配置主干网络，其中维护了一个model_info_dict字典</p><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">class</span> <span class="token class-name">Detector3DTemplate_voxel_mae</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span><span class="token comment">#部分代码</span>    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> model_cfg<span class="token punctuation">,</span> num_class<span class="token punctuation">,</span> dataset<span class="token punctuation">)</span><span class="token punctuation">:</span>        <span class="token builtin">super</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>        self<span class="token punctuation">.</span>model_cfg <span class="token operator">=</span> model_cfg        self<span class="token punctuation">.</span>num_class <span class="token operator">=</span> num_class        self<span class="token punctuation">.</span>dataset <span class="token operator">=</span> dataset        self<span class="token punctuation">.</span>class_names <span class="token operator">=</span> dataset<span class="token punctuation">.</span>class_names        self<span class="token punctuation">.</span>register_buffer<span class="token punctuation">(</span><span class="token string">'global_step'</span><span class="token punctuation">,</span> torch<span class="token punctuation">.</span>LongTensor<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">.</span>zero_<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span>        self<span class="token punctuation">.</span>module_topology <span class="token operator">=</span> <span class="token punctuation">[</span>            <span class="token string">'vfe'</span><span class="token punctuation">,</span> <span class="token string">'backbone_3d'</span>        <span class="token punctuation">]</span>    <span class="token decorator annotation punctuation">@property</span>    <span class="token keyword">def</span> <span class="token function">mode</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span><span class="token punctuation">:</span>        <span class="token keyword">return</span> <span class="token string">'TRAIN'</span> <span class="token keyword">if</span> self<span class="token punctuation">.</span>training <span class="token keyword">else</span> <span class="token string">'TEST'</span>    <span class="token keyword">def</span> <span class="token function">update_global_step</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span><span class="token punctuation">:</span>        self<span class="token punctuation">.</span>global_step <span class="token operator">+=</span> <span class="token number">1</span>    <span class="token keyword">def</span> <span class="token function">build_networks</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span><span class="token punctuation">:</span>        model_info_dict <span class="token operator">=</span> <span class="token punctuation">&#123;</span>            <span class="token string">'module_list'</span><span class="token punctuation">:</span> <span class="token punctuation">[</span><span class="token punctuation">]</span><span class="token punctuation">,</span>            <span class="token string">'num_rawpoint_features'</span><span class="token punctuation">:</span> self<span class="token punctuation">.</span>dataset<span class="token punctuation">.</span>point_feature_encoder<span class="token punctuation">.</span>num_point_features<span class="token punctuation">,</span>            <span class="token string">'num_point_features'</span><span class="token punctuation">:</span> self<span class="token punctuation">.</span>dataset<span class="token punctuation">.</span>point_feature_encoder<span class="token punctuation">.</span>num_point_features<span class="token punctuation">,</span>            <span class="token string">'grid_size'</span><span class="token punctuation">:</span> self<span class="token punctuation">.</span>dataset<span class="token punctuation">.</span>grid_size<span class="token punctuation">,</span>            <span class="token string">'point_cloud_range'</span><span class="token punctuation">:</span> self<span class="token punctuation">.</span>dataset<span class="token punctuation">.</span>point_cloud_range<span class="token punctuation">,</span>            <span class="token string">'voxel_size'</span><span class="token punctuation">:</span> self<span class="token punctuation">.</span>dataset<span class="token punctuation">.</span>voxel_size<span class="token punctuation">,</span>            <span class="token string">'depth_downsample_factor'</span><span class="token punctuation">:</span> self<span class="token punctuation">.</span>dataset<span class="token punctuation">.</span>depth_downsample_factor        <span class="token punctuation">&#125;</span>        <span class="token keyword">for</span> module_name <span class="token keyword">in</span> self<span class="token punctuation">.</span>module_topology<span class="token punctuation">:</span>            <span class="token triple-quoted-string string">"""            使用 getattr 函数从当前对象 self 中获取这个方法。getattr 接受一个对象和一个字符串作为参数，返回对象的属性或方法。            调用获取到的方法，并传递 model_info_dict=model_info_dict 作为参数。这样，构建方法就可以更新 model_info_dict，并返回构建的模块。            """</span>            module<span class="token punctuation">,</span> model_info_dict <span class="token operator">=</span> <span class="token builtin">getattr</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> <span class="token string">'build_%s'</span> <span class="token operator">%</span> module_name<span class="token punctuation">)</span><span class="token punctuation">(</span>                model_info_dict<span class="token operator">=</span>model_info_dict            <span class="token punctuation">)</span>            <span class="token comment">#将构建的模块 module 添加到当前对象 self 中，并为它指定一个名称 module_name。</span>            self<span class="token punctuation">.</span>add_module<span class="token punctuation">(</span>module_name<span class="token punctuation">,</span> module<span class="token punctuation">)</span>        <span class="token keyword">return</span> model_info_dict<span class="token punctuation">[</span><span class="token string">'module_list'</span><span class="token punctuation">]</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">from</span> <span class="token punctuation">.</span>detector3d_template_voxel_mae <span class="token keyword">import</span> Detector3DTemplate_voxel_mae<span class="token keyword">class</span> <span class="token class-name">Voxel_MAE</span><span class="token punctuation">(</span>Detector3DTemplate_voxel_mae<span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> model_cfg<span class="token punctuation">,</span> num_class<span class="token punctuation">,</span> dataset<span class="token punctuation">)</span><span class="token punctuation">:</span>        <span class="token builtin">super</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span>model_cfg<span class="token operator">=</span>model_cfg<span class="token punctuation">,</span> num_class<span class="token operator">=</span>num_class<span class="token punctuation">,</span> dataset<span class="token operator">=</span>dataset<span class="token punctuation">)</span>        self<span class="token punctuation">.</span>module_list <span class="token operator">=</span> self<span class="token punctuation">.</span>build_networks<span class="token punctuation">(</span><span class="token punctuation">)</span>    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> batch_dict<span class="token punctuation">)</span><span class="token punctuation">:</span>        <span class="token keyword">for</span> cur_module <span class="token keyword">in</span> self<span class="token punctuation">.</span>module_list<span class="token punctuation">:</span>            batch_dict <span class="token operator">=</span> cur_module<span class="token punctuation">(</span>batch_dict<span class="token punctuation">)</span>        <span class="token keyword">if</span> self<span class="token punctuation">.</span>training<span class="token punctuation">:</span>            loss<span class="token punctuation">,</span> tb_dict<span class="token punctuation">,</span> disp_dict <span class="token operator">=</span> self<span class="token punctuation">.</span>get_training_loss<span class="token punctuation">(</span><span class="token punctuation">)</span>            ret_dict <span class="token operator">=</span> <span class="token punctuation">&#123;</span>                <span class="token string">'loss'</span><span class="token punctuation">:</span> loss            <span class="token punctuation">&#125;</span>            <span class="token keyword">return</span> ret_dict<span class="token punctuation">,</span> tb_dict<span class="token punctuation">,</span> disp_dict        <span class="token keyword">else</span><span class="token punctuation">:</span>            pred_dicts<span class="token punctuation">,</span> recall_dicts <span class="token operator">=</span> self<span class="token punctuation">.</span>post_processing<span class="token punctuation">(</span>batch_dict<span class="token punctuation">)</span>            <span class="token keyword">return</span> pred_dicts<span class="token punctuation">,</span> recall_dicts    <span class="token keyword">def</span> <span class="token function">get_training_loss</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span><span class="token punctuation">:</span>        disp_dict <span class="token operator">=</span> <span class="token punctuation">&#123;</span><span class="token punctuation">&#125;</span>         loss_rpn<span class="token punctuation">,</span> tb_dict <span class="token operator">=</span> self<span class="token punctuation">.</span>backbone_3d<span class="token punctuation">.</span>get_loss<span class="token punctuation">(</span><span class="token punctuation">)</span>        tb_dict <span class="token operator">=</span> <span class="token punctuation">&#123;</span>            <span class="token string">'loss_rpn'</span><span class="token punctuation">:</span> loss_rpn<span class="token punctuation">.</span>item<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span>            <span class="token operator">**</span>tb_dict        <span class="token punctuation">&#125;</span>        loss <span class="token operator">=</span> loss_rpn        <span class="token keyword">return</span> loss<span class="token punctuation">,</span> tb_dict<span class="token punctuation">,</span> disp_dict<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p><strong>这里主要看BACKBONE_3D，具体包括这些：</strong></p><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">from</span> <span class="token punctuation">.</span>pointnet2_backbone <span class="token keyword">import</span> PointNet2Backbone<span class="token punctuation">,</span> PointNet2MSG<span class="token keyword">from</span> <span class="token punctuation">.</span>spconv_backbone <span class="token keyword">import</span> VoxelBackBone8x<span class="token punctuation">,</span> VoxelResBackBone8x<span class="token keyword">from</span> <span class="token punctuation">.</span>spconv_unet <span class="token keyword">import</span> UNetV2<span class="token keyword">from</span> <span class="token punctuation">.</span>voxel_mae <span class="token keyword">import</span> Voxel_MAE<span class="token keyword">from</span> <span class="token punctuation">.</span>voxel_mae_res <span class="token keyword">import</span> Voxel_MAE_res__all__ <span class="token operator">=</span> <span class="token punctuation">&#123;</span>    <span class="token string">'VoxelBackBone8x'</span><span class="token punctuation">:</span> VoxelBackBone8x<span class="token punctuation">,</span>    <span class="token string">'UNetV2'</span><span class="token punctuation">:</span> UNetV2<span class="token punctuation">,</span>    <span class="token string">'Voxel_MAE'</span><span class="token punctuation">:</span> Voxel_MAE<span class="token punctuation">,</span>    <span class="token string">'Voxel_MAE_res'</span><span class="token punctuation">:</span> Voxel_MAE_res<span class="token punctuation">,</span>    <span class="token string">'PointNet2Backbone'</span><span class="token punctuation">:</span> PointNet2Backbone<span class="token punctuation">,</span>    <span class="token string">'PointNet2MSG'</span><span class="token punctuation">:</span> PointNet2MSG<span class="token punctuation">,</span>    <span class="token string">'VoxelResBackBone8x'</span><span class="token punctuation">:</span> VoxelResBackBone8x<span class="token punctuation">,</span><span class="token punctuation">&#125;</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>核心部分，先看<a href="https://github.com/chaytonmin/Occupancy-MAE/blob/main/pcdet/models/backbones_3d/voxel_mae.py">Voxel_MAE.py</a><br>这段代码定义了一个名为 <code>Voxel_MAE</code> 的 PyTorch 模型。以下是对代码的一些主要解释：</p><ol><li><p><strong>SparseBasicBlock 类</strong>：这是一个基本的稀疏块，用于构建深度神经网络。它继承自 <code>spconv.SparseModule</code>，其中包含了一系列稀疏卷积、批量归一化和激活函数的层。该类被用作 Voxel_MAE 模型中的基本构建块。</p></li><li><p><strong>Voxel_MAE 类</strong>：这是主要的模型类。以下是该类的一些关键部分：</p><ul><li><p><strong>初始化方法</strong>：模型的初始化方法定义了网络的结构，包括一系列的稀疏卷积层 (<code>SparseSequential</code> 和 <code>SparseConv3d</code>)、归一化层 (<code>BatchNorm1d</code> 和 <code>BatchNorm3d</code>)、激活函数 (<code>ReLU</code>) 以及转置卷积层 (<code>ConvTranspose3d</code>)。模型的结构主要包括四个卷积阶段 (<code>conv1</code> 到 <code>conv4</code>)，每个阶段都包含了多个基本块 (<code>SparseBasicBlock</code>)。</p></li><li><p><strong>前向传播方法</strong>：<code>forward</code> 方法实现了模型的前向传播过程。在前向传播中，输入的点云数据首先通过一系列的稀疏卷积层，然后经过转置卷积层进行上采样。同时，根据模型配置，选择性地对输入进行掩码操作。最终，模型输出包括预测的稀疏张量和与输入相对应的目标。</p></li><li><p><strong>get_loss 方法</strong>：用于计算模型的损失。在这里，使用二进制交叉熵损失 (<code>BCEWithLogitsLoss</code>) 计算预测值与目标值之间的差异。</p></li></ul></li></ol><p>总体而言，这段代码实现了一个处理稀疏点云数据的神经网络模型，其中使用了稀疏卷积和转置卷积层，以及一些基本的块来构建网络结构。</p><p><strong>以下是 <code>Voxel_MAE</code> 模型的前向传播方法的逐行介绍：</strong></p><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> batch_dict<span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token triple-quoted-string string">"""    Args:        batch_dict:            batch_size: int            vfe_features: (num_voxels, C)            voxel_coords: (num_voxels, 4), [batch_idx, z_idx, y_idx, x_idx]    Returns:        batch_dict:            encoded_spconv_tensor: sparse tensor            point_features: (N, C)    """</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>这个前向传播方法接受一个 <code>batch_dict</code> 参数，其中包含了输入数据的一些关键信息，如批大小、体素特征 (<code>vfe_features</code>) 和体素坐标 (<code>voxel_coords</code>)。返回一个更新后的 <code>batch_dict</code>，其中包含了编码的稀疏卷积张量 (<code>encoded_spconv_tensor</code>) 和点特征 (<code>point_features</code>)。</p><pre class="line-numbers language-python" data-language="python"><code class="language-python">voxel_features<span class="token punctuation">,</span> voxel_coords <span class="token operator">=</span> batch_dict<span class="token punctuation">[</span><span class="token string">'voxel_features'</span><span class="token punctuation">]</span><span class="token punctuation">,</span> batch_dict<span class="token punctuation">[</span><span class="token string">'voxel_coords'</span><span class="token punctuation">]</span><span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>从输入的 <code>batch_dict</code> 中提取体素特征和体素坐标。</p><pre class="line-numbers language-python" data-language="python"><code class="language-python">select_ratio <span class="token operator">=</span> <span class="token number">1</span> <span class="token operator">-</span> self<span class="token punctuation">.</span>masked_ratio <span class="token comment"># ratio for select voxel</span><span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>计算用于选择体素的比率，即 1 减去模型配置中指定的 <code>masked_ratio</code>。</p><pre class="line-numbers language-python" data-language="python"><code class="language-python">voxel_coords_distance <span class="token operator">=</span> <span class="token punctuation">(</span>voxel_coords<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span><span class="token number">2</span><span class="token punctuation">]</span><span class="token operator">**</span><span class="token number">2</span> <span class="token operator">+</span> voxel_coords<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span><span class="token number">3</span><span class="token punctuation">]</span><span class="token operator">**</span><span class="token number">2</span><span class="token punctuation">)</span><span class="token operator">**</span><span class="token number">0.5</span><span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>计算体素坐标在 y 和 x 方向上的欧氏距离。</p><pre class="line-numbers language-python" data-language="python"><code class="language-python">select_30 <span class="token operator">=</span> voxel_coords_distance<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">]</span><span class="token operator">&lt;=</span><span class="token number">30</span>select_30to50 <span class="token operator">=</span> <span class="token punctuation">(</span>voxel_coords_distance<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">]</span><span class="token operator">></span><span class="token number">30</span><span class="token punctuation">)</span> <span class="token operator">&amp;</span> <span class="token punctuation">(</span>voxel_coords_distance<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">]</span><span class="token operator">&lt;=</span><span class="token number">50</span><span class="token punctuation">)</span>select_50 <span class="token operator">=</span> voxel_coords_distance<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">]</span><span class="token operator">></span><span class="token number">50</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre><p>根据欧氏距离将体素划分为三个区域，分别是距离小于等于 30、30 到 50、大于 50。</p><pre class="line-numbers language-python" data-language="python"><code class="language-python">id_list_select_30 <span class="token operator">=</span> torch<span class="token punctuation">.</span>argwhere<span class="token punctuation">(</span>select_30<span class="token operator">==</span><span class="token boolean">True</span><span class="token punctuation">)</span><span class="token punctuation">.</span>reshape<span class="token punctuation">(</span>torch<span class="token punctuation">.</span>argwhere<span class="token punctuation">(</span>select_30<span class="token operator">==</span><span class="token boolean">True</span><span class="token punctuation">)</span><span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">)</span>id_list_select_30to50 <span class="token operator">=</span> torch<span class="token punctuation">.</span>argwhere<span class="token punctuation">(</span>select_30to50<span class="token operator">==</span><span class="token boolean">True</span><span class="token punctuation">)</span><span class="token punctuation">.</span>reshape<span class="token punctuation">(</span>torch<span class="token punctuation">.</span>argwhere<span class="token punctuation">(</span>select_30to50<span class="token operator">==</span><span class="token boolean">True</span><span class="token punctuation">)</span><span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">)</span>id_list_select_50 <span class="token operator">=</span> torch<span class="token punctuation">.</span>argwhere<span class="token punctuation">(</span>select_50<span class="token operator">==</span><span class="token boolean">True</span><span class="token punctuation">)</span><span class="token punctuation">.</span>reshape<span class="token punctuation">(</span>torch<span class="token punctuation">.</span>argwhere<span class="token punctuation">(</span>select_50<span class="token operator">==</span><span class="token boolean">True</span><span class="token punctuation">)</span><span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre><p>为每个区域生成相应的索引列表。</p><pre class="line-numbers language-python" data-language="python"><code class="language-python">shuffle_id_list_select_30 <span class="token operator">=</span> id_list_select_30random<span class="token punctuation">.</span>shuffle<span class="token punctuation">(</span>shuffle_id_list_select_30<span class="token punctuation">)</span>shuffle_id_list_select_30to50 <span class="token operator">=</span> id_list_select_30to50random<span class="token punctuation">.</span>shuffle<span class="token punctuation">(</span>shuffle_id_list_select_30to50<span class="token punctuation">)</span>shuffle_id_list_select_50 <span class="token operator">=</span> id_list_select_50random<span class="token punctuation">.</span>shuffle<span class="token punctuation">(</span>shuffle_id_list_select_50<span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>对每个区域的索引列表进行随机打乱。</p><pre class="line-numbers language-python" data-language="python"><code class="language-python">slect_index <span class="token operator">=</span> torch<span class="token punctuation">.</span>cat<span class="token punctuation">(</span><span class="token punctuation">(</span>shuffle_id_list_select_30<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token builtin">int</span><span class="token punctuation">(</span>select_ratio<span class="token operator">*</span><span class="token builtin">len</span><span class="token punctuation">(</span>shuffle_id_list_select_30<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">]</span><span class="token punctuation">,</span>                          shuffle_id_list_select_30to50<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token builtin">int</span><span class="token punctuation">(</span><span class="token punctuation">(</span>select_ratio<span class="token operator">+</span><span class="token number">0.2</span><span class="token punctuation">)</span><span class="token operator">*</span><span class="token builtin">len</span><span class="token punctuation">(</span>shuffle_id_list_select_30to50<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">]</span><span class="token punctuation">,</span>                          shuffle_id_list_select_50<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token builtin">int</span><span class="token punctuation">(</span><span class="token punctuation">(</span>select_ratio<span class="token operator">+</span><span class="token number">0.2</span><span class="token punctuation">)</span><span class="token operator">*</span><span class="token builtin">len</span><span class="token punctuation">(</span>shuffle_id_list_select_50<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span></span></code></pre><p>将选择的索引合并成一个索引列表，以实现按比率选择体素。</p><pre class="line-numbers language-python" data-language="python"><code class="language-python">nums <span class="token operator">=</span> voxel_features<span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span>voxel_fratures_all_one <span class="token operator">=</span> torch<span class="token punctuation">.</span>ones<span class="token punctuation">(</span>nums<span class="token punctuation">,</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">.</span>to<span class="token punctuation">(</span>voxel_features<span class="token punctuation">.</span>device<span class="token punctuation">)</span>voxel_features_partial<span class="token punctuation">,</span> voxel_coords_partial <span class="token operator">=</span> voxel_features<span class="token punctuation">[</span>slect_index<span class="token punctuation">,</span><span class="token punctuation">:</span><span class="token punctuation">]</span><span class="token punctuation">,</span> voxel_coords<span class="token punctuation">[</span>slect_index<span class="token punctuation">,</span><span class="token punctuation">:</span><span class="token punctuation">]</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span></span></code></pre><p><strong>生成所有体素特征为 1 的全 1 张量，并根据选择的索引提取部分体素特征和坐标。</strong></p><pre class="line-numbers language-python" data-language="python"><code class="language-python">batch_size <span class="token operator">=</span> batch_dict<span class="token punctuation">[</span><span class="token string">'batch_size'</span><span class="token punctuation">]</span>input_sp_tensor <span class="token operator">=</span> spconv<span class="token punctuation">.</span>SparseConvTensor<span class="token punctuation">(</span>    features<span class="token operator">=</span>voxel_features_partial<span class="token punctuation">,</span>    indices<span class="token operator">=</span>voxel_coords_partial<span class="token punctuation">.</span><span class="token builtin">int</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span>    spatial_shape<span class="token operator">=</span>self<span class="token punctuation">.</span>sparse_shape<span class="token punctuation">,</span>    batch_size<span class="token operator">=</span>batch_size<span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>使用 <code>spconv.SparseConvTensor</code> 构建稀疏卷积张量，输入包括部分体素特征、坐标、稀疏形状和批大小。</p><pre class="line-numbers language-python" data-language="python"><code class="language-python">input_sp_tensor_ones <span class="token operator">=</span> spconv<span class="token punctuation">.</span>SparseConvTensor<span class="token punctuation">(</span>    features<span class="token operator">=</span>voxel_fratures_all_one<span class="token punctuation">,</span>    indices<span class="token operator">=</span>voxel_coords<span class="token punctuation">.</span><span class="token builtin">int</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span>    spatial_shape<span class="token operator">=</span>self<span class="token punctuation">.</span>sparse_shape<span class="token punctuation">,</span>    batch_size<span class="token operator">=</span>batch_size<span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>构建所有体素特征为 1 的稀疏卷积张量，用于计算损失。</p><pre class="line-numbers language-python" data-language="python"><code class="language-python">x <span class="token operator">=</span> self<span class="token punctuation">.</span>conv_input<span class="token punctuation">(</span>input_sp_tensor<span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>将输入的稀疏卷积张量传递给第一个稀疏卷积层。</p><pre class="line-numbers language-python" data-language="python"><code class="language-python">x_conv1 <span class="token operator">=</span> self<span class="token punctuation">.</span>conv1<span class="token punctuation">(</span>x<span class="token punctuation">)</span>x_conv2 <span class="token operator">=</span> self<span class="token punctuation">.</span>conv2<span class="token punctuation">(</span>x_conv1<span class="token punctuation">)</span>x_conv3 <span class="token operator">=</span> self<span class="token punctuation">.</span>conv3<span class="token punctuation">(</span>x_conv2<span class="token punctuation">)</span>x_conv4 <span class="token operator">=</span> self<span class="token punctuation">.</span>conv4<span class="token punctuation">(</span>x_conv3<span class="token punctuation">)</span>out <span class="token operator">=</span> self<span class="token punctuation">.</span>conv_out<span class="token punctuation">(</span>x_conv4<span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>依次通过模型的各个卷积层，得到最终的输出 <code>out</code>。</p><pre class="line-numbers language-python" data-language="python"><code class="language-python">self<span class="token punctuation">.</span>forward_re_dict<span class="token punctuation">[</span><span class="token string">'target'</span><span class="token punctuation">]</span> <span class="token operator">=</span> input_sp_tensor_ones<span class="token punctuation">.</span>dense<span class="token punctuation">(</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>将目标值（全 1 张量）添加到 <code>forward_re_dict</code> 中。</p><pre class="line-numbers language-python" data-language="python"><code class="language-python">x_up1 <span class="token operator">=</span> self<span class="token punctuation">.</span>deconv1<span class="token punctuation">(</span>out<span class="token punctuation">.</span>dense<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span>x_up2 <span class="token operator">=</span> self<span class="token punctuation">.</span>deconv2<span class="token punctuation">(</span>x_up1<span class="token punctuation">)</span>x_up3 <span class="token operator">=</span> self<span class="token punctuation">.</span>deconv3<span class="token punctuation">(</span>x_up2<span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre><p>通过一系列的转置卷积层进行上采样。</p><pre class="line-numbers language-python" data-language="python"><code class="language-python">self<span class="token punctuation">.</span>forward_re_dict<span class="token punctuation">[</span><span class="token string">'pred'</span><span class="token punctuation">]</span> <span class="token operator">=</span> x_up3<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>将预测值添加到 <code>forward_re_dict</code> 中。</p><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">return</span> batch_dict<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>返回更新后的 <code>batch_dict</code>。</p><p>损失计算。</p><pre class="line-numbers language-python" data-language="python"><code class="language-python">self<span class="token punctuation">.</span>criterion <span class="token operator">=</span> nn<span class="token punctuation">.</span>BCEWithLogitsLoss<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token comment">#二元交叉熵损失函数</span>self<span class="token punctuation">.</span>forward_re_dict <span class="token operator">=</span> <span class="token punctuation">&#123;</span><span class="token punctuation">&#125;</span><span class="token keyword">def</span> <span class="token function">get_loss</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> tb_dict<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">)</span><span class="token punctuation">:</span>   tb_dict <span class="token operator">=</span> <span class="token punctuation">&#123;</span><span class="token punctuation">&#125;</span> <span class="token keyword">if</span> tb_dict <span class="token keyword">is</span> <span class="token boolean">None</span> <span class="token keyword">else</span> tb_dict   pred <span class="token operator">=</span> self<span class="token punctuation">.</span>forward_re_dict<span class="token punctuation">[</span><span class="token string">'pred'</span><span class="token punctuation">]</span>   target <span class="token operator">=</span> self<span class="token punctuation">.</span>forward_re_dict<span class="token punctuation">[</span><span class="token string">'target'</span><span class="token punctuation">]</span>   loss <span class="token operator">=</span> self<span class="token punctuation">.</span>criterion<span class="token punctuation">(</span>pred<span class="token punctuation">,</span> target<span class="token punctuation">)</span>   tb_dict <span class="token operator">=</span> <span class="token punctuation">&#123;</span>      <span class="token string">'loss_rpn'</span><span class="token punctuation">:</span> loss<span class="token punctuation">.</span>item<span class="token punctuation">(</span><span class="token punctuation">)</span>   <span class="token punctuation">&#125;</span>   <span class="token keyword">return</span> loss<span class="token punctuation">,</span> tb_dict<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p><strong><a href="https://github.com/chaytonmin/Occupancy-MAE/blob/main/pcdet/models/backbones_3d/voxel_mae_res.py">Voxel_MAE_res.py</a>区别不是很大，具体可以看源码</strong></p><h4 id="补充"><a href="#补充" class="headerlink" title="补充"></a>补充</h4><p><strong><a href="https://github.com/chaytonmin/Occupancy-MAE/blob/main/pcdet/models/backbones_3d/vfe/__init__.py">VFE</a>模块的包括如下：</strong></p><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">from</span> <span class="token punctuation">.</span>mean_vfe <span class="token keyword">import</span> MeanVFE<span class="token keyword">from</span> <span class="token punctuation">.</span>pillar_vfe <span class="token keyword">import</span> PillarVFE<span class="token keyword">from</span> <span class="token punctuation">.</span>dynamic_mean_vfe <span class="token keyword">import</span> DynamicMeanVFE<span class="token keyword">from</span> <span class="token punctuation">.</span>dynamic_pillar_vfe <span class="token keyword">import</span> DynamicPillarVFE<span class="token keyword">from</span> <span class="token punctuation">.</span>image_vfe <span class="token keyword">import</span> ImageVFE<span class="token keyword">from</span> <span class="token punctuation">.</span>vfe_template <span class="token keyword">import</span> VFETemplate__all__ <span class="token operator">=</span> <span class="token punctuation">&#123;</span>    <span class="token string">'VFETemplate'</span><span class="token punctuation">:</span> VFETemplate<span class="token punctuation">,</span>    <span class="token string">'MeanVFE'</span><span class="token punctuation">:</span> MeanVFE<span class="token punctuation">,</span>    <span class="token string">'PillarVFE'</span><span class="token punctuation">:</span> PillarVFE<span class="token punctuation">,</span>    <span class="token string">'ImageVFE'</span><span class="token punctuation">:</span> ImageVFE<span class="token punctuation">,</span>    <span class="token string">'DynMeanVFE'</span><span class="token punctuation">:</span> DynamicMeanVFE<span class="token punctuation">,</span>    <span class="token string">'DynPillarVFE'</span><span class="token punctuation">:</span> DynamicPillarVFE<span class="token punctuation">,</span><span class="token punctuation">&#125;</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p><strong>这里主要看<a href="https://github.com/chaytonmin/Occupancy-MAE/blob/main/pcdet/models/backbones_3d/vfe/mean_vfe.py">MeanVFE.py</a>:</strong></p><p>这是一个用于点云特征提取的模块，该模块实现了一个均值池化的 VFE（Voxel Feature Extractor）。以下是对代码的解释：</p><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">import</span> torch<span class="token keyword">from</span> <span class="token punctuation">.</span>vfe_template <span class="token keyword">import</span> VFETemplate<span class="token keyword">class</span> <span class="token class-name">MeanVFE</span><span class="token punctuation">(</span>VFETemplate<span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> model_cfg<span class="token punctuation">,</span> num_point_features<span class="token punctuation">,</span> <span class="token operator">**</span>kwargs<span class="token punctuation">)</span><span class="token punctuation">:</span>        <span class="token builtin">super</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span>model_cfg<span class="token operator">=</span>model_cfg<span class="token punctuation">)</span>        self<span class="token punctuation">.</span>num_point_features <span class="token operator">=</span> num_point_features    <span class="token keyword">def</span> <span class="token function">get_output_feature_dim</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span><span class="token punctuation">:</span>        <span class="token keyword">return</span> self<span class="token punctuation">.</span>num_point_features    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> batch_dict<span class="token punctuation">,</span> <span class="token operator">**</span>kwargs<span class="token punctuation">)</span><span class="token punctuation">:</span>        <span class="token triple-quoted-string string">"""        Args:            batch_dict:                voxels: (num_voxels, max_points_per_voxel, C)                voxel_num_points: optional (num_voxels)            **kwargs:        Returns:            vfe_features: (num_voxels, C)        """</span>        voxel_features<span class="token punctuation">,</span> voxel_num_points <span class="token operator">=</span> batch_dict<span class="token punctuation">[</span><span class="token string">'voxels'</span><span class="token punctuation">]</span><span class="token punctuation">,</span> batch_dict<span class="token punctuation">[</span><span class="token string">'voxel_num_points'</span><span class="token punctuation">]</span>        points_mean <span class="token operator">=</span> voxel_features<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span> <span class="token punctuation">:</span><span class="token punctuation">,</span> <span class="token punctuation">:</span><span class="token punctuation">]</span><span class="token punctuation">.</span><span class="token builtin">sum</span><span class="token punctuation">(</span>dim<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">,</span> keepdim<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">)</span>        normalizer <span class="token operator">=</span> torch<span class="token punctuation">.</span>clamp_min<span class="token punctuation">(</span>voxel_num_points<span class="token punctuation">.</span>view<span class="token punctuation">(</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token builtin">min</span><span class="token operator">=</span><span class="token number">1.0</span><span class="token punctuation">)</span><span class="token punctuation">.</span>type_as<span class="token punctuation">(</span>voxel_features<span class="token punctuation">)</span>        points_mean <span class="token operator">=</span> points_mean <span class="token operator">/</span> normalizer        batch_dict<span class="token punctuation">[</span><span class="token string">'voxel_features'</span><span class="token punctuation">]</span> <span class="token operator">=</span> points_mean<span class="token punctuation">.</span>contiguous<span class="token punctuation">(</span><span class="token punctuation">)</span>        <span class="token keyword">return</span> batch_dict<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><ol><li><p><strong>类结构：</strong></p><ul><li><code>MeanVFE</code> 类继承自 <code>VFETemplate</code> 类，表示它是一个 VFE 模块的具体实现。</li></ul></li><li><p><strong>初始化方法：</strong></p><ul><li><code>__init__</code> 方法接受模型配置 (<code>model_cfg</code>) 和点的特征数量 (<code>num_point_features</code>) 作为参数，并通过调用父类的初始化方法来初始化模块。</li></ul></li><li><p><strong>输出特征维度获取：</strong></p><ul><li><code>get_output_feature_dim</code> 方法返回 VFE 模块的输出特征维度，即 <code>num_point_features</code>。</li></ul></li><li><p><strong>前向传播方法：</strong></p><ul><li><code>forward</code> 方法用于执行前向传播。</li><li>接收一个字典 <code>batch_dict</code> 作为输入，其中包含了 <code>voxels</code>（体素特征）和可选的 <code>voxel_num_points</code>（每个体素中的点的数量）。</li><li>计算每个体素中点的均值，然后更新 <code>batch_dict[&#39;voxel_features&#39;]</code> 为计算得到的均值特征。</li><li>返回更新后的 <code>batch_dict</code>。</li></ul></li><li><p><strong>均值池化过程：</strong></p><ul><li>通过对 <code>voxel_features</code> 在第二个维度（点的维度）进行求和，得到每个体素内点特征的总和。</li><li>创建一个 <code>normalizer</code>，其值为每个体素内的点的数量，用于规范化均值的计算。</li><li>利用 <code>normalizer</code> 对点特征总和进行均值计算。</li><li>将计算得到的均值作为更新后的 <code>voxel_features</code>。</li></ul></li></ol><p>这个模块的主要作用是在每个体素中对点的特征进行均值池化，以获得更紧凑的体素特征表示。</p>]]></content>
      
      
      <categories>
          
          <category> MAE </category>
          
      </categories>
      
      
        <tags>
            
            <tag> mae </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>室外大规模点云自监督学习理论篇(MAE，持续更新)</title>
      <link href="/2024/01/02/mae3d/"/>
      <url>/2024/01/02/mae3d/</url>
      
        <content type="html"><![CDATA[<h1 id="室外大规模点云自监督学习理论篇-MAE，持续更新"><a href="#室外大规模点云自监督学习理论篇-MAE，持续更新" class="headerlink" title="室外大规模点云自监督学习理论篇(MAE，持续更新)"></a>室外大规模点云自监督学习理论篇(MAE，持续更新)</h1><p><a href="https://arxiv.org/pdf/2207.00531">一：Voxel-MAE:Masked Autoencoder for Self-Supervised Pre-training on Lidar Point Clouds</a></p><p><a href="https://github.com/georghess/voxel-MAE">代码链接</a></p><p><a href="https://georghess.se/projects/voxel-MAE/">项目链接</a></p><p><a href="https://arxiv.org/pdf/2206.09900">二：Occupancy-MAE,原版名为Voxel-MAE(v1~v7)</a></p><p><a href="https://github.com/chaytonmin/Occupancy-MAE">代码链接</a><br><a href="https://zhuanlan.zhihu.com/p/576288307">原版Voxel-MAE讲解</a><br><a href="https://blog.csdn.net/weixin_45657478/article/details/132197567">Occupancy-MAE讲解</a></p><p><a href="https://openaccess.thecvf.com/content/WACV2024/papers/Krispel_MAELi_Masked_Autoencoder_for_Large-Scale_LiDAR_Point_Clouds_WACV_2024_paper.pdf">三：MAELi: Masked Autoencoder for Large-Scale LiDAR Point Clouds</a></p><p><a href="">代码暂未开源：基于OpenPCDet</a></p><h2 id="第一篇：Voxel-MAE-Masked-Autoencoder-for-Self-Supervised-Pre-training-on-Lidar-Point-Clouds"><a href="#第一篇：Voxel-MAE-Masked-Autoencoder-for-Self-Supervised-Pre-training-on-Lidar-Point-Clouds" class="headerlink" title="第一篇：Voxel-MAE:Masked Autoencoder for Self-Supervised Pre-training on Lidar Point Clouds"></a>第一篇：Voxel-MAE:Masked Autoencoder for Self-Supervised Pre-training on Lidar Point Clouds</h2><h3 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h3><p>掩码自动编码已经成为文本、图像和最近的点云Transformer模型的成功预训练范例。原始汽车数据集适合用于自我监督预训练，因为与用于3D目标检测(OD)等任务的注释相比，它们的收集成本通常更低。然而，针对点云的掩码自编码器的开发只关注于合成数据和室内数据。因此，现有的方法将其表示和模型定制为具有均匀点密度的小而密集的点云。在这项工作中，我们研究了汽车场景中点云的掩码自动编码，这些点云是稀疏的，并且在同一场景中不同物体之间的点密度可能会发生巨大变化。为此，我们提出了Voxel-MAE，这是一种用于体素表示的简单掩码自动编码预训练方案。我们预训练一个基于Transformer的3D物体检测器的主干来重建掩码体素，并区分空体素和非空体素。在具有挑战性的nuScenes数据集上，我们的方法将3D OD性能提高了1.75个mAP点和1.05个NDS。此外，我们表明，通过使用Voxel-MAE进行预训练，我们只需要40%的注释数据就可以优于随机初始化的等效数据。</p><h3 id="引言"><a href="#引言" class="headerlink" title="引言"></a>引言</h3><p>自监督学习可以在不需要人工注释的情况下从数据中提取丰富的特征。这开辟了新的途径，可以在更大的数据集上训练模型。在鲁棒表示的推动下，自监督模型在自然语言处理(NLP)[3,12,32]和计算机视觉(CV)[5,8,19]等领域取得了巨大成功。具体来说，掩码语言建模[BERT]和掩码图像建模[BEIT,MAE]已经被证明是简单而有效的预训练策略。这两种方法都训练模型从部分掩码的输入中重建句子或图像。随后，模型可以很好地调整到下游任务，通常比完全监督的等效模型表现得更好。</p><p><img src="/pic/mae3d1.png" alt="图1:MAE(左)将图像划分为固定大小的不重叠的小块。现有的掩码点建模方法(中)是利用最远点采样和k近邻来创建固定数量的点云补丁。我们的方法(右)使用具有动态点数的非重叠体素。飞机图来自Point-MAE"></p><p>自动驾驶是一个非常适合自监督预训练策略的应用，包括掩码自动编码。在汽车领域，原始数据的收集相对便宜，而目标检测(OD)、跟踪和语义分割等常见任务的注释则昂贵且耗时。特别是对于3D数据，激光雷达和雷达传感器的稀疏性可能会使标记变得费力甚至模糊。因此，自我监督的预训练是创建鲁棒和通用特征表示的一个有吸引力的替代方案，并最终减少对人工注释数据的需求。</p><p>近年来，已有多篇论文将掩码点建模技术应用于点云编码器的预训练。这些方法在形状分类、形状分割、少镜头分类、室内3D OD等下游任务上都取得了良好的效果，表明了掩码自编码器在点云领域的有效性。然而，评估主要集中在ShapeNet和ModelNet40等合成数据，以及ScanObjectNN、ScanNet和SUN RGB-D等室内数据集上。与汽车点云相比，这些数据集包含所有物体的许多点，并且点密度在扫描中通常是恒定的，这使得物体的检测和分类变得不那么具有挑战性。</p><p>此外，现有的方法针对数据集特征定制了点云表示和模型选择等设计选择。例如，每个场景更少的点降低了对计算效率的要求，并允许使用普通的transformer。此外，以前的工作完全依赖于最远点采样(FPS)和k近邻(kNN)将点云划分为相同数量点的子集，如图1所示。当点云均匀分布时，这种方法效果很好，并且简化了预训练期间的重建，因为模型为每个子集预测固定数量的点。然而，这种表示对于有效解决汽车领域的下游任务来说是次优的。首先，存在丢弃点的风险，如图1中机翼尖端所示。这种潜在的信息丢失使得它不适合安全关键型应用程序。其次，由于子集可能重叠，表示是冗余的，从而产生不必要的计算负载。</p><p>在这项工作中，我们建议在汽车设置中使用掩码点建模。为此，我们提出了VoxelMAE，一种体素化点云的掩码自编码器预训练策略，并将其部署在大规模汽车数据集nuScenes[4]上，研究其对3D OD的影响。体素表示由于能够有效地描述大型点云而被广泛应用于3D OD中，但尚未用于掩码自编码器的预训练。为了在重建过程中捕捉体素的独特性质，我们提出了一组独特的损失函数来同时捕获形状、点密度和缺位点。与PointBERT和POS-BERT等先前的方法相比，我们的方法更简单，因为它不依赖于训练一个单独的标记器来嵌入和重建点云。</p><p>继自监督Transformer在NLP和CV中的成功之后，Voxel-MAE利用Transformer主干提取点云特征。之所以选择Transformer架构，是因为在部署广泛掩码时，它的预训练规模更有利，因为只有未掩码的数据被嵌入到编码器中。此外，该模型通过仅处理非空体素来有效地处理稀疏点云。有趣的是，只有少数用于汽车点云的Transformer主干存在，并且它们的自监督预训练之前没有被探索过。</p><p>在这项工作中，<strong>我们使用单步稀疏Transformer(Single-stride Sparse Transformer, SST)作为我们的点云编码器</strong>，它直接对体素化的点云应用移位窗口Transformer，类似于图像的SwinTransformer[26]。SST在3D物体检测方面取得了有竞争力的结果，在计算效率高的同时捕获了精细的细节，使其成为一个强大的基线来改进。对于预训练，我们遵循MAE[19]的范式，并为模型配备一个轻量级的解码器，该解码器在结构上与编码器相似。</p><p>综上所述，我们提出以下贡献:</p><ul><li>我们提出了一种在体素化点云上部署MAE式自监督预训练的方法Voxel-MAE，并在大规模汽车点云数据集nuScenes上对其进行了评估。我们的方法是第一个自监督预训练方案，该方案使用Transformer主干用于汽车点云。</li><li>我们针对体素表示定制我们的方法，并使用一组独特的重建任务来捕获体素化点云的特征。</li><li>我们证明了我们的方法是数据高效的，并且减少了对注释数据的需求。通过预训练，我们在仅使用40%的注释数据时优于完全监督的等效方法。</li><li>此外，我们表明，Voxel-MAE将基于transformer的检测器的性能在mAP中提高了1.75%，在NDS中提高了1.05个百分点，与现有的自监督方法相比，性能提高了2倍。</li></ul><p><img src="/pic/mae3d2.png" alt="图2:我们的Voxel-MAE方法.首先，用固定的体素大小对点云进行体素化。为了可视化的目的，图中的体素大小被夸大了。在预训练期间，很大一部分(70%)的非空体素被随机掩码掉。然后，编码器仅应用于使用**动态体素特征嵌入**的可见体素[MVF](https://arxiv.org/pdf/1910.06528v2.pdf)。被掩码的非空体素和随机选择的空体素使用相同的可学习掩码toke嵌入。然后由解码器处理掩码toke序列和编码的可见体素，以重建掩码点云并区分空体素和非空体素。预训练后，丢弃解码器，将编码器应用于解掩点云。"></p><h3 id="相关工作"><a href="#相关工作" class="headerlink" title="相关工作"></a>相关工作</h3><p><strong>用于语言和图像的掩码自动编码器</strong>。掩码语言建模(MLM)及其衍生工具，如BERT[12]和GPT[3,32,33]，在NLP中已经非常成功。这些方法通过掩码输入句子的一部分来学习数据表示，并训练模型来预测缺失的部分。这些方法的可扩展性很好，可以在前所未有的数据集上进行训练，并且它们的表示可以推广到各种下游任务。受其成功的启发，多种方法将类似的技术应用于图像域[2,7,13,19,40]。最近，[19]的作者提出了MAE，这是一种简单的方法，其中随机图像补丁被掩码，并将其像素值作为重建目标。此外，他们部署了非对称编码器-解码器架构，其中编码器只嵌入可见的补丁，并且使用轻量级解码器进行重建。与完全监督的基线相比，MAE在一系列下游任务上的性能得到了提高。Voxel-MAE遵循这一设计理念，对稀疏点云数据进行非平凡的转换。</p><p><strong>点云的掩码自动编码器</strong>。受MLM在自然语言处理和MAE在计算机视觉上的成功启发，人们提出了对点云域的多种适应。Point-BERT[41]首先引入了BERT风格的点云预训练，掩码和重建部分输入。虽然取得了有竞争力的结果，但他们的方法依赖于训练一个单独的离散变分自动编码器(dVEA)来标记点云补丁，增加了复杂性和对标记器性能的依赖。Point-MAE[31]去除标记器，直接重建点补丁，使用Chamfer距离来测量预测点云与真实点云之间的相似性。与Point-BERT相比，这加快了训练速度，也提高了下游性能。MaskPoint[24]通过去除点云重构进一步加快了预训练。相反，解码器被训练来区分被掩码的点补丁和随机采样的假的空的点补丁。</p><p><strong>3D目标检测的自监督学习</strong>。虽然户外3D检测可以从自我监督学习中获益良多，但该领域通常尚未得到充分开发。STRL[20]遵循BYOL[18]方法，训练两个点云编码器，在呈现两个时间相关的点云时创建一致的潜在表示。然而，训练两个编码器可能会限制模型的大小，因为在预训练期间内存需求增加。GCC3D[23]通过训练模型应用对比学习，在呈现同一点云的两个增强视图时产生体素方面的相似特征。使用两个后续点云进行预训练，并训练模型以估计帧之间的场景流。这可以看作是掩码自动编码器的一个特殊情况，其中掩码是暂时完成的。然而，他们的方法依赖于一种特殊的交替训练方案，在自我监督和监督训练之间切换。相比之下，<strong>我们的方法支持一种更简单的顺序训练策略，首先对模型进行预训练，然后根据需要进行微调</strong>。因此，我们避免了每次将模型训练到下游任务时必须处理大型未注释数据集的问题。</p><h3 id="方法"><a href="#方法" class="headerlink" title="方法"></a>方法</h3><p>本工作旨在将MAE风格的预训练[19]扩展到体素化点云。核心思想仍然是使用编码器从输入的部分观察中创建丰富的潜在表示，然后使用解码器重建原始输入，如图2所示。经过预训练后，编码器被用作三维目标检测器的主干。但是，由于图像和点云之间的根本差异，需要进行一些修改才能有效地训练Voxel-MAE，如下所述。</p><h4 id="掩码和体素嵌入"><a href="#掩码和体素嵌入" class="headerlink" title="掩码和体素嵌入"></a>掩码和体素嵌入</h4><p>与将图像划分为不重叠的小块类似，首先将点云划分为体素。体素为不规则的点云带来结构，使高效的处理成为可能，同时为密集的预测任务(如3D OD)保留足够的细节。然而，与图像补丁相比，体素也带来了独特的挑战。</p><ul><li>首先，由于遮挡和激光雷达数据固有的稀疏性，视场中很大一部分体素通常是空的。而不是使用所有体素，<strong>我们丢弃空体素，以避免不必要的计算压力</strong>。</li><li>在预训练中，我们掩码了很大一部分(70%)的非空体素，并<strong>用编码器只处理可见体素</strong>，进一步提高了计算效率。场景之间不同数量的可见体素通过Transformer的多对多映射巧妙地处理。其次，由于点密度的变化，分配给单个体素的点的数量可以从一个到几百个不等。为了将每个可见体素中的所有点嵌入到单个特征向量中，我们使用动态体素特征编码器<a href="https://blog.csdn.net/qq_38650028/article/details/105323922">MVF</a>。掩码体素被嵌入一个共享的、可学习的掩码token。</li></ul><h4 id="编码"><a href="#编码" class="headerlink" title="编码"></a>编码</h4><p>对于编码可见体素，我们使用单步稀疏转换器(SST)的编码器[15]。SST是一种基于transformer的3D目标检测器，操作在体素上，可以很容易地将预训练的骨干权重转移到3D OD的下游任务。</p><ul><li>SST编码器是通过堆叠多个Transformer编码器层构建的，其中非空体素被视为单独的token，点云被认为是这些token的序列。此外，每个标记都伴随着基于体素在视场中的位置的位置嵌入。</li><li>由于自注意机制中的二次复杂度，Transformer对序列长度的扩展能力较差，因此SST引入了区域分组和区域移位。受Swin Transformer[26]中移位窗口的启发，将视场划分为不重叠的3D区域。自注意仅在同一区域内的体素之间计算，与全局自注意相比，大大减少了计算负荷。为了实现来自不同区域的体素之间的交互，每个编码器层都会移动这些区域，并根据新区域对体素进行分组。区域分组和仅处理非空体素的结合限制了使用Voxel-MAE预训练SST的计算空间，特别是在广泛掩码的情况下。</li></ul><h4 id="解码"><a href="#解码" class="headerlink" title="解码"></a>解码</h4><p>在对可见体素进行编码后，使用解码器利用丰富的潜在表示来重建原始点云。请注意，解码器仅在预训练期间使用，并且在对下游任务的模型进行微调时被丢弃。如图2所示，</p><ul><li>嵌入体素的序列被掩码体素扩展。它们作为共享的、学习的掩码token以及它们各自的位置嵌入一起嵌入，这样解码器就可以区分它们。</li><li>除了编码和掩码体素外，我们还添加了一组空的掩码体素，类似于[24]。我们通过在视场中的空体素中随机采样，并以与非空体素相同的方式嵌入它们来实现这一点。增加了空掩码体素，使重建任务更加困难，有效地促进了编码器的学习。通过只处理包含点的体素，模型将具有接近完美的占用知识，从而不必学习点云的这一属性。</li><li>相反，我们迫使解码器学习区分非空和空掩码体素，并忽略空体素进行重建。根据经验，我们发现采样10%的空体素可以提供良好的性能，而不会引入不必要的计算开销。解码器具有与编码器相似的结构，由SST编码器层组成，但使用较少的层。部分原因可能是预训练所需的时间减少了，但我们也发现，当与较小的解码器一起训练时，编码器可以获得更高的下游任务性能，类似于[19]中的结果。</li></ul><h4 id="重建目标"><a href="#重建目标" class="headerlink" title="重建目标"></a>重建目标</h4><p>解码器由三个不同的重建任务监督，每个任务监督点云固有的某个特征。对于每个任务，我们对解码器输出应用一个单独的线性层，以将嵌入投影到合适的维度。下面描述了这三个任务及其相应的损失函数。</p><p>如前所述，每个体素包含不同数量的点。对于精确的重建，这将<strong>要求预测头为每个体素预测不同数量的点。这可以使用递归神经网络来实现</strong>，但代价是简单。相反，我们建议预测固定数量的点n，从而可以使用简单的线性层来预测这些点。这种重建是用Chamfer距离来监督的，Chamfer距离测量两组点之间的距离，并允许两组点具有不同的基数。设Pgt &#x3D; {Pgt i}N i&#x3D;1为被遮挡点云，该点云被划分为N个体素，其中每个体素Pgt i&#x3D; {xj}ni j&#x3D;1包含ni个点，其中ni可以在体素之间变化。类似地，预测的点云Ppre &#x3D; {Ppre i}N i&#x3D;1包含N个体素Ppre i&#x3D; {xj} N j&#x3D;1,其中所有i的N固定。我们计算每个掩码体素的Chamfer距离，并定义我们的Chamfer损失为</p><p><img src="/pic/mae3d3.png" alt="Chamfer损失"></p><p>当预测点的数量n超过体素中真实点的数量ni时，该模型仍然可以通过在同一位置放置重复点来最小化Chamfer损失。对于另一种情况，n &lt; ni，研究表明[38]，即使在基数不匹配的情况下，Chamfer损失也会促使模型预测捕获真实点云中的细节。</p><p>为了进一步明确地学习点云分布的不均匀性，我们还<strong>预测了每个非空掩码体素的点数ni</strong>。由于目标ni的范围可以从1到几百，我们使用平滑L1损失来监督预测，以避免梯度爆炸</p><p><img src="/pic/mae3d4.png" alt="非空掩码体素的点数ni损失"></p><p>最后，<strong>对于每个被掩码体素，我们预测它是空的还是非空的</strong>。这个任务是用一个简单的二元交叉熵损失Locc来监督的。预训练的总损失是</p><p><img src="/pic/mae3d5.png" alt="非空掩码体素的点数ni损失"></p><h2 id="第二篇-Occupancy-MAE-Self-supervised-Pre-training-Large-scale-LiDAR-Point-Clouds-with-Masked-Occupancy-Autoencoders"><a href="#第二篇-Occupancy-MAE-Self-supervised-Pre-training-Large-scale-LiDAR-Point-Clouds-with-Masked-Occupancy-Autoencoders" class="headerlink" title="第二篇:Occupancy-MAE: Self-supervised Pre-training Large-scale LiDAR Point Clouds with Masked Occupancy Autoencoders"></a>第二篇:Occupancy-MAE: Self-supervised Pre-training Large-scale LiDAR Point Clouds with Masked Occupancy Autoencoders</h2><h3 id="摘要-1"><a href="#摘要-1" class="headerlink" title="摘要"></a>摘要</h3><p>当前的自动驾驶感知模型严重依赖于大规模标记的3D数据，这些数据的标注既昂贵又耗时。这项工作提出了一种解决方案，通过使用掩码自动编码器(MAE)对大规模未标记的户外LiDAR点云进行预训练，来减少对标记3D训练数据的依赖。现有的掩码点自动编码方法主要集中在小规模室内点云或基于柱的大规模室外LiDAR数据上，而我们的方法引入了一种新的自监督掩码占用预训练方法，称为Occupancy-MAE，专门针对基于体素的大规模室外LiDAR点云设计。Occupancy-MAE利用了室外LiDAR点云逐渐稀疏的体素占用结构，并结合了<strong>距离感知随机掩码策略和占用预测的Pretext Task</strong>。通过根据体素与激光雷达的距离随机屏蔽体素，并预测整个3D周围场景的被屏蔽占用结构，<strong>Occupancy-MAE鼓励提取高级语义信息，仅使用少量可见体素重建被屏蔽体素</strong>。大量的实验证明了Occupancy-MAE在几个下游任务中的有效性。对于3D物体检测，Occupancy-MAE将KITTI数据集上汽车检测所需的标记数据减少了一半，并将Waymo数据集上AP的小物体检测提高了约2%。对于3D语义分割，Occupancy-MAE在mIoU中比从头开始训练高出约2%。对于多目标跟踪，Occupancy-MAE在AMOTA和AMOTP方面从零开始提高了大约1%的训练。</p><p><img src="/pic/mae3d6.png" alt="图1所示。我们的自监督预训练的标签效率。Occupancy-MAE优于从头开始的训练，并且在标记数据较少的情况下实现了相同的检测性能(汽车类别约为50%，行人类别约为75%)。"></p><h3 id="引言-1"><a href="#引言-1" class="headerlink" title="引言"></a>引言</h3><p>准确的3D感知是自动驾驶的核心技术，它使车辆能够获得关于周围环境的精确信息[1]。KITTI[2]、Waymo[3]、nuScenes[4]和ONCE[5]等大量大型户外激光雷达点云数据集已经发布，展示了无人驾驶车辆环境感知的潜力。然而，为3D物体检测和语义分割等常见任务收集和注释大规模LiDAR点云可能非常耗时和费力。例如，熟练工人每天只能注释大约100-200帧[5]。因此，利用大规模无标注的LiDAR点云进行自监督学习对于提高自动驾驶的感知能力至关重要。它可能为开发下一代强大而稳健的行业级自动驾驶感知模型铺平道路[5]。</p><p>近年来，自监督学习取得了重大进展，可以在没有人工注释的情况下对丰富的特征进行预训练。简单的掩码自动编码方法在学习代表性特征方面特别有效，其任务是从未掩码输入中重构掩码数据[7]-[10]。在自然语言处理中，掩码自动编码可以训练大型语言模型，如BERT[7]。同样，在2D视觉中，掩码自动编码优于监督预训练[8]。</p><p>近年来，人们提出了一些关于掩码点自动编码的研究成果，如point - mae[11]、point - bert[12]、MaskPoint[13]和point - m2ae[14]。然而，这些方法主要集中在小尺度点云，如合成点云数据集ShapeNet[15]和室内点云数据集ScanNet[16]。相比之下，大规模户外LiDAR点云的掩模自动编码[2]-[5]受到的关注较少。在大规模标记LiDAR点云上从零开始训练仍然是主流方法[5]。为了将掩码自编码的思想引入到大规模户外LiDAR点云的自监督学习中，我们首先识别了与掩码小规模点云自编码相比的挑战[11]-[14]，然后提出解决方案:</p><ul><li><p>首先，小尺度点云[15]、[16]与大型室外激光雷达点云[2]-[5]在几个方面有所不同。</p><ul><li>(1)小规模点云通常比大规模点云包含的点要少得多，ShapeNet包含大约2k个点，ScanNet被压缩到大约2k个点，用于掩码小规模点云的自动编码工作[11]-[14]。相比之下，像Velodyne hd - 64e激光雷达这样的激光雷达传感器每帧可以扫描192,000个点，覆盖160×160×20米的区域[17]。</li><li>(2)小尺度点云往往分布均匀，而大尺度激光雷达点云随着距离激光雷达传感器的增加而变得稀疏。</li><li>(3)现有的掩码小尺度点云自动编码工作[11]-[14]通常依靠最远点采样(FPS)和k近邻(kNN)将点划分为相等的子集，不适合大规模户外LiDAR点云。在之前的工作中[17]，[18]已经强调了高效、实时地处理这种大规模点云的挑战，这使得对这些数据集的预训练成为一项更具挑战性的任务。</li></ul></li><li><p>其次，现有的掩模小尺度点云自动编码工作[11]、[12]、[14]主要集中在缺失点的回归上。然而，大规模LiDAR点云的特征并不适合通过回归来重建，因为这些特征包含了必要的空间信息[6]。此外，体素的位置编码可以为解码器提供一条捷径[19]。为了解决这一问题，我们<strong>将重点转向大规模激光雷达点云的占用分布，并设计占用预测目标作为pretext task</strong>。通过这样做，网络被迫学习具有代表性的特征来恢复3D场景的整体结构，使得这个简单的任务成为我们预训练方法的有效解决方案。</p></li><li><p>第三，现有的掩模小尺度点云自动编码作品[11]-[14]采用的随机掩模策略，由于大规模户外LiDAR点云分布不均匀，可能不适用于大规模户外LiDAR点云。与小尺度点云不同，大尺度激光雷达点云的密度根据与激光雷达传感器的距离而变化。因此，对所有体素采用统一的随机掩码策略并不是最优的。为了解决这个问题，我们<strong>提出了一种大规模激光雷达点云的距离感知随机掩码策略</strong>。该策略根据体素与LiDAR传感器的距离调整掩码比，掩码比随着与传感器距离的增加而减小。</p></li></ul><p>Transformer已经开始在处理点云中出现[20]。例如，PCT[21]在整个点云上执行全局自关注，而PointASNL[22]、PointFormer[23]和VoxSet[24]将基于Transformer的架构应用于每个点的局部邻域。然而，这些局部Transformer的效率受到邻域查询和特征重构的限制。SST[25]和FlatFormer[26]采用基于窗口的点云Transformer。这些模型将点云投影到鸟瞰图(BEV)中，并将BEV空间划分为相同空间大小的不重叠窗口。然而，这些方法可能会在纵轴中丢失有价值的信息。我们在<strong>编码器中使用3D稀疏卷积[6]和位置编码模块</strong>。这使得网络可以专注于可见的3D体素，而不是处理整个点云，从而大大降低了计算成本。<strong>位置编码模块，类似于Transformer中使用的位置嵌入，将体素的空间信息编码成固定大小的嵌入向量。</strong></p><p>最近，针对大规模户外LiDAR点云，已经提出了几种基于掩模自编码器的自监督预训练算法(如:Voxel-MAE[27]、MV-JAR[28]、GeoMAE[29]、GD-MAE[30])。然而，这些算法是专门为预训练基于柱子的室外LiDAR点云方法SST [25] (SST用Transformer[32]代替Pointpillars[31]骨干，不能处理3D体素)而量身定制的，忽略了点云的关键高度信息。</p><p>激光雷达点云中的信息冗余是指点云数据集中存在重复或高度相似的数据点[37]，[38]。这种冗余可能来自各种来源[39]，[40]，如重叠的LiDAR条、多次返回、采样密度、传感器伪影和噪声。重叠点包含相似的地形信息，这些信息可能是冗余的。多次返回通常从不同角度包含关于同一物理特征的信息，从而导致冗余。高采样密度可能导致冗余，因为多个点可能描述相同的地面，特别是在平坦或均匀的区域。这些工件可能导致冗余的数据点，而这些数据点不能代表地面上的实际特征。因此，<strong>大量的冗余空间信息仍然隐藏在LiDAR点云中，使它们适合于掩模自动编码方法来学习丰富和代表性的特征。</strong></p><p>作为激光雷达点云的压缩表示，占位可以定义为3D网格中的体素是否包含点。在自动驾驶任务中，占用率预测是一种常用的3D世界表示方法[33]。它包括将环境划分为网格，并估计每个单元被占用或空闲的概率。该方法已应用于各种任务，如障碍物检测、路径规划和同步定位和映射(SLAM)[34] -[36]。<br>F<br>在上述分析的推动下，我们提出了第一个自监督掩码占用自动编码框架，称为Occupancy-MAE，用于预训练大规模户外激光雷达点云。图2展示了我们的Occupancy-MAE的工作流程，它首先<strong>采用距离感知掩码策略来随机掩码体素，然后将未掩码的体素馈送到3D稀疏编码器中</strong>。<strong>三维解码器的输出是每个体素包含点的概率，我们计算二元占用分类损失来预训练网络</strong>。掩码占用分类的pretext task训练鼓励编码器网络对整个物体形状具有体素感知，从而学习3D感知的代表性特征。</p><p><img src="/pic/mae3d7.png" alt="图2所示整体架构。我们首先将大规模不规则LiDAR点云转换为体积表示，根据体素与LiDAR传感器的距离随机掩码(即距离感知掩码策略)，然后使用非对称自编码器网络重建一般3D世界的几何占用结构。我们采用以位置编码为编码骨干的3D空间稀疏卷积[6]。我们使用二元占用分类作为pretext task来区分体素是否包含点。预训练后，丢弃轻量级解码器，使用编码器对下游任务的主干进行预热。"></p><p>通过对实验结果的分析，我们得出结论，Occupancy-MAE是一个简单有效的自监督学习框架，可以很好地泛化到各种下游任务。在3D目标检测任务中，我们的方法在ONCE数据集上比最先进的自监督学习方法高出0.5% ~ 6% mAP, ONCE数据集是最近发表的用于大规模LiDAR点云自监督学习的数据集。此外，使用Occupancy-MAE进行预训练可以显著提高流行的3D检测器的性能，如SECOND[6]、PV-RCNN[41]、CenterPoint[42]和PV-RCNN[43]，这些检测器在KITTI、Waymo和nuScenes数据集上从头开始训练，特别是对于小物体。如图1所示，Occupancy-MAE是一种数据高效学习器，可以用有限的注释3D数据有效地训练大规模LiDAR点云。对于3D语义分割任务，采用双层解码器的Occupancy-MAE可以将从头开始的训练提高约2% mIoU。对于多目标跟踪任务，Occupancy-MAE在AMOTA和AMOTP方面将从零开始的训练提高了约1%。我们还证明了该方法在无监督域适应任务中的有效性，这证实了occuancymae的迁移学习能力。即使掩码率为90%，由于大规模LiDAR点云是信息冗余的，Occupancy-MAE仍然可以学习代表性特征，最终提高了3D感知的性能。</p><p>本工作的主要贡献如下:</p><ul><li>我们提出了一种新的自监督掩码占用自动编码框架，称为Occupancy-MAE，用于预训练大规模户外LiDAR点云，减少了对昂贵的注释3D数据的需求。</li><li>我们提出了一个3D占用预测pretext task，该任务利用大规模LiDAR点云逐渐稀疏的占用结构。通过从少量可见体素中恢复3D周围世界的掩模占用分布，迫使网络提取高级语义信息。</li><li>引入距离感知随机掩码策略，利用大规模LiDAR点云密度变化的优势，提高预训练性能。</li><li>我们提出的Occupancy-MAE在各种下游任务上显著优于从头开始的训练，包括3D目标检测、语义分割、多目标跟踪和无监督域自适应。</li></ul><h3 id="相关工作-1"><a href="#相关工作-1" class="headerlink" title="相关工作"></a>相关工作</h3><h4 id="基于激光雷达的3D感知"><a href="#基于激光雷达的3D感知" class="headerlink" title="基于激光雷达的3D感知"></a>基于激光雷达的3D感知</h4><p>基于lidar的三维感知模型具有准确的三维空间信息，已广泛应用于自动驾驶领域[44]，[45]。基于lidar的三维目标检测器可分为基于点的[46]、[47]、基于体素的[6]、[31]、[48]和基于点体素的[41]、[49]。基于点的目标检测器利用PointNet从原始点云中提取判别特征[50]，并以每个点为中心生成建议，计算成本高。基于体素的检测器将不规则的点云转化为体表示，这将降低细粒度定位的精度。基于点体素的方法利用了点检测器的定位精度和体素检测器的计算效率。基于lidar的三维分割方法分为基于网格的[51]、[52]和基于体素的[18]、[53]。基于网格的方法侧重于将三维点云转换为二维正视图像或距离图像，无法对三维几何信息进行建模。基于体素的方法将点云转换成体积表示。现有的三维感知方法是使用大规模标记的三维数据进行训练的。如何设计自监督学习网络以减少对三维标注的依赖，这方面的研究很少。</p><h4 id="Self-supervised学习"><a href="#Self-supervised学习" class="headerlink" title="Self-supervised学习"></a>Self-supervised学习</h4><p>近年来，自监督学习(Self-supervised Learning, SSL)作为一种避免昂贵数据注释的有效方法得到了广泛的应用。[54]提出了预测图像斑块相对位置的托辞任务。[55] -[57]的方法设计了旋转预测任务，在学习代表性特征方面显示出良好的效果。在[58]中，引入了一种拼图预测任务，它在目标识别的领域自适应中具有很好的泛化性。DeepCluster[59]和SwAV[60]通过kmeans聚类获得伪标签，并使用这些标签来训练网络。其他方法如Moco[61]、PointContrast[19]、BYOL[62]、ProposalContrast[63]和DepthContrast[64]为自监督学习构建对比视图。最近，<strong>MAE[8]通过首先屏蔽输入图像的随机补丁，然后使用简单的自编码器框架重建缺失的像素，在自监督学习中显示出令人鼓舞的结果</strong>。VideoMAE[65]将MAE扩展到从视频中学习时空表征，具有更大的信息冗余。ALSO[66]在一个借口任务上训练模型，该任务是重建3D点被采样的表面。ISCC[67]将对比聚类和隐式表面重建应用于自监督预训练大规模室外LiDAR点云。我们的Occupancy-MAE遵循MAE的设计理念，并基于其几何特征(如稀疏性和密度变化)将其应用于大规模户外LiDAR点云。</p><h4 id="点云的掩码自动编码器"><a href="#点云的掩码自动编码器" class="headerlink" title="点云的掩码自动编码器"></a>点云的掩码自动编码器</h4><p>掩码自编码在NLP[7]和图像[8]中取得了成功，导致了去年点云的掩码自编码技术的发展。point - bert[12]首先引入MAE对小尺度点云进行预训练。point - mae[11]利用Chamfer距离重建小尺度点斑块。MaskPoint[13]设计了用于识别小尺度掩码点补丁的解码器。Point-M2AE[14]将金字塔架构应用于空间几何模型，并捕获3D形状的细粒度和高级语义。然而，这些聚焦于小尺度室内点云的方法，由于场景范围大、密度变化的特点，无法处理大尺度室外激光雷达点云。Voxel-MAE[27]、MV-JAR[28]、GeoMAE[29]和GD-MAE[30]对大规模点云采用掩码自动编码，但仅限于基于柱的方法，会丢失垂直信息。我们提出的Occupancy-MAE克服了这些限制，并能够对体素和基于柱的方法进行大规模LiDAR点云的预训练。</p><h3 id="方法-1"><a href="#方法-1" class="headerlink" title="方法"></a>方法</h3><p>针对大规模LiDAR点云的实例，自监督预训练是利用未标记的数据对网络进行训练，生成具有代表性的特征。受掩码自编码[7]，[8]，[11]优异性能的启发，我们设计了用于三维感知的掩码占用自编码网络。该算法对体素进行随机掩码，然后利用自编码器网络重构体素的占用值。利用二元交叉熵损失训练占用率预测的pretext task。Occupancy-MAE覆盖大部分骨干网，不包括最后的头部部分。</p><p>对于ns未标记的点云数据xi ns i&#x3D;1，我们的目标是首先预训练掩码自动编码网络以学习高级语义。然后我们使用预训练的模型来预热下游任务的网络。我们还将预训练方法扩展到目标点云xj nt j&#x3D;1上的域自适应任务。Occupancy-MAE的详细信息列在表1中。解码器仅由两到三个3D反卷积层组成，因此非常轻巧。</p><p><img src="/pic/mae3d8.png" alt="我们的Occupancy-MAE架构的细节，其中包括一个3d编码器和一个3d解码器。3DSparsecvonv和3DTranscvonv分别表示second[45]中提出的三维稀疏卷积层和常见的三维反卷积层。这里我们在kitti数据集上显示预训练秒的输出大小。"></p><h4 id="距离感知随机掩码"><a href="#距离感知随机掩码" class="headerlink" title="距离感知随机掩码"></a>距离感知随机掩码</h4><p>在这项工作中，我们采用了将LiDAR点云划分为间隔体素的常用方法，该方法经常用于3D感知模型[6]，[18]。对于X × Y × Z轴上尺寸为W × H × D的LiDAR点云，每个体素的大小为vW × vH × vD，总共为nl体素，其中每个包含nv点。基于体素的方法比基于点的方法计算效率更高[46]，使其非常适合处理自动驾驶汽车等应用中的大规模LiDAR点云。</p><p>虽然随机掩码策略已被证明在语言[7]、图像[8]和小规模点云[11]的预训练模型中是有效的，但大规模LiDAR点云的分布是独特的，因为它们的稀疏度水平与与LiDAR传感器的距离相关。<strong>靠近传感器的点是密集的，而远离传感器的点则稀疏得多。因此，我们不能对近距离点和远距离点应用相同的掩码策略</strong>。相反，我们提出了一种距离感知随机掩码策略，该策略可以掩码一小部分远距离点的数据。</p><p>为此，我们引入了考虑距离信息的距离感知随机掩码策略。我们坚持使用常用的距离配置来评估LiDAR点云检测结果[3]，[5]，根据占用体素与LiDAR传感器的距离将其分为三组:0-30米，30-50米和&gt;50米。掩码比随着距离的增加而减小，我们采用分段的方法对每一组应用随机掩码策略。对应的体素数为nv1、nv2、nv3。我们对每一组采用随机掩码策略，掩码比依次递减为r1、r2、r3(即r1 &gt; r2 &gt; r3)。因此，未遮挡的被占用体素数为nun &#x3D; nv1(1−r1) nv2(1−r2) nv3(1−r3)，体素集Vinput∈Rnun×4作为训练数据。ground truth表示体素的占用状态，T∈{0,1}nl×1。每个体素可以包含点(已占用)或空(空闲)。值为1表示已占用体素，值为0表示空闲体素。</p><p>需要注意的是，对于距离感知掩码，也可以使用更多的分割组，但在预训练中会占用大量的预处理时间。因此，我们通过实现三组距离来实现距离感知掩码，从而在精度和速度之间取得平衡。</p><h4 id="3D稀疏卷积编码器"><a href="#3D稀疏卷积编码器" class="headerlink" title="3D稀疏卷积编码器"></a>3D稀疏卷积编码器</h4><p>用于NLP[7]、2D视觉[8]和小规模点云[11]、[13]的掩码自动编码的Transformer网络对训练数据的未掩码部分进行自关注，这些部分不受掩码的影响。然而，在一个3D场景中有数百万个点，即使在屏蔽了90%的体素之后，仍然有数十万个未被屏蔽的体素，这使得Transformer网络无法从如此庞大的输入数据中聚合信息[7]。<strong>一些算法(例如，Voxel-MAE [27]， MVJAR[28])利用transformer进行掩码，但它们仅适用于2D柱结构，而忽略了3D高度信息。</strong></p><p>为了解决这个问题，提出了3D Sparse Convolution[6]，[68]来处理大规模点云。该方法使用位置编码，仅从已占用的体素中聚合信息，从而提高了效率。流行的3D感知方法[6]，[18]，[41]，[42]已经使用该技术开发。基于此，我们采用SECOND[6]中的3D Spatially Sparse Convolution来构建编码器网络，<strong>允许我们使用位置编码模块从未被遮挡的、被占用的体素中聚合信息</strong>。因此，我们的体素掩码策略降低了训练的记忆复杂性，类似于Transformer网络在NLP[7]、2D视觉[8]和小规模点云[11]-[14]中的工作方式。</p><h4 id="轻量级3D解码器"><a href="#轻量级3D解码器" class="headerlink" title="轻量级3D解码器"></a>轻量级3D解码器</h4><p>我们的解码器由3D反卷积层组成，最后一层输出每个体素包含点的概率，得到一个输出张量P∈Rnl×1。在预训练期间，解码器的唯一目的是执行占用体素重建。通过将掩码token转移到解码器，我们鼓励编码器为下游任务学习更好的潜在特征。解码器是轻量级的，仅由两到三个3D反卷积层组成，使其可扩展到更大的感知范围。</p><h4 id="重构占用目标"><a href="#重构占用目标" class="headerlink" title="重构占用目标"></a>重构占用目标</h4><p>大多数掩码自动编码工作的主要目标是通过回归任务重建被掩码部分[11]，[12]，[14]，由于体素的位置编码，这对网络来说并不具有挑战性。然而，在3D感知中，一般3D世界的占用结构在感知模型中起着至关重要的作用[6]，[18]，[48]。例如，特斯拉推出了用于自动驾驶的占用网络[?]。基于此，我们提出了大规模户外LiDAR点云预训练的占用率预测任务，旨在鼓励网络在高级语义上进行推理，从少量可见体素中恢复被掩码的3D场景占用率分布。由于空体素的大量存在，占用率的预测面临着类不平衡的二元分类挑战。为了解决这个问题，我们使用focal损失进行二元占用分类，使用预测占用值P和地面真实占用体素T。</p><p><img src="/pic/mae3d9.png"></p><p>其中Pij表示第i个训练样本中体素j的预测概率，batch表示批大小。正样例&#x2F;负样例的权重因子α设为2，易样例&#x2F;难样例的权重因子γ设为0.25。对于第1类，αt &#x3D; α， Pij t &#x3D; Pij。对于第0类，αt &#x3D; 1−α， Pij t &#x3D; 1−Pij。</p><h4 id="与现有掩码激光雷达方法的比较"><a href="#与现有掩码激光雷达方法的比较" class="headerlink" title="与现有掩码激光雷达方法的比较"></a>与现有掩码激光雷达方法的比较</h4><p>近年来，针对大规模户外LiDAR点云，提出了几种基于掩码自编码器的自监督预训练算法，包括Voxel-MAE[27]、MV-JAR[28]、GeoMAE[29]和GD-MAE[30]。</p><p>我们的方法在四个关键方面不同于这些现有的方法。</p><ul><li>首先，上述掩码激光雷达方法仅限于基于柱的方法，忽略了点云的关键高度信息，而我们的方法既适用于基于体素的3D物体检测，也适用于基于柱的3D物体检测(如VoxelNet[48]和PointPillars[31])，也适用于3D分割(如Cylinder3D[18])和领域自适应算法(如ST3D[69])。</li><li>其次，我们引入了距离感知掩码策略，该策略对不同距离的点云进行了不同的处理，考虑了不同的信息密度。</li><li>第三，由于输入点的位置(x, y, z)可能不适合直接回归LiDAR点云，并且为回归提供了捷径，我们设计了整个周围场景的3D占用预测目标。相反，GeoMAE[29]和MV-JAR[28]使用二维占位预测目标，忽略了沿z轴的关键信息。</li><li>第四，我们的方法利用了稀疏的三维卷积，使其能够专注于三维体素，并有效地融合了体积信息。</li></ul><p>相比之下，上述掩模LiDAR方法使用的是Transformers，它只聚合二维柱特征，而忽略高度信息。</p><h3 id="设置"><a href="#设置" class="headerlink" title="设置"></a>设置</h3><h4 id="数据集"><a href="#数据集" class="headerlink" title="数据集"></a>数据集</h4><p>我们的实验使用了四种流行的自动驾驶数据集:ONCE[5]、KITTI[2]、Waymo[3]和nuScenes[4]。</p><ul><li><strong>ONCE</strong>。ONCE数据集[5]包含100万个LiDAR场景和700万个相应的相机图像。这些数据涵盖了不同的地区、时间段和天气条件，提供了真实世界驾驶场景的全面代表。值得注意的是，ONCE数据集的关键目标之一是促进研究，探索利用大规模未标记数据的潜力，为自动驾驶研究和开发的创新方法开辟机会。</li><li><strong>KITTI</strong>。KITTI数据集[2]是最流行的自动驾驶数据集之一，它提供了7481个训练样本和7518个测试样本。3D边界框注释仅在前置摄像头的视场(FoV)内提供。我们遵循常见的50&#x2F;50训练&#x2F;值分割，并使用官方的KITTI评估指标进行三级评估(简单、中等、困难)，并评估平均精度。</li><li><strong>Waymo开放数据集</strong>。Waymo开放数据集[3]是最近发布的大规模自动驾驶数据集，该数据集包括798个训练序列，约158361个LiDAR样本，202个验证序列，40077个LiDAR样本。遵循流行的点云检测代码库OpenPCDet[70]，我们将所有训练样本中20%的数据(约32k帧)的一帧作为训练集进行子采样。它注释了目标在全360度的领域。Waymo的官方评价指标是难度等级(L1和L2)的平均平均精度(AP)和标题加权的平均平均精度(APH)。</li><li><strong>nuScenes</strong>。nuScenes数据集[4]是另一个流行的自动驾驶数据集。总共有28130个训练样本和6019个验证样本。我们使用nuScenes检测分数(NDS)、平均精度(mAP)、平均平移误差(ATE)、平均尺度误差(ASE)、平均方向误差(AOE)、平均速度误差(AVE)、平均属性误差(AAE)等官方评价指标进行评价。</li></ul><h4 id="实验设置"><a href="#实验设置" class="headerlink" title="实验设置"></a>实验设置</h4><p>我们通过在四个自动驾驶数据集上执行三个下游任务来评估我们提出的模型的有效性[2]-[5]。为了实现3D目标检测和无监督域自适应任务，我们使用了流行的点云检测代码库OpenPCDet<a href="%E7%89%88%E6%9C%AC0.5.2">70</a>。对于3D语义分割任务，我们使用开源的Cylinder3D[18]作为预训练主干，它应用圆柱形体素划分。我们首先在ONCE数据集的未标记原始集上预训练Occupancy-MAE，然后对训练集上的感知模型进行微调。然而，对于KITTI、Waymo和nuScenes数据集，预训练和微调都在训练集上。ONCE数据集为自监督学习方法提供了一个基准，但没有可用的代码。因此，我们只比较了我们的Occupancy-MAE自监督学习方法在ONCE数据集上的结果。</p><p>我们利用预训练的3D编码器来初始化和预热下游任务的骨干，而不会在微调期间冻结3D编码器参数。随后，我们使用与原始模型相同的训练参数训练下游任务。我们将0-30米、30-50米和&gt; 50米的体素掩码率分别设置为90%、70%和50%。预训练epoch的个数为3。更详细的参数设置请参考OpenPCDet[5]、[70]和Cylinder3D[18]。</p><blockquote><p>具体实验结果看原文</p></blockquote><h2 id="第三篇：MAELi-Masked-Autoencoder-for-Large-Scale-LiDAR-Point-Clouds"><a href="#第三篇：MAELi-Masked-Autoencoder-for-Large-Scale-LiDAR-Point-Clouds" class="headerlink" title="第三篇：MAELi: Masked Autoencoder for Large-Scale LiDAR Point Clouds"></a>第三篇：MAELi: Masked Autoencoder for Large-Scale LiDAR Point Clouds</h2><h3 id="摘要-2"><a href="#摘要-2" class="headerlink" title="摘要"></a>摘要</h3><p>大规模激光雷达点云的传感过程不可避免地会产生较大的盲点，即传感器不可见的区域。我们通过设计一个高效的预训练框架来演示如何有效地利用这些固有的采样属性进行自监督表示学习，该框架大大减少了训练最先进目标检测器所需的繁琐的3D注释。我们的Masked AutoEncoder for LiDAR point clouds(MAELi)直观地利用了激光雷达点云在编码器和解码器重建期间的稀疏性。这将产生更具表现力和有用的初始化，可直接应用于下游感知任务，如3D物体检测或自动驾驶的语义分割。<strong>在一种新的重建方法中，MAELi区分了空空间和遮挡空间，并采用了一种新的掩码策略，以激光雷达固有的球形投影为目标</strong>。因此，在没有任何基础真理的情况下，MAELi仅在单帧上进行训练，从而获得了对底层3D场景几何和语义的理解。为了证明MAELi的潜力，我们以端到端方式对骨干进行预训练，并展示了我们的无监督预训练权重在3d目标检测和语义分割任务上的有效性。</p><h3 id="引言-2"><a href="#引言-2" class="headerlink" title="引言"></a>引言</h3><p>由于最近大规模和精心策划的数据集，如Waymo开放数据集[35]，我们见证了在各种各样的3D感知任务中取得的巨大进步，这些任务对自动驾驶至关重要。然而，即使有这样的成本密集型数据集的帮助，模型仍然只能转移到其他领域，同时遭受显著的性能下降[40]。</p><p>自监督表示学习(SSRL)提供了一种减少昂贵标记工作的技术。总体思路是以无监督的方式学习通用特征表示，然后用于特定的下游任务，例如目标检测。3D中最常见的方法之一是通过点云重建来学习表征[1,25,30,37,38,41,56]。在那里，任务是恢复点云的删除部分，从而学习对场景和物体几何结构的隐式理解。这对于由CAD模型(如ModelNet[44]或ShapeNet[6])生成的全3D点云尤其有用。最近，这些方法被用于汽车领域的大规模点云[Voxel-MAE,Occupancy-MAE,GeoMAE]。</p><p>然而，现有的SSRL方法忽略了激光雷达点云固有的基本特性:</p><ul><li>i)我们无法感知撞击表面以外的物体(即我们只能感知2.5D)， </li><li>ii)激光雷达传感器的角度分辨率有限。</li></ul><p>在这项工作中，我们适应了这些特性，并提出了MAELi，一种transformer-less掩模自编码器(MAE)，它不简单地遵循重建原始LiDAR点云的直接方法。相反，我们提出了一种新的重建方法，使我们能够超越(可见)点。因此，MAELi可以从任何观看方向学习物体的样子，从而获得具有良好泛化能力的强预训练权值。如图1所示，它隐式地学习重建整个目标，同时以一种真正无监督的方式，在没有任何基础真理的情况下，对单个帧进行训练。</p><p><img src="/pic/mae3d11.png" alt="图1所示。重构点云。与现有的自监督表示学习(SSRL)方法(简单地重建初始点云(灰色))相比，MAELi学习了一种表达性的特征表示，该特征表示捕获了目标的完整几何目标结构，没有任何基础真值标签。为了可视化的目的，我们用z坐标对点进行了颜色编码，并删除了重建的地平面。"></p><p>直觉上，我们明确区分已占用空间、空白空间和未知空间。当跟踪激光雷达光束从传感器到物体(以及返回)的路径时，中间的空间被认为是空的，物体本身在撞击点占据空间，物体后面的空间由于遮挡而未知。此外，由于激光雷达的分辨率有限，无法对未覆盖的区域做出结论。<strong>因此，我们要求我们的模型重建点云的移除部分，但我们不会因为完成未知区域（即遮挡或未采样区域）中的结构而惩罚它。</strong></p><p>在训练过程中，模型遇到的目标范围很广，姿态和采样密度不同。尽管缺少标签，但我们独特的目标允许模型重建整个目标，即隐式捕获整个几何结构。在这个过程中，MAELi学习了一种更接近底层几何结构的表示，而不是简单地模仿激光雷达点云中可观察到的特定采样模式。</p><p>为了评估，我们选择了最主要的汽车感知任务，即目标检测和语义分割。我们的内存高效，稀疏解码器结构能够在单个GPU上以端到端方式高效地预训练最先进的目标检测器。在Waymo开放数据集[35]、KITTI[2,12,24]和ONCE[26]上的大量实验中，我们证明了MAELi对于预训练各种最先进的3D检测器和语义分割网络是非常有效的。</p><p>总之，我们的贡献有三个方面:</p><ul><li>我们提出了一种激光雷达感知的SSRL方法来预训练适用于各种架构和下游任务的3D骨干网。</li><li>我们引入了一种新的掩码策略和重建损失，用于无监督表示学习，特别是针对激光雷达特性设计的。</li><li>我们展示了我们的预训练，视觉可验证表示的有效性，改进了3D物体检测和语义分割的几个基线。</li></ul><h3 id="相关工作-2"><a href="#相关工作-2" class="headerlink" title="相关工作"></a>相关工作</h3><p><strong>2D图像和3D点云的SSRL</strong>:自监督表示学习努力在引入人工标记的地面真值数据形式的任何监督之前学习有益的表示。这些表示用于改进各自下游任务的结果或减少标记训练数据所需的数量。</p><p>对比学习方法要求模型在使用不同的扩展进行转换时为相同的数据实例维护相似的嵌入。因此，不同的数据实例应该导致不同的嵌入。这些方法最初应用于二维图像[8,16,18,39]，也适用于点云[21,23,28,29,31,31,42,46,54,58]。自然地，诱导一致性损失的粒度定义了模型所同意的语义级别。换句话说，在描述整个图像或点云的全局嵌入上的对比学习更适合于分类等下游任务。然而，像目标检测或语义分割这样的任务需要更细粒度的处理。因果困境是在不知道什么是语义连贯的情况下，以适当的细节水平对语义连贯的区域进行采样。例如，Yin等人[54]在去除地平面后，通过最远点采样和球查询生成建议。TARL[29]和STSSL[43]将分析扩展到多个时间框架，以聚类感兴趣的目标，这需要使用全局配准的点云。这个增加的时间维度引入了一个额外的信息层，将它们与基于单帧的方法区分开来。</p><p><strong>基于重建的SSRL</strong>:最近，生成式自监督表示学习方法正在兴起。它最成功的概念之一是去噪自动编码器。基于编码器的输出嵌入，解码器的任务是重建去噪的输入，如果成功，编码器被迫学习一种有用的抗噪声表示。特别是在不同的应用领域，如自然语言处理(NLP)[11]和2D图像[15]等，对蒙面输入的重建获得了巨大的吸引力。主要的研究重点开始于学习全3D、合成或室内数据集的表示[7,13,19,25,30,38,49,52,56,57]，例如ModelNet[44]或ScanNet[10]。</p><p>近年来，一些研究工作考虑了激光雷达点云，如[17,27,37,45,48,51]。Xie等人[45]需要充分的监督，而我们不需要任何标签进行预训练。MV-JAR[48]使用主干来重建体素的掩码位置编码和内部点分布，强制网络重建精确的采样模式。<strong>在Occupancy-MAE[27]中，作者使用mae方法在没有鸟瞰(BEV)编码器的情况下预训练常见的3D体素骨架</strong>，而GDMAE[51]引入了一种新开发的复杂多级transformer。这两种方法都采用密集解码器，并且不区分激光雷达点云中固有的空空间和遮挡空间。相比之下，<strong>我们的重建策略与我们的稀疏解码器相结合，使我们能够对最常见的3D物体检测器的整个编码器进行预训练，而不会在单个GPU上的未采样区域进行重建。</strong></p><p>只有少数研究考虑了激光雷达点云的固有特性，即其采样分辨率和2.5D感知。在这种情况下，Wang等人[38]在小规模全3D点扫描上合成闭塞，并利用它来预训练编码器。Hu等人[20]通过光线投射生成可见性地图，以减轻不一致的物体增强[50]，作为检测网络的额外输入。Xu等人[47]通过分组相似的实例来增强地面真值目标点，并利用它们来训练辅助任务，估计被遮挡区域被目标占用的可能性。<strong>GeoMAE[37]提出了一种掩码自编码器，该编码器经过训练后可以重建底层点统计和表面属性，即基于相邻体素的法线和曲率估计</strong>。与我们的方法最密切相关的ALSO[3]采用了通过曲面插值的SSRL。它沿着位于探测目标前后的激光雷达光线生成查询点，并指示网络预测这些选定点的占用情况，将遮挡区域视为占用。虽然这种策略很简单，但它可能会抵消自动驾驶汽车上多激光雷达设置的限制，Waymo开放数据集(Open Dataset)中特别使用了这种配置[35]。相比之下，我们的方法能够鲁棒地处理这种复杂的场景，并且可以隐式地学习采样查询点以外的几何结构。一旦网络能够插值下垫面，GeoMAE和ALSO的自我监督任务就被认为是完成的。虽然最终的特征表示已经显示出有希望的结果，但我们的LiDARaware重建展示了进一步的改进，因为我们的模型固有地捕获了整个物体的几何形状。</p><h3 id="大规模激光雷达点云MAE"><a href="#大规模激光雷达点云MAE" class="headerlink" title="大规模激光雷达点云MAE"></a>大规模激光雷达点云MAE</h3><p>我们的目标是通过自监督表示学习(SSRL)显着减少大规模LiDAR点云的昂贵标记工作。我们建立在成功的掩码和重建范式的基础上，但解决了现有方法主要集中于重建原始输入点云的基本限制。虽然这种策略对于完整的3D模型是有效的，例如由CAD渲染生成的模型[6,44]，但它在两个主要方面限制了学习表征对LiDAR点云的有用性:</p><ul><li>首先，激光雷达传感器的有限角分辨率会导致激光雷达光束之间的间隙。简单地重建原始点云将意味着对这些间隙中(正确地)重建点的模型进行惩罚。</li><li>其次，单次激光雷达扫描无法完全捕获目标。一旦光束被表面反射，传感器就无法从该表面后面的物体上捕获任何空间信息。</li></ul><p>因此，基于标准重建的模型在完成被遮挡的部分时会受到惩罚阻碍了对潜在目标的固有理解和对隐含上下文信息的学习。<strong>借助MAELi，我们通过引入激光雷达感知损失来应对这些挑战。这种损失不是均匀地惩罚，而是专门针对激光雷达采样的已知区域。</strong></p><p>虽然我们的方法是通用的，可以应用于广泛的任务，但我们选择证明其在3D检测和语义分割方面的有效性，因为它们在该领域的重要性和广泛应用。<strong>对于3D检测，我们通过在编码器的最后一层附加一个重建解码器来预训练编码器。一旦预训练完成，我们丢弃解码器并利用编码器的权重作为后续检测任务的初始化。对于语义分割，我们使用编码器和解码器的权重作为下游微调的初始化。图2说明了这一点。</strong></p><p>在描述我们的重建目标和掩码策略之前，我们将简要解释我们的稀疏解码器。</p><p><img src="/pic/mae3d12.png" alt="图2。我们MAELi预训练的示意图概述。我们的稀疏解码器的任务是重建被屏蔽的输入点云的缺失部分。因此，编码器被迫学习可用于下游任务的合理表示，例如3D目标检测。由于我们不会因为在激光雷达不可见的区域重建体素而惩罚我们的网络，在看到许多不同的样本后，它学会在没有任何地面真值标签的情况下重建被遮挡的部分，从而获得更具表现力的特征表示。为了可视化的目的，已经删除了地平面。"></p><h4 id="稀疏重建解码器"><a href="#稀疏重建解码器" class="headerlink" title="稀疏重建解码器"></a>稀疏重建解码器</h4><p>我们的目标是通过为各自的下游任务预训练整个主干来获得更具表现力的特征表示。与现有的稀疏的、基于体素的LiDAR点云编码器&#x2F;解码器结构(如Part-A2[33])相反，我们必须解决一个关键的区别。典型的方法是通过专用稀疏卷积(SC)对稀疏数据进行体素化和处理。与密集的对等物不同，SC仅在内核覆盖任何活动位点时才应用。即使使用较小的3 × 3内核，这些活性位点也会迅速稀释，从而增加计算工作量。因此，像[32,50,55]这样的方法使用子流形稀疏卷积(submanifold sparse convolutions, SSC)[14]，其中内核中心只放置在活动位点上，只考虑内核覆盖的活动位点，同时保持有利的内存消耗。<strong>解码器然后在上采样期间重用各自编码器层的活动位点。然而，这使得常见的上采样模式对于重建编码器中不存在的体素无效，使其不适合重建任务。</strong></p><p>为了在点云上执行重建，我们需要一个解码器，它也可以恢复被删除的体素，并扩展到编码器中存在的活动站点之外。对原始空间分辨率进行密集上采样是可能的，但由于内存和计算需求过大，在有限的硬件设置上预训练较大的架构是相当不切实际的。因此，<strong>我们允许每个上采样层(立方)增长，但添加一个学习删除冗余体素的后续修剪层，如图3所示。</strong>为此，我们利用小尺度点云重建的思想[9]，对学习修剪的稀疏特征映射应用1 × 1卷积。这样，<strong>解码器能够重建和完成点云的部分，而修剪层去除多余的体素</strong>。为了获得更强的特征表示，我们提出了一个扩展的重建目标。</p><p><img src="/pic/mae3d13.png" alt="图3。在我们的解码器中重建了一辆真实的汽车。上采样步骤将体素步幅和体素大小减半，并增加体素数量。随后，在剪枝步骤中去除多余的体素。为了可视化目的，我们裁剪了汽车并对z坐标进行了颜色编码。"></p><h4 id="重建目标-1"><a href="#重建目标-1" class="headerlink" title="重建目标"></a>重建目标</h4><p>我们制定了重建目标，以更深入地了解潜在的环境结构。直观地说，模型应该更直接地掌握汽车的完整外观，而不仅仅是重建激光雷达光束捕获的特定点。<strong>此外，预训练的表示应该更紧密地与下游任务的目标保持一致，例如，在整个汽车周围安装一个3D边界框</strong>。因此，在解码器内的重建过程中，我们区分了三种体素:已占用体素、空体素和未知体素。这些类别如图4所示。</p><p><img src="/pic/mae3d14.png" alt="图4。应该只对体素产生损失，因为我们实际上知道空间是否为空。激光雷达光束穿过空体素(蓝色)，直到它们到达一个表面，即一个被占用的体素(红色)。所有其他体素被认为是未知的。空体素的损失由其中心与最近光束(不同深浅的蓝色)的接近程度来加权，以抵消离散体素化造成的不准确性。"></p><p>被占用的体素包含来自原始点云的表面点(在掩码之前)，因此应该是重建的一部分。空体素是激光雷达光束穿过而没有击中任何表面的体素，因此应该保持为空。最后，如果前两种情况都不适用，我们将体素分类为未知，即这些体素要么被遮挡，要么由于有限的角度分辨率而未被光束感知。这种分类使重建能够超越最初感知的点云。直观地说，这是有道理的，因为惩罚网络重构未采样的点是适得其反的，即使它们是底层物体的一部分。</p><p>我们观察到离散体素化会导致不精确，特别是在低分辨率体素网格下。为了减轻这种情况，我们通过从体素中心到最近激光雷达光束的垂直距离ds i来减少空体素的损失。更正式地说，我们将体素的权重定义为：</p><p><img src="/pic/mae3d15.png"></p><p>其中ds v表示步长s处体素对角线的长度。请注意，使用多个LiDAR(例如Waymo[35])的设置可以通过迭代每个LiDAR的分类过程来简单地处理。在那里，由于额外的激光雷达传感器，未知体素的数量将会减少。</p><p>我们需要对低分辨率和大跨步的体素分别进行相同的分类。例如，步长为s的体素在空间上覆盖步长为s&#x2F;2的8个体素(见图3)。但是，这里需要考虑的是，如果对低分辨率体素进行剪枝，那么在上采样过程中，低分辨率体素将不再对其高分辨率体素产生自监督信号。因此，如果一个低分辨率体素包含了任何高分辨率的被占用体素(图4中的红色方块)，那么这个低分辨率体素就被认为是被占用的。同样，如果它包含了任何未知的体素，但没有被占用的体素，那么这个低分辨率体素就是未知的(灰色方块)。否则，低分辨率体素将被归类为空。</p><p>综上所述，自监督预训练的目标是正确分类一个体素是被占用还是空的，同时不惩罚未知的体素。为此，我们使用加权二值交叉熵损失：</p><p><img src="/pic/mae3d16.png"></p><p>式子中S为所有步幅的集合，M为解码器中所有被占用和空的体素；ys i为体素化输入点云的实际占用，即vs I被占用时为1，否则为0;x1是模型预测的占用率。换句话说，<strong>我们对移除被占用的和保留空体素的每个修剪步骤进行惩罚，但我们不会对未知体素造成任何损失。</strong>、</p><h4 id="掩码策略"><a href="#掩码策略" class="headerlink" title="掩码策略"></a>掩码策略</h4><p>在应用于小尺度全3D点云的方法中，一种常见的掩码策略是应用最远点采样(FPS)和k最近邻算法来创建重叠的补丁[30,56]，然后随机移除。然而，将FPS应用于LiDAR点云将主要选择孤立的点，这些点通常是遥远的单个异常值。掩码这些对学习强特征表示没有好处，因为这些离群点通常不对应于真实场景结构，并且只有很少的邻近点。<strong>相反，我们利用已经存在的体素化步骤并随机掩码体素。</strong></p><p>此外，考虑到标准旋转激光雷达传感器的采样分辨率:它发出多个光束，等待它们返回，然后从经过的时间中得出与击中障碍物的距离，即距离。每个光束都有一个倾角，并围绕一个共同的垂直轴旋转，从而定义了一个方位角。由于两个特定的光束围成一个恒定的角度，因此空间分辨率和点的数量随着与被击中物体的距离而减少。<strong>考虑通过点云重建的SSRL，随着距离传感器距离的增加，自监督越少。</strong></p><p>直观地说，如果我们从附近的区域移除点，使它们看起来与更远的稀疏区域相似，那么模型应该能够更好地推广到这些区域。<strong>我们通过降低方位角和倾角的角分辨率来结合这种球形掩码思想。</strong>为了有效地做到这一点，我们对激光雷达的距离图像进行子采样。具体地说，我们随机抽取两个整数1≤mr,mc≤4，并分别过滤范围图像中r mod mr≤0或c mod mc≤0的所有行r和列c。例如，每隔一秒使用一行和一列会使角分辨率减半。<strong>通过这种方式，一个物体被采样，就好像它在更远的地方一样，但我们能够在重建任务中诱导出更强的自我监督信号。</strong></p><h3 id="实验"><a href="#实验" class="headerlink" title="实验"></a>实验</h3><p>我们通过关注汽车感知中的关键任务，即目标检测和语义分割来演示我们的MAELi预训练。为了验证我们的方法，我们采用了一系列架构，并在这些领域中广泛认可的数据集上对它们进行预训练。具体来说，我们使用了Waymo开放数据集[35]、KITTI[2、12、24]和ONCE[26]数据集。</p><p>我们与最先进的方法进行比较，除非另有说明，否则我们报告其官方结果，这些结果来自官方基准或相应的出版物。补充材料中提供了一个额外的消融研究，以分离我们方法中各种成分的贡献。</p><p>**实现细节:**我们将MAELi集成到OpenPCDet[36]框架(v0.5.2)中，并使用Minkowski引擎[9]来设计我们的稀疏解码器。对于KITTI, Waymo和ONCE，我们分别使用[0.05,0.05,0.1]，[0.1,0.1,0.15]和[0.1,0.1,0.2]的体素大小。给定各自的架构和数据集，我们预训练了30个epoch，没有任何标记的ground truth。我们使用Adam[22]和一个单周期策略[34]，最大学习率为0.003。我们在一台NVIDIA®Quadro RTX™8000上进行了所有实验。</p><h4 id="3D目标检测"><a href="#3D目标检测" class="headerlink" title="3D目标检测"></a>3D目标检测</h4><p>我们的预训练方法MAELi非常适合3D物体检测器，因为我们的损失公式让模型学习物体应该是什么样子。这些预训练的权重可以很好地泛化数据集，并允许检测器在目标数据集上进行有效的数据微调。</p><p>对于3D检测，我们设计了一个具有四个blocks的稀疏解码器，每个block反转编码器的一次下采样操作(确切的架构在补充材料中)。我们预训练整个编码器，包括密集的BEV主干[50]。因此，为了利用解码器的稀疏处理能力，我们从稀疏3D编码器的最后一层提取活动体素，并从BEV特征图中采样。为了使用预训练的权重微调不同的3D检测器，我们通过冻结编码器一个epoch来预热检测头，并根据默认的OpenPCDet配置进行端到端训练。</p><p>OpenPCDet提供了几种增强技术，即随机翻转&#x2F;旋转&#x2F;缩放和地面真值采样[50]。在预训练期间，我们利用随机翻转&#x2F;旋转&#x2F;缩放除了我们的掩蔽策略。对于微调，我们在数据效率实验中，除了地面真值采样[50]外，都是按原样使用。我们不能在这里应用它的普通形式，因为它在整个数据集中采样地面真实对象，并将这些随机复制粘贴到只包含少量对象的帧中。如果不进行更改，这将添加来自我们的次采样帧以外的真实对象，从而使这些研究无效。<strong>因此，我们过滤原始的ground truth数据库，并确保它只包含来自实际使用的帧的对象。</strong></p><p><strong>检测结果:</strong></p><p><strong>KITTI 3D数据集和基准</strong>[12]是第一个公开可用的3D物体检测数据集之一，由~ 7.5k激光雷达帧组成，这些帧在前置摄像头视图中被标记。KITTI360[24]是原始KITTI数据集的扩展，包含约80k激光雷达帧，提供额外的模态和更详尽的注释。为了与其他预训练方法进行比较，<strong>我们使用来自KITTI 3D、KITTI360和Waymo的预训练权值，展示了KITTI 3D集上的检测结果</strong>。在我们的实验中，我们报告了基于中等难度水平的评估指标，利用具有40个召回点的官方R40指标。</p><p>表1显示了广泛使用的SECOND[50]和PV-RCNN[32]检测器在使用不同预训练权值时的检测性能。在所有用于预训练的数据集中，MAELi能够对从头开始训练的检测模型进行强大的改进，并产生最佳性能的检测结果。特别是，我们在整体mAP上优于当前最先进的产品，即ALSO，而汽车类别的性能差异仍然在非常窄的范围内，这表明该类别的性能已经饱和。</p><p><img src="/pic/mae3d17.png" alt="表1.在全KITTI 3D训练集上对SECOND和PVRCNN的预训练权值进行微调后的性能比较。使用标准R40度量报告KITTI 3D值集的结果。"></p><p>**大规模的Waymo开放数据集[35]**包含798个训练序列和202个验证序列。为车辆、行人和骑自行车的人提供标签。我们使用Waymo官方评估协议报告我们的结果，在更具挑战性的LEVEL 2难度下报告APH。包括AP分数在内的详细结果在补充材料中提供。</p><p>我们在Waymo训练集上预训练我们的权值，并使用它们初始化SECOND[50]、CenterPoint[55]和PV-RCNN[32]的主干。为了与[27,54]进行公平的比较，我们遵循通用协议，使用整个Waymo训练集的20%(包括vanilla ground truth sampling)来微调检测器。</p><p>表2显示了Waymo与预训练方法Occupancy-MAE[27]、GCC3D[23]和ProposalContrast[54]的对比结果。使用MAELi进行预训练表明，在所有检测器上都有很大的改进。虽然MAELi预训练的权重显然比Occupancy-MAE更适合SECOND，但CenterPoint和PV-RCNN的结果与Occupancy-MAE相当，明显优于GCC-3D或ProposalContrast。</p><p><img src="/pic/mae3d18.png" alt="表2.在20%的Waymo训练集上训练的Waymo val集上的性能比较。我们利用GCC-3D、ProposalContrast、Occupancy-MAE和拟议的MAELi的预训练权重，将从零开始训练的不同检测器与其挂件进行比较。"></p><p><strong>ONCE[26]是一个综合性的数据集</strong>，专为诸如3D物体检测、跟踪和运动预测等任务而设计。该数据集包括100万帧，其中大部分是未标记的。ONCE的主要目标是为利用大规模未标记数据的研究奠定基础。在我们的工作中，我们利用官方的Usmall子集进行预训练，随后在训练集上微调我们的模型，并在验证集上进行评估。</p><p>表3给出了MAELi与其他预训练方法在ONCE上的对比结果。除了我们的良好结果之外，该评估还证明了我们的预训练方法具有强大的跨域泛化能力:对于SECOND[50]，即使使用来自Waymo的预训练权值，我们的表现也优于当前的方法，即55.84 mAP (MAELi在Waymo上预训练)和52.68 mAP(同样在Usmall上预训练)。当然，对ONCE本身进行预训练会进一步改善结果。</p><p><img src="/pic/mae3d19.png" alt="表3.ONCE验证集上的性能比较。我们的初始化，即使是在不同的数据集上进行预训练，也有助于胜过最先进的方法。"></p><blockquote><p>其他实验结果请看原文</p></blockquote>]]></content>
      
      
      <categories>
          
          <category> MAE </category>
          
      </categories>
      
      
        <tags>
            
            <tag> mae </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>点云自监督学习(MAE，持续更新)</title>
      <link href="/2023/12/25/mae/"/>
      <url>/2023/12/25/mae/</url>
      
        <content type="html"><![CDATA[<h1 id="点云自监督学习-MAE，持续更新"><a href="#点云自监督学习-MAE，持续更新" class="headerlink" title="点云自监督学习(MAE，持续更新)"></a>点云自监督学习(MAE，持续更新)</h1><p><a href="https://arxiv.org/abs/2203.06604">PointMAE论文链接</a></p><p><a href="https://github.com/Pang-Yatian/Point-MAE">PointMAE代码链接</a></p><p><a href="https://zhuanlan.zhihu.com/p/568827138">PointMAE讲解链接</a></p><p><a href="https://arxiv.org/abs/2111.14819">Point-BERT论文链接</a></p><p><a href="https://github.com/lulutang0608/Point-BERT">Point-BERT代码链接</a></p><p><a href="https://zhuanlan.zhihu.com/p/484336830">Point-BERT讲解链接</a></p><p> <img src="/pic/mae2.png" alt="目前点云分割的mae"></p><p>PointBERT pipeline（如下图）里的三个问题：</p><ul><li>需要训练一个基于DGCNN的dVAE用于生成点云的离散词表表示，整个pipeline比较复杂（引入了非Transformer的结构（DGCNN）来辅助Transformer训练）</li><li>依赖对比学习和数据增强去学习high-level语义特征（本文用调高mask ratio去解决）</li><li>PointBERT对点云做tokenize之后，被mask掉的token是以一个learnable token + positional embedding表示的，再与visible tokens一起输入Transformer Encoder，存在位置信息的泄漏（positional embedding里有mask token的位置信息，降低了reconstruction任务的难度）</li></ul><p><img src="/pic/mae1.png" alt="Point-BERT的pipeline。我们首先将输入点云划分为几个点pathes(子云)。然后使用一个迷你点网[34]来获得点嵌入序列。在预训练之前，通过基于dvae的点云重建(如图右图所示)学习Tokenizer，其中点云可以转换为一系列离散的点tokens;在预训练期间，我们对点嵌入的某些部分进行了掩码，并用掩码tokens替换它们。然后将掩蔽点嵌入到transformer中。在Tokenizer获得的点tokens的监督下，训练模型恢复原始点tokens。我们还添加了一个辅助的对比学习任务，以帮助transformer捕获高级语义知识。"></p><p>PointMAE pipeline:</p><ul><li>采用MAE的pipeline，在Encoder处仅输入visible tokens + visible token的positional embedding，在Decoder处才将visible tokens 和mask tokens 加上full-set的positional embedding 一起输入</li><li>采用非常高的mask ratio（60%）</li><li>直接对初始的点云xyz进行预测</li><li>整体网络仅由Transformer Blocks构成，没有其他结构</li><li>采用轻量级的decoder（encoder有12个Transformer blocks，decoder只有4个）</li></ul><p> <img src="/pic/mae3.png" alt="Point-MAE总体方案。在左边，我们展示了掩蔽和嵌入的过程。将输入云划分为多个点块，对点块进行随机掩码后嵌入。右侧为自动编码器预训练。编码器只处理可见的标记。将掩码tokens添加到解码器的输入序列中以重建掩码点补丁。"></p><p><strong>编码器仅处理visible tokens的好处：</strong></p><ul><li>可以让编码器更好地学习这些point patchs的high-level的语义特征，而不需要学习去区分visible tokens和mask tokens</li><li>避免了mask tokens的位置信息的泄漏</li><li>提高了网络的预训练效率（因为采用了高的mask ratio（60%），相当于编码器仅处理40%的tokens，自然就很快啦）</li></ul><p><strong>MAE pretrain效率高的原因:</strong></p><ul><li>编码器仅处理visible tokens</li><li>解码器虽然要处理全部tokens，但非常轻量级</li></ul>]]></content>
      
      
      <categories>
          
          <category> MAE </category>
          
      </categories>
      
      
        <tags>
            
            <tag> mae </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>终端神器tmux:多任务管理大师</title>
      <link href="/2023/12/21/tmux/"/>
      <url>/2023/12/21/tmux/</url>
      
        <content type="html"><![CDATA[<h1 id="终端神器tmux-多任务管理大师"><a href="#终端神器tmux-多任务管理大师" class="headerlink" title="终端神器tmux:多任务管理大师"></a>终端神器tmux:多任务管理大师</h1><h2 id="tumx是什么"><a href="#tumx是什么" class="headerlink" title="tumx是什么"></a>tumx是什么</h2><p>命令行的典型使用方式是，打开一个终端窗口（terminal window，以下简称”窗口”），在里面输入命令。用户与计算机的这种临时的交互，称为一次”会话”（session）。</p><p>会话的一个重要特点是，窗口与其中启动的进程是连在一起的。打开窗口，会话开始；关闭窗口，会话结束，会话内部的进程也会随之终止，不管有没有运行完。</p><p>一个典型的例子就是，SSH 登录远程计算机，打开一个远程窗口执行命令。这时，网络突然断线，再次登录的时候，是找不回上一次执行的命令的。因为上一次 SSH 会话已经终止了，里面的进程也随之消失了。</p><p>为了解决这个问题，会话与窗口可以”解绑”：窗口关闭时，会话并不终止，而是继续运行，等到以后需要的时候，再让会话”绑定”其他窗口。</p><p><strong>Tmux 就是会话与窗口的”解绑”工具，将它们彻底分离。</strong></p><blockquote><p>（1）它允许在单个窗口中，同时访问多个会话。这对于同时运行多个命令行程序很有用。<br>（2） 它可以让新窗口”接入”已经存在的会话。<br>（3）它允许每个会话有多个连接窗口，因此可以多人实时共享会话。<br>（4）它还支持窗口任意的垂直和水平拆分。</p></blockquote><h2 id="常用操作"><a href="#常用操作" class="headerlink" title="常用操作"></a>常用操作</h2><blockquote><p>1.新建会话tmux new -s my_session。<br>2.在 Tmux 窗口运行所需的程序。<br>3.按下快捷键Ctrl+b d将会话分离。<br>4.下次使用时，重新连接到会话tmux attach-session -t my_session。</p></blockquote><h2 id="基本用法"><a href="#基本用法" class="headerlink" title="基本用法"></a>基本用法</h2><h3 id="安装与进入"><a href="#安装与进入" class="headerlink" title="安装与进入"></a>安装与进入</h3><pre class="line-numbers language-none"><code class="language-none"># Ubuntu 或 Debian$ sudo apt-get install tmux# CentOS 或 Fedora$ sudo yum install tmux# Mac$ brew install tmux#键入tmux命令，就进入了 Tmux 窗口。$ tmux<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h3 id="前缀键Ctrl-b"><a href="#前缀键Ctrl-b" class="headerlink" title="前缀键Ctrl+b"></a>前缀键Ctrl+b</h3><p>Tmux 窗口有大量的快捷键。所有快捷键都要通过前缀键唤起。默认的前缀键是Ctrl+b，即先按下Ctrl+b，快捷键才会生效。</p><p>举例来说，帮助命令的快捷键是Ctrl+b ?。它的用法是，在 Tmux 窗口中，先按下Ctrl+b，再按下?，就会显示帮助信息。</p><p>然后，按下ESC 键或q键，就可以退出帮助。</p><h2 id="会话管理"><a href="#会话管理" class="headerlink" title="会话管理"></a>会话管理</h2><pre class="line-numbers language-none"><code class="language-none"># 新建一个指定名称的会话。$ tmux new -s &lt;session-name&gt;#分离会话$ tmux detach# 或者按下Ctrl+b d# 接入会话# 使用会话编号（attach 可以写成a）$ tmux attach -t 0$ tmux a -t 0# 使用会话名称$ tmux attach -t &lt;session-name&gt;$ tmux a -t &lt;session-name&gt;# 杀死会话# 使用会话编号$ tmux kill-session -t 0# 使用会话名称$ tmux kill-session -t &lt;session-name&gt;# 切换会话# 使用会话编号$ tmux switch -t 0# 使用会话名称$ tmux switch -t &lt;session-name&gt;# 重命名会话$ tmux rename-session -t 0 &lt;new-name&gt;<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h2 id="窗格快捷键"><a href="#窗格快捷键" class="headerlink" title="窗格快捷键"></a>窗格快捷键</h2><blockquote><p><strong>Ctrl+b d：分离当前会话。（后台继续运行）</strong><br>Ctrl+b s：列出所有会话。（tmux ls）<br>Ctrl+b $：重命名当前会话。<br><strong>Ctrl+b %：划分左右两个窗格。</strong><br><strong>Ctrl+b “：划分上下两个窗格。</strong><br>Ctrl+b <arrow key>：光标切换到其他窗格。是指向要切换到的窗格的方向键，比如切换到下方窗格，就按方向键↓。<br>Ctrl+b ;：光标切换到上一个窗格。<br>Ctrl+b o：光标切换到下一个窗格。<br>Ctrl+b {：当前窗格与上一个窗格交换位置。<br>Ctrl+b }：当前窗格与下一个窗格交换位置。<br>Ctrl+b Ctrl+o：所有窗格向前移动一个位置，第一个窗格变成最后一个窗格。<br>Ctrl+b Alt+o：所有窗格向后移动一个位置，最后一个窗格变成第一个窗格。<br><strong>Ctrl+b x：关闭当前窗格。</strong><br>Ctrl+b !：将当前窗格拆分为一个独立窗口。<br><strong>Ctrl+b z：当前窗格全屏显示，再使用一次会变回原来大小。</strong><br>Ctrl+b Ctrl+<arrow key>：按箭头方向调整窗格大小。<br><strong>Ctrl+b q：显示窗格编号。</strong></p></blockquote><h2 id="窗口快捷键"><a href="#窗口快捷键" class="headerlink" title="窗口快捷键"></a>窗口快捷键</h2><blockquote><p><strong>Ctrl+b c：创建一个新窗口，状态栏会显示多个窗口的信息。<br>Ctrl+b p：切换到上一个窗口（按照状态栏上的顺序）。<br>Ctrl+b n：切换到下一个窗口。<br>Ctrl+b <number>：切换到指定编号的窗口，其中的是状态栏上的窗口编号。<br>Ctrl+b w：从列表中选择窗口。<br>Ctrl+b ,：窗口重命名。</strong></p></blockquote><p>参考链接</p><p><a href="https://blog.csdn.net/Aibiabcheng/article/details/122482786">Tmux使用教程</a></p><p><a href="https://blog.csdn.net/qq_30883899/article/details/132871423">tmux的常用操作</a></p><p><a href="https://www.bilibili.com/video/BV1ML411h7tF/?spm_id_from=333.337.search-card.all.click&vd_source=c3ce2553ecbf3f37d2f4be6f466233fa">视频配置便捷tumx快捷键</a></p>]]></content>
      
      
      <categories>
          
          <category> tmux </category>
          
      </categories>
      
      
        <tags>
            
            <tag> tmux </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Learning Inter-Superpoint Affinity for Weakly Supervised 3D Instance Segmentation（未完待续）</title>
      <link href="/2023/12/04/superpoint3/"/>
      <url>/2023/12/04/superpoint3/</url>
      
        <content type="html"><![CDATA[<h1 id="Learning-Inter-Superpoint-Affinity-for-Weakly-Supervised-3D-Instance-Segmentation（未完待续）"><a href="#Learning-Inter-Superpoint-Affinity-for-Weakly-Supervised-3D-Instance-Segmentation（未完待续）" class="headerlink" title="Learning Inter-Superpoint Affinity for Weakly Supervised 3D Instance Segmentation（未完待续）"></a>Learning Inter-Superpoint Affinity for Weakly Supervised 3D Instance Segmentation（未完待续）</h1><h2 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h2><p>由于三维点云的标注很少，如何学习点云的区分特征来分割目标实例是一个具有挑战性的问题。在本文中，我们提出了一个简单而有效的3D实例分割框架，该框架可以通过对每个实例只标注一个点来获得良好的性能。具体地说，为了处理极少的标签例如分割，</p><ul><li>我们首先以无监督的方式将点云过分割成超点，并将点级注释扩展到超点级。</li><li>然后，在超点图的基础上，提出了一种考虑语义和空间关系的超点间亲和力挖掘模块，通过语义感知的随机游走自适应学习超点间亲和力，生成高质量的伪标签。</li><li>最后，我们提出了一个体积感知的实例求精模块，通过在超点图上的聚类中应用目标的体积约束来分割高质量的实例。</li></ul><p>在ScanNet-v2和S3DIS数据集上的大量实验表明，我们的方法在弱监督的点云实例分割任务中取得了最好的性能，甚至优于一些完全监督的方法。</p><h2 id="引言"><a href="#引言" class="headerlink" title="引言"></a>引言</h2><p>点云实例分割是三维计算机视觉中的一项经典任务，可应用于室内导航系统、增强现实、机器人等领域。全监督的实例分割方法[17，2，12]已经取得了令人印象深刻的结果，但它们依赖于大量的人工标记数据。然而，注释大量的点云是非常耗时和昂贵的。因此，以一种需要少量注记的半监督&#x2F;弱监督方式分割点云是有意义的。然而，如何充分利用有限的标签来提高实例分割的性能仍然是一个具有挑战性的问题。</p><p>很少有人致力于半监督&#x2F;弱监督点云实例分割。</p><ul><li>作为开拓者，廖添丁等人。[18]提出了一种以包围盒为监督的半监督点云实例分割方法，利用网络生成包围盒方案。实例分割是通过细化边界框内的点云来实现的。</li><li>此外，陶渊明等人也提出了自己的观点。[24]提出了一种两阶段分段监督3D实例和语义分割方法，该方法首先利用分段分组网络为整个场景生成伪标签，然后将生成的伪点级标签作为地面真值对网络进行训练。然而，这些简单的伪标签生成策略不能有效地生成高质量的伪标签，导致3D实例分割结果不佳。</li></ul><p>本文提出了一种简单而有效的弱监督3D实例分割框架，<strong>每个实例只需一个点的标注就能获得令人印象深刻的分割结果</strong>。对于标注较少的弱监督点云实例分割，我们的直觉体现在两个方面：</p><ul><li>(1)在稀有标注下，有效的标注传播是产生高质量伪标注的关键，尤其是在3D实例分割中。</li><li>(2)弱监督3D实例分割比弱监督3D语义分割更具挑战性，因此我们考虑引入目标体积约束来改善实例分割结果。具体地说，<ul><li>我们首先使用一种无监督的方法[14]将点云过度分割成超点并构建超点图。通过这种方式，点级别的标签可以扩展到超点级别的标签。</li><li>然后，我们提出了一个超点间亲和力挖掘模块，基于少量标注的超点级标签生成高质量的伪标签。在超点图的基础上，利用相邻超点的语义和空间信息自适应地学习超点间的亲和力，通过语义感知的随机游走将超点标记沿着超点图传播。</li><li>最后，我们提出了一个体积感知的实例精化模块来提高实例分割的性能。在超点传播训练模型的基础上，通过超点聚类得到粗略的实例分割结果，进而从实例分割结果中推断出目标的体积信息。目标体积信息包含体素的数量和目标的半径。将推断出的物体体积信息作为相应实例的地面真实，对网络进行再训练。在测试阶段，基于目标体积信息，利用预测的目标体积信息，提出了一种体积感知的实例聚类算法，用于分割高质量的实例。</li></ul></li></ul><p>在ScanNet-v2[6]和S3DIS[1]数据集上的大量实验证明了该方法的有效性。本论文的主要贡献如下：</p><ul><li>提出了一种考虑语义和空间关系的超点间亲和力挖掘模块，用于基于随机游走的标签传播自适应学习超点间亲和力。</li><li>提出了一种体积感知的实例求精模块，该模块利用对象体积信息指导超点图上的超点聚类进行实例分割。</li><li>我们简单而有效的框架在流行的数据集ScanNet-v2和S3DIS上实现了最先进的弱监督3D实例分割性能。</li></ul>]]></content>
      
      
      <categories>
          
          <category> DL </category>
          
      </categories>
      
      
        <tags>
            
            <tag> seg </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Superpoint Transformer for 3D Scene Instance Segmentation（未完待续）</title>
      <link href="/2023/12/03/superpoint2/"/>
      <url>/2023/12/03/superpoint2/</url>
      
        <content type="html"><![CDATA[<h1 id="Superpoint-Transformer-for-3D-Scene-Instance-Segmentation（未完待续）"><a href="#Superpoint-Transformer-for-3D-Scene-Instance-Segmentation（未完待续）" class="headerlink" title="Superpoint Transformer for 3D Scene Instance Segmentation（未完待续）"></a>Superpoint Transformer for 3D Scene Instance Segmentation（未完待续）</h1><p><a href="https://arxiv.org/abs/2211.15766">论文地址</a><br><a href="https://github.com/sunjiahao1999/SPFormer?utm_source=catalyzex.com">代码地址</a><br><a href="https://zhuanlan.zhihu.com/p/661437298">论文解读</a></p><h2 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h2><p>现有的大多数方法通过扩展用于3D对象检测或3D语义分割的模型来实现3D实例分割。然而，这些非直截了当的方法有两个缺点：</p><ul><li>1)不精确的边界框或不令人满意的语义预测限制了整体3D实例分割框架的性能。</li><li>2)现有方法需要耗时的聚合中间步骤。</li></ul><p>针对这些问题，本文提出了一种基于超点变换的端到端3D实例分割方法SPFormer。它将点云中的潜在特征分组为超点，并通过查询向量直接预测实例，而不依赖于对象检测或语义分割的结果。</p><ul><li>该框架的关键是设计了一种带transformer的查询解码器，通过超点交叉注意机制捕获实例信息，并生成实例的超点掩码。</li><li>通过基于超点掩码的二分图匹配，SPFormer无需中间聚合步骤即可实现网络训练，加快了网络的运行速度。</li><li>在ScanNetv2和S3DIS基准测试程序上的大量实验验证了该方法的简明性和有效性。值得注意的是，在ScanNetv2隐藏测试集上，SPFormer在MAP方面比现有的方法高出4.3%，同时保持了快速的推理速度(每帧247ms)。</li></ul><h2 id="引言"><a href="#引言" class="headerlink" title="引言"></a>引言</h2><p>3D场景理解被认为是许多应用的基本要素，包括增强&#x2F;虚拟现实(Park等人)。2020)、自动驾驶(周等人2020)和机器人导航(谢等人2021年)。通常情况下，实例分割是三维场景理解中的一项具有挑战性的任务，其目的不仅是检测稀疏点云上的实例，而且为每个实例提供清晰的掩码。</p><p><img src="/pic/Super1.png" alt="图1：不同方法的关键流程。(A)是输入点云。(B)基于proposal的方法首先检测对象。(C)基于分组的方法将点偏移到它们自己的实例中心和组点。(D)我们的方法通过超点交叉注意来突出感兴趣区域。"></p><p>现有的最先进的方法可以分为<strong>基于proposal的方法</strong>(Yang等人)。2019年；刘等人。2020)和<strong>基于分组</strong>(酱等人2020年；Chen等人。2021年；梁等人。2021年；Vu等人。2022年)。</p><ul><li>基于proposal的方法将3D实例分割视为一条自上而下的pipeline。它们首先生成区域proposals(即边界框)，如图1(B)所示，然后预测proposes区域中的实例掩码。这些方法受到MASK-RCNN巨大成功的鼓舞(他等人)。2017)在2D实例分段字段上。然而，由于域间隙的原因，这些方法在点云上遇到了困难。在三维领域中，包围盒具有更多的自由度(DoF)，增加了拟合的难度。此外，点通常只存在于物体表面的一部分，这导致无法检测到物体的几何中心。此外，低质量的区域proposal影响基于盒的二分图匹配(Yang等人。2019年)，并进一步降低了模型的性能。</li><li>相反，基于分组的方法采用自下而上的pipeline。它们学习逐点语义标签和实例中心偏移量。然后，它们使用偏移点和语义预测来聚集成实例，如图1(C)所示。在过去的两年中，基于分组的方法在3D实例分割任务中取得了很大的改进(梁等人)。2021年；Vu等人。2022年)。但也存在一些不足：<ul><li>(1)基于分组的方法依赖于它们的语义分割结果，这可能导致错误的预测。将这些错误预测传播到后续处理会抑制网络性能。</li><li>(2)这些方法需要一个中间的聚合步骤，增加了训练和推理时间。聚合步骤独立于网络训练，缺乏监督，需要额外的细化模块。</li></ul></li></ul><p>本文提出了一种基于超点变换的端到端两阶段3D实例分割方法SPFormer。SPFormer自下而上地将点云中的潜在特征分组到超点中，并通过查询向量将实例proposes作为自上而下的pipeline。</p><ul><li>在自下而上的分组阶段，利用稀疏的三维U-net提取自下而上的逐点特征。提出了一种简单的超点池化层，用于将潜在的逐点特征分组为超点。超点(Landrieu和Simonovsky 2018)可以利用几何规则来表示均匀的邻接点。与以前的方法(梁等人)不同2021)，<strong>我们的超点特征是潜在的</strong>，避免了通过非直截了当的语义和中心距离标签来监督特征。&#x3D;&#x3D;我们将超点作为3D场景潜在的中层表示，并直接使用实例标签来训练整个网络。&#x3D;&#x3D; </li><li>在自上而下的提proposal阶段，提出了一种新的带transformers的查询解码器。我们利用可学习的查询向量从潜在的超点特征中提出实例预测，作为自顶向下的pipeline。<strong>可学习查询向量通过超点交叉注意机制捕获实例信息</strong>。图1(D)示出了这样的过程，即椅子的部分越红，查询向量就越关注。利用携带实例信息和超点特征的查询向量，查询解码器直接生成实例类、得分和掩码预测。最后，通过基于超点掩码的二分图匹配，SPFormer可以实现端到端的训练，而不需要耗时的聚合步骤。此外，SPFormer没有像非最大值抑制(NMS)那样的后处理，进一步加快了网络速度。</li></ul><p>SPFormer在ScanNetv2和S3DIS基准测试中都达到了最先进的水平。特别是，SPFormer在定性和定量指标以及推理速度方面都超过了同类最先进的方法。SPFormer采用了一种新的流水线，可以作为3D实例分割的通用框架。总而言之，我们的贡献如下：</p><ul><li>我们提出了一种端到端的两阶段方法SPFormer，该方法不依赖于目标检测或语义分割的结果来表示具有潜在超文本特征的3D场景。</li><li>设计了一个带有transformers的查询解码器，其中可学习的查询向量可以通过超点交叉注意来捕获实例信息。通过查询向量，查询解码器可以直接生成实例预测。</li><li>通过基于超点掩码的二分图匹配，SPFormer可以实现网络训练，而不需要耗时的中间聚合步骤，也不需要复杂的推理后处理。</li></ul><h2 id="相关工作"><a href="#相关工作" class="headerlink" title="相关工作"></a>相关工作</h2><p><strong>以proposal为基础的方法。</strong></p><p>例如，基于proposal的方法采用自上而下的方法进行分割。以前的方法(Yi等人)2019年；侯、戴和尼埃纳2019年；成田等人。2019)专注于将2D图像要素和点云要素融合到一个体积网格中，并从该网格生成区域proposals。3D-Bonet(Yang et al.2019)使用PointNet++(齐等人2017a，b)从点云中提取特征，并将3D包围盒生成任务视为最优分配问题。GICN(Liu et al.2020)预测高斯热图以选择实例中心候选，并在提议的边界框内产生实例掩码。3D-MPA(Engelmann等人2020)样本预测质心和质心附近的聚集点以形成最终实例遮罩。大多数基于proposals的方法都基于3D边界框。然而，低质量的包围盒预测会影响实例分割模型的性能。</p><p><strong>基于分组的方法。</strong></p><p>基于分组的方法将3D实例分割视为一条自下而上的pipeline。MTML(Lahoud et al.2019)利用多任务策略学习特征嵌入。PointGroup(酱等人)2020)从原始和中心移动的点云中聚集点，并设计ScoreNet来评估聚集的质量。PE(Zhang And Wonka 2021)引入了一种新的概率嵌入空间。Dyco3D(何、沈和van den Hengel 2021)引入了动态卷积核。HAIS(Chen et al.2021)使用分层聚集来扩展PointGroup，并过滤实例预测内的噪声点。SSTNet(梁等人)2021)构建语义超点树，并通过拆分不相似节点获得实例预测。SoftGroup(Vu等人)2022)使用较低的聚类阈值来解决错误的语义硬预测，并使用微小的3D U-net来优化实例。尽管基于分组的方法可能有一个自上而下的精化模块，但它们仍然不可避免地依赖于中间聚合步骤。</p><p><strong>使用Transformer进行2D实例分割。</strong></p><p>最近，Transform(Vaswani et al.)2017)被引入到图像分类中(Dosovitski等人。2020年；Touvron等人。2021年；刘等人。2021)、目标检测(Carion等人2020年；戴相龙等人。2021)和分段(cheng，Schwing，and Kirillov，2021；cheng et al.2022A；郭某等人。2021年)。还有一些实例分割方法(Fang等人)。2021年；程等人。2022b)受到transformer的启发。Mask2Former(程等人)2022A)成功地应用tranformers构建了二维图像语义、实例和全景分割的通用网络。</p><p>受到Transform在2D分割任务中的成功应用的启发，我们有动力将Transform引入到3D实例分割中。然而，由于注意机制的复杂性，transformer不能简单地应用于稀疏卷积主干的输出，因为它会引入很高的计算开销。</p><p>本文中，我们将<strong>设计一种新的查询解码器用于3D实例分割，并使用超点在主干和查询解码器之间架起一座桥梁</strong>。</p><h2 id="方法"><a href="#方法" class="headerlink" title="方法"></a>方法</h2><p><img src="/pic/Super2.png" alt="图2"></p><p><img src="/pic/Super3.png"><br>SPFormer的体系结构如图2所示。首先，使用稀疏的3D U-net来提取自下而上的点状特征。提出了一种简单的超点池化层，用于将潜在的逐点特征分组为超点。其次，提出了一种新的带变换的查询解码器，其中可学习的查询向量通过超点交叉注意来获取实例信息。最后，通过基于超点掩码的二分图匹配，SPFormer可以实现端到端的训练，而不需要耗时的聚合步骤。</p><h3 id="Backbone-and-Superpoints"><a href="#Backbone-and-Superpoints" class="headerlink" title="Backbone and Superpoints"></a>Backbone and Superpoints</h3><p><strong>稀疏3D U-net。</strong></p><p>假设输入点云有N个点，输入可以表示为P∈RN×6。每个点都有颜色r，g，b和坐标x，y，z。在前面的实现(Graham，Engelcke和Van Der Maten 2018)之后，我们将点云体素化用于常规输入，并使用子流形稀疏卷积(SSC)或稀疏卷积(SC)组成的U-net骨干来提取点特征P∈RN×C。我们在补充材料中给出了稀疏3D U-net的细节。与常用的基于分组的方法不同，我们的方法不增加额外的语义分支和偏置分支。</p><p><strong>超点池化层。</strong></p><p>为了构建端到端框架，我们直接将逐点功能P∈RN×C馈送到基于预计算超点的超点池层(Landrieu和Simonovsky 2018年)。超点池化层通过对每个超点内部的逐点平均池化，简单地获得超点特征S∈Rm×C。在不失去一般性的情况下，我们假设从输入点云计算出M个超点。值得注意的是，超点池化层可靠地将输入点云下采样到数百个超点，这显著降低了后续处理的计算开销，并优化了整个网络的表示能力。</p><p><a href="https://blog.csdn.net/Dujing2019/article/details/104091750?spm=1001.2101.3001.6650.1&utm_medium=distribute.pc_relevant.none-task-blog-2~default~CTRLIST~Rate-1-104091750-blog-105473851.235%5Ev39%5Epc_relevant_3m_sort_dl_base4&depth_1-utm_source=distribute.pc_relevant.none-task-blog-2~default~CTRLIST~Rate-1-104091750-blog-105473851.235%5Ev39%5Epc_relevant_3m_sort_dl_base4&utm_relevant_index=2">Large-scale point cloud semantic segmentation with superpoint graphs.</a></p><h3 id="查询解码器"><a href="#查询解码器" class="headerlink" title="查询解码器"></a>查询解码器</h3><p>查询解码器由实例分支和掩码分支组成。在MASK分支中，一个简单的多层感知器旨在提取掩码感知特征SMASK∈Rm×D。实例分支由一系列transformer解码器层组成。它们通过超点交叉注意来解码可学习的查询向量。假设有K个可学习的查询向量。我们预定义了来自各transformer解码层的查询向量的特征为Z∈RK×D，D为嵌入维度，&#x3D;1，2，3…是层索引。</p>]]></content>
      
      
      <categories>
          
          <category> DL </category>
          
      </categories>
      
      
        <tags>
            
            <tag> seg </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Learning Superpoint Graph Cut for 3D Instance Segmentation （未完待续）</title>
      <link href="/2023/12/02/superpoint/"/>
      <url>/2023/12/02/superpoint/</url>
      
        <content type="html"><![CDATA[<h1 id="Learning-Superpoint-Graph-Cut-for-3D-Instance-Segmentation-（未完待续）"><a href="#Learning-Superpoint-Graph-Cut-for-3D-Instance-Segmentation-（未完待续）" class="headerlink" title="Learning Superpoint Graph Cut for 3D Instance Segmentation （未完待续）"></a>Learning Superpoint Graph Cut for 3D Instance Segmentation （未完待续）</h1><p><a href="https://proceedings.neurips.cc/paper_files/paper/2022/file/ef0af61ccfba2bf9fad4f4df6dfcb7c3-Paper-Conference.pdf">论文地址</a><br><a href="https://github.com/fpthink/GraphCut">代码地址(待发布)</a></p><h2 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h2><p>由于点云中目标的复杂局部几何结构，3D实例分割是一项具有挑战性的任务。在本文中，我们提出了一种基于学习的<strong>超点图割方法</strong>，它<strong>显式地</strong>学习点云的局部几何结构，用于3D实例分割。具体地说，</p><ul><li>我们首先将原始点云过分割成超点，并构造超点图。</li><li>然后，我们提出了一个边分数预测网络来预测超点图的边分数，其中通过交叉图注意力在坐标和特征空间中学习到的相邻节点的相似性向量被用来回归边分数。通过迫使同一实例的两个相邻节点在坐标和特征空间中靠近实例中心，我们提出了一种几何感知的边缘损失(Geometry-Aware Edge Loss)来训练边缘得分预测网络。</li><li>最后，我们开发了一个超点图割网络，该网络利用学习到的<strong>边分数和预测的节点语义类别</strong>来生成实例，其中提出了双边图注意力来提取坐标空间和特征空间上的区分特征来预测语义标签和实例分数。在ScanNet v2和S3DIS这两个具有挑战性的数据集上的大量实验表明，该方法在3D实例分割上达到了最新的性能。</li></ul><h2 id="1-引言"><a href="#1-引言" class="headerlink" title="1.引言"></a>1.引言</h2><p>近年来，随着激光雷达、Kinect摄像机等三维传感器的发展，各种三维计算机视觉任务受到越来越多的关注。3D场景分割是3D场景理解中的一项基本任务，在自动驾驶汽车、虚拟现实、机器人导航等领域有着广泛的应用。尽管近年来3D实例分割的进展令人鼓舞，但&#x3D;&#x3D;由于复杂几何结构的3D场景中3D点的不规则性和上下文不确定性&#x3D;&#x3D;，分割仍然是一项具有挑战性的任务。</p><p>许多人致力于3D实例分割，并取得了令人满意的性能。这些方法主要可以分为两类：基于检测的方法[44，45]和基于聚类的方法[40，18]。</p><ul><li>在基于检测的方法中，3D-Bonet[44]首先检测3D包围盒，然后使用掩码预测网络预测目标掩码以用于3D实例分割。然而，对于具有复杂几何结构的目标，基于检测的方法[45]无法获得准确的3D边界框，从而降低了实例分割的性能。</li><li>基于聚类的方法SGPN[40]基于语义分割对三维点进行聚类生成实例。与SGPN不同的是，酱等人。[18]提出了一种基于双重坐标空间(包括原始坐标空间和移动坐标空间)中的语义预测来聚类点的偏移量分支。此外，一些后续方法利用树结构[25]、层次聚集[3]和软语义分割[37]来提高3D实例分割的性能。然而，这些基于聚类的方法大多依赖于中心偏移量和语义来分割实例，不能有效地捕捉点云的几何上下文信息。因此，点云中具有复杂几何结构的目标往往限制了实例分割的性能。</li></ul><p>在本文中，我们提出了一种基于学习的超点图切割方法，<strong>该方法显式地学习点云的局部几何结构来分割3D实例</strong>。具体地说， &#x3D;&#x3D;我们构造超点图来学习超点的几何上下文相似性，并将实例分割转化为边的二进制分类&#x3D;&#x3D;。</p><p>我们的方法包括一个用于预测边缘分数的边缘分数预测网络和一个用于生成实例的超点图割网络。</p><ul><li>在我们的方法中，我们将原始点云过分割成超点，并通过链接坐标空间中k个最近的超点来构造超点图。</li><li>在边分数预测网络中，我们首先对两个相邻节点的局部邻域进行交叉图注意力，以提取局部几何特征来衡量节点的相似性。<ul><li>然后，基于从坐标空间和特征空间学习到的相似性向量，采用边缘分数分支来预测边缘分数。</li><li>此外，我们还提出了一种<strong>几何感知的边缘损失</strong>来训练边缘得分预测网络，该方法迫使同一实例的相邻节点在坐标空间和特征空间中都靠近实例中心。</li></ul></li><li>在超点图割网络中，我们使用学习到的边分数结合节点的语义类别来切割边，以形成object proposals。<ul><li>proposals是通过在超点图上应用广度优先搜索算法来聚合同一连通分量中的节点而得到的。</li><li>在每个proposals中，我们应用双边图注意力来聚合局部几何特征，以提取可区分的特征来预测类别和数十个proposals。</li><li>此外，我们采用掩码学习分支来过滤proposals中的低置信度超点以生成实例。</li></ul></li></ul><p>综上所述，我们提出了一种边缘分数预测网络，该网络学习相邻节点的局部几何特征来生成边缘分数。</p><ul><li>为了训练它，我们提出了一种几何感知的边缘损失，以同时在坐标空间和特征空间保持实例的紧凑性。</li><li>提出了一种超点图割网络，该网络通过在坐标和特征空间中利用双边图注意力来提取可区分的实例特征来生成准确的实例。在ScanNet v2[7]和S3DIS[1]数据集上的大量实验表明，该方法在3D实例分割上达到了最新的性能。在ScanNet v2的在线测试集上，我们的方法在MAP方面达到了55.2%的性能，比目前最好的结果高出4.6%[25]。对于S3DIS，我们的方法在MAP方面比当前最好的结果[37]要好2%以上。</li></ul><h2 id="2-相关工作"><a href="#2-相关工作" class="headerlink" title="2.相关工作"></a>2.相关工作</h2><h3 id="3D语义分割"><a href="#3D语义分割" class="headerlink" title="3D语义分割"></a>3D语义分割</h3><p>从不规则的三维点云中提取特征是3D语义分割的关键。</p><ul><li>qi等人[30]首先提出了<strong>PointNet</strong>，通过多层感知器网络从点集学习逐点特征进行语义分割。在此之后，人们提出了许多改进语义分割性能的努力[24，34，46，15，47，4，2]。早期的基于点的方法[36，43，31，14]设计了各种局部特征聚合策略来提取可区分的逐点特征用于语义分割。</li><li>受成功的2D卷积网络的启发，<strong>基于视图</strong>的方法[23，35，17，19]将点云投影到多个规则的2D视图中，在其中应用规则的2D卷积来提取特征。</li><li>除了基于视点的方法外，<strong>基于体积</strong>的方法[27，39，10，6]首先将点云体素化为规则的3D网格，然后应用3D卷积来提取点云的局部特征。</li><li>为了捕捉点云的局部几何结构，<strong>基于图</strong>的方法[42，22，38，5，16]在点云上构造图形，并利用图的卷积来聚集局部几何信息进行语义分割。</li></ul><h3 id="3D实例分割"><a href="#3D实例分割" class="headerlink" title="3D实例分割"></a>3D实例分割</h3><p>3D实例分割是一项更具挑战性的任务，它进一步需要识别每个实例。目前的方法大致可以分为两类：基于检测的方法和基于聚类的方法。</p><ul><li>基于检测的方法[45，13，26]首先检测点云中每个目标的3D包围盒，然后在每个包围盒上应用掩码预测网络来预测3D实例分割中的目标掩码。在[44]中，提出了一种称为3D-BoNet的3D实例分割框架，该框架直接回归所有实例的3D边界框，并预测每个实例的点级掩码。Yi等人的研究成果。[45]提出了一种生成式形状proposal网络，该网络通过从场景中的噪声观测重建形状来生成proposal，用于3D实例分割。此外，使用几何体和RGB输入，[13]开发了联合2D-3D特征学习网络，该网络结合2D和3D特征以回归3D目标边界框并预测实例masks。</li><li>基于聚类的方法通常使用点相似性[40]、语义地图[11，20]或几何移位[18，3，25，33]来将3D点聚类成对象实例。文献[40]中提出了一种相似度分组proposal网络，通过学习逐点相似度来聚类点以生成实例。[29]提出了一种多任务学习框架，该框架同时学习3D点的语义类别和高维嵌入，以将点聚类为对象实例。在[41]中，引入了一个分割框架来学习语义感知的逐点实例嵌入，用于关联分割点云实例和语义。Hanet al.[11]提出了一种占用率感知的方法来预测每个实例的占用体素数。PointGroup[18]通过使用预测的逐点中心偏移向量和逐点语义标签来聚类点。后续方法[3]采用分层聚集策略进行3D实例分割，首先进行点聚集，将点聚类成初步集合，然后再进行集合聚集，将集合聚类成实例。最近，Vu等人。[37]提出了一种软分组策略，通过将每个点与多个类别相关联来缓解语义预测错误的问题，从而在3D实例分割中获得显著的性能提升。此外，文献[25]还提出了一种语义超点树网络，称为SSTNet，用于分割实例中的点云。该算法首先对语义特征相似的超点进行分组，构建一棵二叉树，然后通过树的遍历和分裂生成实例。为了提高网络的效率，将动态卷积网络与小型变压器网络相结合，提出了一种轻量级的3D实例分割方法[12]。</li></ul><h2 id="3-方法"><a href="#3-方法" class="headerlink" title="3.方法"></a>3.方法</h2><p>基于学习的超点图割方法的概述如图1所示。基于超点图，边分数预测网络(SEC。3.1)从坐标和特征空间中提取边缘嵌入，用于预测边缘得分。之后，超点图割网络(SEC.3.2)通过学习区分实例特征来预测类别和实例分数，从而生成准确的对象实例。最后，在SEC3.3中，我们描述了如何从点云中训练我们的方法和推理实例。</p><p><img src="/pic/SuperPoint1.png" alt="图1：基于学习的超点图切割方法的框架。在给定超点图的情况下，我们首先**提取超点特征(图中省略)**。然后，我们构建了一个边缘分数预测网络，从坐标和特征空间中提取边缘嵌入，以预测边缘分数。最后，基于学习到的边分数，我们开发了一个超点图割网络来生成准确的实例。"></p><h3 id="3-1-边缘得分预测网络"><a href="#3-1-边缘得分预测网络" class="headerlink" title="3.1.边缘得分预测网络"></a>3.1.边缘得分预测网络</h3><p>给定一个原始点云，我们将其过分割成超点，并构造超点图G&#x3D;(V，E)，<strong>其中V表示超点的节点集，E表示边集</strong>。由于超点表示比点表示更粗糙，直接从超点表示学习特征不能有效地捕捉点云的局部几何结构。</p><ul><li>因此，我们在点云上应用子流形稀疏卷积[10]来提取点级特征，并使用点级特征通过平均汇集来初始化超点级特征。</li><li>之后，我们应用边缘条件卷积[32]来提取超点特征，记为F∈R|V|×C，其中C是特征维度。</li></ul><p><a href="https://blog.csdn.net/qq_53086461/article/details/129152257">子流形稀疏卷积解读</a><br><a href="https://blog.csdn.net/yuanmiyu6522/article/details/124943607">边缘条件卷积解读</a></p><h4 id="3-1-1-边缘特征嵌入-Edge-Feature-Embedding"><a href="#3-1-1-边缘特征嵌入-Edge-Feature-Embedding" class="headerlink" title="3.1.1.边缘特征嵌入(Edge Feature Embedding)"></a>3.1.1.边缘特征嵌入(Edge Feature Embedding)</h4><p>一旦我们获得超点特征，边缘得分预测网络就学习边缘嵌入(edge embeddings)来预测分割实例的边缘得分。给定相邻节点(u，v)∈E，期望学习边嵌入可以有效地识别节点u和v是否属于同一实例。为了解决这个问题，我们将<strong>交叉图注意力</strong>应用于双空间(坐标空间和特征空间)中的超点图，以学习超点相似性。学习到的节点u和v的相似度向量被用来形成用于预测边缘得分的边缘嵌入。</p><p><strong>在坐标空间中边缘嵌入。</strong></p><p>为了刻画节点u和v的相似性，</p><ul><li>我们首先将它们移向坐标空间中对应的实例质心。这里，多层感知器(MLP)网络对F进行编码以产生|V|偏移向量O&#x3D;{o1，.。。，o|V|}∈R|V|×3。给出原始超点坐标X&#x3D;{x1，.。。，x|V|}∈R|V|×3，移位的超点坐标Xˆ&#x3D;{ˆx1，.。。，ˆx|V|}由Xˆ&#x3D;X+O得到，这样就增加了属于不同实例的节点之间的几何距离，从而增强了对超点的区分。</li><li>然后，基于移动后的坐标空间，对于节点u，利用其k个最近的超点(即Nu)来构造局部k-NN图Gu。同样，我们可以得到节点v的图GV。</li><li>接着，我们对Gu和Gv进行交叉图注意力，通过学习的特征向量来表征节点的相似性，如图1所示。以节点u为例，交叉图注意力的权重α定义为：</li></ul><p> <img src="/pic/SuperPoint2.png" alt="公式1"></p><ul><li>其中ˆxi和ˆXu是移动后的坐标。请注意，我列举了两个图中的所有2*k个邻neighbors。因此，最终输出特征向量可表示为：</li></ul><p> <img src="/pic/SuperPoint3.png" alt="公式2"></p><ul><li>其中ˆαu，i是权重αu，i在Softmax之后，bi是可学习的偏差。学习的特征向量hu∈RC可以通过自适应地学习两个图上的几何差异来表征几何相似性。同样，我们可以得到另一个节点v的特征向量hv。我们将特征向量hv和hv组合为嵌入在坐标空间中的边：eu，v&#x3D;[hu，hv]。</li></ul><p><strong>特征空间中的边缘嵌入。</strong></p><p>除了考虑坐标空间外，我们还考虑了特征空间来提取可区分的边缘嵌入。</p><ul><li>首先，一个MLP网络对F进行编码以产生嵌入Z∈Rd的初始特征。通过将实例的特征嵌入相互推开，扩大了不同实例在特征空间中的距离。</li><li>给定一对结点(u，v)∈E，我们在特征空间中分别构造了k-NN图ˆGu和ˆGV。通过这种方式，可以预期每个图可以聚合同一实例中的超点。</li><li>然后，我们在ˆGU和ˆGV上执行交叉图注意力来刻画特征空间中节点的相似性。最后得到特征向量ˆhu∈Rc和ˆhv∈Rc。如果u和v属于同一实例，则它们在特征空间中共享相似的k-NN图，从而使得学习到的特征向量hˆu和ˆhv彼此相似。在这里，我们还结合特征向量来得到在特征空间中边缘嵌入：ˆeu，v&#x3D;[ˆhu，ˆhv]。</li></ul><p><strong>边缘分数预测。</strong><br>在获得坐标空间和特征空间中的边缘嵌入后，我们利用一个简单的MLP网络来生成边缘分数，其定义如下：</p><p><img src="/pic/SuperPoint4.png" alt="公式3"></p><p> 其中[·，···]表示串联运算，σ表示Sigmoid函数，du，v表示节点u和v在移位坐标空间中的几何距离。在实验中，如果边得分au，v&gt;0.5，则意味着节点u和v之间的边应该从超点图中截断。我们使用二进制交叉熵损失Ledge来最小化边缘得分。</p><h4 id="3-1-2-几何感知边缘损失（Geometry-Aware-Edge-Loss）"><a href="#3-1-2-几何感知边缘损失（Geometry-Aware-Edge-Loss）" class="headerlink" title="3.1.2.几何感知边缘损失（Geometry-Aware Edge Loss）"></a>3.1.2.几何感知边缘损失（Geometry-Aware Edge Loss）</h4><p>为了训练边分数预测网络，我们使用超点图的几何结构来形成几何感知边损失，如图2所示。</p><p><img src="/pic/SuperPoint5.png" alt="图2：几何感知边缘损失的详细信息。图(A)和(C)中的u和v属于同一实例，而图(B)和(D)中的u和v属于不同的实例。"></p><p>具体地说，给定节点u、v及其对应的实例质心cu和cv，我们通过最小化L2距离du、cu和dv、cv来向它们的实例质心绘制节点。此外，当(u，v)∈E属于同一实例时，期望它们可以通过最小化三角形UVC的面积来协作地移动到相同的实例质心c。如果(u，v)∈E属于不同的实例，但期望它们可以通过最小化三角形uvcu和uvcv的面积来协作地转移到它们自己的实例质心cu和cv。坐标空间中的面积约束写成：</p><p><img src="/pic/SuperPoint6.png" alt="公式4"></p><p>其中i(u，v)是指示函数，如果u和v属于同一实例，则i(u，v)等于1，否则等于0。请注意，“×”表示用于计算三角形面积的矢量的外积。对于来自同一实例的节点u和v，u和v同时靠近公共实例质心。因此，它们在坐标空间中拉得很近，这有助于将u和v组合到同一实例中。对于来自不同实例的节点u和v，u和v分别接近对应的实例质心。因此，它们在坐标空间中被推开，这有助于将u和v分成两个不同的实例。</p><p>同样，我们期望同一实例中的节点通过约束其特征嵌入而在特征空间中是紧凑的。对于(u，v)∈E属于同一实例，我们将u和v的嵌入画向实例的平均嵌入，并将它们拉到彼此之间。对于(u，v)∈E属于不同的实例，我们将u和v的嵌入相互推开。此外，通过增加实例自身的平均嵌入距离，实例彼此推开。因此，特征空间中的约束写成：</p><p><img src="/pic/SuperPoint7.png" alt="公式5"></p><p>其中zu∈Rd和zv∈Rd是特征嵌入。注意，gu∈Rd和gv∈Rd分别指示u和v所属的实例的平均特征嵌入。阈值δ和β设置为0.1和1.5，以确保实例间距离大于实例内距离。最后，几何感知边缘损失被定义为：</p><p><img src="/pic/SuperPoint9.png" alt="公式6"></p><h3 id="3-2-超点图割网络-Superpoint-Graph-Cut-Network"><a href="#3-2-超点图割网络-Superpoint-Graph-Cut-Network" class="headerlink" title="3.2.超点图割网络(Superpoint Graph Cut Network)"></a>3.2.超点图割网络(Superpoint Graph Cut Network)</h3><h4 id="3-2-1-基于超点图割的proposal生成方法"><a href="#3-2-1-基于超点图割的proposal生成方法" class="headerlink" title="3.2.1.基于超点图割的proposal生成方法"></a>3.2.1.基于超点图割的proposal生成方法</h4><p>给定边得分A&#x3D;{au，v}∈R|E|×1，我们提出了一种proposal生成算法，通过<strong>同时利用学习的边得分和预测的节点语义类别(即超点)来生成候选proposal</strong>。具体地说，</p><ul><li>为了减少语义预测误差，我们遵循[37]，采用soft阈值θ将节点与多个类别相关联。给定超点的语义分数S&#x3D;{s1，.。。，S|V||si∈RN，i&#x3D;1，.。。，|V|}，其中N是类的数目，如果si v&gt;θ，则第v个超点可以与第i个类相关联。这样，对于第i类，我们可以对超点图上的一个超点子集Ci进行切片，其中第i类索引上的超点的语义得分高于θ。</li><li>然后，在超点图上，对于边(u，v)∈E，如果节点u∈Ci和v∈Ci，边(u，v)将被保留，否则边将被删除。换句话说，我们去掉了两个语义不同的超点节点之间的边。</li><li>接着，对于超点图上保留的边(u，v)，我们利用边分数au，v来确定是否应该从超点图中切出边。在实验中，切割边缘的阈值设置为0.5。如果边得分高于0.5，则将从超点图中剪切该边。</li><li>最后，我们在超点图上应用<strong>广度优先搜索算法</strong>来聚集同一连通分支中的节点，以生成第i类的proposal。通过这种方式，我们可以通过迭代N个类来生成N个类的proposal。详细信息如算法1所示。</li></ul><p> <img src="/pic/SuperPoint8.png" alt="proposal生成算法"></p><h4 id="3-2-2-proposal嵌入中的双边图注意力"><a href="#3-2-2-proposal嵌入中的双边图注意力" class="headerlink" title="3.2.2.proposal嵌入中的双边图注意力"></a>3.2.2.proposal嵌入中的双边图注意力</h4><p>随着我们获得proposals I&#x3D;{I1，.。。从点云数据出发，通过在坐标空间和特征空间应用注意机制，我们提出了双边图注意来提取生成实例的proposal嵌入。具体地说，给定第i个proposal，我们首先通过平均移位的超点坐标来计算方proposal质心Ci。然后，我们采用相应超点的反距离加权平均来对proposals 质心的嵌入进行插补，其公式如下：</p>]]></content>
      
      
      <categories>
          
          <category> DL </category>
          
      </categories>
      
      
        <tags>
            
            <tag> seg </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Efficient LiDAR Point Cloud Oversegmentation Network</title>
      <link href="/2023/12/01/overseg/"/>
      <url>/2023/12/01/overseg/</url>
      
        <content type="html"><![CDATA[<h1 id="Efficient-LiDAR-Point-Cloud-Oversegmentation-Network"><a href="#Efficient-LiDAR-Point-Cloud-Oversegmentation-Network" class="headerlink" title="Efficient LiDAR Point Cloud Oversegmentation Network"></a>Efficient LiDAR Point Cloud Oversegmentation Network</h1><p><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Hui_Efficient_LiDAR_Point_Cloud_Oversegmentation_Network_ICCV_2023_paper.pdf">论文地址</a><br><a href="https://openaccess.thecvf.com/content/ICCV2023/supplemental/Hui_Efficient_LiDAR_Point_ICCV_2023_supplemental.pdf">补充材料</a><br><a href="https://github.com/fpthink/SuperLiDAR">代码地址(待发布)</a></p><h2 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h2><p>点云过分割是一项具有挑战性的任务，因为它需要产生具有感知意义的点云分区(即超点)。现有的大多数过分割方法都不能有效地从大规模LiDAR点云中生成超点，这是因为过程复杂且效率低下。本文提出了一种简单而高效的端到端LiDAR过分割网络，该网络通过基于低级点嵌入的分组来分割LiDAR点云中的超点。具体地说，</p><ul><li>我们首先从构建的局部邻域中学习点的相似性，通过<strong><strong>局部区分损失</strong></strong>来获得低级点嵌入。</li><li>然后，为了从稀疏的LiDAR点云中生成齐次超点，我们提出了一种同时考虑<strong>点嵌入相似性和点在三维空间中的欧几里德距离</strong>的LiDAR点分组算法。</li><li>最后，我们设计了一个超点求精模块，用于准确地将硬边界点分配到相应的超点。</li></ul><p>在两个大规模室外数据集SemancKITTI和nuScenes上的广泛结果表明，我们的方法在LiDAR过分割方面取得了新的进展。值得注意的是，我们的方法的推理时间比其他方法快100倍。此外，我们将学习到的超点应用到LiDAR语义分割任务中，结果表明，使用超点可以显著提高基线网络的LiDAR语义分割。</p><h2 id="介绍"><a href="#介绍" class="headerlink" title="介绍"></a>介绍</h2><p>在现代自动驾驶汽车中，3D LiDAR传感器可以获取周围物体及其表面特征的精确距离测量，以用于大规模户外场景理解。近年来，LiDAR语义分割[21]得到了广泛的研究，出现了各种方法并取得了令人印象深刻的结果。然而，激光雷达过分割在三维计算机视觉中很少被研究。与LiDAR语义分割不同，<strong>LiDAR过分割输出有感知意义的点云细分。得到的超点是一组点，这些点在对象的局部区域中在语义和几何上是同质的。超点表示法能够自适应、灵活地表示物体的局部几何结构。</strong>因此，研究激光雷达过分割对于基于激光雷达点云的应用是非常有意义的。然而，激光雷达点云的稀疏性、噪声和不规则性给激光雷达过分割带来了巨大的挑战。</p><p>早期的点云过分割方法通常是基于优化的方法。</p><ul><li>Lin等人的研究。[17]将超点分割问题归结为子集选择问题，提出了一种利用点云局部信息通过最小化能量函数来分割超点的启发式算法。</li><li>Guinard等人。[8]将点云过分割问题描述为一个结构化优化问题，并使用手工创建的局部描述符通过贪婪的图割算法生成几何简单的超点[14]。然而，由于LiDAR点云稀疏，计算的手工创建的特征的区分性较差，因此生成的超点无法在相似对象之间产生清晰的边界。</li><li>Landrieu等人。[13]引入了一种深度网络来提取点嵌入，用来代替[14]中的手工特征来分割超点。由于它是一个两阶段的方法，超点分割的处理过程复杂且耗时。</li><li>最近，Huy等人提出了一个新的观点。[11]提出了一种端到端的超点网络，该网络迭代地学习点-超点关联映射以聚类超点。然而，它需要后处理来滤除噪声点。总之，由于程序复杂和效率低下，上述方法不能有效地从大规模LiDAR点云生成超点。</li></ul><p>在本文中，我们提出了一种简单而高效的LiDAR过分割网络SuperLiDAR，它直接从LiDAR点云输出超点，而不需要任何额外的处理过程。超点分割的核心思想是基于<strong>低级点嵌入对点进行分组</strong>。具体地说，为了学习低级点嵌入，</p><ul><li>我们首先将其描述为一个由点云上定义的局部邻域构造的深度度量学习问题。引入局部判别损失，将3D点嵌入到同一物体的局部邻域内，从而保证了点的嵌入是相似的。</li><li>在获得低级点嵌入后，我们提出了一种LiDAR点分组算法，该算法通过广度优先搜索(BFS)对点进行分组以生成超点。利用点嵌入的相似性和三维点坐标的欧几里德距离，应用BFS算法生成紧致超点。</li><li>最后，我们提出了一个超点求精模块，该模块学习硬边界点与其k-最近邻候选超点之间的亲和力。通过将硬边界点与对应的相似度最高的超点进行赋值，可以得到高质量的超点。</li></ul><p>值得注意的是，我们的LiDAR过分割网络可以灵活地与下游任务集成，例如语义分割。为了评估学习的超点的有效性，我们引入了一个简单的<strong>多尺度超点聚合模型</strong>来进行LiDAR语义分割。为了验证该方法的有效性，我们在语义KITTI和nuScenes这两个大规模基准上进行了实验。激光雷达过分割实验表明，该方法不仅达到了最好的性能，而且比其他方法快100倍。此外，LiDAR语义分割实验表明，使用超点可以显著提高基线网络的性能。</p><p>本文的主要贡献如下：</p><ul><li>提出了一种高效的LiDAR过分割网络，用于学习大规模LiDAR点云的超点分割。</li><li>我们的方法在获得最先进的LiDAR过分割性能的同时，比现有的过分割方法快100倍。</li><li>我们证明了所提出的LiDAR过分割网络可以端到端的方式集成到LiDAR语义分割网络中，进一步提高了LiDAR语义分割的性能。</li></ul><h2 id="相关工作"><a href="#相关工作" class="headerlink" title="相关工作"></a>相关工作</h2><p><strong>点云过分割</strong></p><p>现有的点云过分割方法大致可以分为两类：基于优化的方法和基于深度学习的方法。</p><ul><li>基于优化的方法通常使用手工创建的描述符来提取点特征以用于超点分割。<ul><li>Papon等人。[21]是3D数据过分割领域的先驱。提出的体素云连通性分割方法利用颜色和深度信息以及三维几何关系将RGB-D数据分割成超点。</li><li>Lin等人的研究。[17]将超点分割问题描述为一个子集选择问题。基于手工生成的点云局部信息，通过直接最小化能量函数来分割超点。</li><li>Guinard等人。[8]将点云过分割问题视为一个结构化优化问题。他们使用贪婪的图割算法[14]来产生几何简单的超点，其中手工制作的局部描述符(如线性度、平面度、散射度和垂直度)被用来描述每个点的局部几何体。然而，当周围环境的点云中存在具有复杂局部几何结构的物体时，手工构造的特征通常不能提供区分特征来生成高质量的超点，特别是在稀疏的LiDAR点云中。</li></ul></li><li>基于深度学习的方法利用深度网络来提取区分点特征，以提高过分割性能。<ul><li>Guinard等人。[13]将点云过分割作为深度度量学习问题。提出了一种基于图结构的对比度损失来学习辨识点特征。通过使用学习的特征来代替贪婪的图割算法[14]中使用的手工特征，它可以从点云生成比[8]更高质量的超点。然而，由于两阶段超点分割策略，它不能被端到端训练。</li><li>最近，Huy等人提出了一个新的观点。[11]提出了一种端到端的超点网络，通过迭代学习点-超点关联图来聚类超点。尽管如此，聚集的超点在超点边界附近可能会有很多噪声，特别是在稀疏的LiDAR点云中。因此，需要对图像进行后处理来滤除噪声，增加了应用的复杂性和时间成本。</li></ul></li></ul><p>现有的点云过分割方法受到复杂处理过程的限制，不能灵活地应用于下游任务。因此，有必要设计一种高效的、具有可扩展性和灵活性的点云过分割</p><p><strong>激光雷达语义分割</strong></p><p>激光雷达语义分割[30，40，9，6，27，34，42，24，26，10]已经出现了基于不同点云表示的各种方法。</p><ul><li>为了利用传统的2D分割方案，将LiDAR点云投影到图像平面中的<strong>基于投影的方法</strong>，获得渲染图像[16]、快照[2]、切线图像[28]、距离图像[32，33]和鸟瞰图像[18]。虽然基于投影的方法是激光雷达分割的有效方法，但由于空间压缩，不可避免地会丢失空间几何信息。</li><li>为了直接利用3D几何信息，<strong>基于点的方法</strong>[22]通常使用多层感知器网络来处理3D LiDAR点，使用不同设计的局部描述符，包括最大汇集函数[23，31]、自适应加权[29，41]和非局部加权[38，4]。尽管与基于投影的方法相比，基于点的方法具有更高的性能，但由于大规模LiDAR点云中复杂的局部聚集操作和大量的点，因此<strong>基于点的方法通常是耗时的</strong>。</li><li>目前，最先进的技术是<strong>基于体素的方法</strong>，它们使用&#x3D;&#x3D;稀疏卷积[19]&#x3D;&#x3D;来处理大规模LiDAR点云。与传统的三维卷积算法相比，该算法只对非空体素进行卷积运算，大大降低了计算成本和内存消耗。<ul><li>基于SparseConv，不同的方法使用网络结构搜索[25]、多尺度特征聚集[5]、柱面分割和非对称3D卷积网络[43]、距离-点体素融合网络[35]和多模式网络[37]来提高分割性能。虽然基于体素的方法已经取得了令人印象深刻的结果，但体素的质量对其分辨率是敏感的。</li><li>体素大小越大，质量越低。这促使我们探索激光雷达点云的超点表示。</li></ul></li></ul><h2 id="方法"><a href="#方法" class="headerlink" title="方法"></a>方法</h2><h3 id="激光雷达过分割网络"><a href="#激光雷达过分割网络" class="headerlink" title="激光雷达过分割网络"></a>激光雷达过分割网络</h3><p><strong>低级点嵌入</strong></p><p>对于LiDAR过分割，我们使用点云的低级嵌入来分割超点。如图1所示，我们将其描述为一个由点云上定义的局部邻域构造的深度度量学习问题。</p><p><img src="/pic/Overseg1.png" alt="图1.我们的LiDAR过分割网络概述。给定一个LiDAR点云，在利用稀疏3D网络提取点特征后，我们首先学习低级点嵌入。在此基础上，提出了由点云生成超点的LiDAR点分组算法。最后，我们使用超点求精模块将未分配的空闲点分配给相应的超点。"></p><p>具体地说，给定LiDAR点云P&#x3D;{pi}Ni&#x3D;1，我们遵循[37]并使用稀疏3D网络来获得特征地图X∈RN×C，其中N是邻域的数量。我们使用MLP将X映射到低维嵌入F∈RN×D中，通过计算点与其周围点之间的欧几里德距离来构造每个点的局部邻域，表示为N&#x3D;{Ni}Ni&#x3D;1。每个局部邻域Ni是一组三维点，包含位于第i个局部邻域中的点。</p><p>在构造了LiDAR点云的局部邻域后，提出了一种局部判别损失LSP算法，将点特征映射到具有相似局部几何结构的低维嵌入空间。对于第i个局部邻域Ni，我们可以根据点标签，即基本真实语义标签(图1(A)中不同颜色的圆圈)来识别不同的语义部分。在有限范围内构造的局部邻域中，具有相同语义标签的点可视为具有相似的几何结构。局部区分性损失LSP由两个项组成：</p><p><img src="/pic/Overseg2.png" alt="公式1"></p><p>其中，相同项Lident将嵌入的点拉向相应语义部分的平均嵌入，而距离项Ldist将嵌入的点推离其他语义部分。具体表述如下：</p><p><img src="/pic/Overseg3.png" alt="公式2，3"></p><p>其中i(j，k)是指标函数。如果点j属于第k个语义部分，则i(j，k)等于1，否则等于0。Ti是第i个局部邻域Ni中语义部分的个数。此外，Fj∈Rd表示第i点的点嵌入，Zk∈Rd表示包含第i点的对应语义部分的平均值点嵌入。在实验中，阈值α和β分别被设置为0.01和0.2.</p><p><strong>激光雷达点分组</strong></p><p>为了提高激光雷达超点分割的推理速度，提出了一种简单而高效的激光雷达点分组算法。如图1所示，其关键思想是通过应用广度优先搜索(BFS)算法，基于学习的低级点嵌入对点进行分组。</p><p>具体地说，为了在3D空间中生成齐次紧致的超点，我们同时考虑了嵌入空间和3D坐标空间。</p><ul><li>在给定LiDAR点云的情况下，我们首先随机选择一个种子点作为BFS起点，该起点尚未分配给超点。</li><li>然后，根据起始点执行BFS算法，对周围的点进行分组，形成一个超点。程序如图1(B)所示。在BFS过程中，如果一个点可以被分配到相应的超点，那么它应该满足两个约束。<ul><li>对于一个约束，点嵌入之间的欧几里德距离应该小于阈值β，该阈值被认为是用来将点嵌入推开的余量如等式(3)。</li><li>对于另一个约束，点之间的欧几里德距离应该小于阈值γ，该阈值用于保持超点在3D坐标空间中的紧致性。</li><li>注意，在BFS期间，如果超点大小(即，超点内的点数)大于最大大小Nmax，则当前的超点增长过程将被终止。</li><li>在BFS之后，如果生成的超点大小小于最小大小Nmin，则会丢弃该超点。LiDAR点分组算法重复BFS过程，从LiDAR点云生成新的超点，直到它不能满足生成条件。具体步骤如算法1所示。</li></ul></li></ul><p> <img src="/pic/Overseg4.png" alt="激光雷达点分组算法"></p><p>在超点生成中，也可以采用基于聚类的算法，但它们会在超点上引入噪声点。为了消除噪声点，需要进行后处理，这会带来额外的时间成本。相比之下，BFS算法可以在O(N)的时间复杂度下生成没有噪声点的良好超点，其中N是LiDAR点数。请注意，**我们不是在稀疏的3D空间中执行BFS，而是将稀疏的LiDAR点云转换为密集的距离图像(dense range image)**。由于距离图像中邻接点的边缘是相邻像素网格的边缘，这可以大大节省边缘遍历的时间，因此对深度图像进行边界过滤可以有效地减少三维空间的边界过滤时间。因此，BFS的复杂度约为O(N)。</p><p><strong>超点精化</strong></p><p>在LiDAR点分组中，我们丢弃了生成的不符合超点最小尺寸的超点，因此一些LiDAR点仍然没有分配给任何超点。处理这些未指定的LiDAR点的简单方法是将点指定给坐标空间中最近的超点中心。但是，未指定的点大多是边界点。因此，很难根据距离将边界点准确地分配到相应的超点。为了保持超点分割的效率，我们提出了一种简单而有效的超点细化模块，通过学习该点与其K-近邻超点之间的亲和力来准确地将该点分配到相应的超点，如图1所示。</p><p>具体地说，给定一个未分配的LiDAR点i，我们首先根据该点与超点之间的欧几里德距离在3D坐标空间中找到K-近邻超点(Ki)。点i和超点j∈ki之间的亲和力定义为：</p><p><img src="/pic/Overseg5.png" alt="公式4"></p><p>其中，连接(·)表示连接操作。矢量fi−zj和pi−xj分别捕捉嵌入空间和坐标空间中的点和超点之间的差异。请注意，zj∈Rd和xj∈R3是超点的嵌入和坐标。它们是通过平均点坐标和嵌入而获得的。这样，我们就可以得到该点与其K-最近超点之间的亲和力矩阵Ai∈Rk×(4d+1)。之后，我们映射亲和度矩阵以获得亲和度分数，其定义为：</p><p><img src="/pic/Overseg6.png" alt="公式5"></p><p>其中W∈R1×(4d+1)是要学习的权重。最后，我们将softmax函数应用于亲和度得分YI∈RK，以获得亲和度概率。通过将点i分配给概率最高的超点，我们可以从LiDAR点云生成高质量的超点。请注意，第i点的基本事实是具有相同语义标签的最近超点。在训练期间，它由交叉熵损失监督，由LSR表示(见图1(C))。</p><h3 id="基于超点的LiDAR图像分割"><a href="#基于超点的LiDAR图像分割" class="headerlink" title="基于超点的LiDAR图像分割"></a>基于超点的LiDAR图像分割</h3><p>为了显示LiDAR过分割网络的可扩展性和灵活性，我们将学习到的超点应用到LiDAR语义分割任务中。具体地说，我们提出了一个简单的多尺度超点聚合模块，并将其与我们的LiDAR过度分割网络相集成，形成了一个端到端的LiDAR语义分割框架，如图2所示。</p><p><img src="/pic/Overseg7.png" alt="图2.我们的端到端LiDAR语义分割框架概述。"></p><p><strong>多尺度超点聚集</strong></p><p>在LiDAR过分割网络中，稀疏卷积[19]用于提取点的局部特征。然而，基于体素的表示方法不能很好地刻画点云的局部几何结构，从而产生粗特征图X∈RN×C。基于粗略的点特征，我们设计了一个简单的多尺度超点聚集模块，通过融合多尺度局部特征来增强点特征。具体地说，在超点生成过程中，</p><ul><li>我们首先通过调整超点的最小和最大尺寸来获得多尺度超点。对于第i个点，我们可以得到相应的L尺度的超点，用Si1，Si2，.SiL表示。</li><li>然后，我们对相应超点内的点特征采用最大合并函数来聚集超点特征(图2中的三个圆锥)。</li><li>在此之后，超点特征由一系列层组成的MLP独立处理，包括线性、RELU[20]和批归一化[12]。由此产生的超点特征由Ei 1，Ei 2，EiL。最后，融合L尺度的超点特征，聚集点云的多尺度几何信息。生成的第i个点的新特征写为：</li></ul><p><img src="/pic/Overseg8.png" alt="公式6"></p><p>与基于体素表示的粗点特征xi∈RC相比，新的点特征Xˆi∈RC∗(L+1)能够通过多尺度超点精细地刻画点的局部几何结构。在融合后的点特征的基础上，采用分类头来预测点标签。</p><p>本质上，<strong>超点提供了一个自适应邻域</strong>，该邻域是沿点云曲面的几何结构生成的。因此，与基于体素的邻域、基于球查询的邻域和基于k-NN的邻域相比，自适应邻域能够提取更具区分性的局部特征，从而提高了性能。</p><p>损失函数由于所提出的LiDAR语义分割框架是一个端到端的网络，因此可以通过联合损失函数直接进行优化。为了训练它，将联合损失函数定义为：</p><p><img src="/pic/Overseg9.png" alt="公式7"></p><p>其中，LSP是公式1中定义的训练超点的局部区分性损失。LSR和LSEM分别是超点求精和语义分割的交叉熵损失。</p><h2 id="实验"><a href="#实验" class="headerlink" title="实验"></a>实验</h2><h3 id="数据集和指标"><a href="#数据集和指标" class="headerlink" title="数据集和指标"></a>数据集和指标</h3><p>在本文中，对于LiDAR过分割和语义分割，我们使用了两个大规模的室外数据集，即SemancKITTI[1]和nuScenes[3]。数据集和评估指标的详细信息如下：</p><ul><li>SemancKITTI是一款拥有20个语义类别的大型自动驾驶汽车户外标杆。它包含由64波束LiDAR传感器收集的22个序列，其中序列00到10用于训练集(序列08用作验证集)，序列11到22用于在线隐藏测试集。序列00至10具有用于每次扫描的密集语义注释。</li><li>NuScenes包含由32光束LiDAR传感器收集的1000个场景。总共有1000个场景被分为训练(750)、验证(150)和测试(150)集。训练集和验证集被标注了17个语义类别，而测试集的标签被保存以用于在线盲测。</li><li>评估指标为了评估超点的质量，我们<strong>使用Oracle整体准确度(OOA)、边界查全率(BR)和边界精度(BP)作为[13]中定义的评估指标</strong>。OOA分析使用超点来衡量语义分割的理论上界，而BR和BP则衡量超点边界的质量。<ul><li>为了平衡BR和BP，我们报告F1分数，其公式为F1&#x3D;2BP·BR&#x2F;(BR+BP)。</li><li>为了评估LiDAR语义分割的性能，我们使用了均值交集对并集(MIoU)。</li></ul></li></ul><h3 id="网络结构"><a href="#网络结构" class="headerlink" title="网络结构"></a>网络结构</h3><p>我们使用与[2DPASS，37]中相同的稀疏3D网络作为我们的LiDAR过分割网络的主干。</p><ul><li>在激光雷达点分组算法中，将边界条件下距离约束的阈值γ设置为1.5。</li><li>在超点求精模块中，超参数K被设置为5。</li><li>在多尺度超点聚合模块中，我们使用了三种尺度的超点，它们由不同的超点大小阈值控制。</li></ul><h3 id="激光雷达过分割"><a href="#激光雷达过分割" class="headerlink" title="激光雷达过分割"></a>激光雷达过分割</h3><p>对于LiDAR过分割，我们将所提出的SuperLiDAR与SPG[15]、SSP[13]和SPNet[11]进行了比较。请注意，SPG使用手工制作的特征，而SSP和SPNet使用深度特征进行超点分割。我们运行官方代码从激光雷达点云生成超点。超点评估结果是分别在语义KITTI和nuScenes数据集的验证集上计算的。</p><p><strong>SemantiKITTI的结果。</strong></p><p><img src="/pic/Overseg10.png" alt="表1.语义KITTI和nuScenes验证集上生成的超点的比较结果。"></p><p>表1给出了基于语义KITTI数据集的超点分割的定量结果。为了进行公平的比较，不同方法生成的超点的数量保持相似(约1000个超点)。可以看出，我们的方法在四个指标上取得了最好的结果。</p><ul><li>特别是，我们的SuperLiDAR在OOA方面比其他方法高出约4%，这代表了语义分割性能的理论上限。因此，这意味着我们的方法能够以更高的精度生成高质量的超点。</li><li>此外，我们的方法具有较高的BR和BP，这表明我们的方法可以在稀疏的LiDAR点云中生成边界清晰的超点。<ul><li>由于稀疏的LiDAR点云，SPG[15]的手工制作特征的区分性较差，导致性能较差。</li><li>虽然SSP[13]使用了深度特征，但它通过基于优化的方法[14]生成超点，这限制了稀疏LiDAR点云的性能。</li><li>在稀疏点云中，基于聚类的方法SPNet[11]生成的超点很可能包含噪声，从而影响性能。</li></ul></li><li>在图3中，我们展示了不同方法在不同超点数目下的性能曲线。可以观察到，我们的方法的曲线在四个度量方面都高于其他方法。超点越多，超点越小，反之亦然。结果表明，该方法可以生成不同大小的高质量超点。</li></ul><p><img src="/pic/Overseg11.png" alt="图3.语义KITTI验证集上不同方法的性能"></p><p><strong>nuScenes的结果</strong></p><p>nuScenes数据集上超点分割的定量结果显示在表1中.请注意，不同方法生成的超点数量保持相同(<strong>约400个超点</strong>)。与SemancKITT的64波束点云相比，nuScenes的32波束点云更加稀疏。从表中可以看出，我们的方法在所有四个指标上仍然取得了最好的结果。在nuScenes数据集上的实验结果进一步表明，该方法可以有效地生成稀疏LiDAR点云中的高质量超点。</p><p><strong>可视化结果</strong></p><p>在图4中，我们展示了不同过分割方法的可视化结果。可以观察到，我们的SuperLiDAR生成的超点可以有效地区分道路和人行道。注意，<strong>道路和人行道几乎在同一水平面上，因此如果没有颜色信息</strong>，很难区分它们。尽管如此，我们的方法仍然能够学习区分点特征，利用局部区分性损失来分离它们。</p><p><img src="/pic/Overseg12.png" alt="图4.在语义KITTI(第一行)和nuScenes(第二行)的验证集上生成的超点的可视化"></p><p><strong>时间代价</strong></p><p>推理时间是超点分割的一个重要标准。</p><ul><li>对于基于优化的方法SPG[15]，我们使用单个Corei5 CPU来计算推理时间。</li><li>对于基于学习的方法SSP[13]、SPNet[11]和我们的SuperLiDAR，我们使用单个NVIDIA RTX 3090来运行基于PyTorch深度学习平台的代码。</li></ul><p>表2报告了不同方法在语义KITTI验证数据集的每次扫描上计算的平均推理时间。可以看出，我们的方法的推理时间仅为72ms，比其他方法快100倍。虽然SSP和SPNET使用网络来提取点特征，但它们也将手工制作的点特征馈送到网络中，导致数据处理时间较长。请注意，SSP和SPNET的数据处理相同，因此它们的数据处理时间相同。由于SPG和SSP采用相同的基于优化的方法来生成超点，所以它们的超点生成时间较长。由于sPNET的超点生成采用迭代策略，因此比我们的方法耗时更多。</p><p><img src="/pic/Overseg13.png" alt="表2.不同方法在语义KITTI验证集上的推理时间请注意，推理时间包括数据处理和超点生成。"></p><p><strong>消融研究</strong></p><p>在低级点嵌入学习中，我们在<strong>距离图像(range image)<strong>中而不是在3D空间中构造局部邻域，并在语义KITTI数据集上进行实验。表3展示了在三维空间中构建邻域的结果。可以观察到，建筑街区在3D空间中的表现(“Build nei.在3D空间中“)比在距离图像(”Default(SuperLiDAR)“)中差。与3D空间相比，距离像中的点更加均匀。由此得到的稠密邻域对应用</strong>局部区分损失</strong>学习好点嵌入是有益的。</p><p><img src="/pic/Overseg14.png" alt="表3.不同设置下的消融研究结果。"></p><p>为了解决LiDAR点分组后未分配点的问题，提出了一种超点细化模块。为了验证其有效性，我们通过将点分配到坐标空间中最近的超点中心来替换超点求精模块。表3结果(“无超点参考”)低于使用超点细化模块(“Default(SuperLiDAR)”)。由于简单的距离准则，硬边界点不能有效地分配给相应的超点。通过自适应地学习点和超点之间的亲和力，我们可以更准确地将点分配到相应的超点。</p><p>为了解决LiDAR点分组后未分配点的问题，提出了一种超点细化模块。为了验证其有效性，我们通过将点分配到坐标空间中最近的超点中心来替换超点求精模块。在选项卡中。3、结果(“无超点参考”)低于使用超点细化模块(“Default(SuperLiDAR)”)。由于简单的距离准则，硬边界点不能有效地分配给相应的超点。通过自适应地学习点和超点之间的亲和力，我们可以更准确地将点分配到相应的超点。</p><h3 id="LiDAR语义分割"><a href="#LiDAR语义分割" class="headerlink" title="LiDAR语义分割"></a>LiDAR语义分割</h3><p>对于LiDAR语义分割，我们在语义KITTI和nuScenes数据集上进行了实验，以验证学习超点的有效性。我们的LiDAR语义分割框架基于LiDAR过度分割网络，并使用多尺度超点聚合模块进行语义预测。我们使用LiDAR过分割网络的主干(即稀疏3D网络)作为基线。由于我们的基线是在[37]中使用的稀疏3D网络，因此我们直接将[37]的基线结果作为我们的方法在语义KITTI和nuScenes数据集上的基线结果。</p><p><img src="/pic/Overseg15.png" alt="表4.语义分割在语义KITTI和nuScenes测试集上的结果。结果在2022年11月11日之前进行了比较。“L”和“C”分别表示激光雷达和相机。请注意，我们只列出已发表作品的结果。"></p><p><strong>SemantiKITTI的结果。</strong></p><p>表4给出了语义分割在语义KITTI在线测试集上的实验结果。可以观察到，我们的SuperLiDAR的MIU值比基线高出2%。此外，图5显示了基线和我们的方法的可视化结果(第一行)。实验结果表明，利用学习的超点来增强点云的局部几何信息，可以进一步提高语义分割的性能。由于采用了复杂的点-体素或距离-点-体素融合框架，RPVNet[35]和AF2-S3Net[5]的性能要高于我们的基于体素的方法。此外，2DPASS[37]是一种多模式方法，它使用2D图像和知识提取来实现更高的性能和更短的推理时间。由于采用了简单有效的多尺度超点聚合模型，在不增加太多时间开销的情况下，有效地提高了基线的性能。</p><p><img src="/pic/Overseg16.png" alt="图5.语义分割在语义KITTI(第一行)和nuScenes(第二行)的验证集上的可视化"></p><p><strong>NuScenes的结果。</strong></p><p>表4给出了在nuScenes在线测试集上的语义分割结果。可以观察到，我们的SuperLiDAR的MIU值超过了基线约1%。此外，图5显示了基线和我们的方法的可视化结果(第二行)。由于nuScenes的点云比SemancKITTI的点云更稀疏，定量和可视化结果表明，学习的超点可以有效地提高分割性能。与仅使用LiDAR数据的方法相比，该方法可以在更短的推理时间内获得更高的性能。请注意，即使我们只使用LiDAR数据，我们的方法的性能也要好于使用LiDAR点云和相机图像的PMF[44]。此外，多模式方法2D3DNet[7]和2DPASS[37]采用复杂的多尺度融合网络以获得更高的性能。</p><h2 id="结论"><a href="#结论" class="headerlink" title="结论"></a>结论</h2><p>提出了一种高效的端到端LiDAR过分割网络，用于从LiDAR点云中分割超点。通过使用所提出的LiDAR点分组算法，我们的方法可以从稀疏的LiDAR点云中生成高质量的超点。值得注意的是，我们的方法在激光雷达过分割方面取得了新的进展，并且推理时间比现有方法快100倍。此外，在LiDAR语义分割上，通过使用超点，我们的方法可以显著提高两个大规模基准测试(即SemancKITTI和nuScenes)的基线结果。我们相信，提出的高效LiDAR过分割网络可以应用于更多的下游任务，如3D检测和跟踪。</p>]]></content>
      
      
      <categories>
          
          <category> DL </category>
          
      </categories>
      
      
        <tags>
            
            <tag> seg </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>BEVDet系列（持续更新）</title>
      <link href="/2023/11/30/bevdet-list/"/>
      <url>/2023/11/30/bevdet-list/</url>
      
        <content type="html"><![CDATA[<h1 id="BEVDet系列"><a href="#BEVDet系列" class="headerlink" title="BEVDet系列"></a>BEVDet系列</h1><p><a href="https://zhuanlan.zhihu.com/p/557613388">BEVDet系列源码解读</a><br><a href="https://blog.csdn.net/guangqianzhang/article/details/129615932">BEVDet网络结构</a><br><a href="https://zhuanlan.zhihu.com/p/492106899">BEVDet4D 强大而不失优雅的三维目标检测范式</a><br><a href="https://zhuanlan.zhihu.com/p/638452999">BEVDet4D讲解</a></p><h2 id="BEVDet4D-Exploit-Temporal-Cues-in-Multi-camera-3D-Object-Detection"><a href="#BEVDet4D-Exploit-Temporal-Cues-in-Multi-camera-3D-Object-Detection" class="headerlink" title="BEVDet4D: Exploit Temporal Cues in Multi-camera 3D Object Detection"></a>BEVDet4D: Exploit Temporal Cues in Multi-camera 3D Object Detection</h2><p>BEVDet4D首次尝试访问时间域中的丰富信息。它只是通过保留先前帧中的中间BEV特征来扩展朴素的BEVDet。然后通过空<strong>间对齐操作和拼接操作</strong>将保留的特征与当前帧中对应的特征进行融合。除此之外，论文保持了框架的大部分其他细节不变。这样，论文只在推理过程中投入了可以忽略的额外计算预算，<strong>同时使范例能够通过查询和比较两个候选特征来访问时间线索</strong>。虽然构建BEVDet4D的框架很简单，但构建其健壮的性能并不是一件容易的事情。BEVDet4D的空间对齐操作和学习目标应精心设计，以配合优雅的框架，从而<strong>简化速度预测任务</strong>，并获得优异的<strong>泛化性能</strong>。</p><p><strong>BEVFormer的速度精度是通过融合多个相邻帧(即总共4帧)的特征来实现的</strong>，这类似于大多数基于LiDAR的方法[2，46]，其中包含来自多次扫描的点。这与拟议的BEVDet4D有根本的不同，<strong>BEVDet4D只使用两个相邻的帧</strong>，并在更优雅的图案中实现了更高的速度精度。</p><p><img src="/pic/BEVDet1.png" alt="网络架构图"></p><p>如上图所示，BEVDet4D的总体框架建立在BEVDet基线之上，该基线由四种模块组成：<strong>图像-视图编码器、视图转换器、BEV编码器和特定于任务的头部</strong>（an image-view encoder, a view transformer, a BEV encoder, and a task-specific head）。</p><h3 id="网络架构图"><a href="#网络架构图" class="headerlink" title="网络架构图"></a>网络架构图</h3><p>为了利用时间线索，BEVDet4D通过保<strong>留由前一帧中的视图转换器生成的&#x3D;&#x3D;BEV特征&#x3D;&#x3D;来扩展基线</strong>。然后将保留的特征与当前帧中的特征合并。在此之前，将进行对齐操作以简化学习目标。论文应用一个简单的串联操作来合并这些特征，以验证BEVDet4D范例。更复杂的融合策略在本文中没有被开发出来。</p><p>此外，<strong>视图转换器生成的特征是稀疏的</strong>，对于后续模块来说过于粗糙，无法利用时间线索。因此，在时间融合之前，采用&#x3D;&#x3D;额外的BEV编码器&#x3D;&#x3D;来调整候选特征。实际上，额外的BEV编码器由<strong>两个朴素的残差单元</strong>[13]组成，其通道号设置为与输入特征相同。</p><h3 id="简化速度学习任务"><a href="#简化速度学习任务" class="headerlink" title="简化速度学习任务"></a>简化速度学习任务</h3><p><img src="/pic/BEVDet2.png" alt="说明对齐操作的效果。在没有对齐操作(即第一行)的情况下，需要以下模块来研究与自我运动相关的对象运动的更复杂的分布。通过在第二行中应用对齐操作，可以简化学习目标"></p><p>由于有多帧信息，不再直接预测目标速度，而是<strong>预测目标在两个连续帧中的位移</strong>。即将速度预测简化为移除了时间因素的，通过两个BEV特征间的差异来衡量的位置偏移预测。</p><p><img src="/pic/BEVDet3.png" alt="自车空间的BEV特征图"></p><p>此外，BEVDet4D使得位移与ego自身的运动无关进一步简化学习任务(因为ego车辆自身的运动会使目标运动复杂化)。因为ego的运动会使得在global坐标系中static的物体在ego坐标系中变为动态物体。由于BEV特征的感受野是围绕ego对称的，同样，ego运动会使连续两帧对应的BEV特征的global坐标系下的感受野变化。</p><p><img src="/pic/BEVDet4.png" alt="没有对齐下静止物体ego坐标下的位移差"></p><p>上图可以看到，如果直接concat两帧特征，则最后得到的两个特征的位移和ego运动相关(红色方框)，因此需要先将上一帧乘以(红色方框)的逆来消除自车(ego)运动的影响。如下图所示</p><p><img src="/pic/BEVDet5.png" alt="对齐下静止物体ego坐标下的位移差"></p><p>学习目标设为在ego坐标系下当前帧物体的运动，与ego运动无关。</p><h3 id="鲁棒性实验"><a href="#鲁棒性实验" class="headerlink" title="鲁棒性实验"></a>鲁棒性实验</h3><p><img src="/pic/BEVDet6.png" alt="表：nuScenes Val集合的消融研究结果。对齐操作包括旋转(R)和平移(T)。Extra表示额外的Bev编码器。Aug表示在选择相邻帧时在时间维度上的增大。"></p><p>如何建立BEVDet4D的鲁棒性能。即在view-transformer中通过调整伪点云来实现空间对齐操作。结果如上表所示，主要测试了以下几种空间对齐的方案。</p><ul><li>方案A，即直接将当前帧的特征与先前帧拼接。模型性能大幅降低，尤其是在速度和位移预测方面。推测可能是由于ego运动导致静态和动态目标更复杂导致，因此需要移除ego运动。</li><li>方案B，只进行平移对齐操作。模型可以利用位置对齐的候选特征来更好的感知静态目标，同时移除了ego运动简化了速度预测。此时，位移误差已经低于baseline，但速度误差仍高于baseline，这可能是由于相邻帧时间间隔不一致导致位移分布与速度分布相差较远。</li><li>方案C，进一步移除时间因素，让模型直接预测两帧间目标的位移。简化了学习目标，使训练更鲁棒。大幅降低速度预测误差。</li><li>方案D，在拼接两个候选特征前引入一个额外的BEV编码器。轻微地增加了大约2.8%的计算成本，推理速度基本保持不变，但是使得模型在C上有了更全面的提升。</li><li>方案E，在D的基础上通过调节速度预测损失的权重，进一步降低了速度误差。</li><li>方案F，在对齐操作时考虑ego位姿的旋转方差。进一步降低速度误差，表明更精确的对齐操作能够提高速度预测精度。</li><li>方案G，探索当前帧与参考帧之间最优的时间间隔。采用12Hz的数据，两帧时间间隔为T≈0.083s，训练时选择三个不同的时间间隔，并通过在测试比较它们来判断调整方向，这样可以避免超参搜索时的训练干扰。结果见下图，组价间隔为15T，设置为本文默认的测试时间间隔。训练时在[3T,27T]中随机采样来进行数据增强。这种方法进一步降低了速度误差。</li></ul><p><img src="/pic/BEVDet7.png" alt="当前帧和参考帧之间的时间间隔。以相同颜色绘制的点处于相同的训练配置中"></p><h3 id="代码分析"><a href="#代码分析" class="headerlink" title="代码分析"></a>代码分析</h3><p>以下是对BEVDet4D模型代码的详细介绍：</p><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token decorator annotation punctuation">@DETECTORS<span class="token punctuation">.</span>register_module</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token keyword">class</span> <span class="token class-name">BEVDet4D</span><span class="token punctuation">(</span>BEVDet<span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre><p>这是一个注册了BEVDet4D类的检测器模块。BEVDet4D继承自BEVDet，表示BEVDet4D是BEVDet的一种变体。</p><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span>             pre_process<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">,</span>             align_after_view_transfromation<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">,</span>             num_adj<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">,</span>             with_prev<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span>             <span class="token operator">**</span>kwargs<span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token builtin">super</span><span class="token punctuation">(</span>BEVDet4D<span class="token punctuation">,</span> self<span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token operator">**</span>kwargs<span class="token punctuation">)</span>    <span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>初始化方法接受多个参数，包括预处理网络的配置（pre_process），是否在视图变换后对齐BEV特征（align_after_view_transformation），相邻帧的数量（num_adj），以及其他超参数。此外，它调用了父类（BEVDet）的初始化方法。</p><pre class="line-numbers language-python" data-language="python"><code class="language-python">self<span class="token punctuation">.</span>align_after_view_transfromation <span class="token operator">=</span> align_after_view_transfromationself<span class="token punctuation">.</span>num_frame <span class="token operator">=</span> num_adj <span class="token operator">+</span> <span class="token number">1</span>self<span class="token punctuation">.</span>with_prev <span class="token operator">=</span> with_prev<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre><p>这里设置了BEV特征是否在视图变换后对齐（align_after_view_transformation）、相邻帧的数量（num_adj）以及是否使用前一帧的特征（with_prev）。</p><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token decorator annotation punctuation">@force_fp32</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token keyword">def</span> <span class="token function">shift_feature</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> <span class="token builtin">input</span><span class="token punctuation">,</span> trans<span class="token punctuation">,</span> rots<span class="token punctuation">,</span> bda<span class="token punctuation">,</span> bda_adj<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre><p><code>shift_feature</code> 方法用于将输入的特征进行位移，包括旋转、平移和BEV数据增强。</p><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">def</span> <span class="token function">prepare_bev_feat</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> img<span class="token punctuation">,</span> rot<span class="token punctuation">,</span> tran<span class="token punctuation">,</span> intrin<span class="token punctuation">,</span> post_rot<span class="token punctuation">,</span> post_tran<span class="token punctuation">,</span>                     bda<span class="token punctuation">,</span> mlp_input<span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre><p><code>prepare_bev_feat</code> 方法准备BEV特征，包括图像编码、视图变换和预处理。</p><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">def</span> <span class="token function">extract_img_feat_sequential</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> inputs<span class="token punctuation">,</span> feat_prev<span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><p><code>extract_img_feat_sequential</code> 方法提取顺序图像特征，包括对先前特征进行对齐和使用BEV编码器。</p><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">def</span> <span class="token function">prepare_inputs</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> inputs<span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><p><code>prepare_inputs</code> 方法准备输入数据，将输入图像和相关信息分割为每一帧的数据。</p><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">def</span> <span class="token function">extract_img_feat</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span>                     img<span class="token punctuation">,</span>                     img_metas<span class="token punctuation">,</span>                     pred_prev<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">,</span>                     sequential<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">,</span>                     <span class="token operator">**</span>kwargs<span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p><code>extract_img_feat</code> 方法是特征提取的主要入口，根据参数决定是提取顺序特征还是一般特征。</p><p>上述是对BEVDet4D模型代码的主要部分的详细介绍，包括初始化、特征提取等</p>]]></content>
      
      
      <categories>
          
          <category> DL </category>
          
      </categories>
      
      
        <tags>
            
            <tag> bev </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>弱监督下的BEV检测和占据预测(部分汇总)</title>
      <link href="/2023/11/29/weakly-supervised-det/"/>
      <url>/2023/11/29/weakly-supervised-det/</url>
      
        <content type="html"><![CDATA[<h1 id="弱监督下的BEV检测和占据预测-部分汇总"><a href="#弱监督下的BEV检测和占据预测-部分汇总" class="headerlink" title="弱监督下的BEV检测和占据预测(部分汇总)"></a>弱监督下的BEV检测和占据预测(部分汇总)</h1><h2 id="Weakly-Supervised-Class-agnostic-Motion-Prediction-for-Autonomous-Driving"><a href="#Weakly-Supervised-Class-agnostic-Motion-Prediction-for-Autonomous-Driving" class="headerlink" title="Weakly Supervised Class-agnostic Motion Prediction for Autonomous Driving"></a>Weakly Supervised Class-agnostic Motion Prediction for Autonomous Driving</h2><p><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Li_Weakly_Supervised_Class-Agnostic_Motion_Prediction_for_Autonomous_Driving_CVPR_2023_paper.pdf">论文链接</a></p><p><a href="%E5%BE%85%E5%85%AC%E5%B8%83">代码链接</a></p><p>了解动态环境中的运动行为对于自动驾驶至关重要，这使得激光雷达点云中与类别无关的运动预测受到越来越多的关注。室外场景通常可以分解为运动前景和静态背景，这使得我们能够将运动理解与场景解析联系起来。</p><p>基于这一观察结果，我们研究了一种新的弱监督运动预测范式，其中完全或部分(1%，0.1%)注释的前景&#x2F;背景二值掩模用于监督，而不是使用昂贵的运动注释。为此，我们提出了一种两阶段弱监督方法，</p><ul><li>其中在第一阶段用不完全二值掩码训练的分割模型将通过提前估计可能的运动前景来促进第二阶段运动预测网络的自监督学习。</li><li>此外，对于稳健的自监督运动学习，我们通过利用多帧信息并显式地抑制潜在的离群点来设计一致性感知的切角距离损失。</li></ul><p>综合实验表明，在完全或部分二值掩码作为监督的情况下，我们的弱监督模型比自监督模型有较大幅度的提高，性能与一些监督模型相当。</p><p> <img src="/pic/weakly4.png" alt="网络架构"></p><p>上图是两阶段弱监督运动预测方法概述。在阶段1中，我们使用部分注释的掩码训练前景&#x2F;背景(FG&#x2F;BG)分割网络PreSegNet。在阶段2中，我们训练了一个运动预测网络WeakMotionNet，它以一系列同步的Bev图作为输入，预测每个细胞的FG&#x2F;BG类别Xfb和未来的运动位移Xmot。在没有运动数据的情况下，我们从Stage1训练的PreSegNet生成FG&#x2F;BG点，并使用一致性感知切角损失来以自监督的方式训练WeakMotionNet的运动预测头部。</p><p> <img src="/pic/weakly5.png" alt="在nuScenes测试集上的运动预测评估结果"></p><p>在表1中，我们将我们的弱监督方法与各种基于nuScenes的SOTA运动预测方法进行了比较。PillarMotion是最好的自我监督方法，它利用现成的光流估计网络和额外的2D图像进行训练。在不使用图像或光流的任何知识的情况下，我们的模型通过1%或0.1%的注释FG&#x2F;BG面具训练，在所有评估指标上都比自我监督的PillarMotion高出约35%。比较我们的弱监督模型和全监督模型，我们观察到我们的模型在慢速和快速组上都比FlowNet3D[25]、HPLFlowNet[14]和PointRCNN[34]表现得更好。特别是，在快速组上，我们的模型比完全监督的场景流模型FlowNet3D和HPLFlowNet分别高出约70%和50%。比较表明，弱监督方法在标注工作量和性能之间取得了很好的折衷，缩小了与完全监督方法的差距。</p><h2 id="Weakly-Supervised-3D-Object-Detection-from-Point-Clouds"><a href="#Weakly-Supervised-3D-Object-Detection-from-Point-Clouds" class="headerlink" title="Weakly Supervised 3D Object Detection from Point Clouds"></a>Weakly Supervised 3D Object Detection from Point Clouds</h2><p><a href="https://arxiv.org/abs/2007.13970">论文链接</a><br><a href="https://github.com/Zengyi-Qin/Weakly-Supervised-3D-Object-Detection?utm_source=catalyzex.com">代码链接</a><br><a href="https://zhuanlan.zhihu.com/p/411282095">讲解链接</a><br><a href="https://zhuanlan.zhihu.com/p/411282095">讲解链接2</a></p><p> <img src="/pic/weakly1.png" alt="网络架构"></p><p>第一个关键组件是无监督的三维对象建议模块(UPM)，它基于归一化的点云密度选择三维锚点，生成潜在的3D框</p><p>第二个组件是一个跨模态转移学习模块，通过利用在图像数据集上预先训练的教师模型，它将信息，包括对象分类和旋转回归，从图像数据集转移到基于点云的三维物体检测器中，对建议进行分类和改进，以产生最终的预测 （能不能从Kitti到waymo？）</p><p>其中激光雷达扫描仪并不需要提供输入点云，而输入点云也可以从单目图像或一对立体图像中获得。假设每一帧的点云在训练集中都有一个成对的图像，但在只需要点云的测试时并不需要这一点</p><h2 id="Weakly-Supervised-3D-Object-Detection-from-Lidar-Point-Cloud"><a href="#Weakly-Supervised-3D-Object-Detection-from-Lidar-Point-Cloud" class="headerlink" title="Weakly Supervised 3D Object Detection from Lidar Point Cloud"></a>Weakly Supervised 3D Object Detection from Lidar Point Cloud</h2><p><a href="https://arxiv.org/abs/2007.11901">论文链接</a><br><a href="https://github.com/hlesmqh/WS3D">代码链接</a><br><a href="https://zhuanlan.zhihu.com/p/379025799">讲解链接</a></p><p>弱监督的3D点云目标检测，训练数据是少量的弱标签（BEV目标中心点）+少量的kitti-groundtruth，效果能与全监督性能相近，甚至更好。基于此，还做了一个自动标注器。</p><p> <img src="/pic/weakly2.png" alt="弱监管的数据注释策略"></p><p>原先的标注过程：标注者首先借助相机图像中的视觉内容在三维场景中查找对象，然后标注一个粗糙的立方体和方向箭头，最后，最佳标注（上图(c)）通过逐步调整正交视图中投影的二维框获得。尽管标注的结果质量很高，但是过程耗时又耗力。</p><p>作者的弱监督数据只包含带对象中心注释的BEV map，这可以很容易地获得。</p><p>具体来说，标注者首先粗略地点击相机前视图上的目标，然后放大BEV map，并显示初始点击周围的区域，以获得更准确的标注点。由于这个注释过程没有引用任何三维视图，所以它非常简单和快速；大多数注释只用通过点击两次按钮即可完成。这个标注包含的信息很弱，没有y轴中的高度和长方体的大小信息。</p><p> <img src="/pic/weakly3.png" alt="网络架构"></p><p> 整体结构如图所示。由两个阶段组成。第一阶段由圆柱形三维提案生成结果，第二阶段进行立方体预测。最后产生结果。</p><h2 id="FGR-Frustum-Aware-Geometric-Reasoning-for-Weakly-Supervised-3D-Vehicle-Detection"><a href="#FGR-Frustum-Aware-Geometric-Reasoning-for-Weakly-Supervised-3D-Vehicle-Detection" class="headerlink" title="FGR: Frustum-Aware Geometric Reasoning for Weakly Supervised 3D Vehicle Detection"></a>FGR: Frustum-Aware Geometric Reasoning for Weakly Supervised 3D Vehicle Detection</h2><p><a href="https://arxiv.org/pdf/2105.07647.pdf">论文链接</a><br><a href="https://github.com/weiyithu/FGR">代码链接</a></p><h2 id="WeakM3D-Towards-Weakly-Supervised-Monocular-3D-Object-Detection"><a href="#WeakM3D-Towards-Weakly-Supervised-Monocular-3D-Object-Detection" class="headerlink" title="WeakM3D: Towards Weakly Supervised Monocular 3D Object Detection"></a>WeakM3D: Towards Weakly Supervised Monocular 3D Object Detection</h2><p><a href="https://arxiv.org/abs/2203.08332">论文链接</a><br><a href="https://github.com/SPengLiang/WeakM3D?utm_source=catalyzex.com">代码链接</a></p><p>单目 3D 物体检测是 3D 场景理解中最具挑战性的任务之一。由于单目图像的不适定性质，现有的单目 3D 检测方法高度依赖于 LiDAR 点云上手动注释的 3D 框标签的训练。这个注释过程非常费力且昂贵。为了摆脱对 3D 框标签的依赖，在本文中，我们探索了弱监督的单目 3D 检测。具体来说，</p><ul><li>我们首先检测图像上的 2D 框。</li><li>然后，我们采用生成的2D框来选择相应的RoI LiDAR点作为弱监督。</li><li>最终，我们采用网络来预测 3D 框，它可以与相关的 RoI LiDAR 点紧密对齐。</li></ul><p>该网络是通过最小化我们新提出的 3D 框估计和相应的 RoI LiDAR 点之间的 3D 对齐损失来学习的。我们将说明上述学习问题的潜在挑战，并通过在我们的方法中引入几种有效的设计来解决这些挑战。</p>]]></content>
      
      
      <categories>
          
          <category> DL </category>
          
      </categories>
      
      
        <tags>
            
            <tag> bev </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>BEVFormer学习总结（结合代码）</title>
      <link href="/2023/11/29/bevformer-learning/"/>
      <url>/2023/11/29/bevformer-learning/</url>
      
        <content type="html"><![CDATA[<h1 id="BEVFormer学习总结（结合代码）"><a href="#BEVFormer学习总结（结合代码）" class="headerlink" title="BEVFormer学习总结（结合代码）"></a>BEVFormer学习总结（结合代码）</h1><p><a href="https://arxiv.org/pdf/2203.17270.pdf">论文链接</a><br><a href="https://drive.google.com/file/d/1dKnD6gUHhBXZ8gT733cIU_A7dHEEzNTP/view">中文论文链接</a><br><a href="https://github.com/fundamentalvision/BEVFormer">代码链接</a><br><a href="https://zhuanlan.zhihu.com/p/543335939">万字长文理解BEVFormer</a><br><a href="https://www.bilibili.com/video/BV1rK411o7PS/?spm_id_from=333.788&vd_source=2055b62125c0277a0d6878f41c89fec2">手撕BEVFormer视频</a><br><a href="https://blog.csdn.net/weixin_42108183/article/details/128426721?spm=1001.2014.3001.5501">BEVFormer代码流程梳理1</a><br><a href="https://blog.csdn.net/weixin_42108183/article/details/128433381?spm=1001.2014.3001.5501">BEVFormer代码流程梳理2</a><br><a href="https://ttxsai.blog.csdn.net/article/details/123450282?spm=1001.2101.3001.6650.3&utm_medium=distribute.pc_relevant.none-task-blog-2~default~CTRLIST~Rate-3-123450282-blog-122300131.235%5Ev38%5Epc_relevant_anti_vip_base&depth_1-utm_source=distribute.pc_relevant.none-task-blog-2~default~CTRLIST~Rate-3-123450282-blog-122300131.235%5Ev38%5Epc_relevant_anti_vip_base&utm_relevant_index=6">nuscenes数据集介绍</a><br><a href="https://zhuanlan.zhihu.com/p/48508221">Transformer学习笔记</a><br><a href="https://zhuanlan.zhihu.com/p/48508221">Vistion Transformer学习笔记</a><br><a href="https://zhuanlan.zhihu.com/p/657666107">Vistion Transformer学习笔记</a><br><a href="https://blog.csdn.net/weixin_42108183/article/details/128680716?spm=1001.2014.3001.5501">DeformableDETR原理+代码解析1</a><br><a href="https://zhuanlan.zhihu.com/p/612756240">DeformableDETR原理+代码解析2</a><br><a href="https://zhuanlan.zhihu.com/p/596303361">DeformableDETR原理+代码解析3</a><br><a href="https://blog.csdn.net/qq_52053775/article/details/126468394">DeformableDETR原理解析</a></p><h2 id="介绍"><a href="#介绍" class="headerlink" title="介绍"></a>介绍</h2><p>该篇论文提出了一个采用纯视觉（camera）做感知任务的算法模型 BEVFormer。BEVFormer 通过提取环视相机采集到的图像特征，并将提取的环视特征通过模型学习的方式转换到 BEV 空间（模型去学习如何将特征从图像坐标系转换到BEV坐标系），从而实现 3D 目标检测和地图分割任务，并取得了 SOTA 的效果， 利用询问向量来查找空间&#x2F;时间域,并相应地聚合时空信息,因此有利于更强的感知任务表征。</p><h2 id="官方的模型仓库"><a href="#官方的模型仓库" class="headerlink" title="官方的模型仓库"></a>官方的模型仓库</h2><table><thead><tr><th align="center">Backbone</th><th align="center">Method</th><th align="center">Lr Schd</th><th align="center">NDS</th><th align="center">mAP</th><th align="center">memroy</th><th align="center">Config</th><th align="center">Download</th></tr></thead><tbody><tr><td align="center">R50</td><td align="center">BEVFormer-tiny_fp16</td><td align="center">24ep</td><td align="center">35.9</td><td align="center">25.7</td><td align="center">-</td><td align="center"><a href="projects/configs/bevformer_fp16/bevformer_tiny_fp16.py">config</a></td><td align="center"><a href="https://github.com/zhiqi-li/storage/releases/download/v1.0/bevformer_tiny_fp16_epoch_24.pth">model</a>&#x2F;<a href="https://github.com/zhiqi-li/storage/releases/download/v1.0/bevformer_tiny_fp16_epoch_24.log">log</a></td></tr><tr><td align="center">R50</td><td align="center">BEVFormer-tiny</td><td align="center">24ep</td><td align="center">35.4</td><td align="center">25.2</td><td align="center">6500M</td><td align="center"><a href="projects/configs/bevformer/bevformer_tiny.py">config</a></td><td align="center"><a href="https://github.com/zhiqi-li/storage/releases/download/v1.0/bevformer_tiny_epoch_24.pth">model</a>&#x2F;<a href="https://github.com/zhiqi-li/storage/releases/download/v1.0/bevformer_tiny_epoch_24.log">log</a></td></tr><tr><td align="center"><a href="https://github.com/zhiqi-li/storage/releases/download/v1.0/r101_dcn_fcos3d_pretrain.pth">R101-DCN</a></td><td align="center">BEVFormer-small</td><td align="center">24ep</td><td align="center">47.9</td><td align="center">37.0</td><td align="center">10500M</td><td align="center"><a href="projects/configs/bevformer/bevformer_small.py">config</a></td><td align="center"><a href="https://github.com/zhiqi-li/storage/releases/download/v1.0/bevformer_small_epoch_24.pth">model</a>&#x2F;<a href="https://github.com/zhiqi-li/storage/releases/download/v1.0/bevformer_small_epoch_24.log">log</a></td></tr><tr><td align="center"><a href="https://github.com/zhiqi-li/storage/releases/download/v1.0/r101_dcn_fcos3d_pretrain.pth">R101-DCN</a></td><td align="center">BEVFormer-base</td><td align="center">24ep</td><td align="center">51.7</td><td align="center">41.6</td><td align="center">28500M</td><td align="center"><a href="projects/configs/bevformer/bevformer_base.py">config</a></td><td align="center"><a href="https://github.com/zhiqi-li/storage/releases/download/v1.0/bevformer_r101_dcn_24ep.pth">model</a>&#x2F;<a href="https://github.com/zhiqi-li/storage/releases/download/v1.0/bevformer_r101_dcn_24ep.log">log</a></td></tr><tr><td align="center"><a href="https://pan.baidu.com/s/1Jh5Aq2YwcD6tdj7Sl5BB3g?pwd=5rij">R50</a></td><td align="center">BEVformerV2-t1-base</td><td align="center">24ep</td><td align="center">42.6</td><td align="center">35.1</td><td align="center">23952M</td><td align="center"><a href="projects/configs/bevformerv2/bevformerv2-r50-t1-base-24ep.py">config</a></td><td align="center"><a href="https://pan.baidu.com/s/1ynzlAt1DQbH8NkqmisatTw?pwd=fdcv">model&#x2F;log</a></td></tr><tr><td align="center"><a href="https://pan.baidu.com/s/1Jh5Aq2YwcD6tdj7Sl5BB3g?pwd=5rij">R50</a></td><td align="center">BEVformerV2-t1-base</td><td align="center">48ep</td><td align="center">43.9</td><td align="center">35.9</td><td align="center">23952M</td><td align="center"><a href="projects/configs/bevformerv2/bevformerv2-r50-t1-base-48ep.py">config</a></td><td align="center"><a href="https://pan.baidu.com/s/1ynzlAt1DQbH8NkqmisatTw?pwd=fdcv">model&#x2F;log</a></td></tr><tr><td align="center"><a href="https://pan.baidu.com/s/1Jh5Aq2YwcD6tdj7Sl5BB3g?pwd=5rij">R50</a></td><td align="center">BEVformerV2-t1</td><td align="center">24ep</td><td align="center">45.3</td><td align="center">38.1</td><td align="center">37579M</td><td align="center"><a href="projects/configs/bevformerv2/bevformerv2-r50-t1-24ep.py">config</a></td><td align="center"><a href="https://pan.baidu.com/s/1ynzlAt1DQbH8NkqmisatTw?pwd=fdcv">model&#x2F;log</a></td></tr><tr><td align="center"><a href="https://pan.baidu.com/s/1Jh5Aq2YwcD6tdj7Sl5BB3g?pwd=5rij">R50</a></td><td align="center">BEVformerV2-t1</td><td align="center">48ep</td><td align="center">46.5</td><td align="center">39.5</td><td align="center">37579M</td><td align="center"><a href="projects/configs/bevformerv2/bevformerv2-r50-t1-48ep.py">config</a></td><td align="center"><a href="https://pan.baidu.com/s/1ynzlAt1DQbH8NkqmisatTw?pwd=fdcv">model&#x2F;log</a></td></tr><tr><td align="center"><a href="https://pan.baidu.com/s/1Jh5Aq2YwcD6tdj7Sl5BB3g?pwd=5rij">R50</a></td><td align="center">BEVformerV2-t2</td><td align="center">24ep</td><td align="center">51.8</td><td align="center">42.0</td><td align="center">38954M</td><td align="center"><a href="projects/configs/bevformerv2/bevformerv2-r50-t2-24ep.py">config</a></td><td align="center"><a href="https://pan.baidu.com/s/1ynzlAt1DQbH8NkqmisatTw?pwd=fdcv">model&#x2F;log</a></td></tr><tr><td align="center"><a href="https://pan.baidu.com/s/1Jh5Aq2YwcD6tdj7Sl5BB3g?pwd=5rij">R50</a></td><td align="center">BEVformerV2-t2</td><td align="center">48ep</td><td align="center">52.6</td><td align="center">43.1</td><td align="center">38954M</td><td align="center"><a href="projects/configs/bevformerv2/bevformerv2-r50-t2-48ep.py">config</a></td><td align="center"><a href="https://pan.baidu.com/s/1ynzlAt1DQbH8NkqmisatTw?pwd=fdcv">model&#x2F;log</a></td></tr><tr><td align="center"><a href="https://pan.baidu.com/s/1Jh5Aq2YwcD6tdj7Sl5BB3g?pwd=5rij">R50</a></td><td align="center">BEVformerV2-t8</td><td align="center">24ep</td><td align="center">55.3</td><td align="center">46.0</td><td align="center">40392M</td><td align="center"><a href="projects/configs/bevformerv2/bevformerv2-r50-t8-24ep.py">config</a></td><td align="center"><a href="https://pan.baidu.com/s/1ynzlAt1DQbH8NkqmisatTw?pwd=fdcv">model&#x2F;log</a></td></tr></tbody></table><p><strong>可以看到这里也有BEVFormerV2,后续会进行讲解，下面以BEVFormer-base为例进行讲解</strong></p><h2 id="Pipeline"><a href="#Pipeline" class="headerlink" title="Pipeline"></a>Pipeline</h2><ul><li>Backbone + Neck （ResNet-101-DCN + FPN）提取环视图像的多尺度特征；</li><li>论文提出的 Encoder 模块（包括 Temporal Self-Attention 模块和 Spatial Cross-Attention 模块）完成环视图像特征向 BEV 特征的建模；</li><li>Decoder 模块使用类似 Deformable DETR 的 完成 3D 目标检测的分类和定位任务；</li><li>正负样本的定义（采用 Transformer 中常用的匈牙利匹配算法，Focal Loss + L1 Loss 的总损失和最小）；</li><li>损失的计算（Focal Loss 分类损失 + L1 Loss 回归损失）；</li><li>反向传播，更新网络模型参数；</li></ul><p><img src="/pic/BEVFormer1.png" alt="制作的结构图"></p><p><strong>接下来将从输入数据格式，网络特征提取，BEV特征产生，BEV 特征解码完成 3D 框预测、正负样本定义、损失计算这六个方面完成 BEVFormer 的解析</strong></p><h2 id="输入的数据格式"><a href="#输入的数据格式" class="headerlink" title="输入的数据格式"></a>输入的数据格式</h2><p>对于 BEVFormer 网络模型而言，输入的数据是一个 6 维的张量：（bs，queue，cam，C，H，W）；</p><ul><li>bs：batch size 大小；</li><li>queue：连续帧的个数；由于 BEVFormer 采用了时序信息的思想（我认为加入时序信息后，可以一定程度上缓解遮挡问题），所以输入到网络模型中的数据要包含除当前帧之外，之前几帧的数据；</li><li>cam：每帧中包含的图像数量，对于nuScenes数据集而言，由于一辆车带有六个环视相机传感器，可以实现 360° 全场景的覆盖，所以一帧会包含六个环视相机拍摄到的六张环视图片；</li><li>C，H，W：图片的通道数，图片的高度，图片的宽度；</li></ul><h3 id="Nuscenes数据集简介"><a href="#Nuscenes数据集简介" class="headerlink" title="Nuscenes数据集简介"></a>Nuscenes数据集简介</h3><p>NuScenes数据集是一个包含两个城市1000个驾驶场景的大规模自动驾驶数据集。850个场景用于培训&#x2F;验证，150个场景用于测试。每一场戏都有20多秒长。它有40K个关键帧和整个传感器套件，包括6个摄像头（CAM）、1个激光雷达（LIDAR）、5个雷达（RADAR）、IMU和GPS。摄像机图像分辨率为1600×900。同时，发布了相应的HD-Map和CanBus数据，以探索多个输入的辅助。由于NuScenes提供了多样化的多传感器设置，因此它在学术文献中越来越受欢迎；数据规模没有Waymo的大，这使得在这个基准上快速验证想法变得高效。</p><p><strong>传感器在采集车上的布置如下图所示</strong></p><p> <img src="/pic/BEVFormer2.png" alt="采集车的结构图"></p><p>可以看出，相机（CAM）有六个，分别分布在前方（Front）、右前方（Front Right）、左前方（Front Left）、后方（Back）、右后方（Back Right）、左后方（Back Left）；激光雷达（LIDAR）有1个，放置在车顶（TOP）；毫米波雷达有五个，分别放置在前方（Front）、右前方（Front Right）、左前方（Front Left）、右后方（Back Right）、左后方（Back Left）。</p><h3 id="transformer的一些知识"><a href="#transformer的一些知识" class="headerlink" title="transformer的一些知识"></a>transformer的一些知识</h3><h4 id="自注意力机制（self-attention）"><a href="#自注意力机制（self-attention）" class="headerlink" title="自注意力机制（self-attention）"></a>自注意力机制（self-attention）</h4><p>自注意力机制是一种用于处理序列数据的机制，它能够在序列中的每个位置上计算该位置与其他位置之间的关联程度，并根据这些关联程度来加权组合序列中的信息。</p><p>概念：</p><ul><li>查询(Query)：查询是你想要了解的信息或者你想要从文本中提取的特征。它类似于你对文中的某个词语提出的问题或者你想要了解的内容。</li><li>键(Key)：键是文本中每个词语的表示。它类似于每个词语的标识符或者关键信息，用于帮助计算查询与其他词语之间的关联程度。</li><li>值(Value)：值是与每个词语相关的具体信息或特征。它类似于每个词语的具体含义或者特征向量。</li></ul><p>在自注意力机制中，具体步骤是：</p><ul><li>Stp1：从输入值a乘以矩阵Wq、Wk和Wv(这三个矩阵是模型参数，需要通过训练数据来学习)获取查询(Q)、键(K)、值(V),一般可以在输入a加上位置向量后再计算对应的Q、K、V</li><li>Step2:通过计算查询(Q)与键(K)之间的点积，来衡量查询与其他词语之间的关联程度，然后，通过对这些关联程度进行归一化处理（一般采用softma归一化），得到每个词语的注意力权重。</li><li>Step3:然后，根据这些注意力权重，对每个词语的值(V)进行加权求和，得到一个新的表示，该表示会更加关注与查询相关的信息。</li><li>Step4:最后，把Self Attention层的输出给全连接神经网络学习更多的信息</li></ul><p>以下是Transformer完整的架构，总体来看，它由**编码器(Encoder)<strong>和</strong>解码器(Decoder)**两部分组成。</p><p> <img src="/pic/transformer1.png" alt="完整架构图"></p><ul><li><strong>编码器(Encoder)</strong><ul><li>左边是编码器部分，主要作用是将输入数据编码成计算机能理解的高维抽象表示。</li><li>它的结构也就是前面一节我们所说的多头注意力机制+全连接神经网络的结构。此外，这里用了残差连接Q(Residual connection)的方式，将输入和多头注意力层或全连接神经网络的输出相加，再传递给下一层，避免梯度递减的问题。</li></ul></li><li><strong>解码器(Decoder)</strong><ul><li>右边是解码器的部分，主要作用是利用高维表示信息生成目标序列。它的结构大致与编码器相同，不同的有两点：<ul><li>第一，采用了<strong>掩码多头自注意力(Masked-Multi-.head self attention)</strong>,即在计算注意力得分时，模型只能关注生成内容的当前位置之前的信息，避免未来信息的泄漏。比如，这里计算输出b2时，就只使用了a1、a2两个位置的注意力得分进行加权。</li><li>第二，中间部分，利用了Encoder的输出结果计算交叉注意力(Cross Attention)。同之前的注意力机制类似，Cross Attention通过计算<strong>解码器当前位置(Q)的表示与编码器上下文表示(K)之间</strong>的注意力权重，将编码器上下文表示(V)加权，然后将该加权表示与解码器当前位置的表示进行融合。</li></ul></li></ul></li></ul><h4 id="Deformable-DETR"><a href="#Deformable-DETR" class="headerlink" title="Deformable DETR"></a>Deformable DETR</h4><p>在transformer中，特征点的特征向量Value可以由一个网络学习到，但是这个Value并不能表示全局的建模关系，于是就由另外两个网络为分别为每个特征点学习一个query和key，然后利用当前特征点的query与所有特征点的key做点乘，然后进行softmax，这样可以计算出每个特征点与其他特征点的权重关系，然后利用这个权重关系，将所有特征点的Value进行加权求和，得到每个特征点最终的Value。实时上，每个特征点并非需要与其他每个特征点做self-attention，比如图片上的左上角的特征点与右下角的特征点的关系是十分微弱的，甚至毫无关系。</p><ul><li>在Deformable DETR 中，每个特征点只与周围的几个特征点(默认为4)进行self-attention，也就是每个特征点的Value是由其周围4个特征点的的Value加权求和得到的。</li><li>相对于DETR，在Deformable DETR中，引入了多尺度的特征(能够同时兼顾大目标与小目标的识别)，因此每个特征点都能够在每个特征层上找到一个自己的采样点，然后在每个采样点周围采样4个偏移点作为self-attention的对象，即利用 4 * 4 &#x3D; 16 个偏移点特征向量Value来计算当前特征点的Value。</li><li>这里有个问题，在transformer中，当前特征点的Value加权求和时是将自己的Value包括在内的，而在deformable detr中，是将自己value除外的。</li></ul><p>已经基本明确在 Deformable DETR中， 特征点要与哪些偏移点怎么做self-attention了，那么后续可以分为两个部分:</p><ul><li>1、如何找到这些采偏移点，</li><li>2、这些偏移点的权重系数是多少。</li></ul><p>文章是利用两个网络来实现的，一个网络通过特征点的Value预测16个偏移点(四个特征层)的位置，另一个网络利用特征点的Value预测16个偏移点的权重系数，如下图所示。</p><p> <img src="/pic/BEVFormer3.png" alt="完整架构图"></p><p>图中左边所示为4种尺度的特征层，以最上方特征层中的一个特征点(0.3，0.3)为例，它在每个特征层上都有一个采样点(相对坐标一致)，正常来说每个采样点会与周围的四个点(绿色点)进行self-attention，但是这四个点最好的通过网络自己来学习，于是蓝色的点是网络学习到的偏移点，但是偏移点的坐标一般不会为整数，因此，蓝色特征点的Value就会有其附近的四个特征点(黄色)进行双线性差值得到，因此，一个特征点就采样到了16个偏移点，那么这个特征点的特征向量Value就由这16个偏移点的特征向量Value加权求和得到</p><h2 id="BEV特征的产生"><a href="#BEV特征的产生" class="headerlink" title="BEV特征的产生"></a>BEV特征的产生</h2><p>BEV 特征的产生用到的就是论文中最核心的部分 —— Encoder 模块</p><p>Encoder 模块包含两个子模块 <strong>Temporal Self-Attention模块 以及 Spatial Cross-Attention模块</strong>；接下来我会分别介绍一下这两个模块；</p><p>在梳理具体的代码实现之前，首先介绍下在 Temporal Self-Attention 模块和 Spatial Cross-Attention 模块中都要用到的一个组件 ——** 多尺度的可变形注意力模块**；这个模块是将 Transformer 的全局注意力变为局部注意力的一个非常关键的组件，用于减少训练时间，提高 Transformer 的收敛速度；（该思想最早出现在 Deformable DETR 中）</p><p>简单概括下多尺度的可变形注意力模块对数据处理的Pipeline，概括如下：</p><p> <img src="/pic/BEVFormer5.png" alt="Deformable Attention 模块 Pipeline"></p><p>通过流程图可知，输入到 Deformable Attention Module CUDA 扩展的变量主要有五个，分别是采样位置（Sample Location）、注意力权重（Attention Weights）、映射后的 Value 特征、多尺度特征每层特征起始索引位置、多尺度特征图的空间大小（便于将采样位置由归一化的值变成绝对位置）；</p><p>多尺度可变形注意力模块与 Transformer 中常见的先生成 Attention Map，再计算加权和的方式不同；常规而言 Attention Map &#x3D; Query 和 Key 做内积运算，将 Attention Map 再和 Value 做加权；但是由于这种方式计算量开销会比较大，所以在 Deformable DETR 中用局部注意力机制代替了全局注意力机制，只对几个采样点进行采样，而<strong>采样点的位置相对于参考点的偏移量和每个采样点在加权时的比重均是靠 Query 经过 Linear 层学习得到的</strong>。具体可以看下图</p><p> <img src="/pic/BEVFormer4.png" alt="Deformable Attention 模块 "></p><h3 id="Temporal-Self-Attention-模块"><a href="#Temporal-Self-Attention-模块" class="headerlink" title="Temporal Self-Attention 模块"></a>Temporal Self-Attention 模块</h3><h4 id="功能"><a href="#功能" class="headerlink" title="功能"></a>功能</h4><p>通过引入时序信息（插图中的 History BEV）与当前时刻的 BEV Query 进行融合，提高 BEV Query 的建模能力；</p><h4 id="代码实现"><a href="#代码实现" class="headerlink" title="代码实现"></a>代码实现</h4><p>对于 Temporal Self-Attention 模块而言，需要 bev_query、bev_pos、prev_bev、ref_point、value等参数（需要用到的参数参考 Deformable Attention Pipeline 图解）</p><ul><li>参数 bev_query<ul><li>一个完全 learnable parameter，通过 nn.Embedding() 函数得到，形状 shape &#x3D; (200 * 200，256)；200，200 分别代表 BEV 特征平面的长和宽；</li></ul></li><li>参数 bev_pose<ul><li>感觉也是一个完全 learnable parameter，与 2D 检测中常见的正余弦编码方式不同，感觉依旧是把不同的 grid 位置映射到一个高维的向量空间，shape &#x3D; （bs，256，200，200）代码如下：<pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token triple-quoted-string string">""" bev_pose 的生成过程 """</span><span class="token comment"># w, h 分别代表 bev 特征的空间尺寸 200 * 200</span>x <span class="token operator">=</span> torch<span class="token punctuation">.</span>arange<span class="token punctuation">(</span>w<span class="token punctuation">,</span> device<span class="token operator">=</span>mask<span class="token punctuation">.</span>device<span class="token punctuation">)</span>y <span class="token operator">=</span> torch<span class="token punctuation">.</span>arange<span class="token punctuation">(</span>h<span class="token punctuation">,</span> device<span class="token operator">=</span>mask<span class="token punctuation">.</span>device<span class="token punctuation">)</span><span class="token comment"># self.col_embed 和 self.row_embed 分别是两个 Linear 层，将(200, )的坐标向高维空间做映射</span>x_embed <span class="token operator">=</span> self<span class="token punctuation">.</span>col_embed<span class="token punctuation">(</span>x<span class="token punctuation">)</span>  <span class="token comment"># (200, 128)</span>y_embed <span class="token operator">=</span> self<span class="token punctuation">.</span>row_embed<span class="token punctuation">(</span>y<span class="token punctuation">)</span>  <span class="token comment"># (200, 128)</span><span class="token comment"># pos shape: (bs, 256, 200, 200)</span>pos <span class="token operator">=</span> torch<span class="token punctuation">.</span>cat<span class="token punctuation">(</span><span class="token punctuation">(</span>x_embed<span class="token punctuation">.</span>unsqueeze<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">)</span><span class="token punctuation">.</span>repeat<span class="token punctuation">(</span>h<span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">,</span> y_embed<span class="token punctuation">.</span>unsqueeze<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">.</span>repeat<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> w<span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">,</span> dim<span class="token operator">=</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">.</span>permute<span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">.</span>unsqueeze<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">)</span><span class="token punctuation">.</span>repeat<span class="token punctuation">(</span>mask<span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre></li></ul></li><li>参数 ref_point<ul><li>这个参数根据当前 Temporal Self-Attention 模块是否有 prev_bev 特征输入而言，会对应不同的情况，之所以会出现不同，是考虑到了前后时刻 BEV 特征存在特征不对齐的问题，BEV 特征不对齐主要体现在以下两个方面<ul><li>车自身是不断运动的。上一时刻和当前时刻，由于车自身的不断运动，两个时刻的 BEV 特征在空间上是不对齐的；针对这一问题，为了实现两个时刻特征的空间对齐，需要用到 can_bus 数据中有关车自身旋转角度和偏移的信息，从而对上一时刻的 BEV 特征与当前时刻的 BEV 特征在空间上实现特征对齐；</li><li>车周围的物体也在一定范围内运动。针对车周围的物体可能在不同时刻也有移动，这部分的特征对齐就是靠网络自身的注意力模块去学习实现修正了。</li></ul></li><li>综上，对于 Temporal Self-Attention 模块没有输入 prev_bev（第一帧没有前一时刻的 BEV 特征）的情况，其 ref_point &#x3D; ref_2d；对于存在输入 prev_bev 的情况，其 ref_point &#x3D; ref_2d + shift；</li><li>涉及到的ref_2d、shift参数，核心代码如下：</li></ul></li></ul> <pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token triple-quoted-string string">"""shift 参数的生成"""</span> <span class="token comment"># obtain rotation angle and shift with ego motion</span>delta_x <span class="token operator">=</span> kwargs<span class="token punctuation">[</span><span class="token string">'img_metas'</span><span class="token punctuation">]</span><span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">[</span><span class="token string">'can_bus'</span><span class="token punctuation">]</span><span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span>delta_y <span class="token operator">=</span> kwargs<span class="token punctuation">[</span><span class="token string">'img_metas'</span><span class="token punctuation">]</span><span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">[</span><span class="token string">'can_bus'</span><span class="token punctuation">]</span><span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span>ego_angle <span class="token operator">=</span> kwargs<span class="token punctuation">[</span><span class="token string">'img_metas'</span><span class="token punctuation">]</span><span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">[</span><span class="token string">'can_bus'</span><span class="token punctuation">]</span><span class="token punctuation">[</span><span class="token operator">-</span><span class="token number">2</span><span class="token punctuation">]</span> <span class="token operator">/</span> np<span class="token punctuation">.</span>pi <span class="token operator">*</span> <span class="token number">180</span>rotation_angle <span class="token operator">=</span> kwargs<span class="token punctuation">[</span><span class="token string">'img_metas'</span><span class="token punctuation">]</span><span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">[</span><span class="token string">'can_bus'</span><span class="token punctuation">]</span><span class="token punctuation">[</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">]</span>grid_length_y <span class="token operator">=</span> grid_length<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span>grid_length_x <span class="token operator">=</span> grid_length<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span>translation_length <span class="token operator">=</span> np<span class="token punctuation">.</span>sqrt<span class="token punctuation">(</span>delta_x <span class="token operator">**</span> <span class="token number">2</span> <span class="token operator">+</span> delta_y <span class="token operator">**</span> <span class="token number">2</span><span class="token punctuation">)</span>translation_angle <span class="token operator">=</span> np<span class="token punctuation">.</span>arctan2<span class="token punctuation">(</span>delta_y<span class="token punctuation">,</span> delta_x<span class="token punctuation">)</span> <span class="token operator">/</span> np<span class="token punctuation">.</span>pi <span class="token operator">*</span> <span class="token number">180</span><span class="token keyword">if</span> translation_angle <span class="token operator">&lt;</span> <span class="token number">0</span><span class="token punctuation">:</span>   translation_angle <span class="token operator">+=</span> <span class="token number">360</span>bev_angle <span class="token operator">=</span> ego_angle <span class="token operator">-</span> translation_angleshift_y <span class="token operator">=</span> translation_length <span class="token operator">*</span> \   np<span class="token punctuation">.</span>cos<span class="token punctuation">(</span>bev_angle <span class="token operator">/</span> <span class="token number">180</span> <span class="token operator">*</span> np<span class="token punctuation">.</span>pi<span class="token punctuation">)</span> <span class="token operator">/</span> grid_length_y <span class="token operator">/</span> bev_hshift_x <span class="token operator">=</span> translation_length <span class="token operator">*</span> \   np<span class="token punctuation">.</span>sin<span class="token punctuation">(</span>bev_angle <span class="token operator">/</span> <span class="token number">180</span> <span class="token operator">*</span> np<span class="token punctuation">.</span>pi<span class="token punctuation">)</span> <span class="token operator">/</span> grid_length_x <span class="token operator">/</span> bev_wshift_y <span class="token operator">=</span> shift_y <span class="token operator">*</span> self<span class="token punctuation">.</span>use_shiftshift_x <span class="token operator">=</span> shift_x <span class="token operator">*</span> self<span class="token punctuation">.</span>use_shiftshift <span class="token operator">=</span> bev_queries<span class="token punctuation">.</span>new_tensor<span class="token punctuation">(</span><span class="token punctuation">[</span>shift_x<span class="token punctuation">,</span> shift_y<span class="token punctuation">]</span><span class="token punctuation">)</span>  <span class="token comment"># shape (2,) </span><span class="token comment"># 通过`旋转`和`平移`变换实现 BEV 特征的对齐，对于平移部分是通过对参考点加上偏移量`shift`体现的</span><span class="token keyword">if</span> prev_bev <span class="token keyword">is</span> <span class="token keyword">not</span> <span class="token boolean">None</span><span class="token punctuation">:</span>   <span class="token keyword">if</span> prev_bev<span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span> <span class="token operator">==</span> bev_h <span class="token operator">*</span> bev_w<span class="token punctuation">:</span>      prev_bev <span class="token operator">=</span> prev_bev<span class="token punctuation">.</span>permute<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">)</span>   <span class="token keyword">if</span> self<span class="token punctuation">.</span>rotate_prev_bev<span class="token punctuation">:</span>      num_prev_bev <span class="token operator">=</span> prev_bev<span class="token punctuation">.</span>size<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">)</span>      prev_bev <span class="token operator">=</span> prev_bev<span class="token punctuation">.</span>reshape<span class="token punctuation">(</span>bev_h<span class="token punctuation">,</span> bev_w<span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">.</span>permute<span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span>  <span class="token comment"># sequence -> grid</span>      prev_bev <span class="token operator">=</span> rotate<span class="token punctuation">(</span>prev_bev<span class="token punctuation">,</span> rotation_angle<span class="token punctuation">,</span> center<span class="token operator">=</span>self<span class="token punctuation">.</span>rotate_center<span class="token punctuation">)</span>      prev_bev <span class="token operator">=</span> prev_bev<span class="token punctuation">.</span>permute<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">)</span><span class="token punctuation">.</span>reshape<span class="token punctuation">(</span>bev_h <span class="token operator">*</span> bev_w<span class="token punctuation">,</span> num_prev_bev<span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token triple-quoted-string string">"""ref_2d 参数的生成，常规的 2D 网格生成的规则坐标点"""</span>ref_y<span class="token punctuation">,</span> ref_x <span class="token operator">=</span> torch<span class="token punctuation">.</span>meshgrid<span class="token punctuation">(</span>torch<span class="token punctuation">.</span>linspace<span class="token punctuation">(</span><span class="token number">0.5</span><span class="token punctuation">,</span> H <span class="token operator">-</span> <span class="token number">0.5</span><span class="token punctuation">,</span> H<span class="token punctuation">,</span> dtype<span class="token operator">=</span>dtype<span class="token punctuation">,</span> device<span class="token operator">=</span>device<span class="token punctuation">)</span><span class="token punctuation">,</span>                              torch<span class="token punctuation">.</span>linspace<span class="token punctuation">(</span><span class="token number">0.5</span><span class="token punctuation">,</span> W <span class="token operator">-</span> <span class="token number">0.5</span><span class="token punctuation">,</span> W<span class="token punctuation">,</span> dtype<span class="token operator">=</span>dtype<span class="token punctuation">,</span> device<span class="token operator">=</span>device<span class="token punctuation">)</span><span class="token punctuation">)</span>ref_y <span class="token operator">=</span> ref_y<span class="token punctuation">.</span>reshape<span class="token punctuation">(</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">[</span><span class="token boolean">None</span><span class="token punctuation">]</span> <span class="token operator">/</span> Href_x <span class="token operator">=</span> ref_x<span class="token punctuation">.</span>reshape<span class="token punctuation">(</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">[</span><span class="token boolean">None</span><span class="token punctuation">]</span> <span class="token operator">/</span> Wref_2d <span class="token operator">=</span> torch<span class="token punctuation">.</span>stack<span class="token punctuation">(</span><span class="token punctuation">(</span>ref_x<span class="token punctuation">,</span> ref_y<span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span>ref_2d <span class="token operator">=</span> ref_2d<span class="token punctuation">.</span>repeat<span class="token punctuation">(</span>bs<span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">.</span>unsqueeze<span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><ul><li><p>参数 value</p><ul><li>对应着bev_query去查询的特征；<ul><li>对于 Temporal Self-Attention 模块输入包含 prev_bev时，value &#x3D; [prev_bev，bev_query]，对应的参考点 ref_point &#x3D; [ref_2d + shift，ref_2d]；如果输入不包含 prev_bev时，value &#x3D; [bev_query，bev_query]，对应的参考点ref_point &#x3D; [ref_2d，ref_2d]。</li><li>相应的，之前介绍的 bev_query 在输入包含 prev_bev时，bev_query &#x3D; [value[0]，bev_query]；输入不包含 prev_bev时，value &#x3D; [bev_query，bev_query]；</li><li>整体的思路还是计算在计算 self-attention，无论是否存在prev_bev，都是在计算prev_bev以及bev_query自身的相似性，最后将两组计算得到的bev_query结果做一下平均。</li></ul></li></ul></li><li><p>内部参数 Offset、Weights、 Sample Location</p><ul><li>参数Offset的计算是同时考虑了value[0]和bev_query的信息，在映射空间的维度上进行了concat，并基于 concat 后的特征，去计算 Offset以及attention weights ，涉及到的核心代码如下，这里解释一下为什么 level &#x3D; 1，由于 BEV 特征只有一层，所以只会对一层 200 * 200 空间大小的 BEV 特征，基于每个位置采样四个点，重新构造新的 BEV 特征；</li></ul><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token triple-quoted-string string">""" bev_query 按照通道维度进行 concat """</span>query <span class="token operator">=</span> torch<span class="token punctuation">.</span>cat<span class="token punctuation">(</span><span class="token punctuation">[</span>value<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">:</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">,</span> query<span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span>  <span class="token comment"># (bs, 40000, 512)</span><span class="token triple-quoted-string string">""" value 经过 Linear 做映射 """</span>value <span class="token operator">=</span> self<span class="token punctuation">.</span>value_proj<span class="token punctuation">(</span>value<span class="token punctuation">)</span><span class="token triple-quoted-string string">""" offsets 以及 attention weights 的生成过程 """</span><span class="token comment"># sampling_offsets: shape = (bs, num_query, 8, 1, 4, 2)</span><span class="token comment"># 对 query 进行维度映射得到采样点的偏移量</span>sampling_offsets <span class="token operator">=</span> self<span class="token punctuation">.</span>sampling_offsets<span class="token punctuation">(</span>query<span class="token punctuation">)</span><span class="token punctuation">.</span>view<span class="token punctuation">(</span>bs<span class="token punctuation">,</span> num_query<span class="token punctuation">,</span> self<span class="token punctuation">.</span>num_heads<span class="token punctuation">,</span> self<span class="token punctuation">.</span>num_levels<span class="token punctuation">,</span> self<span class="token punctuation">.</span>num_points<span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">)</span><span class="token comment"># 对 query 进行维度映射得到注意力权重</span>attention_weights <span class="token operator">=</span> self<span class="token punctuation">.</span>attention_weights<span class="token punctuation">(</span>query<span class="token punctuation">)</span><span class="token punctuation">.</span>view<span class="token punctuation">(</span>bs<span class="token punctuation">,</span> num_query<span class="token punctuation">,</span> self<span class="token punctuation">.</span>num_heads<span class="token punctuation">,</span> self<span class="token punctuation">.</span>num_levels <span class="token operator">*</span> self<span class="token punctuation">.</span>num_points<span class="token punctuation">)</span>  attention_weights <span class="token operator">=</span> attention_weights<span class="token punctuation">.</span>softmax<span class="token punctuation">(</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token comment"># attention_weights: shape = (bs, num_query, 8, 1, 4)</span>attention_weights <span class="token operator">=</span> attention_weights<span class="token punctuation">.</span>view<span class="token punctuation">(</span>bs<span class="token punctuation">,</span> num_query<span class="token punctuation">,</span> self<span class="token punctuation">.</span>num_heads<span class="token punctuation">,</span> self<span class="token punctuation">.</span>num_levels<span class="token punctuation">,</span> self<span class="token punctuation">.</span>num_points<span class="token punctuation">)</span> <span class="token triple-quoted-string string">""" sample location 的生成过程 通过代码可以观察到两点：1. 通过 query 学到的 sampling_offsets 偏移量是一个绝对量，不是相对量，所以需要做 normalize；2. 最终生成的 sampling_locations 是一个相对量；"""</span>offset_normalizer <span class="token operator">=</span> torch<span class="token punctuation">.</span>stack<span class="token punctuation">(</span><span class="token punctuation">[</span>spatial_shapes<span class="token punctuation">[</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">,</span> spatial_shapes<span class="token punctuation">[</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span>sampling_locations <span class="token operator">=</span> reference_points<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span> <span class="token punctuation">:</span><span class="token punctuation">,</span> <span class="token boolean">None</span><span class="token punctuation">,</span> <span class="token punctuation">:</span><span class="token punctuation">,</span> <span class="token boolean">None</span><span class="token punctuation">,</span> <span class="token punctuation">:</span><span class="token punctuation">]</span> \               <span class="token operator">+</span> sampling_offsets <span class="token operator">/</span> offset_normalizer<span class="token punctuation">[</span><span class="token boolean">None</span><span class="token punctuation">,</span> <span class="token boolean">None</span><span class="token punctuation">,</span> <span class="token boolean">None</span><span class="token punctuation">,</span> <span class="token punctuation">:</span><span class="token punctuation">,</span> <span class="token boolean">None</span><span class="token punctuation">,</span> <span class="token punctuation">:</span><span class="token punctuation">]</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre></li><li><p>输出bev query</p><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token triple-quoted-string string">""" 各个参数的 shape 情况 1. value: (2，40000，8，32） # 2: 代表前一时刻的 BEV 特征和后一时刻的 BEV 特征，两个特征在计算的过程中是互不干扰的，                           # 40000: 代表 bev_query 200 * 200 空间大小的每个位置                           # 8: 代表8个头，# 32: 每个头表示为 32 维的特征2. spatial_shapes: (200, 200) # 方便将归一化的 sampling_locations 反归一化3. level_start_index: 0 # BEV 特征只有一层4. sampling_locations: (2, 40000, 8, 1, 4, 2)5. attention_weights: (2, 40000, 8, 1, 4)6. output: (2, 40000, 8, 32)"""</span>output <span class="token operator">=</span> MultiScaleDeformableAttnFunction<span class="token punctuation">.</span><span class="token builtin">apply</span><span class="token punctuation">(</span>value<span class="token punctuation">,</span>                                                 spatial_shapes<span class="token punctuation">,</span>                                                 level_start_index<span class="token punctuation">,</span>                                                 sampling_locations<span class="token punctuation">,</span>                                                attention_weights<span class="token punctuation">,</span>                                                 self<span class="token punctuation">.</span>im2col_step<span class="token punctuation">)</span><span class="token string">""</span>" 最后将前一时刻的 bev_query 与当前时刻的 bev_query 做平均output <span class="token operator">=</span> output<span class="token punctuation">.</span>permute<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">)</span>output <span class="token operator">=</span> <span class="token punctuation">(</span>output<span class="token punctuation">[</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token punctuation">:</span>bs<span class="token punctuation">]</span> <span class="token operator">+</span> output<span class="token punctuation">[</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">,</span> bs<span class="token punctuation">:</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token operator">/</span>self<span class="token punctuation">.</span>num_bev_queue<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre></li></ul><p>至此，Temporal Self-Attention 模块的逻辑到此结束，将生成的 bev_query 送入到后面的 Spatial Cross-Attention 模块中。</p><h3 id="Spatial-Cross-Attention-模块"><a href="#Spatial-Cross-Attention-模块" class="headerlink" title="Spatial Cross-Attention 模块"></a>Spatial Cross-Attention 模块</h3><h4 id="功能-1"><a href="#功能-1" class="headerlink" title="功能"></a>功能</h4><p>利用 Temporal Self-Attention 模块输出的 bev_query， 对主干网络和 Neck 网络提取到的多尺度环视图像特征进行查询，生成 BEV 空间下的BEV Embedding特征；</p><h4 id="代码实现-1"><a href="#代码实现-1" class="headerlink" title="代码实现"></a>代码实现</h4><p>对于 Spatial Cross-Attention 模块而言，与 Temporal Self-Attention 模块需要的参数很类似，但是并不需要 bev_pos 参数，只需要 bev_query、ref_point、value（就是 concat 到一起的多尺度特征）；虽不需要 bev_pose，但是整体流程与 Deformable Attention Pipeline 图解类似</p><ul><li><p>参数bev_query</p><ul><li>bev_query参数来自于 Temporal Self-Attention 模块的输出；</li></ul></li><li><p>参数value</p><ul><li>对于 Transformer 而言，由于其本身是处理文本序列的模型，而文本序列都是一组组一维的数据，所以需要将前面提取的多尺度特征做 flatten() 处理，并将所有层的特征汇聚到一起，方便之后做查询；对应的核心代码如下：<pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token triple-quoted-string string">""" 首先将多尺度的特征每一层都进行 flatten() """</span><span class="token keyword">for</span> lvl<span class="token punctuation">,</span> feat <span class="token keyword">in</span> <span class="token builtin">enumerate</span><span class="token punctuation">(</span>mlvl_feats<span class="token punctuation">)</span><span class="token punctuation">:</span>   bs<span class="token punctuation">,</span> num_cam<span class="token punctuation">,</span> c<span class="token punctuation">,</span> h<span class="token punctuation">,</span> w <span class="token operator">=</span> feat<span class="token punctuation">.</span>shape   spatial_shape <span class="token operator">=</span> <span class="token punctuation">(</span>h<span class="token punctuation">,</span> w<span class="token punctuation">)</span>   feat <span class="token operator">=</span> feat<span class="token punctuation">.</span>flatten<span class="token punctuation">(</span><span class="token number">3</span><span class="token punctuation">)</span><span class="token punctuation">.</span>permute<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">)</span>     <span class="token keyword">if</span> self<span class="token punctuation">.</span>use_cams_embeds<span class="token punctuation">:</span>      feat <span class="token operator">=</span> feat <span class="token operator">+</span> self<span class="token punctuation">.</span>cams_embeds<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span> <span class="token boolean">None</span><span class="token punctuation">,</span> <span class="token boolean">None</span><span class="token punctuation">,</span> <span class="token punctuation">:</span><span class="token punctuation">]</span><span class="token punctuation">.</span>to<span class="token punctuation">(</span>feat<span class="token punctuation">.</span>dtype<span class="token punctuation">)</span>      feat <span class="token operator">=</span> feat <span class="token operator">+</span> self<span class="token punctuation">.</span>level_embeds<span class="token punctuation">[</span><span class="token boolean">None</span><span class="token punctuation">,</span> <span class="token boolean">None</span><span class="token punctuation">,</span> lvl<span class="token punctuation">:</span>lvl <span class="token operator">+</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token punctuation">:</span><span class="token punctuation">]</span><span class="token punctuation">.</span>to<span class="token punctuation">(</span>feat<span class="token punctuation">.</span>dtype<span class="token punctuation">)</span>      spatial_shapes<span class="token punctuation">.</span>append<span class="token punctuation">(</span>spatial_shape<span class="token punctuation">)</span>      feat_flatten<span class="token punctuation">.</span>append<span class="token punctuation">(</span>feat<span class="token punctuation">)</span><span class="token triple-quoted-string string">""" 对每个 camera 的所有层级特征进行汇聚 """</span>feat_flatten <span class="token operator">=</span> torch<span class="token punctuation">.</span>cat<span class="token punctuation">(</span>feat_flatten<span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">)</span>  <span class="token comment"># (cam, bs, sum(h*w), 256)</span>spatial_shapes <span class="token operator">=</span> torch<span class="token punctuation">.</span>as_tensor<span class="token punctuation">(</span>spatial_shapes<span class="token punctuation">,</span> dtype<span class="token operator">=</span>torch<span class="token punctuation">.</span><span class="token builtin">long</span><span class="token punctuation">,</span> device<span class="token operator">=</span>bev_pos<span class="token punctuation">.</span>device<span class="token punctuation">)</span><span class="token comment"># 计算每层特征的起始索引位置</span>level_start_index <span class="token operator">=</span> torch<span class="token punctuation">.</span>cat<span class="token punctuation">(</span><span class="token punctuation">(</span>spatial_shapes<span class="token punctuation">.</span>new_zeros<span class="token punctuation">(</span><span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">,</span> spatial_shapes<span class="token punctuation">.</span>prod<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">.</span>cumsum<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">)</span><span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token comment"># 维度变换</span>feat_flatten <span class="token operator">=</span> feat_flatten<span class="token punctuation">.</span>permute<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">)</span>  <span class="token comment"># (num_cam, sum(H*W), bs, embed_dims)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre></li></ul></li><li><p>参数ref_point</p><ul><li><p>首先说一下ref_3d坐标点，这个ref_3d是基于 BEV 空间产生的三维空间规则网格点，同时在 z 轴方向上人为的选择了 4 个坐标点。</p></li><li><p>这里要使用 z 轴，并在 z 轴方向上采样的物理意义，我的理解是为了提取每个 BEV 位置处不同高度的特征；可以理解一下，假如对于 BEV 平面上的（x，y）处有一辆汽车，它所对应的特征应该由车底、车身、车顶处等位置的特征汇聚而成，但是这些位置对应的高度是不一致的，而为了更好的获取在 BEV 空间下的（x，y）处的特征，就将（x，y）的坐标进行了 lift ，从而将 BEV 坐标系下的三维点映射回图像平面后可以去查询并融合更加准确的特征；</p></li><li><p>而在映射的过程中，论文中也提到，由于每个参考点映射回图像坐标系后，不会落到六个图像上，只可能落在其中的某些图像的某些位置上，所以只对这些参考点附近的位置进行采样，可以提高模型的收敛速度（借鉴了 Deformable DETR 的思想）如下图所示：<br><img src="/pic/BEVFormer6.png"></p></li><li><p>ref_3d参数生成、3D 坐标向图像平面转换等过程的核心代码如下，真正用在 Spatial Cross-Attention 模块中的参考点是下面代码段中的reference_points_cam 。</p><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token triple-quoted-string string">""" ref_3d 坐标生成 """</span>zs <span class="token operator">=</span> torch<span class="token punctuation">.</span>linspace<span class="token punctuation">(</span><span class="token number">0.5</span><span class="token punctuation">,</span> Z <span class="token operator">-</span> <span class="token number">0.5</span><span class="token punctuation">,</span> num_points_in_pillar<span class="token punctuation">,</span> dtype<span class="token operator">=</span>dtype<span class="token punctuation">,</span> device<span class="token operator">=</span>device<span class="token punctuation">)</span><span class="token punctuation">.</span>view<span class="token punctuation">(</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">.</span>expand<span class="token punctuation">(</span>num_points_in_pillar<span class="token punctuation">,</span> H<span class="token punctuation">,</span> W<span class="token punctuation">)</span> <span class="token operator">/</span> Zxs <span class="token operator">=</span> torch<span class="token punctuation">.</span>linspace<span class="token punctuation">(</span><span class="token number">0.5</span><span class="token punctuation">,</span> W <span class="token operator">-</span> <span class="token number">0.5</span><span class="token punctuation">,</span> W<span class="token punctuation">,</span> dtype<span class="token operator">=</span>dtype<span class="token punctuation">,</span> device<span class="token operator">=</span>device<span class="token punctuation">)</span><span class="token punctuation">.</span>view<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> W<span class="token punctuation">)</span><span class="token punctuation">.</span>expand<span class="token punctuation">(</span>num_points_in_pillar<span class="token punctuation">,</span> H<span class="token punctuation">,</span> W<span class="token punctuation">)</span> <span class="token operator">/</span> Wys <span class="token operator">=</span> torch<span class="token punctuation">.</span>linspace<span class="token punctuation">(</span><span class="token number">0.5</span><span class="token punctuation">,</span> H <span class="token operator">-</span> <span class="token number">0.5</span><span class="token punctuation">,</span> H<span class="token punctuation">,</span> dtype<span class="token operator">=</span>dtype<span class="token punctuation">,</span> device<span class="token operator">=</span>device<span class="token punctuation">)</span><span class="token punctuation">.</span>view<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> H<span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">.</span>expand<span class="token punctuation">(</span>num_points_in_pillar<span class="token punctuation">,</span> H<span class="token punctuation">,</span> W<span class="token punctuation">)</span> <span class="token operator">/</span> Href_3d <span class="token operator">=</span> torch<span class="token punctuation">.</span>stack<span class="token punctuation">(</span><span class="token punctuation">(</span>xs<span class="token punctuation">,</span> ys<span class="token punctuation">,</span> zs<span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span>  <span class="token comment"># (4, 200, 200, 3)  (level, bev_h, bev_w, 3) 3代表 x,y,z 坐标值</span>ref_3d <span class="token operator">=</span> ref_3d<span class="token punctuation">.</span>permute<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">.</span>flatten<span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">.</span>permute<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span>  <span class="token comment"># (4, 200 * 200, 3)</span>ref_3d <span class="token operator">=</span> ref_3d<span class="token punctuation">[</span><span class="token boolean">None</span><span class="token punctuation">]</span><span class="token punctuation">.</span>repeat<span class="token punctuation">(</span>bs<span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span>  <span class="token comment"># (1, 4, 200 * 200, 3)</span><span class="token triple-quoted-string string">""" BEV 空间下的三维坐标点向图像空间转换的过程代码中的`lidar2img`需要有两点需要注意1. BEV 坐标系 这里指 lidar 坐标系2. 这里提到的`lidar2img`是经过坐标变换的，一般分成三步   第一步：lidar 坐标系 -> ego vehicle 坐标系   第二步：ego vehicle 坐标系 -> camera 坐标系   第三部：camera 坐标系 通过相机内参 得到像素坐标系   以上这三步用到的所有平移和旋转矩阵都合并到了一起，形成了 `lidar2img` 旋转平移矩阵同时需要注意：再与`lidar2img`矩阵乘完，还需要经过下面两步坐标系转换，才是得到了三维坐标点在二维图像平面上的点"""</span><span class="token comment"># (level, bs, cam, num_query, 4)</span>坐标系转换第一步：reference_points_cam <span class="token operator">=</span> torch<span class="token punctuation">.</span>matmul<span class="token punctuation">(</span>lidar2img<span class="token punctuation">.</span>to<span class="token punctuation">(</span>torch<span class="token punctuation">.</span>float32<span class="token punctuation">)</span><span class="token punctuation">,</span> reference_points<span class="token punctuation">.</span>to<span class="token punctuation">(</span>torch<span class="token punctuation">.</span>float32<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">.</span>squeeze<span class="token punctuation">(</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span>  eps <span class="token operator">=</span> <span class="token number">1e-5</span>bev_mask <span class="token operator">=</span> <span class="token punctuation">(</span>reference_points_cam<span class="token punctuation">[</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">:</span><span class="token number">3</span><span class="token punctuation">]</span> <span class="token operator">></span> eps<span class="token punctuation">)</span>  <span class="token comment"># (level, bs, cam, num_query, 1)</span>坐标系转换第二步：reference_points_cam <span class="token operator">=</span> reference_points_cam<span class="token punctuation">[</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">:</span><span class="token number">2</span><span class="token punctuation">]</span> <span class="token operator">/</span> torch<span class="token punctuation">.</span>maximum<span class="token punctuation">(</span>reference_points_cam<span class="token punctuation">[</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">:</span><span class="token number">3</span><span class="token punctuation">]</span><span class="token punctuation">,</span> torch<span class="token punctuation">.</span>ones_like<span class="token punctuation">(</span>reference_points_cam<span class="token punctuation">[</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">:</span><span class="token number">3</span><span class="token punctuation">]</span><span class="token punctuation">)</span> <span class="token operator">*</span> eps<span class="token punctuation">)</span><span class="token comment"># reference_points_cam = (bs, cam = 6, 40000, level = 4, xy = 2)</span>reference_points_cam<span class="token punctuation">[</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">]</span> <span class="token operator">/=</span> img_metas<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">[</span><span class="token string">'img_shape'</span><span class="token punctuation">]</span><span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span>  <span class="token comment"># 坐标归一化</span>reference_points_cam<span class="token punctuation">[</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">]</span> <span class="token operator">/=</span> img_metas<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">[</span><span class="token string">'img_shape'</span><span class="token punctuation">]</span><span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span>  <span class="token comment"># 坐标归一化</span><span class="token comment"># bev_mask 用于评判某一 三维坐标点 是否落在了 二维坐标平面上</span><span class="token comment"># bev_mask = (bs, cam = 6, 40000, level = 4)</span>bev_mask <span class="token operator">=</span> <span class="token punctuation">(</span>bev_mask <span class="token operator">&amp;</span> <span class="token punctuation">(</span>reference_points_cam<span class="token punctuation">[</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">:</span><span class="token number">2</span><span class="token punctuation">]</span> <span class="token operator">></span> <span class="token number">0.0</span><span class="token punctuation">)</span>                     <span class="token operator">&amp;</span> <span class="token punctuation">(</span>reference_points_cam<span class="token punctuation">[</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">:</span><span class="token number">2</span><span class="token punctuation">]</span> <span class="token operator">&lt;</span> <span class="token number">1.0</span><span class="token punctuation">)</span>                     <span class="token operator">&amp;</span> <span class="token punctuation">(</span>reference_points_cam<span class="token punctuation">[</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">:</span><span class="token number">1</span><span class="token punctuation">]</span> <span class="token operator">&lt;</span> <span class="token number">1.0</span><span class="token punctuation">)</span>                     <span class="token operator">&amp;</span> <span class="token punctuation">(</span>reference_points_cam<span class="token punctuation">[</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">:</span><span class="token number">1</span><span class="token punctuation">]</span> <span class="token operator">></span> <span class="token number">0.0</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre></li></ul></li></ul><p>需要注意的是，上述得到的bev_query以及reference_points_cam参数并不是直接用在了 Spatial Cross-Attention 模块中，而是选择了有用的部分进行使用（减少模型的计算量，提高训练过程的收敛速度），这里还是根据 Deformable Attention Pipeline 中涉及的参数进行说明：</p><ul><li><p>参数queries_rebatch</p><ul><li>之前也有提到，并不是 BEV 坐标系下的每个三维坐标都会映射到环视相机的所有图像上，而只会映射到其中的某几张图片上，所以使用所有来自 Temporal Self-Attention 模块的所有bev_query会消耗很大的计算量，所以这里是对bev_query进行了重新的整合，涉及的核心代码如下：<pre class="line-numbers language-python" data-language="python"><code class="language-python">   indexes <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span><span class="token comment"># 根据每张图片对应的`bev_mask`结果，获取有效query的index</span><span class="token keyword">for</span> i<span class="token punctuation">,</span> mask_per_img <span class="token keyword">in</span> <span class="token builtin">enumerate</span><span class="token punctuation">(</span>bev_mask<span class="token punctuation">)</span><span class="token punctuation">:</span>   index_query_per_img <span class="token operator">=</span> mask_per_img<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">.</span><span class="token builtin">sum</span><span class="token punctuation">(</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">.</span>nonzero<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>squeeze<span class="token punctuation">(</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span>   indexes<span class="token punctuation">.</span>append<span class="token punctuation">(</span>index_query_per_img<span class="token punctuation">)</span>queries_rebatch <span class="token operator">=</span> query<span class="token punctuation">.</span>new_zeros<span class="token punctuation">(</span><span class="token punctuation">[</span>bs <span class="token operator">*</span> self<span class="token punctuation">.</span>num_cams<span class="token punctuation">,</span> max_len<span class="token punctuation">,</span> self<span class="token punctuation">.</span>embed_dims<span class="token punctuation">]</span><span class="token punctuation">)</span>reference_points_rebatch <span class="token operator">=</span> reference_points_cam<span class="token punctuation">.</span>new_zeros<span class="token punctuation">(</span><span class="token punctuation">[</span>bs <span class="token operator">*</span> self<span class="token punctuation">.</span>num_cams<span class="token punctuation">,</span> max_len<span class="token punctuation">,</span> D<span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">]</span><span class="token punctuation">)</span> <span class="token keyword">for</span> i<span class="token punctuation">,</span> reference_points_per_img <span class="token keyword">in</span> <span class="token builtin">enumerate</span><span class="token punctuation">(</span>reference_points_cam<span class="token punctuation">)</span><span class="token punctuation">:</span>   <span class="token keyword">for</span> j <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span>bs<span class="token punctuation">)</span><span class="token punctuation">:</span>      index_query_per_img <span class="token operator">=</span> indexes<span class="token punctuation">[</span>i<span class="token punctuation">]</span>      <span class="token comment"># 重新整合 `bev_query` 特征，记作 `query_rebatch</span>      queries_rebatch<span class="token punctuation">[</span>j <span class="token operator">*</span> self<span class="token punctuation">.</span>num_cams <span class="token operator">+</span> i<span class="token punctuation">,</span> <span class="token punctuation">:</span><span class="token builtin">len</span><span class="token punctuation">(</span>index_query_per_img<span class="token punctuation">)</span><span class="token punctuation">]</span> <span class="token operator">=</span> query<span class="token punctuation">[</span>j<span class="token punctuation">,</span> index_query_per_img<span class="token punctuation">]</span>      <span class="token comment"># 重新整合 `reference_point`采样位置，记作`reference_points_rebatch`</span>      reference_points_rebatch<span class="token punctuation">[</span>j <span class="token operator">*</span> self<span class="token punctuation">.</span>num_cams <span class="token operator">+</span> i<span class="token punctuation">,</span> <span class="token punctuation">:</span><span class="token builtin">len</span><span class="token punctuation">(</span>index_query_per_img<span class="token punctuation">)</span><span class="token punctuation">]</span> <span class="token operator">=</span> reference_points_per_img<span class="token punctuation">[</span>j<span class="token punctuation">,</span> index_query_per_img<span class="token punctuation">]</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre></li></ul></li><li><p>参数reference_points_rebatch</p><ul><li>与产生query_rebatch的原因相同，获得映射到二维图像后的有效位置，对原有的reference_points进行重新的整合reference_points_rebatch。</li></ul></li><li><p>内部参数Offset、Weights、Sample Locations</p></li></ul> <pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token triple-quoted-string string">""" 获取 sampling_offsets，依旧是对 query 做 Linear 做维度的映射，但是需要注意的是这里的 query 指代的是上面提到的 `quries_rebatch` """</span><span class="token comment"># sample 8 points for single ref point in each level.</span><span class="token comment"># sampling_offsets: shape = (bs, max_len, 8, 4, 8, 2)</span>sampling_offsets <span class="token operator">=</span> self<span class="token punctuation">.</span>sampling_offsets<span class="token punctuation">(</span>query<span class="token punctuation">)</span><span class="token punctuation">.</span>view<span class="token punctuation">(</span>bs<span class="token punctuation">,</span> num_query<span class="token punctuation">,</span> self<span class="token punctuation">.</span>num_heads<span class="token punctuation">,</span> self<span class="token punctuation">.</span>num_levels<span class="token punctuation">,</span> self<span class="token punctuation">.</span>num_points<span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">)</span>attention_weights <span class="token operator">=</span> self<span class="token punctuation">.</span>attention_weights<span class="token punctuation">(</span>query<span class="token punctuation">)</span><span class="token punctuation">.</span>view<span class="token punctuation">(</span>bs<span class="token punctuation">,</span> num_query<span class="token punctuation">,</span> self<span class="token punctuation">.</span>num_heads<span class="token punctuation">,</span> self<span class="token punctuation">.</span>num_levels <span class="token operator">*</span> self<span class="token punctuation">.</span>num_points<span class="token punctuation">)</span>attention_weights <span class="token operator">=</span> attention_weights<span class="token punctuation">.</span>softmax<span class="token punctuation">(</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token comment"># attention_weights: shape = (bs, max_len, 8, 4, 8)</span>attention_weights <span class="token operator">=</span> attention_weights<span class="token punctuation">.</span>view<span class="token punctuation">(</span>bs<span class="token punctuation">,</span> num_query<span class="token punctuation">,</span>                                          self<span class="token punctuation">.</span>num_heads<span class="token punctuation">,</span>                                          self<span class="token punctuation">.</span>num_levels<span class="token punctuation">,</span>                                          self<span class="token punctuation">.</span>num_points<span class="token punctuation">)</span><span class="token triple-quoted-string string">""" 生成 sampling location """</span>offset_normalizer <span class="token operator">=</span> torch<span class="token punctuation">.</span>stack<span class="token punctuation">(</span><span class="token punctuation">[</span>spatial_shapes<span class="token punctuation">[</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">,</span> spatial_shapes<span class="token punctuation">[</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span>reference_points <span class="token operator">=</span> reference_points<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span> <span class="token punctuation">:</span><span class="token punctuation">,</span> <span class="token boolean">None</span><span class="token punctuation">,</span> <span class="token boolean">None</span><span class="token punctuation">,</span> <span class="token boolean">None</span><span class="token punctuation">,</span> <span class="token punctuation">:</span><span class="token punctuation">,</span> <span class="token punctuation">:</span><span class="token punctuation">]</span>sampling_offsets <span class="token operator">=</span> sampling_offsets <span class="token operator">/</span> offset_normalizer<span class="token punctuation">[</span><span class="token boolean">None</span><span class="token punctuation">,</span> <span class="token boolean">None</span><span class="token punctuation">,</span> <span class="token boolean">None</span><span class="token punctuation">,</span> <span class="token punctuation">:</span><span class="token punctuation">,</span> <span class="token boolean">None</span><span class="token punctuation">,</span> <span class="token punctuation">:</span><span class="token punctuation">]</span>sampling_locations <span class="token operator">=</span> reference_points <span class="token operator">+</span> sampling_offsets<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><ul><li>输出bev_embedding</li><li>将上述处理好的参数，送入到多尺度可变形注意力模块中生成bev_embedding特征；</li></ul>  <pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token triple-quoted-string string">"""1. value: shape = (cam = 6, sum(h_i * w_i) = 30825, head = 8, dim = 32)2. spatial_shapes = ([[116, 200], [58, 100], [29,  50], [15,  25]])3. level_start_index= [0, 23200, 29000, 30450]4. sampling_locations = (cam, max_len, 8, 4, 8, 2)5. attention_weights = (cam, max_len, 8, 4, 8)6. output = (cam, max_len, 8, 32)"""</span>output <span class="token operator">=</span> MultiScaleDeformableAttnFunction<span class="token punctuation">.</span><span class="token builtin">apply</span><span class="token punctuation">(</span>value<span class="token punctuation">,</span> spatial_shapes<span class="token punctuation">,</span> level_start_index<span class="token punctuation">,</span> sampling_locations<span class="token punctuation">,</span>               attention_weights<span class="token punctuation">,</span> self<span class="token punctuation">.</span>im2col_step<span class="token punctuation">)</span><span class="token triple-quoted-string string">"""最后再将六个环视相机查询到的特征整合到一起，再求一个平均值 """</span><span class="token keyword">for</span> i<span class="token punctuation">,</span> index_query_per_img <span class="token keyword">in</span> <span class="token builtin">enumerate</span><span class="token punctuation">(</span>indexes<span class="token punctuation">)</span><span class="token punctuation">:</span>   <span class="token keyword">for</span> j <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span>bs<span class="token punctuation">)</span><span class="token punctuation">:</span>  <span class="token comment"># slots: (bs, 40000, 256)</span>      slots<span class="token punctuation">[</span>j<span class="token punctuation">,</span> index_query_per_img<span class="token punctuation">]</span> <span class="token operator">+=</span> queries<span class="token punctuation">[</span>j <span class="token operator">*</span> self<span class="token punctuation">.</span>num_cams <span class="token operator">+</span> i<span class="token punctuation">,</span> <span class="token punctuation">:</span><span class="token builtin">len</span><span class="token punctuation">(</span>index_query_per_img<span class="token punctuation">)</span><span class="token punctuation">]</span>count <span class="token operator">=</span> bev_mask<span class="token punctuation">.</span><span class="token builtin">sum</span><span class="token punctuation">(</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span> <span class="token operator">></span> <span class="token number">0</span>count <span class="token operator">=</span> count<span class="token punctuation">.</span>permute<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">)</span><span class="token punctuation">.</span><span class="token builtin">sum</span><span class="token punctuation">(</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span>count <span class="token operator">=</span> torch<span class="token punctuation">.</span>clamp<span class="token punctuation">(</span>count<span class="token punctuation">,</span> <span class="token builtin">min</span><span class="token operator">=</span><span class="token number">1.0</span><span class="token punctuation">)</span>slots <span class="token operator">=</span> slots <span class="token operator">/</span> count<span class="token punctuation">[</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token boolean">None</span><span class="token punctuation">]</span>  <span class="token comment"># maybe normalize.</span>slots <span class="token operator">=</span> self<span class="token punctuation">.</span>output_proj<span class="token punctuation">(</span>slots<span class="token punctuation">)</span>  <span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>以上就是 Spatial Cross-Attention 模块的整体逻辑。</p><p>将 Temporal Self-Attetion 模块和 Spatial Cross-Attention 模块堆叠在一起，并重复六次，最终得到的 BEV Embedding 特征作为下游 3D 目标检测和道路分割任务的 BEV 空间特征。</p><h2 id="Decoder模块"><a href="#Decoder模块" class="headerlink" title="Decoder模块"></a>Decoder模块</h2><p>上述产生 BEV 特征的过程是用了当前输入到网络模型中除当前帧外，之前所有帧的特征去迭代修正去获得prev_bev的特征；所以在利用 Decoder 模块进行解码之前，<strong>需要对当前时刻环视的 6 张图片同样利用 Backbone + Neck 提取多尺度的特征，然后利用上述的 Temporal Self-Attention 模块和 Spatial Cross-Attention 模块的逻辑生成当前时刻的bev_embedding</strong>，然后将这部分特征送入到 Decoder 中进行 3D 目标检测。</p><p>下面分析 Decoder 模块是如何获得预测框和分类得分的。</p><ul><li><p>query、query_pos </p><ul><li>首先是object_query_embed参数，该参数同样是沿用了 2D 目标检测中的 Deformable DETR 的思想。query和query_pose 全都是可学习的。模型直接用 nn.Embedding() 生成一组（900，512）维的张量。然后将 512 维的张量分成两组，分别构成了query &#x3D; (900，256)和query_pos &#x3D; (900，256) 。</li></ul></li><li><p>referece_points</p><ul><li>之前介绍过，对于多尺度可变形注意力模块是需要参考点的，但是在预测过程中是没有参考点的，这就需要网络学习出来，网络是靠 query_pos学习得到的，核心代码如下：<pre class="line-numbers language-python" data-language="python"><code class="language-python">reference_points <span class="token operator">=</span> self<span class="token punctuation">.</span>reference_points<span class="token punctuation">(</span>query_pos<span class="token punctuation">)</span>  <span class="token comment"># (bs, 900, 3)  3 代表 (x, y, z) 坐标</span>reference_points <span class="token operator">=</span> reference_points<span class="token punctuation">.</span>sigmoid<span class="token punctuation">(</span><span class="token punctuation">)</span>  <span class="token comment"># absolute -> relative</span>init_reference_out <span class="token operator">=</span> reference_points <span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre></li></ul></li><li><p>Decoder 逻辑</p><ul><li>在获取到需要用到的query、query_pos、reference_points参数后，后面的逻辑有些类似 Deformabe DETR 的 Decoder 过程，简单概括如下几点：<ul><li>利用query和query_pos去做常规的 Self-Attention 运算更新query；</li><li>利用 Self-Attention 得到的 query，之前获得的 bev_embedding作为value，query_pos，由 query生成的reference_points（虽然生成的x，y，z参考点位置，但是 BEV Embedding 是二维的，所以参考点只选择了前两维）仿照 Deformable Attention Module 的 pipeline 做可变形注意力；</li></ul></li><li>可变形注意力核心代码如下：</li></ul><pre class="line-numbers language-python" data-language="python"><code class="language-python">   <span class="token triple-quoted-string string">""" 由 query 生成 sampling_offsets 和 attention_weights """</span>sampling_offsets <span class="token operator">=</span> self<span class="token punctuation">.</span>sampling_offsets<span class="token punctuation">(</span>query<span class="token punctuation">)</span><span class="token punctuation">.</span>view<span class="token punctuation">(</span>            bs<span class="token punctuation">,</span> num_query<span class="token punctuation">,</span> self<span class="token punctuation">.</span>num_heads<span class="token punctuation">,</span> self<span class="token punctuation">.</span>num_levels<span class="token punctuation">,</span> self<span class="token punctuation">.</span>num_points<span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">)</span>  <span class="token comment"># (bs, 900, 8, 1, 4, 2)</span>attention_weights <span class="token operator">=</span> self<span class="token punctuation">.</span>attention_weights<span class="token punctuation">(</span>query<span class="token punctuation">)</span><span class="token punctuation">.</span>view<span class="token punctuation">(</span>            bs<span class="token punctuation">,</span> num_query<span class="token punctuation">,</span> self<span class="token punctuation">.</span>num_heads<span class="token punctuation">,</span> self<span class="token punctuation">.</span>num_levels <span class="token operator">*</span> self<span class="token punctuation">.</span>num_points<span class="token punctuation">)</span>  <span class="token comment"># (bs, 900, 8, 4)</span>attention_weights <span class="token operator">=</span> attention_weights<span class="token punctuation">.</span>softmax<span class="token punctuation">(</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span>attention_weights <span class="token operator">=</span> attention_weights<span class="token punctuation">.</span>view<span class="token punctuation">(</span>bs<span class="token punctuation">,</span> num_query<span class="token punctuation">,</span>                                                   self<span class="token punctuation">.</span>num_heads<span class="token punctuation">,</span>                                                   self<span class="token punctuation">.</span>num_levels<span class="token punctuation">,</span>                                                   self<span class="token punctuation">.</span>num_points<span class="token punctuation">)</span>  <span class="token comment"># (bs, 900, 8, 1, 4)</span><span class="token triple-quoted-string string">""" sampling_offsets 和 reference_points 得到 sampling_locations """</span>offset_normalizer <span class="token operator">=</span> torch<span class="token punctuation">.</span>stack<span class="token punctuation">(</span>               <span class="token punctuation">[</span>spatial_shapes<span class="token punctuation">[</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">,</span> spatial_shapes<span class="token punctuation">[</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span>sampling_locations <span class="token operator">=</span> reference_points<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span> <span class="token punctuation">:</span><span class="token punctuation">,</span> <span class="token boolean">None</span><span class="token punctuation">,</span> <span class="token punctuation">:</span><span class="token punctuation">,</span> <span class="token boolean">None</span><span class="token punctuation">,</span> <span class="token punctuation">:</span><span class="token punctuation">]</span> \               <span class="token operator">+</span> sampling_offsets \               <span class="token operator">/</span> offset_normalizer<span class="token punctuation">[</span><span class="token boolean">None</span><span class="token punctuation">,</span> <span class="token boolean">None</span><span class="token punctuation">,</span> <span class="token boolean">None</span><span class="token punctuation">,</span> <span class="token punctuation">:</span><span class="token punctuation">,</span> <span class="token boolean">None</span><span class="token punctuation">,</span> <span class="token punctuation">:</span><span class="token punctuation">]</span><span class="token triple-quoted-string string">""" 多尺度可变形注意力模块 """</span><span class="token comment"># value: shape = (bs, 40000, 8, 32)</span><span class="token comment"># spatial_shapes = (200, 200)</span><span class="token comment"># level_start_index = 0</span><span class="token comment"># sampling_locations = (bs, 900, 8, 1, 4, 2)</span><span class="token comment"># attention_weights = (bs, 900, 8, 1, 4)</span><span class="token comment"># output = (bs, 900, 256)</span>output <span class="token operator">=</span> MultiScaleDeformableAttnFunction<span class="token punctuation">.</span><span class="token builtin">apply</span><span class="token punctuation">(</span>value<span class="token punctuation">,</span> spatial_shapes<span class="token punctuation">,</span> level_start_index<span class="token punctuation">,</span> sampling_locations<span class="token punctuation">,</span>               attention_weights<span class="token punctuation">,</span> self<span class="token punctuation">.</span>im2col_step<span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><ul><li><p>在获得查询到的特征后，会利用回归分支（FFN 网络）对提取的特征计算回归结果，预测 10 个输出；</p><ul><li>这 10 个维度的含义为：[xc，yc，w，l，zc，h，rot.sin()，rot.cos()，vx，vy]；</li><li>[预测框中心位置的x方向偏移，预测框中心位置的y方向偏移，预测框的宽，预测框的长，预测框中心位置的z方向偏移，预测框的高，旋转角的正弦值，旋转角的余弦值，x方向速度，y方向速度]；</li></ul></li><li><p>然后根据预测的偏移量，对参考点的位置进行更新，为级联的下一个 Decoder 提高精修过的参考点位置，核心代码如下：</p></li></ul><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">if</span> reg_branches <span class="token keyword">is</span> <span class="token keyword">not</span> <span class="token boolean">None</span><span class="token punctuation">:</span>  <span class="token comment"># update the reference point.</span> tmp <span class="token operator">=</span> reg_branches<span class="token punctuation">[</span>lid<span class="token punctuation">]</span><span class="token punctuation">(</span>output<span class="token punctuation">)</span>  <span class="token comment"># (bs, 900, 256) -> (bs, 900, 10) 回归分支的预测输出</span> <span class="token keyword">assert</span> reference_points<span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">]</span> <span class="token operator">==</span> <span class="token number">3</span> new_reference_points <span class="token operator">=</span> torch<span class="token punctuation">.</span>zeros_like<span class="token punctuation">(</span>reference_points<span class="token punctuation">)</span> <span class="token comment"># 预测出来的偏移量是绝对量</span> new_reference_points<span class="token punctuation">[</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token punctuation">:</span><span class="token number">2</span><span class="token punctuation">]</span> <span class="token operator">=</span> tmp<span class="token punctuation">[</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token punctuation">:</span><span class="token number">2</span><span class="token punctuation">]</span> <span class="token operator">+</span> inverse_sigmoid<span class="token punctuation">(</span>reference_points<span class="token punctuation">[</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token punctuation">:</span><span class="token number">2</span><span class="token punctuation">]</span><span class="token punctuation">)</span>  <span class="token comment"># 框中心处的 x, y 坐标</span> new_reference_points<span class="token punctuation">[</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">:</span><span class="token number">3</span><span class="token punctuation">]</span> <span class="token operator">=</span> tmp<span class="token punctuation">[</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token number">4</span><span class="token punctuation">:</span><span class="token number">5</span><span class="token punctuation">]</span> <span class="token operator">+</span> inverse_sigmoid<span class="token punctuation">(</span>reference_points<span class="token punctuation">[</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">:</span><span class="token number">3</span><span class="token punctuation">]</span><span class="token punctuation">)</span>  <span class="token comment"># 框中心处的 z 坐标</span> <span class="token comment"># 参考点坐标是一个归一化的坐标</span> new_reference_points <span class="token operator">=</span> new_reference_points<span class="token punctuation">.</span>sigmoid<span class="token punctuation">(</span><span class="token punctuation">)</span> reference_points <span class="token operator">=</span> new_reference_points<span class="token punctuation">.</span>detach<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token triple-quoted-string string">""" 最后将每层 Decoder 产生的特征 = (bs, 900, 256)，以及参考点坐标 = (bs, 900, 3) 保存下来。"""</span><span class="token keyword">if</span> self<span class="token punctuation">.</span>return_intermediate<span class="token punctuation">:</span>   intermediate<span class="token punctuation">.</span>append<span class="token punctuation">(</span>output<span class="token punctuation">)</span>   intermediate_reference_points<span class="token punctuation">.</span>append<span class="token punctuation">(</span>reference_points<span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><ul><li>然后将层级的 bev_embedding特征以及参考点通过 for loop 的形式，一次计算每个 Decoder 层的分类和回归结果：</li></ul><pre class="line-numbers language-python" data-language="python"><code class="language-python">bev_embed<span class="token punctuation">,</span> hs<span class="token punctuation">,</span> init_reference<span class="token punctuation">,</span> inter_references <span class="token operator">=</span> outputshs <span class="token operator">=</span> hs<span class="token punctuation">.</span>permute<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">)</span>  <span class="token comment"># (decoder_level, bs, 900, 256)</span>outputs_classes <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>outputs_coords <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span><span class="token keyword">for</span> lvl <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span>hs<span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">:</span>   <span class="token keyword">if</span> lvl <span class="token operator">==</span> <span class="token number">0</span><span class="token punctuation">:</span>      reference <span class="token operator">=</span> init_reference   <span class="token keyword">else</span><span class="token punctuation">:</span>      reference <span class="token operator">=</span> inter_references<span class="token punctuation">[</span>lvl <span class="token operator">-</span> <span class="token number">1</span><span class="token punctuation">]</span>   reference <span class="token operator">=</span> inverse_sigmoid<span class="token punctuation">(</span>reference<span class="token punctuation">)</span>   outputs_class <span class="token operator">=</span> self<span class="token punctuation">.</span>cls_branches<span class="token punctuation">[</span>lvl<span class="token punctuation">]</span><span class="token punctuation">(</span>hs<span class="token punctuation">[</span>lvl<span class="token punctuation">]</span><span class="token punctuation">)</span>  <span class="token comment"># (bs, 900, num_classes)</span>   tmp <span class="token operator">=</span> self<span class="token punctuation">.</span>reg_branches<span class="token punctuation">[</span>lvl<span class="token punctuation">]</span><span class="token punctuation">(</span>hs<span class="token punctuation">[</span>lvl<span class="token punctuation">]</span><span class="token punctuation">)</span>  <span class="token comment"># (bs, 900, 10)</span>   <span class="token keyword">assert</span> reference<span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">]</span> <span class="token operator">==</span> <span class="token number">3</span>   tmp<span class="token punctuation">[</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">:</span><span class="token number">2</span><span class="token punctuation">]</span> <span class="token operator">+=</span> reference<span class="token punctuation">[</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">:</span><span class="token number">2</span><span class="token punctuation">]</span>  <span class="token comment"># (x, y)</span>   tmp<span class="token punctuation">[</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">:</span><span class="token number">2</span><span class="token punctuation">]</span> <span class="token operator">=</span> tmp<span class="token punctuation">[</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">:</span><span class="token number">2</span><span class="token punctuation">]</span><span class="token punctuation">.</span>sigmoid<span class="token punctuation">(</span><span class="token punctuation">)</span>   tmp<span class="token punctuation">[</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token number">4</span><span class="token punctuation">:</span><span class="token number">5</span><span class="token punctuation">]</span> <span class="token operator">+=</span> reference<span class="token punctuation">[</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">:</span><span class="token number">3</span><span class="token punctuation">]</span>   tmp<span class="token punctuation">[</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token number">4</span><span class="token punctuation">:</span><span class="token number">5</span><span class="token punctuation">]</span> <span class="token operator">=</span> tmp<span class="token punctuation">[</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token number">4</span><span class="token punctuation">:</span><span class="token number">5</span><span class="token punctuation">]</span><span class="token punctuation">.</span>sigmoid<span class="token punctuation">(</span><span class="token punctuation">)</span>   tmp<span class="token punctuation">[</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">:</span><span class="token number">1</span><span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token punctuation">(</span>tmp<span class="token punctuation">[</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">:</span><span class="token number">1</span><span class="token punctuation">]</span> <span class="token operator">*</span> <span class="token punctuation">(</span>self<span class="token punctuation">.</span>pc_range<span class="token punctuation">[</span><span class="token number">3</span><span class="token punctuation">]</span> <span class="token operator">-</span> self<span class="token punctuation">.</span>pc_range<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">)</span> <span class="token operator">+</span> self<span class="token punctuation">.</span>pc_range<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">)</span>   tmp<span class="token punctuation">[</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">:</span><span class="token number">2</span><span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token punctuation">(</span>tmp<span class="token punctuation">[</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">:</span><span class="token number">2</span><span class="token punctuation">]</span> <span class="token operator">*</span> <span class="token punctuation">(</span>self<span class="token punctuation">.</span>pc_range<span class="token punctuation">[</span><span class="token number">4</span><span class="token punctuation">]</span> <span class="token operator">-</span> self<span class="token punctuation">.</span>pc_range<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">)</span> <span class="token operator">+</span> self<span class="token punctuation">.</span>pc_range<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">)</span>   tmp<span class="token punctuation">[</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token number">4</span><span class="token punctuation">:</span><span class="token number">5</span><span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token punctuation">(</span>tmp<span class="token punctuation">[</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token number">4</span><span class="token punctuation">:</span><span class="token number">5</span><span class="token punctuation">]</span> <span class="token operator">*</span> <span class="token punctuation">(</span>self<span class="token punctuation">.</span>pc_range<span class="token punctuation">[</span><span class="token number">5</span><span class="token punctuation">]</span> <span class="token operator">-</span> self<span class="token punctuation">.</span>pc_range<span class="token punctuation">[</span><span class="token number">2</span><span class="token punctuation">]</span><span class="token punctuation">)</span> <span class="token operator">+</span> self<span class="token punctuation">.</span>pc_range<span class="token punctuation">[</span><span class="token number">2</span><span class="token punctuation">]</span><span class="token punctuation">)</span>   outputs_coord <span class="token operator">=</span> tmp   outputs_classes<span class="token punctuation">.</span>append<span class="token punctuation">(</span>outputs_class<span class="token punctuation">)</span>   outputs_coords<span class="token punctuation">.</span>append<span class="token punctuation">(</span>outputs_coord<span class="token punctuation">)</span> <span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><ul><li>分类分支的网络结构：</li></ul><pre class="line-numbers language-python" data-language="python"><code class="language-python">Sequential<span class="token punctuation">(</span><span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">)</span><span class="token punctuation">:</span> Linear<span class="token punctuation">(</span>in_features<span class="token operator">=</span><span class="token number">256</span><span class="token punctuation">,</span> out_features<span class="token operator">=</span><span class="token number">256</span><span class="token punctuation">,</span> bias<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span><span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">:</span> LayerNorm<span class="token punctuation">(</span><span class="token punctuation">(</span><span class="token number">256</span><span class="token punctuation">,</span><span class="token punctuation">)</span><span class="token punctuation">,</span> eps<span class="token operator">=</span><span class="token number">1e-05</span><span class="token punctuation">,</span> elementwise_affine<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span><span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">:</span> ReLU<span class="token punctuation">(</span>inplace<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span><span class="token punctuation">(</span><span class="token number">3</span><span class="token punctuation">)</span><span class="token punctuation">:</span> Linear<span class="token punctuation">(</span>in_features<span class="token operator">=</span><span class="token number">256</span><span class="token punctuation">,</span> out_features<span class="token operator">=</span><span class="token number">256</span><span class="token punctuation">,</span> bias<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span><span class="token punctuation">(</span><span class="token number">4</span><span class="token punctuation">)</span><span class="token punctuation">:</span> LayerNorm<span class="token punctuation">(</span><span class="token punctuation">(</span><span class="token number">256</span><span class="token punctuation">,</span><span class="token punctuation">)</span><span class="token punctuation">,</span> eps<span class="token operator">=</span><span class="token number">1e-05</span><span class="token punctuation">,</span> elementwise_affine<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span><span class="token punctuation">(</span><span class="token number">5</span><span class="token punctuation">)</span><span class="token punctuation">:</span> ReLU<span class="token punctuation">(</span>inplace<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span><span class="token punctuation">(</span><span class="token number">6</span><span class="token punctuation">)</span><span class="token punctuation">:</span> Linear<span class="token punctuation">(</span>in_features<span class="token operator">=</span><span class="token number">256</span><span class="token punctuation">,</span> out_features<span class="token operator">=</span><span class="token number">10</span><span class="token punctuation">,</span> bias<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><ul><li>回归分支的网络结构<pre class="line-numbers language-python" data-language="python"><code class="language-python">Sequential<span class="token punctuation">(</span><span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">)</span><span class="token punctuation">:</span> Linear<span class="token punctuation">(</span>in_features<span class="token operator">=</span><span class="token number">256</span><span class="token punctuation">,</span> out_features<span class="token operator">=</span><span class="token number">256</span><span class="token punctuation">,</span> bias<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span><span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">:</span> ReLU<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">:</span> Linear<span class="token punctuation">(</span>in_features<span class="token operator">=</span><span class="token number">256</span><span class="token punctuation">,</span> out_features<span class="token operator">=</span><span class="token number">256</span><span class="token punctuation">,</span> bias<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span><span class="token punctuation">(</span><span class="token number">3</span><span class="token punctuation">)</span><span class="token punctuation">:</span> ReLU<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">(</span><span class="token number">4</span><span class="token punctuation">)</span><span class="token punctuation">:</span> Linear<span class="token punctuation">(</span>in_features<span class="token operator">=</span><span class="token number">256</span><span class="token punctuation">,</span> out_features<span class="token operator">=</span><span class="token number">10</span><span class="token punctuation">,</span> bias<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre></li></ul></li></ul><h2 id="正负样本的定义"><a href="#正负样本的定义" class="headerlink" title="正负样本的定义"></a>正负样本的定义</h2><p>正负样本的定义用到的就是匈牙利匹配算法，分类损失和类似回归损失的总损失和最小；</p><ul><li><p>分类损失的计算代码如下：</p> <pre class="line-numbers language-python" data-language="python"><code class="language-python">cls_pred <span class="token operator">=</span> cls_pred<span class="token punctuation">.</span>sigmoid<span class="token punctuation">(</span><span class="token punctuation">)</span>  <span class="token comment"># calculate the neg_cost and pos_cost by focal loss.</span>neg_cost <span class="token operator">=</span> <span class="token operator">-</span><span class="token punctuation">(</span><span class="token number">1</span> <span class="token operator">-</span> cls_pred <span class="token operator">+</span> self<span class="token punctuation">.</span>eps<span class="token punctuation">)</span><span class="token punctuation">.</span>log<span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token operator">*</span> <span class="token punctuation">(</span><span class="token number">1</span> <span class="token operator">-</span> self<span class="token punctuation">.</span>alpha<span class="token punctuation">)</span> <span class="token operator">*</span> cls_pred<span class="token punctuation">.</span><span class="token builtin">pow</span><span class="token punctuation">(</span>self<span class="token punctuation">.</span>gamma<span class="token punctuation">)</span>pos_cost <span class="token operator">=</span> <span class="token operator">-</span><span class="token punctuation">(</span>cls_pred <span class="token operator">+</span> self<span class="token punctuation">.</span>eps<span class="token punctuation">)</span><span class="token punctuation">.</span>log<span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token operator">*</span> self<span class="token punctuation">.</span>alpha <span class="token operator">*</span> <span class="token punctuation">(</span><span class="token number">1</span> <span class="token operator">-</span> cls_pred<span class="token punctuation">)</span><span class="token punctuation">.</span><span class="token builtin">pow</span><span class="token punctuation">(</span>self<span class="token punctuation">.</span>gamma<span class="token punctuation">)</span>cls_cost <span class="token operator">=</span> pos_cost<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span> gt_labels<span class="token punctuation">]</span> <span class="token operator">-</span> neg_cost<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span> gt_labels<span class="token punctuation">]</span>cls_cost <span class="token operator">=</span> cls_cost <span class="token operator">*</span> self<span class="token punctuation">.</span>weight<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span></span></code></pre></li><li><p>类回归损失的计算代码如下：</p><ul><li>这里介绍一下，gt_box 的表示方式，gt_box 的维度是九维的，分别是 [xc，yc，zc，w，l，h，rot，vx，vy]；而预测结果框的维度是十维的，所以要对 gt_box 的维度进行转换，转换为的维度表示为 [xc，yc，w，l，cz，h，rot.sin()，rot.cos()，vx，vy]</li><li>对应代码如下：</li></ul><pre class="line-numbers language-python" data-language="python"><code class="language-python">cx <span class="token operator">=</span> bboxes<span class="token punctuation">[</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">:</span><span class="token number">1</span><span class="token punctuation">]</span>cy <span class="token operator">=</span> bboxes<span class="token punctuation">[</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">:</span><span class="token number">2</span><span class="token punctuation">]</span>cz <span class="token operator">=</span> bboxes<span class="token punctuation">[</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">:</span><span class="token number">3</span><span class="token punctuation">]</span>w <span class="token operator">=</span> bboxes<span class="token punctuation">[</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">:</span><span class="token number">4</span><span class="token punctuation">]</span><span class="token punctuation">.</span>log<span class="token punctuation">(</span><span class="token punctuation">)</span>l <span class="token operator">=</span> bboxes<span class="token punctuation">[</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token number">4</span><span class="token punctuation">:</span><span class="token number">5</span><span class="token punctuation">]</span><span class="token punctuation">.</span>log<span class="token punctuation">(</span><span class="token punctuation">)</span>h <span class="token operator">=</span> bboxes<span class="token punctuation">[</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token number">5</span><span class="token punctuation">:</span><span class="token number">6</span><span class="token punctuation">]</span><span class="token punctuation">.</span>log<span class="token punctuation">(</span><span class="token punctuation">)</span>rot <span class="token operator">=</span> bboxes<span class="token punctuation">[</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token number">6</span><span class="token punctuation">:</span><span class="token number">7</span><span class="token punctuation">]</span>vx <span class="token operator">=</span> bboxes<span class="token punctuation">[</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token number">7</span><span class="token punctuation">:</span><span class="token number">8</span><span class="token punctuation">]</span> vy <span class="token operator">=</span> bboxes<span class="token punctuation">[</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token number">8</span><span class="token punctuation">:</span><span class="token number">9</span><span class="token punctuation">]</span>normalized_bboxes <span class="token operator">=</span> torch<span class="token punctuation">.</span>cat<span class="token punctuation">(</span><span class="token punctuation">(</span>cx<span class="token punctuation">,</span> cy<span class="token punctuation">,</span> w<span class="token punctuation">,</span> l<span class="token punctuation">,</span> cz<span class="token punctuation">,</span> h<span class="token punctuation">,</span> rot<span class="token punctuation">.</span>sin<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span> rot<span class="token punctuation">.</span>cos<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span> vx<span class="token punctuation">,</span> vy<span class="token punctuation">)</span><span class="token punctuation">,</span> dim<span class="token operator">=</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre></li><li><p>计算类回归损失（L1 Loss）</p><ul><li>这里有一点需要注意的是，在正负样本定义中计算 L1 Loss 的时候，只对前预测框和真值框的前 8 维计算损失：<pre class="line-numbers language-python" data-language="python"><code class="language-python">self<span class="token punctuation">.</span>reg_cost<span class="token punctuation">(</span>bbox_pred<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span> <span class="token punctuation">:</span><span class="token number">8</span><span class="token punctuation">]</span><span class="token punctuation">,</span> normalized_gt_bboxes<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span> <span class="token punctuation">:</span><span class="token number">8</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre></li></ul></li></ul><h2 id="损失的计算"><a href="#损失的计算" class="headerlink" title="损失的计算"></a>损失的计算</h2><p>损失的计算就是分类损失以及 L1 Loss，这里的 L1 Loss 就是对真值框和预测框的10个维度计算 L1 Loss了，计算出来损失，反向传播更新模型的参数。</p>]]></content>
      
      
      <categories>
          
          <category> DL </category>
          
      </categories>
      
      
        <tags>
            
            <tag> bev </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>BEV感知中的时序融合方法（部分待更新）</title>
      <link href="/2023/11/28/bev-time-fusion/"/>
      <url>/2023/11/28/bev-time-fusion/</url>
      
        <content type="html"><![CDATA[<h1 id="BEV感知中的时序融合方法（部分待更新）"><a href="#BEV感知中的时序融合方法（部分待更新）" class="headerlink" title="BEV感知中的时序融合方法（部分待更新）"></a>BEV感知中的时序融合方法（部分待更新）</h1><p><a href="https://zhuanlan.zhihu.com/p/583682754">参考链接</a></p><p>首先展示目前的融合时序的BEV感知算法</p><p> <img src="/pic/time2.jpg" alt="BEV感知模型时序融合方法精简汇总"></p><p>在传统感知算法中,时序融合是提高感知算法准确性和连续性的关键,可以弥补单帧感知的局限性,增加感受野,改善目标检测(Object Detection)帧间跳变和目标遮挡问题,更加准确地判断目标运动速度,同时也对目标预测(Prediction)和跟踪(Tracking)有重要作用.在BEV感知中,时序融合同样可以发挥相应的作用.同时,由于前序帧相关信息可以直接从缓存中读取,并不会带来性能上的大幅下降.下图直观地展示了时序融合的工作原理:</p><p> <img src="/pic/time1.png" alt="参考BEVFormer论文"></p><ul><li>传统的时序融合主要是在后处理中使用RNN或卡尔曼滤波等方式进行融合,这种方式由于要增加额外的开销,影响模型的性能,所以近年来大量采用的是特征级融合.</li><li>特征级融合是继前融合,后融合新提出来的方法,不仅可以用在多传感器融合,也可以用在时序融合,具有跨模态,跨时空的特点.</li><li>而BEV感知由于自身的特点,存在两个特征域:图像域(自车camera图像坐标系)和BEV域(自车lidar坐标系),这一点可以区别于传统感知算法只有图像域特征,从而BEV感知的时序融合可以在两个特征域任意一个进行,具体融合的方法也有两种:<ul><li>基于CNN的方式，其中基于CNN的方式又可使用2D卷积和3D卷积</li><li>基于Transformer的方式,</li><li>基于CNN和Transformer结合的方法.</li></ul></li><li>本文对BEV时序方法的分类主要基于以上几个方面,论文来源基本是2022年的工作.另外由于本文篇幅较长,文末提供精简归纳表格,欢迎阅读下篇获取.</li></ul><p>在具体的时序融合方法上,我们主要关注以下几个对融合结果影响较大的方面:</p><ul><li>一是如何选择前序帧,这个决定了时序融合的有效范围,</li><li>二是如何进行时空对齐(alignment),即将前序帧特征通过ego-motion进行转换,使之与当前帧特征处于同一个坐标系下,这样才可以进行准确的融合,</li><li>三是融合的具体方法,</li><li>最后是融合的分辨率,是融合效果和性能的折中选择.</li><li>数据集方面,以下大部分模型都使用nuscenes数据集,该数据集有1000个场景(scenes),每个scene包括20+精细标注的关键帧(key frame),间隔0.5秒,每两个关键帧之间存在若干无精细标注的非关键帧(sweeps).</li></ul><h2 id="基于Transformer的BEV特征融合"><a href="#基于Transformer的BEV特征融合" class="headerlink" title="基于Transformer的BEV特征融合"></a>基于Transformer的BEV特征融合</h2><h3 id="BEVFormer"><a href="#BEVFormer" class="headerlink" title="BEVFormer"></a>BEVFormer</h3><p>BEVFormer是相对比较早的一个经典BEV感知模型,主体框架是基于transformer生成bev feature,再做基于DETR的目标检测,在之前博客里有详细介绍,主要motion是针对DETR3D的改进,</p><ul><li>一是DETR3D只有基于稀疏的object query的decoder, BEVFormer增加了基于稠密的bev query的encoder,可以生成稠密的bev feature,</li><li>二是由于有了bev feature,方便进行稠密的任务,如语义分割等,也方便进行时序融合.时序融合在encoder中的Temporal Self-Attention中实现,这个模块本质上就是deformable attention(来自于deformable DETR),只是<strong>query做了前序帧和当前帧的拼接</strong>.</li></ul><p>BEVformer在前序帧的选择上,是在<strong>前面4帧中随机选3帧(只包括关键帧)</strong>,所以时序范围为2秒,这3帧不是一次性输入,而是<strong>迭代地进行两两融合</strong>,第一帧由于没有前序帧,只与自己本身融合,也就是每个iteration需要跑<strong>4次前向传播和1次反向传播</strong>.前序bev feature在缓存中直接读取,不会降低推理的效率.</p><p>时空对齐方面,由于是<strong>BEV特征域融合</strong>,而两帧的bev特征分别在两帧的自车lidar坐标系下,所以<strong>需要将前序帧的lidar坐标通过ego-motion转换到当前帧的lidar坐标</strong>.这里面又包括两种方式:</p><ul><li>变换bev feature和变换reference_points(即密集query对应的坐标值),两种方法需要做的变换略有不同.论文中的做法是旋转feature,平移reference_points,</li><li>这里存在一个问题就是论文中旋转feature的方式会产生全0的黑边,不利于后续的融合,而变换reference_points在后续的grid_sample环节会有插值作用,会更加准确.</li></ul><p>具体的融合方式上,论文中是在Temporal Self-Attention模块中把时空对齐后的前序bev feature和当前bev feature分别做deformable attention,再在h*w平面做算术平均进行融合.这里算术平均有点简单粗暴,也可以修改为自适应的融合方式.融合分辨率也就是encoder中query的数量，论文中用的比较大,是200*200,而decoder的query数量与与之类似的经典bev3D模型DETR3D相同,为900.其中,DETR3D无时序版本,并且只有decoder.</p><h3 id="PolarDETR-华中科大-地平线机器人"><a href="#PolarDETR-华中科大-地平线机器人" class="headerlink" title="PolarDETR(华中科大,地平线机器人)"></a>PolarDETR(华中科大,地平线机器人)</h3><p>[Polar Parametrization for Vision-based Surround-View 3D Detection](<a href="https://arxiv.org/abs/">https://arxiv.org/abs/</a><br>2206.10965)<br><a href="https://github.com/hustvl/PolarDETR">代码链接</a></p><p> <img src="/pic/time3.png" alt="网络框架图"></p><p>PolarDETR在整体框架上接近于DETR3D,主要不同点一是bev特征和目标位置的表征和从笛卡尔坐标系转换到了极坐标系,即由半径r,方位角α, 高度z进行表征,二是加入了时序融合.</p><p>为什么使用极坐标系的解释,如下图,假设目标At1和At2由于位置和朝向刚好匹配,在两个2D视角内的呈现是完全相同的,bev有效检测范围是d,这时候在笛卡尔坐标系中At1将被过滤掉,而At2会被保留下来,这对于模型训练来讲显然是不利于收敛的,问题就在于笛卡尔坐标系的各个边界点距中心的距离不一致.而如果使用极坐标系,只要两个目标距自车距离相等,就将被同等对待.</p><p> <img src="/pic/time4.png"></p><p>关于时序融合,本文采用的方式和BEVFormer类似,也是<strong>基于transformer的bev特征融合</strong>,只是这里融合的是</p><ul><li>&#x3D;&#x3D;代表目标的object query,而不是代表bev feature的bev query&#x3D;&#x3D;.</li><li>时序对齐的方法是在极坐标bev下,把当前帧采样点投影到前序帧获取特征,类似于变换reference_points.</li><li>融合方法是<strong>所有帧channel维度拼接后做self attention</strong>,区别于BEVFormer的两两迭代融合,但具体用了多少帧由于代码还未公开所以不确定.</li></ul><p> <img src="/pic/time5.png" alt="性能对比表"></p><h2 id="基于CNN-2D-x2F-3D-conv-的BEV特征融合"><a href="#基于CNN-2D-x2F-3D-conv-的BEV特征融合" class="headerlink" title="基于CNN(2D&#x2F;3D conv)的BEV特征融合"></a>基于CNN(2D&#x2F;3D conv)的BEV特征融合</h2><h3 id="BEVDet4D-x2F-BEVDepth4D-鉴智机器人"><a href="#BEVDet4D-x2F-BEVDepth4D-鉴智机器人" class="headerlink" title="BEVDet4D&#x2F;BEVDepth4D(鉴智机器人)"></a>BEVDet4D&#x2F;BEVDepth4D(鉴智机器人)</h3><p> <img src="/pic/time6.png" alt="网络结构图"></p><p>BEVDet4D和BEVDepth4D是基于BEVDet和BEVDepth增加时序融合的版本.二者框架非常类似,与BEVFormer属于不同的两种bev表征的方式。BEVDet系列属于基于LSS思想,多视角特征先通过深度估计网络进行像素级的深度估计,再投影到bev空间,通过基于CNN的bev encoder进行编码后连接Centerpoint检测头.BEVDepth4D的主要改进是增加了对深度估计网络的监督,使结果更准确.</p><p>具体来说，BEVDet4D的时序融合<strong>发生在投影到bev空间得到bev feature后</strong>,与前序帧先经过时空对齐,在channel维度拼接,再送入bev encoder进行融合.这里的</p><ul><li>时空对齐是使用grid_sample把前序帧特征warp到当前帧,和直接旋转平移feature或reference_points本质上相同,但博主认为,如果后续还要做deformable self attention进行融合的话,这样处理效率较低,因为还需要再做一次grid_sample来取相应的value,还是直接对reference_points进行变换可以获得较高的效率.不过</li><li>这里后续是使用CNN进行融合,影响不大.CNN这里用的是2D卷积.因为这种架构需要额外的深度估计网络,所以bev feature分辨率不能太大,文中采用了16倍下采样.</li><li>在前序帧的选择上,训练阶段是在前3帧或后3帧随机选1帧,推理阶段只在前3帧随机选一帧.训练阶段把后续帧也加进来可以提高鲁棒性.</li></ul><h3 id="BEVerse"><a href="#BEVerse" class="headerlink" title="BEVerse"></a>BEVerse</h3><p><a href="https://arxiv.org/abs/2205.09743">论文链接</a><br><a href="https://github.com/zhangyp15/BEVerse">代码链接</a><br><a href="https://zhuanlan.zhihu.com/p/518147623">学习文章</a></p><p> <img src="/pic/time7.png" alt="网络结构图"></p><p>BEVerse是一个感知预测一体化模型,</p><ul><li>主体基于LSS生成bev feature,</li><li>再经过spatial-temporal bev encoder进行时空编码,-</li><li>再进行下游的检测分割和预测任务.</li></ul><p>由于需要做预测,时序融合成为重要部分,并且需要前序帧和后续帧都要加入训练,选择的帧数相应也会比较多.</p><ul><li>时序对齐的方法仍然类似于BEVDet4D,只是BEVDet4D只使用1帧前序帧,BEVerse使用的是前2帧+后4帧,每帧都用grid_sample warp到当前帧再进行channel维度的拼接.</li><li>拼接完成后,模型设计了Temporal3DConvModel进行时序的融合,和上文两个基于CNN融合的模型不同,BEVerse由于使用的帧数比较多,采用3D卷积和3D池化对所有帧进行融合.3D卷积是处理连续帧信息的一个重要方式.分辨率使用的是128*128.</li></ul><h2 id="基于transformer的图像特征融合"><a href="#基于transformer的图像特征融合" class="headerlink" title="基于transformer的图像特征融合"></a>基于transformer的图像特征融合</h2><p>前文所介绍的模型有一个共同点,即都是在bev空间下对bev feature做时序融合.由于每一帧的bev feature只有一个,所以bev空间下的时序融合比较简单直接,可直接通过warp的方式将前序帧与当前帧融合,而且需要的缓存空间也比较小.但这种方法也有不足之处,</p><ul><li>一是会带来可融合区域的浪费,丢失有用信息,</li><li>二是在融合过程中只能使用固定权重,无法自适应地调整前序帧权重,</li><li>三是可用的时序区间也比较短,因为时序过长,可融合区域会更小,难以起到加强作用.<strong>BEVFormer的实验中,融合3帧,也就是2s的时序区间效果达到了峰值.</strong></li></ul><h3 id="UniFusion-浙江大学-大疆-上海AI-lab"><a href="#UniFusion-浙江大学-大疆-上海AI-lab" class="headerlink" title="UniFusion(浙江大学,大疆,上海AI lab)"></a>UniFusion(浙江大学,大疆,上海AI lab)</h3><p><a href="https://arxiv.org/abs/2207.08536">论文链接</a><br><a href="https://github.com/cfzd/UniFusion">代码链接</a></p><p> <img src="/pic/time10.png" alt="网络结构图"></p><p>Uniformer解释了基于warp的融合方式为什么会带来信息丢失.如下图所示,图(b)的灰色部分是连续两帧实际可融合区域,图(a)的灰色部分是生成一定范围内的矩形的bev feature后实际融合的区域,可见融合范围大大缩小,所以很多有用的信息被浪费了.</p><p> <img src="/pic/time8.png" alt="不同视角"></p><p>为了更好地融合时序信息,可以不在bev空间通过warp的方式进行融合,而是把这一过程提前到<strong>图像空间</strong>,通过缓存前序帧的图像特征,并把前序帧的lidar2img参数,也就是相机外参转换到当前帧,那就<strong>等同于当前帧又多了很多个相机视角</strong>,同时可以看到更大范围的信息,图上图(c)所示.在这种架构下,多帧时间的融合和多视角空间的融合被统一起来了,所以模型命名为Uniformer.下图更加直观地展示了两种方法的区别:</p><p>  <img src="/pic/time9.png"></p><p>Uniformer架构可以解决上述warp方法的全部缺陷.</p><ul><li>第一点,它不造成信息浪费,可以融合当前帧和前序帧相机视角所能覆盖的所有区域,</li><li>第二点,它可以自适应地学习每个视角的权重,不区分当前和前序帧,</li><li>第三,只要缓存空间允许,它可以融合很长的时序区间.当然这种方法的<strong>劣势是需要缓存多视角特征,无法使用较大的分辨率</strong>,一般需要高倍下采样,最后再进行上采样.Uniformer为这种方法取名为”virtual views”即虚拟视角方法.</li></ul><p>在具体实现上,Uniformer的前序帧选取前6帧,时序对齐的方式如上文所述,通过外参转换的方式将前序帧变为当前帧的虚拟视角,然后做基于transformer的融合,包括self attention和cross attention,只是cross attention同时融合了时间和空间信息.最后还设计了self-regression自回归模块来融合多层transformer结果,最后得到bev feature,并指出这种方法也能达到类似于BEVFormer将前序帧和当前帧bev feature进行concate再融合的提升效果.bev feature分辨率采用50*50再用4倍上采样.实验效果对比如下图所示:</p>]]></content>
      
      
      <categories>
          
          <category> DL </category>
          
      </categories>
      
      
        <tags>
            
            <tag> bev </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>BEV感知总结</title>
      <link href="/2023/11/28/bev-reserch/"/>
      <url>/2023/11/28/bev-reserch/</url>
      
        <content type="html"><![CDATA[<h1 id="BEV感知总结"><a href="#BEV感知总结" class="headerlink" title="BEV感知总结"></a>BEV感知总结</h1><h2 id="参考论文和好用的工具箱："><a href="#参考论文和好用的工具箱：" class="headerlink" title="参考论文和好用的工具箱："></a>参考论文和好用的工具箱：</h2><p><a href="https://arxiv.org/abs/2209.05324">Delving into the Devils of Bird’s-eye-view Perception: A Review， Evaluation and Recipe(深入研究鸟瞰感知的魔咒：综述、评估和秘诀)</a><br><a href="https://github.com/OpenDriveLab/Birds-eye-view-Perception">BEV 感知论文和工具箱</a><br><a href="https://www.youtube.com/watch?v=j0z4FweCy4M">(2021) Tesla AI Day. Online</a><br><a href="https://zhuanlan.zhihu.com/p/597554089?utm_id=0">自动驾驶中的BEV感知与建图小记</a></p><p><a href="https://zhuanlan.zhihu.com/p/598164131">CaDDN论文+源码解读</a><br><a href="https://zhuanlan.zhihu.com/p/546194237?utm_id=0">GitNet: 基于几何先验的转换用于BEV分割</a><br><a href="https://blog.csdn.net/qq_39967751/article/details/126290811">Monocular 3D Object Detection with Depth from Motion (DfM)</a><br><a href="https://zhuanlan.zhihu.com/p/508794328">理解DD3D目标检测</a><br><a href="https://blog.csdn.net/weixin_41610241/article/details/126404303">CenterPoint:Center-based 3D Object Detection and Tracking (Based: KITTI)</a><br><a href="https://zhuanlan.zhihu.com/p/445975451">SPVCNN:使用稀疏点体素卷积搜索高效 3D 架构</a><br><a href="https://zhuanlan.zhihu.com/p/432149359">Multi-Level Fusion (CVPR 2018)：解耦单目3D任务的早期尝试</a><br><a href="https://zhuanlan.zhihu.com/p/597394576">UVTR:Unifying Voxel-based Representation with Transformer for 3D Object Detection</a><br><a href="https://blog.csdn.net/m0_63604019/article/details/126878682">PointPainting: Sequential Fusion for 3D Object Detection(3D物体检测的顺序融合)</a><br><a href="https://blog.csdn.net/qq_43456497/article/details/129112072">PointAugmenting: Cross-Modal Augmentation for 3D Object Detection总结</a><br><a href="https://zhuanlan.zhihu.com/p/416084515">MVFuseNet : Improving End-to-End Object Detection and Motion Forecasting through Multi-View Fusion of LiDAR Data</a><br><a href="http://681314.com/A/5A3VK5CZoJ">学习笔记之BEV模型学习小结</a><br><a href="https://zhuanlan.zhihu.com/p/597554089">自动驾驶中的BEV感知与建图小记</a></p><p>在感知任务的鸟瞰(BEV)中学习强大的表征是一种趋势，并引起了工业界和学术界的广泛关注。大多数自动驾驶算法的常规方法在前视或透视角中执行检测、分割、跟踪等。随着传感器配置变得越来越复杂，集成来自不同传感器的多源信息并在统一的视角中表示特征变得至关重要。BEV感知继承了几个优点，因为在BEV中表示周围场景是直观和融合友好的；并且在BEV中表示目标最适合于后续模块，如在规划和&#x2F;或控制中。</p><p>BEV感知的核心问题在于：</p><ul><li>(A)如何通过从透视角到BEV的视角转换来重建丢失的3D信息；</li><li>(B)如何在BEV网格中获取注释的真值 ；</li><li>(C)如何融合来自不同传感器和视角下的特征；</li><li>(D)如何在不同场景下适应和泛化算法。</li></ul><p>接下来回顾了关于Bev感知的最新工作，并对不同的解决方案进行了深入的分析。此外，还描述了业内几种BEV方法的系统设计。此外，还介绍了一整套实用指南，以提高Bev感知任务的性能，包括Camera、LiDAR和融合输入。最后，指出了该领域未来的研究方向。</p><h2 id="引言"><a href="#引言" class="headerlink" title="引言"></a>引言</h2><p>与2D视域中被广泛研究的前视角或透视角相比，Bev表示具有几个固有的优点。</p><ul><li>首先，它没有2D任务中常见的<strong>遮挡或缩放</strong>问题，识别有遮挡或交叉交通的车辆可以更好地解决。</li><li>此外，以BEV的形式表示目标或道路元素将有利于后续模块(例如规划、控制)的开发和部署。</li></ul><h2 id="概述"><a href="#概述" class="headerlink" title="概述"></a>概述</h2><p>根据输入的数据，将Bev感知研究主要分为三个部分–<strong>BevCamera、Bev LiDAR和Bev融合</strong>。图1描绘了Bev感知的总体情况。具体而言，</p><ul><li>Bev Camera指的是从周围多个Camera中检测或分割3D目标的纯视觉或以视觉为中心的算法；</li><li>Bev LiDAR描述了从点云输入进行检测或分割的任务；</li><li>Bev Fusion描述了来自Camera、LiDAR、GNSS、里程计、HD-Map、CAN-Bus等多个传感器输入的融合机制。</li></ul><p> <img src="/pic/BEV1.png" alt="图1"></p><p>如图1所示，对基本感知算法(分类、检测、分割、跟踪等)进行分组和分类。将自动驾驶任务分为三个层次，其中Bev感知的概念位于中间。基于传感器输入层、基础任务和产品场景的不同组合，某一种BEV感知算法可以相应地指示。</p><ul><li>例如，M2BEV和BEVFormer属于来自多个Camera的BEVCamera轨迹，以执行包括3D目标检测和BEV地图分割在内的多个任务。</li><li>BEVFusion在Bev空间设计了一种融合策略，可以同时从Camera和LiDAR输入执行3D检测和跟踪。</li><li>特斯拉发布了其系统pipeline，用于在L2高速公路导航和智能召唤中检测矢量空间(BEV)中的目标和车道线。</li></ul><h2 id="BEV感知的动机研究"><a href="#BEV感知的动机研究" class="headerlink" title="BEV感知的动机研究"></a>BEV感知的动机研究</h2><h3 id="意义重大"><a href="#意义重大" class="headerlink" title="意义重大"></a><strong>意义重大</strong></h3><p>众所周知，仅摄像解决方案和LiDAR解决方案之间存在巨大的性能差距。例如，截至2022年8月提交，一流的纯Camera和LiDAR方法在nuScenes数据集上的差距超过20%，在Waymo基准上的差距超过30%。这自然促使调查只有Camera的解决方案是否可以超越或与LiDAR方法平起平坐。</p><ul><li>从学术角度来看，设计基于Camera的pipeline以使其优于LiDAR的本质是<strong>更好地理解从2D外观输入到3D几何图形输出的视角转换过程</strong>。如何将Camera特征转换为几何表示，就像在点云中所做的那样，给学术界留下了一个有意义的影响。</li><li>在工业方面，将一套LiDAR设备安装到SDV(Software Defined Vehicles，软件定义汽车)中的成本很高；OEM(原始设备制造商，如福特、宝马等)更喜欢廉价且准确的软件算法部署。将仅有Camera的算法改进到LiDAR自然就属于这个目标，因为Camera的成本通常是LiDAR的十分之一。<ul><li>此外，基于摄像头的pipeline可以识别远距离目标和基于颜色的道路元素(例如红绿灯)，而这两者都是LiDAR方法所不能做到的。</li><li>基于Lidar的方法，BEV是最好的方案之一；不过最近的研究也表示，对于对Camera的输入，BEV仍有很大的进步空间。无论是Camera还是Lidar的数据都能够很好的映射到BEV空间，而且也能够更好的以一种统一的方式进行数据融合。</li></ul></li></ul><h3 id="空间-努力方向"><a href="#空间-努力方向" class="headerlink" title="空间(努力方向)"></a><strong>空间(努力方向)</strong></h3><ul><li>一个问题，Bev感知背后的主旨是从Camera和LiDAR输入中学习稳健和可泛化的特征表示。这在LiDAR分支中很容易，因为输入(点云)具有这样的3D属性。在Camera分支中，从单目或多视设置中学习3D空间信息是困难的。看到有一些尝试通过姿势估计[EPro-PnP]或时间运动[Dfm]来学习更好的2D-3D对应，但Bev感知背后的核心问题需要对原始传感器输入的&#x3D;&#x3D;深度估计进行实质性的创新&#x3D;&#x3D;。</li><li>另一个关键问题是如何在pipeline的早期或中期进行<strong>特征融合</strong>。大多数传感器融合算法将该问题视为简单的<strong>目标级融合或沿着通道的朴素特征拼接</strong>。如何&#x3D;&#x3D;从多通道输入中对齐和集成特征&#x3D;&#x3D;起着至关重要的作用，从而留下了广泛的创新空间。</li></ul><h3 id="准备工作"><a href="#准备工作" class="headerlink" title="准备工作"></a><strong>准备工作</strong></h3><p>由于Bev感知同时需要Camera和LiDAR，因此<strong>高质量的注释以及2D和3D目标之间的准确对齐</strong>是此类基准的两个关键评估。</p><ul><li>虽然Kitti[11]是全面的，在早期的自动驾驶研究中吸引了很多关注，但大规模和多样化的基准测试，如**Waymo[8]、nuScenes[7]、ArgoVerse[12]**，为验证Bev感知想法提供了坚实的平台。这些新提出的基准通常具有高质量的标签；场景多样性和数据量也在很大程度上扩大了。</li><li>至于算法方面，近年来见证了普通视觉的巨大发展，其中**Transformer[14]、VIT[15，16]、Masked Auto-encoders(MAE)[17]和CLIP[18]**等方法比传统方法获得了令人印象深刻的收益。相信，这些工作将有益于并激励BEV感知研究的伟大。</li></ul><p>基于以上三个方面的讨论，得出结论：<strong>BEV感知研究具有巨大的潜在影响，值得学术界和产业界长期大力关注。</strong></p><h2 id="3D感知中的背景"><a href="#3D感知中的背景" class="headerlink" title="3D感知中的背景"></a>3D感知中的背景</h2><p>接下来将回顾执行感知任务的传统方法，包括基于单目Camera的3D目标检测、基于LiDAR的3D目标检测和分割以及传感器融合策略。还介绍了3D感知中的主要数据集，如Kitti数据集[11]、nuScenes数据集[7]和Waymo Open数据集[8]。</p><h3 id="任务定义及相关工作"><a href="#任务定义及相关工作" class="headerlink" title="任务定义及相关工作"></a>任务定义及相关工作</h3><ul><li><p>基于单目Camera的3D目标检测。</p><ul><li>基于单目Camera的方法<strong>将RGB图像作为输入</strong>，并尝试预测每个目标的3D位置和类别。</li><li>单目3D检测的主 要挑战是RGB图像<strong>缺乏深度信息</strong>，因此这类方法需要对深度进行预测。由于从单幅图像估计深度是一个不适定的问题，通常基于单目Camera的方法的性能低于基于LiDAR的方法。</li></ul></li><li><p>激光雷达检测与分割。</p><ul><li>激光雷达使用3D空间中的<strong>一组点</strong>来描述周围环境，</li><li>这些点捕获了目标的几何信息。尽管缺乏颜色和纹理信息，感知范围有限，但由于<strong>深度先验</strong>，基于LiDAR的方法比基于Camera的方法有很大的优势。</li></ul></li><li><p>传感器融合。</p><ul><li>现代自动驾驶汽车配备了不同的传感器，如摄像头、激光雷达和雷达。每种传感器都有优缺点。Camera数据包含密集的颜色和纹理信息，但无法捕获深度信息。激光雷达提供了准确的深度和结构信息，但存在范围有限和稀疏性的问题。雷达比LiDAR更稀疏，但感知范围更长，可以捕获运动物体的信息。</li><li>理想情况下，传感器融合将推动感知系统的<strong>性能上限</strong>，然而如何融合来自不同模式的数据仍然是一个具有挑战性的问题。</li></ul></li></ul><h3 id="数据集和指标"><a href="#数据集和指标" class="headerlink" title="数据集和指标"></a>数据集和指标</h3><p>接下来介绍一些流行的自动驾驶数据集和常用的评估指标。表1总结了现行BEV感知基准的主要统计数据。通常，一个数据集由各种场景组成，每个场景在不同的数据集中具有不同的长度。总持续时间从几十分钟到数百小时不等。</p><p>对于Bev感知任务，3D包围框标注和3D分割标注是必不可少的，高清地图配置已成为主流趋势。它们中的大多数都可以在不同的任务中采用。达成共识，即需要具有多个模态和各种注释的传感器。发布更多类型的数据[7、12、24、25、33、39]，如IMU&#x2F;GPS和CAN-BUS。与Kaggle和EvalAI排行榜类似，公布了每个数据集的提交总数，以表明某个数据集的受欢迎程度。</p><p><img src="/pic/BEV2.png" alt="表1"></p><h4 id="数据集"><a href="#数据集" class="headerlink" title="数据集"></a>数据集</h4><ul><li><p>Kitti数据集。</p><ul><li>KITTI是2012年提出的一个开创性的自动驾驶数据集。它拥有7481张训练图像和7518张测试图像，用于3D目标检测任务。它也有从Velodyne激光扫描仪捕捉到的相应的点云。测试集分为3个部分：简单、中等和困难，主要根据边界框大小和遮挡级别。目标检测评价分为两类：三维目标检测评价和鸟瞰评价。Kitti是第一个针对多个自动驾驶任务的全面数据集，它吸引了社区的大量关注。</li></ul></li><li><p>Waymo数据集。</p><ul><li>Waymo Open DataSet v1.3分别在训练、验证和测试集中包含798、202和80个视频序列。每个序列有5个LiDAR和5个左、左、前、右、右、侧5个视角，图像分辨率为1920×1280像素或1920×886像素。Waymo规模庞大，形式多样。它随着数据集版本的不断更新而不断发展。每年，Waymo公开赛都会定义新的任务，并鼓励社区努力解决这些问题。</li></ul></li><li><p>NuScenes数据集。</p><ul><li>NuScenes数据集是一个包含两个城市1000个驾驶场景的大规模自动驾驶数据集。850个场景用于培训&#x2F;验证，150个场景用于测试。每一场戏都有20多秒长。它有40K个关键帧和整个传感器套件，包括6个摄像头，1个激光雷达和5个雷达。摄像机图像分辨率为1600×900。同时，发布了相应的HD-Map和CanBus数据，以探索多个输入的辅助。由于NuScenes提供了多样化的多传感器设置，因此它在学术文献中越来越受欢迎；数据规模没有Waymo的大，这使得在这个基准上快速验证想法变得高效。</li></ul></li></ul><h4 id="评估指标"><a href="#评估指标" class="headerlink" title="评估指标"></a>评估指标</h4><ul><li><p>Let-3D-APL</p><ul><li>在仅有摄像头的3D检测中，使用LET-3D-APL(Longitudinal Error Tolerant 3D Average Precision with Longitudinal Affinity Weight，具有纵向亲和权重的纵向误差容错 3D 平均精度)代替3D-AP作为度量。与基于并集的3D交集(IOU)相比，LET-3D-APL允许预测包围框在给定容差范围内的纵向定位误差。LET-3D-APL通过使用定位亲和力来衡量精度来惩罚纵向定位误差。LET-3D-APL的定义在数学上定义为：</li></ul><p>  <img src="/pic/BEV3.png" alt="公式"><br> 其中，PL(R)表示纵向亲和度加权精确值，p(r)表示调用r时的精确值，乘数al是被视为Tp(真正)的所有匹配预测的平均纵向亲和度。</p></li><li><p>mAP</p><ul><li>在二维目标检测中，平均平均精度(MAP)类似于著名的AP度量，但匹配策略被从IOU替换为BEV平面上的2D中心距离。AP在不同的距离阈值下计算：0.5米、1米、2米和4米。MAP是通过在上述阈值中对AP求平均来计算的。</li></ul></li><li><p>NDS</p><ul><li>NuScenes检测分数(NDS)是几个度量的组合：MAP、Mate(平均平移误差)、MASE(平均比例误差)、MAOE(平均方向误差)、MAVE(平均速度误差)和MAAE(平均属性误差)。通过使用上述指标的加权和来计算NDS。MAP的权重为5，其余的权重为1。在第一步中，TPerror被转换为TPcore，NDS定义：</li></ul><p>  <img src="/pic/BEV4.png" alt="公式"></p></li></ul><h2 id="BEV感知方法"><a href="#BEV感知方法" class="headerlink" title="BEV感知方法"></a>BEV感知方法</h2><p>主要从学术界和工业界对BEV认知的不同角度进行了详细的描述。根据输入方式的不同将BEV区分为三种设置，BEV Camera(仅摄像头3D感知)、Bev LiDAR和Bev Fusion，并总结了BEV感知的工业设计。</p><p>表2总结了基于输入数据和任务类型的BEV知觉文献分类。表3描述了多年来流行排行榜上3D目标检测和分割的性能收益。</p><p><img src="/pic/BEV5.png" alt="表2"></p><p><img src="/pic/BEV6.png" alt="表3"></p><h3 id="BEV-Camera"><a href="#BEV-Camera" class="headerlink" title="BEV Camera"></a>BEV Camera</h3><h4 id="通用pipeline"><a href="#通用pipeline" class="headerlink" title="通用pipeline"></a>通用pipeline</h4><p>如图2所示，一般的只有Camera的3D感知系统可以分为三个部分：2D特征提取模块、视角转换模块(可选)和3D解码器。</p><p><img src="/pic/BEV7.png" alt="图2"></p><ul><li>特征提取<ul><li>在2D特征提取中，2D感知中存在着大量的经验，可以在3D感知中考虑，以骨干预训练的形式。</li></ul></li><li>视角转换模块<ul><li>与2D感知系统有很大的不同。注意，并不是所有的3D感知方法都具有视角转换模块，并且一些方法直接根据2D空间中的特征来检测3D空间中的目标，如FCOS3D这样的单目3D检测其是直接从2D特征中预测3D信息。</li></ul></li><li>3D解码器<ul><li>接收2D&#x2F;3D空间中的要素，并输出3D边界框、Bev地图分割、3D车道关键点等3D感知结果。</li><li>大多数3D解码器来自基于LiDAR的方法[Voxelnet，SECOND]，其在Voxel空间&#x2F;Bev空间执行检测，但仍有一些Camera3D解码器利用2D空间[FCOS3D，SMOKE，DETR3D]的特征并直接回归3D目标的定位。</li></ul></li></ul><h4 id="View-Transformation-视角转换模块"><a href="#View-Transformation-视角转换模块" class="headerlink" title="View Transformation(视角转换模块)"></a>View Transformation(视角转换模块)</h4><p>在仅有Camera的3D感知中，视角转换模块是关键，因为它是构建3D数据和编码3D先验假设。最近的研究[M2BEV，BEVFormer，Mono3D，PersFormer，BEVDet，PETR，BEVDepth，Polarformer，3d-lanenet]集中于增强这一模块。将视角转换技术分为三大主流。</p><ul><li>第一种方法称为“2D-3D方法”，从2D图像特征开始，通过深度估计将2D特征“提升”到3D空间。</li><li>第二种是3D-2D方法，它起源于3D空间，通过3D到2D的投影映射将2D特征编码到3D空间。前两个流显式建模几何变换关系。</li><li>第三种方法被称为“纯网络方法”，它利用神经网络隐式地获取几何变换。图3给出了执行视角转换的概要路线图，下面将对其进行详细分析。</li></ul><p><img src="/pic/BEV8.png" alt="图3：视角转换的分类。在2D-3D方法中，基于LSS的方法[BEVFusion，CaDDN，BEVDet，BEVDepth，LSS，BEVDet4d，BEVFusion2]根据2D特征预测每个像素的深度分布。在3D-2D方法的基础上，基于单应矩阵的方法[BEVFormer，PersFormer，GitNet]假定稀疏的3D样本点，并通过摄像机参数将其投影到2D平面上。基于纯网络的方法[94，Fishing net，NEAT，97，98]采用MLP或变换来隐式建模从3D空间到2D平面的投影。"></p><ul><li><p>2D-3D方法</p><ul><li>首先由LSS提出，它预测2D特征上的网格深度分布，然后基于深度将2D特征提升到Voxel空间，并执行类似于基于LiDAR的方法的下游任务。这一过程可以表述为：</li></ul><p> <img src="/pic/BEV9.png" alt="公式"></p><ul><li>伪LiDAR方法[Pseudo-lidar，Pseudo-lidar++]从预先训练的深度估计模型中提取深度信息，并且提升过程发生在2D特征提取之前。</li><li>在LSS之后，还有另一项工作遵循了与面元分布相同的思想，即CaDDN(Categorical depth distribution network for monocular 3d object detection)。CADDN使用类似的网络来预测分类深度分布，将Voxel空间特征压缩到BEV空间，并在最后执行3D检测。</li><li>LSS和CaDDN的主要区别在于，<strong>CaDDN使用深度真值信息来指导其分类深度分布预测</strong>，从而具有从2D空间提取3D信息的优越的深度网络。后续工作，例如**BEVDet及其时间版本BEVDet4D、BEVDepth、BEVFusion和其他[Dsgn，DD3D，LIGA-Stereo]**。</li></ul></li><li><p>3D-2D方法</p><ul><li>逆透视映射(IPM)有条件地提出了从3D空间到2D空间的投影，假设3D空间中的对应点位于水平面上。这样的变换矩阵可以从Camera的内部和外部参数数学推导出来。一系列的工作是应用IPM将元素从透视角转换为鸟瞰图，无论是前处理还是后处理。在视角转换方面，OFT-Net首次提出了从3D到2D的特征投影方法。OFT-Net形成一个均匀分布的3DVoxel特征网格，通过聚集来自相应投影区域的图像特征来填充Voxel。然后，通过对Voxel特征进行垂直求和来获得正交Bev特征地图。</li><li>最近，受特斯拉感知系统技术路线图的启发，3D-2D几何投影和神经网络的组合流行起来[BEVFormer，PersFormer，DETR3D，GitNet]。请注意，变压器体系结构中的交叉注意机制在概念上满足了这种几何投影的需要，如以下所示：</li></ul><p> <img src="/pic/BEV10.png" alt="公式"></p><ul><li>其中q、k、v表示查询、键和值，Pxyz是Voxel空间中的预定义锚点，利用Camera参数将Pxyz投影到图像平面，以实现模型的快速收敛。为了获得稳健的检测结果，BEVFormer[4]利用Transformer中的交叉注意机制来增强3D-2D视角转换的建模。其他人Imvoxelnet和MVFCOS3D++减轻了网格取样器的压力，以有效地加速这一过程。尽管如此，<em>这些方法在很大程度上依赖于Camera参数的精确度，这些参数很容易受到长时间驾驶的波动的影响。</em></li></ul></li><li><p>基于纯网络的方法</p><ul><li>无论是2D方法还是3D-2D方法，这两种方法都引入了几何投影中包含的遗传感应偏差。相反，一些方法倾向于利用神经网络来隐式表示Camera投影关系。许多BEV地图分割工作[Hdmapnet，Translating images into maps，CVT]使用多层感知器或Transformer[Attention is all you need]体系结构来隐式地建模3D-2D投影。<strong>VPN引入了视角关系模块–多层感知器(MLP)，用于通过处理来自所有视角的输入来产生地图-视角特征，从而实现了跨多个视角的共享特征表示的获取。HDMapNet采用MLP架构来执行特征地图的视角转换</strong>。BEVSegFormer构建密集的BEV查询，通过MLP从查询特征中直接预测其二维投影点，然后使用可变形注意力更新查询嵌入。CVT将图像特征与基于摄像机内外参数的摄像机感知位置嵌入相结合，并引入了交叉视角注意模块来产生地图视角表示。某些方法不显式构建BEV特征。PETR将源自摄像机参数的3D位置嵌入集成到2D多视角特征中。这一集成使稀疏查询能够通过普通的交叉关注直接与3D位置感知图像功能交互。</li></ul></li></ul><h4 id="关于BEV和透视方法的讨论"><a href="#关于BEV和透视方法的讨论" class="headerlink" title="关于BEV和透视方法的讨论"></a>关于BEV和透视方法的讨论</h4><p>在纯相机3D感知的最初阶段，主要关注的是如何从透视图（也称为2D空间）预测3D对象的定位。这是因为2D感知在那个阶段发展得很好[Faster R-CNN，Fast R-CNN，Mask R-CNN，FCOS]，如何使2D检测器具有感知3D场景的能力成为主流方法[Probabilistic and geometric depth: Detecting objects in perspective，FCOS3D，SMOCK，Multi-Level Fusion based 3D Object Detection from Monocular Images]。后来，一些研究涉及到Bev表示，因为在这种观点下，很容易解决3D空间中相同大小的目标由于距离Camera的距离而在图像平面上具有非常不同的大小的问题。这一系列工作要么预测深度信息，要么利用3D先验假设来补偿Camera输入中3D信息的损失<br>虽然最近基于BEV的方法席卷了3D感知界，但值得注意的是，这种成功主要得益于三个方面。</p><ul><li>第一个原因是流行的nuScenes数据集[7]，它具有多摄像机设置，非常适合在BEV下应用多视角特征聚合。</li><li>第二个原因是，大多数纯Camera的Bev感知方法都从基于LiDAR的方法[Voxelnet，Pointpillars，CenterPoint，SPVCNN，SECOND，PointNet，PointNet++]中获得了很多帮助，表现在检测头的形式和相应的损耗设计上。</li><li>第三个原因是单目方法[FCOS3D，SMOKE，Multi-level fusion based 3d object detection from monocular images]的长期发展使基于BEV的方法蓬勃发展，这是处理透视视角中特征表示形式的一个很好的起点。核心问题是如何从2D图像中重建丢失的3D信息。为此，基于BEV的方法和透视方法是解决同一问题的两种不同方式，它们并不相互排斥。</li></ul><h3 id="BEV-LiDAR"><a href="#BEV-LiDAR" class="headerlink" title="BEV LiDAR"></a>BEV LiDAR</h3><h4 id="通用pipeline-1"><a href="#通用pipeline-1" class="headerlink" title="通用pipeline"></a>通用pipeline</h4><p>图4描绘了Bev LiDAR感知的一般pipeline。</p><ul><li>提取的点云特征被转换为BEV特征地图。常见的检测头产生3D预测结果。</li><li>在特征提取部分，主要有两个分支将点云数据转换为BEV表示。根据pipeline的顺序，将这两个选项分别称为pre-BEV and post-BEV，以表明骨干网络的输入是来自3D表示还是来自BEV表示。</li></ul><p><img src="/pic/BEV11.png" alt="图4"></p><h4 id="Pre-BEV特征提取"><a href="#Pre-BEV特征提取" class="headerlink" title="Pre-BEV特征提取"></a>Pre-BEV特征提取</h4><ul><li>除了基于点的方法对原始点云进行处理外，基于Voxel的方法将点Voxel化为离散的网格，通过离散化连续的三维坐标来提供更高效的表示。基于离散Voxel表示，3D卷积或3D稀疏卷积可用于提取点云特征。</li><li>相关方法。<ul><li>VoxelNet堆叠多个Voxel特征编码(VFE)层，以将Voxel中的点云分布编码为Voxel特征。</li><li>SECOND在处理Voxel表示时引入了稀疏卷积，大大降低了训练和推理速度。</li><li>CenterPoint是一款功能强大的基于中心的无锚3D检测器。PV-RCNN结合了点和Voxel分支来学习更具区分性的点云特征。</li><li>PV-RCNN结合了点和Voxel分支来学习更具区分性的点云特征。</li><li>SA-SSD设计了一种辅助网络，将骨干网络中的Voxel特征转换回点级表示，以显式地利用三维点云的结构信息，减少下采样的损失。</li><li>Voxel R-CNN采用三维卷积主干提取点云特征。然后在BEV上应用2D网络来提供object proposals，这些object proposals通过提取的特征进行细化。它获得了与基于点的方法相当的性能。</li><li>Object DGCNN将3D目标检测任务建模为BEV中动态图上的消息传递。在将点云转换为BEV特征地图后，预测查询点迭代地从关键点采集BEV特征。</li><li>Votr引入了局部注意、扩展注意和快速Voxel查询，以实现对大背景信息的多个Voxel的注意机制。</li><li>SST将提取的Voxel特征视为Query，然后在非重叠区域应用稀疏区域注意和区域摆动，以避免对基于Voxel的网络进行下采样。</li><li>AFDetV2通过引入KeyPoint辅助监控和多任务头部，形成了单级无锚点网络。</li></ul></li></ul><h4 id="Post-BEV特征提取"><a href="#Post-BEV特征提取" class="headerlink" title="Post-BEV特征提取"></a>Post-BEV特征提取</h4><p>由于3D空间中的Voxel稀疏且不规则，应用3D卷积的效率很低。对于工业应用，可能不支持诸如3D卷积之类的运算符；需要合适且高效的3D检测网络。</p><ul><li>MV3D是第一种将点云数据转换为BEV表示的方法。在将点离散到BEV网格中后，根据网格中的点获得高度、强度和密度特征来表示网格特征。由于Bev网格中的点很多，在此处理过程中，信息的损失是相当大的。</li><li>其他作品PIXOR、Hdnet、BirdNet、Rt3d、Yolo3d、Complex-YOLO遵循类似的模式，使用Bev网格中的统计数据来表示点云，例如最大高度和平均强度。</li><li>PointPillars首先引入了柱的概念，柱是一种高度不受限制的特殊Voxel。它利用PointNet的简化版本来学习柱子中点的表示。然后，编码后的特征可以由标准2D卷积网络和检测头处理。虽然PointPillars的性能不如其他3D主干，但它及其变体具有很高的效率，因此适合工业应用。</li></ul><h4 id="总结讨论"><a href="#总结讨论" class="headerlink" title="总结讨论"></a>总结讨论</h4><p>将点云数据转换为任何形式的表示不可避免地会导致信息丢失。</p><ul><li>在Bev前特征提取中，最先进的方法利用细粒度的Voxel，保留了点云数据中的大部分3D信息，从而有利于3D检测。作为权衡，它需要较高的内存消耗和计算成本。</li><li>在Bev后特征提取中，将点云数据直接转换为BEV表示，避免了在3D空间中的复杂操作。随着高度维度的压缩，不可避免地会产生巨大的信息损失。最有效的方法是使用统计方法来表示BEV特征图，但其结果较差。基于PointPillars的方法[45]平衡了性能和成本，成为工业应用的流行选择。如何处理性能和效率之间的权衡成为基于LiDAR的应用面临的重大挑战。</li></ul><h3 id="BEV-Fusion"><a href="#BEV-Fusion" class="headerlink" title="BEV Fusion"></a>BEV Fusion</h3><h4 id="通用pipeline-2"><a href="#通用pipeline-2" class="headerlink" title="通用pipeline"></a>通用pipeline</h4><p>逆透视映射(IPM)提出了利用摄像机内部和外部矩阵的几何约束将像素映射到Bev平面上的方法。尽管它由于平地假设而不准确，但它提供了在BEV中统一图像和点云的可能性。Lift-Splat-Shots(LSS)是第一个预测图像特征深度分布的方法，它引入神经网络来学习Camera到激光雷达的不适定变换问题。其他工作[BEVFormer，UVTR]发展了不同的方法来进行视角转换。给定从透视角到BEV的视角转换方法，</p><p>图5展示了两种典型的BEV融合算法pipeline设计，适用于学术界和工业界。主要区别在于2D到3D的转换和融合模块。在透视变换pipeline(A)中，不同算法的结果首先被转换到3D空间，然后使用先验规则或手工规则进行融合。BEV感知pipeline(B)首先将PV特征转换为BEV，然后融合特征以获得最终预测，从而保留大多数原始信息并避免手工设计，在转换为BEV表示后，将来自不同传感器的特征映射进行融合。在BEV表示中也可以引入时间和自我运动信息。</p><p><img src="/pic/BEV12.png" alt="图5"></p><h4 id="LiDAR-camera-Fusion"><a href="#LiDAR-camera-Fusion" class="headerlink" title="LiDAR-camera Fusion"></a>LiDAR-camera Fusion</h4><ul><li>两个同名的BEVFusion[BEVFusion: Multi-task multi-sensor fusion withunified bird’s-eye view representation，BEVFusion: A simple and robust lidar-camera fusion framework]从不同的方向探索了Bev中的融合。由于Camera到激光雷达的投影[PointPainting，PointAugmenting]抛弃了Camera特征的语义密度，前者BEVFusion设计了一种高效的Camera到Bev转换方法，该方法高效地将Camera特征投影到Bev中，<strong>然后使用卷积层将其与激光雷达Bev特征融合</strong>。后者BEVFusion将BEV融合作为保持感知系统稳定性的鲁棒性主题，它<strong>将摄像头和激光雷达功能编码到同一BEV中，以确保摄像头和激光雷达流的独立</strong>，这种设计使感知系统能够在传感器故障时保持稳定性。</li><li>除了BEVFusion之外，UVTR在没有高度压缩的特定于模式的Voxel空间中表示不同的输入模式，以避免语义歧义并实现进一步的交互。通过将每个视角的图像特征变换到为每个图像生成深度分布的预定义空间来构造图像Voxel空间。点Voxel空间是使用常见的3D卷积网络来构造的。然后在两个Voxel空间之间进行跨通道交互，以增强特定于通道的信息。</li></ul><h4 id="Temporal-Fusion-时域融合"><a href="#Temporal-Fusion-时域融合" class="headerlink" title="Temporal Fusion(时域融合)"></a>Temporal Fusion(时域融合)</h4><ul><li>时间信息在推断物体的运动状态和识别遮挡方面起着重要作用。BEV提供了连接不同时间戳中的场景表示的理想桥梁，因为BEV特征地图的中心位置持续到EGO CAR。</li><li>MVFuseNet同时利用Bev和Range视角进行时间特征提取。其他文献FIERY、BEVerse、BEVDet4D使用自运动将先前的BEV特征与当前坐标对齐，然后融合当前BEV特征以获得时间特征。</li><li>BEVDet4D使用空间对齐操作将先前的特征图与当前帧进行融合，然后连接多个特征图。</li><li>BEVFormer和UniFormer采用了一种软方法来融合时间信息。注意力模块用于分别从先前的BEV特征图和先前的帧中融合时间信息。</li><li>关于自我汽车的运动，注意模块在不同时间戳的表示中的位置也被自我运动信息校正。</li></ul><h4 id="总结与讨论"><a href="#总结与讨论" class="headerlink" title="总结与讨论"></a>总结与讨论</h4><p>由于图像在透视坐标系中，而点云在三维坐标系中，两种模式之间的空间对齐成为一个至关重要的问题。</p><ul><li>虽然利用几何投影关系将点云数据投影到图像坐标上很容易，但点云数据的稀疏性使得提取信息丰富的特征变得困难。相反，由于透视角中缺乏深度信息，将透视角中的图像转换到3D空间将是一个不适定的问题。基于先验知识，前人的工作，如IPM和LSS，使得将透视角中的信息转换为BEV成为可能，为多传感器和时间融合提供了统一的表示。</li><li>在Bev空间融合激光雷达和Camera数据为3D检测任务提供了令人满意的性能。这种方法还保持了不同模式的独立性，这为构建更稳健的感知系统提供了机会。</li><li>对于时间融合，通过考虑自我运动信息，不同时间戳中的表示可以直接在BEV空间中融合。由于Bev坐标与3D坐标一致，通过监控控制和运动信息可以很容易地获得对自我运动的补偿。考虑到鲁棒性和一致性，BEV是多传感器和时间融合的理想表示。</li></ul>]]></content>
      
      
      <categories>
          
          <category> DL </category>
          
      </categories>
      
      
        <tags>
            
            <tag> bev </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>BEV感知经验性的评估和可能改进的方法</title>
      <link href="/2023/11/26/bev-evalution/"/>
      <url>/2023/11/26/bev-evalution/</url>
      
        <content type="html"><![CDATA[<h1 id="BEV感知经验性的评估和可能改进的方法"><a href="#BEV感知经验性的评估和可能改进的方法" class="headerlink" title="BEV感知经验性的评估和可能改进的方法"></a>BEV感知经验性的评估和可能改进的方法</h1><p>总结了在各种基准上实现最佳结果的技巧和最有用的实践。建立在BEVFormer之上的BEVFormer++，用于仅用于相机检测，以及从SPVCNN派生的Voxel-SPVCNN用于LiDAR分割。这些实践经验可以作为无缝集成到其他BEV感知模型和评估效果的参考。基于Camera和基于LiDAR的环境下的数据增强，高效的BEV编码器设计，感知头和损失函数族，有用的测试时间增强(TTA)和集成策略等。</p><h2 id="数据增强"><a href="#数据增强" class="headerlink" title="数据增强"></a>数据增强</h2><p>数据增强在增强感知模型的稳健性和泛化能力方面起着至关重要的作用。通过综合扩展训练数据集的多样性，可以提高模型处理真实场景变化的能力。某些非仿射变换很难应用于图像或BEV空间的数据增强。例如，copy-paste和Mosaic等方法存在问题，因为它们可能会导致图像语义和3D空间信息之间的不一致。</p><h3 id="BEVCamera-Camera-only-Detection"><a href="#BEVCamera-Camera-only-Detection" class="headerlink" title="BEVCamera (Camera-only) Detection"></a>BEVCamera (Camera-only) Detection</h3><p>用于二维识别任务的常见图像数据增强适用于基于相机的Bev感知任务。一般来说，可以将增强分为仅涉及颜色变化的静态增强和移动像素的空间变换。</p><ul><li>基于颜色变化的增强效果可以直接应用。</li><li>对于涉及空间变换的增强，除了相应地变换ground truth之外，相机参数的校准也是必要的。最近仅使用相机的方法中采用的常见增强方法有颜色抖动、翻转、调整大小、旋转、裁剪和栅格蒙版。</li></ul><p>在Bev感知中有两种翻转图像的方法，称之为ImageLevel翻转和Bev-Level翻转。在图像级翻转期间，翻转图像并调整相机内部参数，而相机外部参数和GT框保持不变。这将保留3D到2D的投影关系。请注意，图像级翻转仅增强了2D特征提取，不会对与BEV关联的后续模块产生任何影响。</p><ul><li>在Bev级翻转中，翻转图像并对称地重新排列多视角图像，例如将左前摄像头切换到右前。保留了重叠区域的相干性。在调整外部参数时，摄影机内部参数保持不变，并且翻转Bev平面上的GT长方体。Bev级翻转提升了整个Bev感知模型。</li></ul><p>在BEVFormer++中，使用了颜色抖动、翻转、多尺度调整和网格掩码。输入图像按0.5到1.2之间的系数缩放，按0.5的比率翻转；使用正方形蒙版随机遮罩总面积的最大30%。值得注意的是，BEVFormer++采用BEV级翻转。相关的见表4 ID6实验，表明数据增强对提高3D模型的性能起着至关重要的作用。由于BEVFormer使用序列输入，因此它确保输入增强后序列的每一帧的转换是一致的。</p><h3 id="LiDAR-Segmentaion"><a href="#LiDAR-Segmentaion" class="headerlink" title="LiDAR Segmentaion"></a>LiDAR Segmentaion</h3><p>与检测任务不同，重数据增强可以应用于分割任务，包括随机旋转、缩放、翻转和点平移。</p><ul><li>对于随机旋转，从[0，2π)的范围中拾取一个角度，将旋转应用于x-y平面上的每个点。</li><li>缩放从[0.9，1.1]的范围中选择比例因子，然后在点云坐标上相乘。</li><li>随机翻转沿X轴、Y轴或同时沿X和Y轴进行。</li><li>对于随机平移，每个轴的偏移与正态分布分开采样，平均值为0，标准差为0.1。</li></ul><p>除了坐标和反射率之外，还可以利用额外的信息来提高模型的性能。</p><ul><li>Painting[PointPainting，PointAugmenting]是利用图像信息增强点云数据的常用技术。对于未标注的图像数据，通过将点云标签投影到相应的图像上并对稀疏标注进行加密化，从带标注的点云数据中获得图像上的语义标签。训练图像模型以提供2D语义分割结果。然后，将预测的语义标签绘制为一个热点向量来指向云数据，作为表示图像语义信息的附加通道。</li><li>此外，还可以使用时间信息，因为自动驾驶中的数据集通常是按顺序收集的。过去的连续帧与当前帧串联。附加一个通道来表示不同帧的相对时间信息。为了减少点数，应用了一个小的体素化网络。然后，将体素作为点作为的模型的输入。</li></ul><h2 id="BEV-Encoder"><a href="#BEV-Encoder" class="headerlink" title="BEV Encoder"></a>BEV Encoder</h2><h3 id="BEVCamera-BEVFormer"><a href="#BEVCamera-BEVFormer" class="headerlink" title="BEVCamera: BEVFormer++"></a>BEVCamera: BEVFormer++</h3><p>BEVFormer++有多个编码层，每个编码层都遵循传统的transformers结构，除了三个定制设计，即BEV查询、空间交叉注意和时间自我注意(BEV queries, spatial cross-attention, and temporal self-attention)。具体来说，BEV查询是网格形状的可学习参数，旨在通过注意机制从多camera视图查询BEV空间中的特征。空间交叉注意和时间自我注意是与BEV查询一起工作的关注层，用于从多camera图像中查找和聚集空间特征，以及从历史BEV特征中查找和聚集时间特征。</p><ul><li>在推断过程中，在时间戳t，将多个摄像头图像馈送到骨干网络(例如，ResNet-101)，并获得不同摄像头视图的特征Ft。同时，保留了之前时间戳t−1处的BEV特征Bt−1。<ul><li>在每个编码层中，首先使用BEV查询Q通过时间自注意从先前的BEV特征Bt−1中查询时间信息。</li><li>然后，使用BEV查询Q通过空间交叉注意从多camera特征Ft中查询空间信息。</li><li>在前馈网络[transformer]之后，编码层生成精细化的BEV特征作为下一个编码层的输入。</li><li>在六个堆叠编码层之后，生成当前时间戳t处的统一BEV特征Bt。</li><li>3D检测头和地图分割头以BEV特征Bt为输入，对3D包围盒和语义地图等感知结果进行预测。</li></ul></li></ul><p>为了提高BEV编码器的特征质量，主要从以下三个方面进行了讨论。</p><p> <img src="/pic/BEV21.png" alt="表4"></p><ul><li>(A)2D Feature Extractor(2D特征抽取器)。<ul><li>在2D感知任务中改进主干网络特征表征质量也最有可能提高BEV任务的质量。<ul><li>在图像主干中，采用了在大多数2D感知任务中广泛使用的特征金字塔。如表4所示二维特征抽取器的结构设计，如最先进的图像特征抽取器[Swin transformer]、全局信息交互[Cornernet]、多层特征融合[FPN,Deformable detr]等都有助于更好地表示Bev感知的特征。除了结构设计，<strong>监督主干的辅助任务对BEV感知的性能也很重要</strong>，将在下面讨论。</li></ul></li></ul></li><li>(B)View transformation(视图变换)。<ul><li>该变换接受图像特征并将它们重新组织到BEV空间。超参数，包括<strong>图像特征的采样范围和频率，以及Bev分辨率</strong>，对于Bev感知的性能是至关重要的。<ul><li>采样范围决定图像后面的视锥的多少将被采样到Bev空间。默认情况下，此范围等于LiDAR注释的有效范围。当效率具有更高的优先级时，由于在大多数情况下它只包含不重要的信息，例如天空，所以观察锥体的上部z轴部分可能会受到损害。</li><li>采样频率决定了图像特征的效用。较高的频率确保了该模型以较高的计算成本准确地对每个BEV位置的相应图像特征进行采样。</li><li>Bev分辨率决定了Bev特征的表示颗粒度，其中每个特征都可以精确地追溯到世界坐标中的栅格。为了更好地表示交通灯和行人等小尺度对象，需要高分辨率。相关实验在表格4 ID 2&amp;3.</li><li>在视图变换中，许多BEV感知网络中也存在特征提取操作，例如卷积块或变换器块。在BEV空间中加入更好的特征提取子网络也可以提高BEV感知性能。</li></ul></li></ul></li><li>(C)Bev时序融合(Temporal BEV fusion)。<ul><li>在给定Bev特征结构的情况下，Bev空间中的时间融合通常利用自我姿态信息来对齐时间Bev特征。然而，在这个对准过程中，其他代理的运动没有显式建模，需要通过模型进行额外的学习。</li><li>因此，为了加强对其他运动主体特征的融合，在进行时间融合时增加交叉注意的感知范围是合理的。例如，可以在可变形的注意力模块中放大注意力偏移量的核大小，或者使用全局注意力。在表4中可以观察到相关的改进ID%1。</li></ul></li></ul><h3 id="BEVLiDAR-Voxel-SPVCNN"><a href="#BEVLiDAR-Voxel-SPVCNN" class="headerlink" title="BEVLiDAR: Voxel-SPVCNN"></a>BEVLiDAR: Voxel-SPVCNN</h3><p>由于粗体素化和激进的下采样，现有的3D感知模型对于识别小实例并不理想。</p><ul><li>SPVCNN[83]在基于体素的分支中使用Minkowski UNET[119]。为了保持点云的分辨率，使用了额外的基于点的分支，而不需要进行下采样。基于点的分支和基于体素的分支的特征将在网络的不同阶段彼此传播。</li><li>通过对原始SPVCNN[83]进行两个有效的修改而提出了Voxel-SPVCNN。<ul><li>与简单地对原始输入特征进行体素化相比，采用轻量级的3层MLP来提取点特征，然后进行体素化处理。</li><li>此外，基于点的分支的输入被voxel-as-point分支所代替。该分支的网络结构仍然是MLP；但输入被替换为体素。</li><li>Voxel-SPVCNN的效率更高，因为基于点的分支的计算量大大减少，特别是在输入是多扫描点云的情况下。模型体系结构的改变带来了1.1Mou的改进(见表5 ID 7)。</li></ul></li></ul><h2 id="Loss"><a href="#Loss" class="headerlink" title="Loss"></a>Loss</h2><h3 id="BEVCamera-only-Detection"><a href="#BEVCamera-only-Detection" class="headerlink" title="BEVCamera-only Detection"></a>BEVCamera-only Detection</h3><ul><li><p>BEV特征表示的一个多用途的好处是使模型能够在2D和3D目标检测中提出损失来进行训练。</p><ul><li>如表4 ID 16-20，当利用不同的检测头设计时，可以通过最小的修改来转移相应的损失，例如调整损失权重。</li></ul></li><li><p>除了3D目标的训练损失外，辅助损失在纯camera的Bev检测中也起着重要作用。</p><ul><li>一种类型的辅助损失是在2D特征提取器之上添加2D检测损失。这种监督增强了二维图像特征的局部化，进而有助于通过视觉变换在BEV感知中提供3D表示。在表4 ID 4中可以观察到利用这种辅助损耗的一个例子。</li><li>另一种辅助损失是深度监督[BEVDepth]。当利用LiDAR系统产生的地面真实深度时，可以提高Bev感知的隐含深度估计能力，以获得准确的三维目标定位。</li><li>这两项辅助任务都可以在训练期间应用，以提高成绩。通常采用2D检测或深度预训练骨干作为初始权重[BEVformer，DETR3D]。</li></ul></li></ul><h3 id="LiDAR-segmentation"><a href="#LiDAR-segmentation" class="headerlink" title="LiDAR segmentation"></a>LiDAR segmentation</h3><p>与传统的交叉熵损失不同，Geo损失[139]和Lovasz损失[140]被用来训练所有模型。为了更好地区分不同类别的边界，Geo Lost对细节丰富的体素有很强的响应性。Lovasz Lost是一种可区分的联合交集(IOU)损失，以缓解阶级失衡问题。它将模型性能提高了0.6 Mou，如表5 ID 2。</p><h2 id="展望"><a href="#展望" class="headerlink" title="展望"></a>展望</h2><p>最大的挑战和未来的努力可能是：</p><ul><li>(A)如何设计更准确的深度估计器；</li><li>(B)如何在一种新的融合机制中更好地对齐来自多个传感器的特征表示；</li><li>(C)如何设计一个无参数网络，使得算法性能可以自由地进行姿势变化或传感器位置，从而在不同的场景中实现更好的泛化能力；</li><li>(D)如何结合基础模型的成功知识来促进BEV感知。</li></ul><p>下面详细分开讨论</p><h3 id="Depth-Estimation-深度估计"><a href="#Depth-Estimation-深度估计" class="headerlink" title="Depth Estimation(深度估计)"></a>Depth Estimation(深度估计)</h3><p>基于视觉的Bev感知的核心问题在于准确的深度估计，因为该任务是在3D环境中执行的。目前解决深度预测的方法是</p><ul><li>(A)伪LiDAR生成；</li><li>(B)将特征从2D提升到3D对应；</li><li>(C)LiDAR相机蒸馏；</li><li>(D)双目视差或时间运动。<br>这些方向中的任何一个或组合都是有希望的。为了保证更好的性能，大量的监控数据也是至关重要的[80]。</li></ul><p>另一个有趣而重要的方向是如何在训练过程中利用LiDAR信息(例如，作为深度监控)，而在推理过程中只输入视觉输入。这对OEM(制造商)来说是非常有利的，因为我们经常从多个来源获得方便的大量培训数据，但出于部署考虑，发货产品上只提供摄像头输入。</p><h3 id="Fusion-Mechanism-融合机制"><a href="#Fusion-Mechanism-融合机制" class="headerlink" title="Fusion Mechanism(融合机制)"></a>Fusion Mechanism(融合机制)</h3><p>根据融合模块在pipeline中的位置，到目前为止，大多数融合方法可以分为早期融合、中期融合或晚期融合。</p><ul><li>传感器融合算法最简单的设计是将来自相机和激光雷达的两组特征分别连接起来。然而，如何“对齐”来自不同来源的特性是至关重要的。<ul><li>这意味着：(A)相机的特征表示在3D几何空间而不是2D环境中被适当地描述；</li><li>(B)3D空间中的点云与2D域中的对应点云具有准确的对应关系，这意味着LiDAR和相机之间的软和&#x2F;或硬同步得到了精细的保证。</li></ul></li><li>在上述前提条件的基础上，如何设计出优雅的融合方案需要社区更多的关注。<ul><li>这一部分的未来努力可能是：(A)利用自我和&#x2F;或交叉注意在Transformer中整合来自各种模式的特征表示；</li><li>(B)从一般的多模式文献中获得的知识也可能是有利的，例如，CLIP formulation中的文本-图像对的哲学[18]可以启发自动驾驶领域不同传感器的信息集成。</li></ul></li></ul><h3 id="Parameter-free-Design-to-Improve-Generalization-提高泛化能力的无参数设计"><a href="#Parameter-free-Design-to-Improve-Generalization-提高泛化能力的无参数设计" class="headerlink" title="Parameter-free Design to Improve Generalization(提高泛化能力的无参数设计)"></a>Parameter-free Design to Improve Generalization(提高泛化能力的无参数设计)</h3><p>BEV感知中最大的挑战之一是领域适应。一个数据集中的训练模型在另一个数据集中的表现和泛化程度。人们负担不起高昂的成本(培训、数据、注释等)在每一个数据集中的算法。</p><ul><li>由于Bev感知本质上是对物理世界的3D重建，我们认为一个<strong>好的探测器必须与相机参数捆绑在一起</strong>，例如，外参矩阵。不同的基准有不同的摄像机&#x2F;传感器设置，对应于物理位置、重叠区域、视场(视场)、失真参数等。这些因素都会导致将良好性能从一个场景转移到另一个域的(极端)困难。<ul><li>为此，它敦促我们<strong>将网络与摄像机参数分离</strong>，使特征学习独立于外部和&#x2F;或内部矩阵。学术界(extrinsic free，[190])和产业界(rectify module修正模块，[6])在这个方向上都有一些有趣的工作。尽管如此，这并不是一件微不足道的事情，作为未来的工作，最好从社区中进行更多的调查。<strong>无参数设计对于解决实际应用中由于道路颠簸和摄像机不稳定造成的检测不准确具有很强的鲁棒性。</strong></li></ul></li></ul><h3 id="Foundation-Models-to-Facilitate-BEV-Perception-促进BEV感知的基础模型"><a href="#Foundation-Models-to-Facilitate-BEV-Perception-促进BEV感知的基础模型" class="headerlink" title="Foundation Models to Facilitate BEV Perception(促进BEV感知的基础模型)"></a>Foundation Models to Facilitate BEV Perception(促进BEV感知的基础模型)</h3><p>近年来，在一般视觉社区中，大型或基础模型[14、15、18、191、192]取得了令人印象深刻的性能，并在许多领域和任务中超越了最先进的技术。</p><ul><li>对于Bev的感知，至少有两个方面值得研究。<ul><li>一种是应用大型预先训练模型中的丰富知识，并提供更好的初始检查点来进行微调。然而，像上一节所暗示的那样，某些2D基础模型的直接自适应在3D BEV意义上可能不能很好地工作。如何设计和选择基础车型，以更好地适应自动驾驶任务，是我们可以接受的长期研究问题。</li><li>另一个是如何发展多任务学习的想法，就像在Bev感知的基础模型(通才)中那样。在一般的视觉文献中有一些有趣的工作，其中OFA[193]、Uni-PerceiverMoE[194]、Gato[195]等将执行多个复杂的任务并获得令人满意的结果。我们能否将类似的理念应用到BEV感知中，并将多项任务统一在一个框架中？这是有意义的，因为自动驾驶中的感知和认知领域需要协作来处理复杂的场景，以实现最终的L5目标。</li></ul></li></ul>]]></content>
      
      
      <categories>
          
          <category> DL </category>
          
      </categories>
      
      
        <tags>
            
            <tag> bev </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>BEVDet训练记录</title>
      <link href="/2023/11/18/bevdet/"/>
      <url>/2023/11/18/bevdet/</url>
      
        <content type="html"><![CDATA[<h1 id="BEVDet在服务器上环境搭建以及运行记录"><a href="#BEVDet在服务器上环境搭建以及运行记录" class="headerlink" title="BEVDet在服务器上环境搭建以及运行记录"></a>BEVDet在服务器上环境搭建以及运行记录</h1><h2 id="流程"><a href="#流程" class="headerlink" title="流程"></a>流程</h2><ul><li><p>创建conda虚拟环境：</p><pre class="line-numbers language-python" data-language="python"><code class="language-python">conda create <span class="token operator">-</span><span class="token operator">-</span>name BEVDet python<span class="token operator">=</span><span class="token number">3.8</span> <span class="token operator">-</span>y<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre></li><li><p>在虚拟环境里安装torch：</p><pre class="line-numbers language-python" data-language="python"><code class="language-python">conda install pytorch<span class="token operator">==</span><span class="token number">1.10</span><span class="token number">.0</span> torchvision<span class="token operator">==</span><span class="token number">0.11</span><span class="token number">.0</span> cudatoolkit<span class="token operator">=</span><span class="token number">11.3</span> <span class="token operator">-</span>c https<span class="token punctuation">:</span><span class="token operator">//</span>mirrors<span class="token punctuation">.</span>tuna<span class="token punctuation">.</span>tsinghua<span class="token punctuation">.</span>edu<span class="token punctuation">.</span>cn<span class="token operator">/</span>anaconda<span class="token operator">/</span>cloud<span class="token operator">/</span>pytorch<span class="token operator">/</span>linux<span class="token operator">-</span><span class="token number">64</span><span class="token operator">/</span><span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre></li><li><p>安装mmcv-full:</p><pre class="line-numbers language-python" data-language="python"><code class="language-python">pip install mmcv<span class="token operator">-</span>full<span class="token operator">==</span><span class="token number">1.5</span><span class="token number">.3</span> <span class="token operator">-</span>f https<span class="token punctuation">:</span><span class="token operator">//</span>download<span class="token punctuation">.</span>openmmlab<span class="token punctuation">.</span>com<span class="token operator">/</span>mmcv<span class="token operator">/</span>dist<span class="token operator">/</span>cu113<span class="token operator">/</span>torch1<span class="token punctuation">.</span><span class="token number">10.0</span><span class="token operator">/</span>index<span class="token punctuation">.</span>html  <span class="token operator">-</span>i https<span class="token punctuation">:</span><span class="token operator">//</span>pypi<span class="token punctuation">.</span>tuna<span class="token punctuation">.</span>tsinghua<span class="token punctuation">.</span>edu<span class="token punctuation">.</span>cn<span class="token operator">/</span>simple<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre></li><li><p>安装mmdet：</p><pre class="line-numbers language-python" data-language="python"><code class="language-python">pip install mmdet<span class="token operator">==</span><span class="token number">2.25</span><span class="token number">.1</span> mmsegmentation<span class="token operator">==</span><span class="token number">0.25</span><span class="token number">.0</span>  <span class="token operator">-</span>i https<span class="token punctuation">:</span><span class="token operator">//</span>pypi<span class="token punctuation">.</span>tuna<span class="token punctuation">.</span>tsinghua<span class="token punctuation">.</span>edu<span class="token punctuation">.</span>cn<span class="token operator">/</span>simple<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre></li><li><p>安装其他：</p><pre class="line-numbers language-python" data-language="python"><code class="language-python">pip install pycuda \    lyft_dataset_sdk \    networkx<span class="token operator">==</span><span class="token number">2.2</span> \    numba<span class="token operator">==</span><span class="token number">0.53</span><span class="token number">.0</span> \    numpy<span class="token operator">==</span><span class="token number">1.23</span><span class="token number">.5</span> \    nuscenes<span class="token operator">-</span>devkit \    yapf<span class="token operator">==</span><span class="token number">0.40</span><span class="token number">.1</span>\    setuptools<span class="token operator">==</span><span class="token number">59.5</span><span class="token number">.0</span>\    plyfile \    scikit<span class="token operator">-</span>image \    tensorboard \    trimesh<span class="token operator">==</span><span class="token number">2.35</span><span class="token number">.39</span> <span class="token operator">-</span>i https<span class="token punctuation">:</span><span class="token operator">//</span>pypi<span class="token punctuation">.</span>tuna<span class="token punctuation">.</span>tsinghua<span class="token punctuation">.</span>edu<span class="token punctuation">.</span>cn<span class="token operator">/</span>simple<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre></li><li><p>最后安装整个项目：</p><pre class="line-numbers language-python" data-language="python"><code class="language-python">pip install <span class="token operator">-</span>e <span class="token punctuation">.</span><span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre></li><li><p>多机训练：</p><pre class="line-numbers language-python" data-language="python"><code class="language-python">CUDA_VISIBLE_DEVICES<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">,</span><span class="token number">1</span><span class="token punctuation">,</span><span class="token number">2</span><span class="token punctuation">,</span><span class="token number">3</span> PORT<span class="token operator">=</span><span class="token number">29500</span> <span class="token punctuation">.</span><span class="token operator">/</span>tools<span class="token operator">/</span>dist_train<span class="token punctuation">.</span>sh $<span class="token punctuation">&#123;</span>CONFIG_FILE<span class="token punctuation">&#125;</span> <span class="token number">4</span><span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre></li></ul><pre class="line-numbers language-python" data-language="python"><code class="language-python">CUDA_VISIBLE_DEVICES<span class="token operator">=</span><span class="token number">7</span> PORT<span class="token operator">=</span><span class="token number">29510</span> <span class="token punctuation">.</span><span class="token operator">/</span>tools<span class="token operator">/</span>dist_train<span class="token punctuation">.</span>sh configs<span class="token operator">/</span>bevdet<span class="token operator">/</span>bevdet<span class="token operator">-</span>r50<span class="token punctuation">.</span>py <span class="token number">1</span> <span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><br/><h3 id="报错-TypeError-FormatCode-got-an-unexpected-keyword-argument-‘verify’"><a href="#报错-TypeError-FormatCode-got-an-unexpected-keyword-argument-‘verify’" class="headerlink" title="报错 TypeError: FormatCode() got an unexpected keyword argument ‘verify’"></a>报错 TypeError: FormatCode() got an unexpected keyword argument ‘verify’</h3><p>原因：yapf版本过高<br>由0.40.2 切换成 0.40.1问题解决</p><p>pip install yapf&#x3D;&#x3D;0.40.1</p><h3 id="警告’Creating-a-tensor-from-a-list-of-numpy-ndarrays-is-extremely-slow’"><a href="#警告’Creating-a-tensor-from-a-list-of-numpy-ndarrays-is-extremely-slow’" class="headerlink" title="警告’Creating a tensor from a list of numpy.ndarrays is extremely slow’"></a>警告’Creating a tensor from a list of numpy.ndarrays is extremely slow’</h3><p>总结<br>(1) 对于不含numpy.ndarrays的list而言，list-&gt;tensor明显快于list-&gt;numpy.ndarrays-&gt;tensor (1.7s&lt;2.5s);</p><p>(2) 对于含有numpy.ndarrays的list而言，list-&gt;numpy.ndarrays-&gt;tensor明显快于list-&gt;tensor (18.8s&lt;41.2s).</p><p><a href="https://zhuanlan.zhihu.com/p/429901066">参考链接</a></p><h3 id="报错：-AttributeError-module-‘distutils’-has-no-attribute-‘version’"><a href="#报错：-AttributeError-module-‘distutils’-has-no-attribute-‘version’" class="headerlink" title="报错： AttributeError: module ‘distutils’ has no attribute ‘version’."></a>报错： AttributeError: module ‘distutils’ has no attribute ‘version’.</h3><p>解决： setuptools版本问题”，版本过高导致的问题；setuptools版本</p><p>第一步： pip uninstall setuptools【使用pip，不能使用 conda uninstall setuptools ; 【不能使用conda的命令，原因是，conda在卸载的时候，会自动分析与其相关的库，然后全部删除，如果y的话，整个环境都需要重新配置。</p><p>第二步： pip或者conda install setuptools&#x3D;&#x3D;59.5.0【现在最新的版本已经到了68了，之前的老版本只是部分保留，找不到的版本不行</p><h3 id="Ubuntu安装ninja"><a href="#Ubuntu安装ninja" class="headerlink" title="Ubuntu安装ninja"></a>Ubuntu安装ninja</h3><p>Ninja是一个比Make更快速的小型构建系统。其github地址为：<a href="https://ninja-build.org/">https://ninja-build.org/</a></p><p><a href="https://blog.csdn.net/SHH_1064994894/article/details/129268006">不同安装方式</a></p><p><a href="https://blog.csdn.net/qq_36287943/article/details/105343192?spm=1001.2101.3001.6650.2&utm_medium=distribute.pc_relevant.none-task-blog-2~default~CTRLIST~Rate-2-105343192-blog-129268006.235%5Ev38%5Epc_relevant_anti_vip_base&depth_1-utm_source=distribute.pc_relevant.none-task-blog-2~default~CTRLIST~Rate-2-105343192-blog-129268006.235%5Ev38%5Epc_relevant_anti_vip_base&utm_relevant_index=5">Ninja安装和基本使用</a></p><p>pip3 install ninja</p><h2 id="BEVDet系列源码解读"><a href="#BEVDet系列源码解读" class="headerlink" title="BEVDet系列源码解读"></a><a href="https://zhuanlan.zhihu.com/p/557613388">BEVDet系列源码解读</a></h2><p><img src="/pic/fb9cef23d1dd44e2f67fad259113ad1b.png"></p><p><strong>BEVDet方法:关键是其中的第二步View Transformer将图像特征转化为BEV特征的过程并使用cuda实现了高效的voxel_pooling_v2在后处理中也提出了scale-NMS可以针对不同尺度的物体进行缩放然后进行过滤。</strong></p><p><a href="https://www.zhihu.com/column/c_1502288547017617408">系列解读链接</a></p><h2 id="nuScenes使用介绍以及BEVDet进行训练"><a href="#nuScenes使用介绍以及BEVDet进行训练" class="headerlink" title="nuScenes使用介绍以及BEVDet进行训练"></a>nuScenes使用介绍以及BEVDet进行训练</h2><p><a href="https://blog.csdn.net/weixin_44596312/article/details/122300131">csdn</a></p><p><a href="https://mmdetection3d.readthedocs.io/zh-cn/latest/advanced_guides/datasets/nuscenes.html">mmdet3d解说</a></p><h3 id="在mini-v1-0训练测试"><a href="#在mini-v1-0训练测试" class="headerlink" title="在mini-v1.0训练测试"></a>在mini-v1.0训练测试</h3><p>mini-v1.0里面有三类物体（C.V.、Trailer、Barrier）不包含，其他物体数量也比较少，所以最后的性能上会比较低。</p><p><img src="/pic/6a46792dc50f7f77a616b9c0fc84113c.png"></p><p><img src="/pic/4e597a394f9c1492958cc248049f4fe3.png"></p><p><img src="/pic/e77bbdc1106b3b95edfee3277d314dd3.png" alt="1699529096094.png"></p><h3 id="相关检测参数指标"><a href="#相关检测参数指标" class="headerlink" title="相关检测参数指标"></a>相关检测参数指标</h3><p><a href="https://blog.csdn.net/weixin_45097875/article/details/125846945">参考链接</a></p><h4 id="mAP"><a href="#mAP" class="headerlink" title="mAP:"></a>mAP:</h4><p>在评测时依旧使用目标检测中常用的的AP，不过AP的阈值匹配不使用IoU来计算，而使用在地平面上的2D中心距离d来计算。这样解耦了物体的尺寸和方向对AP计算的影响。d设置为{0.5,1,2,4}米。在计算AP时，去除了低于0.1的recall和precision并用0来代替这些区域。不同类以及不同难度D用来计算mAP：</p><h4 id="mATE："><a href="#mATE：" class="headerlink" title="mATE："></a>mATE：</h4><p>Average Translation Error,平均平移误差(ATE) 是二维欧几里德中心距离(单位为米).</p><h4 id="mASE："><a href="#mASE：" class="headerlink" title="mASE："></a>mASE：</h4><p>Average Scale Error, 平均尺度误差(ASE) 是1 - IoU, 其中IoU 是角度对齐后的三维交并比</p><h4 id="mAOE："><a href="#mAOE：" class="headerlink" title="mAOE："></a>mAOE：</h4><p>Average Orientation Error.平均角度误差(AOE) 是预测值和真实值之间最小的偏航角差。(所有的类别角度偏差都在360∘度内, 除了障碍物这个类别的角度偏差在180∘ 内)</p><h4 id="mAVE："><a href="#mAVE：" class="headerlink" title="mAVE："></a>mAVE：</h4><p> Average Velocity Error.平均速度误差(AVE) 是二维速度差的L2 范数(m&#x2F;s)。</p><h4 id="mAAE："><a href="#mAAE：" class="headerlink" title="mAAE："></a>mAAE：</h4><p>Average Attribute Error,平均属性错误(AAE) 被定义为1−acc, 其中acc 为类别分类准确度。</p><p><img src="/pic/bd1a7e7e4cdf242c91e2ecd6e1f7eabb.png"></p><p><img src="/pic/6a46792dc50f7f77a616b9c0fc84113c.png"></p><br/><p>可以看出每个参数的性能指标</p><h2 id="BEVDet中nuscenes数据集处理"><a href="#BEVDet中nuscenes数据集处理" class="headerlink" title="BEVDet中nuscenes数据集处理"></a>BEVDet中nuscenes数据集处理</h2><p>处理前的结构</p><pre class="line-numbers language-python" data-language="python"><code class="language-python">mmdetection3d├── mmdet3d├── tools├── configs├── data│   ├── nuscenes│   │   ├── maps│   │   ├── samples│   │   ├── sweeps│   │   ├── v1<span class="token punctuation">.</span><span class="token number">0</span><span class="token operator">-</span>test<span class="token operator">|</span>   <span class="token operator">|</span>   ├── v1<span class="token punctuation">.</span><span class="token number">0</span><span class="token operator">-</span>trainval<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>处理使用命令</p><pre class="line-numbers language-python" data-language="python"><code class="language-python">python tools<span class="token operator">/</span>create_data<span class="token punctuation">.</span>py nuscenes <span class="token operator">-</span><span class="token operator">-</span>root<span class="token operator">-</span>path <span class="token punctuation">.</span><span class="token operator">/</span>data<span class="token operator">/</span>nuscenes <span class="token operator">-</span><span class="token operator">-</span>out<span class="token operator">-</span><span class="token builtin">dir</span> <span class="token punctuation">.</span><span class="token operator">/</span>data<span class="token operator">/</span>nuscenes <span class="token operator">-</span><span class="token operator">-</span>extra<span class="token operator">-</span>tag nuscenes<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>处理后的结构</p><pre class="line-numbers language-python" data-language="python"><code class="language-python">mmdetection3d├── mmdet3d├── tools├── configs├── data│   ├── nuscenes│   │   ├── maps│   │   ├── samples│   │   ├── sweeps│   │   ├── v1<span class="token punctuation">.</span><span class="token number">0</span><span class="token operator">-</span>test<span class="token operator">|</span>   <span class="token operator">|</span>   ├── v1<span class="token punctuation">.</span><span class="token number">0</span><span class="token operator">-</span>trainval│   │   ├── nuscenes_database│   │   ├── nuscenes_infos_train<span class="token punctuation">.</span>pkl│   │   ├── nuscenes_infos_val<span class="token punctuation">.</span>pkl│   │   ├── nuscenes_infos_test<span class="token punctuation">.</span>pkl│   │   ├── nuscenes_dbinfos_train<span class="token punctuation">.</span>pkl│   │   ├── nuscenes_infos_train_mono3d<span class="token punctuation">.</span>coco<span class="token punctuation">.</span>json│   │   ├── nuscenes_infos_val_mono3d<span class="token punctuation">.</span>coco<span class="token punctuation">.</span>json│   │   ├── nuscenes_infos_test_mono3d<span class="token punctuation">.</span>coco<span class="token punctuation">.</span>json<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p><img src="/pic/3965332f9620a419a12dff851c3b6de2.png"></p><p><img src="/pic/23ecfbfcf5e0597b8c2a808e2a138b0b.png"></p>]]></content>
      
      
      <categories>
          
          <category> DL </category>
          
      </categories>
      
      
        <tags>
            
            <tag> bev </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>MonoScene训练记录</title>
      <link href="/2023/11/17/monoscene/"/>
      <url>/2023/11/17/monoscene/</url>
      
        <content type="html"><![CDATA[<h1 id="MonoScene在服务器上环境搭建以及运行记录"><a href="#MonoScene在服务器上环境搭建以及运行记录" class="headerlink" title="MonoScene在服务器上环境搭建以及运行记录"></a>MonoScene在服务器上环境搭建以及运行记录</h1><h2 id="conda环境"><a href="#conda环境" class="headerlink" title="conda环境"></a>conda环境</h2><pre class="line-numbers language-python" data-language="python"><code class="language-python">pytorch<span class="token operator">-</span>lightning_libgcc_mutex             <span class="token number">0.1</span>                        main    defaults_openmp_mutex             <span class="token number">5.1</span>                       1_gnu    defaultsabsl<span class="token operator">-</span>py                   <span class="token number">2.0</span><span class="token number">.0</span>                    pypi_0    pypiaiohttp                   <span class="token number">3.8</span><span class="token number">.6</span>                    pypi_0    pypiaiosignal                 <span class="token number">1.3</span><span class="token number">.1</span>                    pypi_0    pypiantlr4<span class="token operator">-</span>python3<span class="token operator">-</span>runtime    <span class="token number">4.8</span>                      pypi_0    pypi<span class="token keyword">async</span><span class="token operator">-</span>timeout             <span class="token number">4.0</span><span class="token number">.3</span>                    pypi_0    pypiasynctest                 <span class="token number">0.13</span><span class="token number">.0</span>                   pypi_0    pypiattrs                     <span class="token number">23.1</span><span class="token number">.0</span>                   pypi_0    pypiblas                      <span class="token number">1.0</span>                         mkl    defaultsca<span class="token operator">-</span>certificates           <span class="token number">2023.08</span><span class="token number">.22</span>           h06a4308_0    defaultscachetools                <span class="token number">5.3</span><span class="token number">.2</span>                    pypi_0    pypicertifi                   <span class="token number">2022.12</span><span class="token number">.7</span>        py37h06a4308_0    defaultscharset<span class="token operator">-</span>normalizer        <span class="token number">3.3</span><span class="token number">.1</span>                    pypi_0    pypicudatoolkit               <span class="token number">10.2</span><span class="token number">.89</span>              hfd86e86_1    defaultscycler                    <span class="token number">0.11</span><span class="token number">.0</span>                   pypi_0    pypifonttools                 <span class="token number">4.38</span><span class="token number">.0</span>                   pypi_0    pypifreetype                  <span class="token number">2.12</span><span class="token number">.1</span>               h4a9f257_0    defaultsfrozenlist                <span class="token number">1.3</span><span class="token number">.3</span>                    pypi_0    pypifsspec                    <span class="token number">2023.1</span><span class="token number">.0</span>                 pypi_0    pypifuture                    <span class="token number">0.18</span><span class="token number">.3</span>                   pypi_0    pypigiflib                    <span class="token number">5.2</span><span class="token number">.1</span>                h5eee18b_3    defaultsgoogle<span class="token operator">-</span>auth               <span class="token number">2.23</span><span class="token number">.3</span>                   pypi_0    pypigoogle<span class="token operator">-</span>auth<span class="token operator">-</span>oauthlib      <span class="token number">0.4</span><span class="token number">.6</span>                    pypi_0    pypigrpcio                    <span class="token number">1.59</span><span class="token number">.0</span>                   pypi_0    pypihydra<span class="token operator">-</span>core                <span class="token number">1.0</span><span class="token number">.5</span>                    pypi_0    pypiidna                      <span class="token number">3.4</span>                      pypi_0    pypiimageio                   <span class="token number">2.31</span><span class="token number">.2</span>                   pypi_0    pypiimportlib<span class="token operator">-</span>metadata        <span class="token number">6.7</span><span class="token number">.0</span>                    pypi_0    pypiimportlib<span class="token operator">-</span>resources       <span class="token number">5.12</span><span class="token number">.0</span>                   pypi_0    pypiintel<span class="token operator">-</span>openmp              <span class="token number">2021.4</span><span class="token number">.0</span>          h06a4308_3561    defaultsjoblib                    <span class="token number">1.3</span><span class="token number">.2</span>                    pypi_0    pypijpeg                      9b                   h024ee3a_2    defaultskiwisolver                <span class="token number">1.4</span><span class="token number">.5</span>                    pypi_0    pypilcms2                     <span class="token number">2.12</span>                 h3be6417_0    defaultsld_impl_linux<span class="token operator">-</span><span class="token number">64</span>          <span class="token number">2.38</span>                 h1181459_1    defaultslibffi                    <span class="token number">3.4</span><span class="token number">.4</span>                h6a678d5_0    defaultslibgcc<span class="token operator">-</span>ng                 <span class="token number">11.2</span><span class="token number">.0</span>               h1234567_1    defaultslibgomp                   <span class="token number">11.2</span><span class="token number">.0</span>               h1234567_1    defaultslibpng                    <span class="token number">1.6</span><span class="token number">.39</span>               h5eee18b_0    defaultslibstdcxx<span class="token operator">-</span>ng              <span class="token number">11.2</span><span class="token number">.0</span>               h1234567_1    defaultslibtiff                   <span class="token number">4.1</span><span class="token number">.0</span>                h2733197_1    defaultslibuv                     <span class="token number">1.44</span><span class="token number">.2</span>               h5eee18b_0    defaultslibwebp                   <span class="token number">1.2</span><span class="token number">.0</span>                h89dd481_0    defaultsllvmlite                  <span class="token number">0.36</span><span class="token number">.0</span>                   pypi_0    pypilz4<span class="token operator">-</span>c                     <span class="token number">1.9</span><span class="token number">.4</span>                h6a678d5_0    defaultsmarkdown                  <span class="token number">3.4</span><span class="token number">.4</span>                    pypi_0    pypimarkupsafe                <span class="token number">2.1</span><span class="token number">.3</span>                    pypi_0    pypimatplotlib                <span class="token number">3.5</span><span class="token number">.3</span>                    pypi_0    pypimkl                       <span class="token number">2021.4</span><span class="token number">.0</span>           h06a4308_640    defaultsmkl<span class="token operator">-</span>service               <span class="token number">2.4</span><span class="token number">.0</span>            py37h7f8727e_0    defaultsmkl_fft                   <span class="token number">1.3</span><span class="token number">.1</span>            py37hd3c417c_0    defaultsmkl_random                <span class="token number">1.2</span><span class="token number">.2</span>            py37h51133e4_0    defaultsmonoscene                 <span class="token number">0.0</span><span class="token number">.0</span>                     dev_0    <span class="token operator">&lt;</span>develop<span class="token operator">></span>multidict                 <span class="token number">6.0</span><span class="token number">.4</span>                    pypi_0    pypincurses                   <span class="token number">6.4</span>                  h6a678d5_0    defaultsnetworkx                  <span class="token number">2.6</span><span class="token number">.3</span>                    pypi_0    pypininja                     <span class="token number">1.10</span><span class="token number">.2</span>               h06a4308_5    defaultsninja<span class="token operator">-</span>base                <span class="token number">1.10</span><span class="token number">.2</span>               hd09550d_5    defaultsnumba                     <span class="token number">0.53</span><span class="token number">.0</span>                   pypi_0    pypinumpy                     <span class="token number">1.20</span><span class="token number">.3</span>                   pypi_0    pypinvidia<span class="token operator">-</span>cublas<span class="token operator">-</span>cu11        <span class="token number">11.10</span><span class="token number">.3</span><span class="token number">.66</span>               pypi_0    pypinvidia<span class="token operator">-</span>cuda<span class="token operator">-</span>nvrtc<span class="token operator">-</span>cu11    <span class="token number">11.7</span><span class="token number">.99</span>                  pypi_0    pypinvidia<span class="token operator">-</span>cuda<span class="token operator">-</span>runtime<span class="token operator">-</span>cu11  <span class="token number">11.7</span><span class="token number">.99</span>                  pypi_0    pypinvidia<span class="token operator">-</span>cudnn<span class="token operator">-</span>cu11         <span class="token number">8.5</span><span class="token number">.0</span><span class="token number">.96</span>                 pypi_0    pypioauthlib                  <span class="token number">3.2</span><span class="token number">.2</span>                    pypi_0    pypiomegaconf                 <span class="token number">2.0</span><span class="token number">.6</span>                    pypi_0    pypiopencv<span class="token operator">-</span>python             <span class="token number">4.5</span><span class="token number">.1</span><span class="token number">.48</span>                 pypi_0    pypiopenssl                   <span class="token number">1.1</span><span class="token punctuation">.</span>1w               h7f8727e_0    defaultspackaging                 <span class="token number">23.2</span>                     pypi_0    pypipillow                    <span class="token number">9.3</span><span class="token number">.0</span>            py37hace64e9_1    defaultspip                       <span class="token number">22.3</span><span class="token number">.1</span>           py37h06a4308_0    defaultsprotobuf                  <span class="token number">3.19</span><span class="token number">.6</span>                   pypi_0    pypipyasn1                    <span class="token number">0.5</span><span class="token number">.0</span>                    pypi_0    pypipyasn1<span class="token operator">-</span>modules            <span class="token number">0.3</span><span class="token number">.0</span>                    pypi_0    pypipydeprecate               <span class="token number">0.3</span><span class="token number">.1</span>                    pypi_0    pypipyparsing                 <span class="token number">3.1</span><span class="token number">.1</span>                    pypi_0    pypipython                    <span class="token number">3.7</span><span class="token number">.16</span>               h7a1cb2a_0    defaultspython<span class="token operator">-</span>dateutil           <span class="token number">2.8</span><span class="token number">.2</span>                    pypi_0    pypipytorch<span class="token operator">-</span>lightning         <span class="token number">1.4</span><span class="token number">.9</span>                    pypi_0    pypipywavelets                <span class="token number">1.3</span><span class="token number">.0</span>                    pypi_0    pypipyyaml                    <span class="token number">5.3</span><span class="token number">.1</span>                    pypi_0    pypireadline                  <span class="token number">8.2</span>                  h5eee18b_0    defaultsrequests                  <span class="token number">2.31</span><span class="token number">.0</span>                   pypi_0    pypirequests<span class="token operator">-</span>oauthlib         <span class="token number">1.3</span><span class="token number">.1</span>                    pypi_0    pypirsa                       <span class="token number">4.9</span>                      pypi_0    pypiscikit<span class="token operator">-</span>image              <span class="token number">0.18</span><span class="token number">.1</span>                   pypi_0    pypiscikit<span class="token operator">-</span>learn              <span class="token number">0.24</span><span class="token number">.0</span>                   pypi_0    pypiscipy                     <span class="token number">1.7</span><span class="token number">.3</span>                    pypi_0    pypisetuptools                <span class="token number">65.6</span><span class="token number">.3</span>           py37h06a4308_0    defaultssix                       <span class="token number">1.16</span><span class="token number">.0</span>             pyhd3eb1b0_1    defaultssqlite                    <span class="token number">3.41</span><span class="token number">.2</span>               h5eee18b_0    defaultstbb                       <span class="token number">2020.2</span>               hff7bd54_0    defaultstensorboard               <span class="token number">2.11</span><span class="token number">.2</span>                   pypi_0    pypitensorboard<span class="token operator">-</span>data<span class="token operator">-</span>server   <span class="token number">0.6</span><span class="token number">.1</span>                    pypi_0    pypitensorboard<span class="token operator">-</span>plugin<span class="token operator">-</span>wit    <span class="token number">1.8</span><span class="token number">.1</span>                    pypi_0    pypithreadpoolctl             <span class="token number">3.1</span><span class="token number">.0</span>                    pypi_0    pypitifffile                  <span class="token number">2021.11</span><span class="token number">.2</span>                pypi_0    pypitk                        <span class="token number">8.6</span><span class="token number">.12</span>               h1ccaba5_0    defaultstorch                     <span class="token number">1.13</span><span class="token number">.1</span>                   pypi_0    pypitorchaudio                <span class="token number">0.7</span><span class="token number">.2</span>                      py37    pytorchtorchmetrics              <span class="token number">0.6</span><span class="token number">.0</span>                    pypi_0    pypitorchvision               <span class="token number">0.8</span><span class="token number">.2</span>                py37_cu102    pytorchtqdm                      <span class="token number">4.49</span><span class="token number">.0</span>                   pypi_0    pypityping_extensions         <span class="token number">4.3</span><span class="token number">.0</span>            py37h06a4308_0    defaultsurllib3                   <span class="token number">2.0</span><span class="token number">.7</span>                    pypi_0    pypiwerkzeug                  <span class="token number">2.2</span><span class="token number">.3</span>                    pypi_0    pypiwheel                     <span class="token number">0.38</span><span class="token number">.4</span>           py37h06a4308_0    defaultsxz                        <span class="token number">5.4</span><span class="token number">.2</span>                h5eee18b_0    defaultsyarl                      <span class="token number">1.9</span><span class="token number">.2</span>                    pypi_0    pypizipp                      <span class="token number">3.15</span><span class="token number">.0</span>                   pypi_0    pypizlib                      <span class="token number">1.2</span><span class="token number">.13</span>               h5eee18b_0    defaultszstd                      <span class="token number">1.4</span><span class="token number">.9</span>                haebb681_0    defaults<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p><strong>需要注意的是这里使用了pytorch-lightning，版本对应安装</strong></p><p><a href="https://lightning.ai/docs/pytorch/latest/starter/introduction.html">官方文档连接</a></p><p><img src="/pic/a93ac42b74810deb39e5ea6dc4f5a66e.png"></p><h2 id="数据集使用SemanticKITTI以及NYUv2"><a href="#数据集使用SemanticKITTI以及NYUv2" class="headerlink" title="数据集使用SemanticKITTI以及NYUv2"></a>数据集使用SemanticKITTI以及NYUv2</h2><p><a href="http://www.semantic-kitti.org/dataset.html#download">Semantic Scene Completion dataset v1.1下载地址</a></p><p><a href="https://www.cvlibs.net/datasets/kitti/eval_odometry.php"> KITTI Odometry Benchmark calibration data (Download odometry data set (calibration files, 1 MB)) and the RGB images (Download odometry data set (color, 65 GB))</a></p><pre class="line-numbers language-python" data-language="python"><code class="language-python">$ export KITTI_PREPROCESS<span class="token operator">=</span><span class="token operator">/</span>home3<span class="token operator">/</span>dataset<span class="token operator">/</span>kitti<span class="token operator">/</span>preprocess$ export KITTI_ROOT<span class="token operator">=</span><span class="token operator">/</span>home3<span class="token operator">/</span>dataset<span class="token operator">/</span>kitti<span class="token operator">/</span>semantic_kitti$ export KITTI_LOG<span class="token operator">=</span><span class="token operator">/</span>home3<span class="token operator">/</span>chenhaiyang<span class="token operator">/</span>MonoScene<span class="token operator">/</span>monoscene<span class="token operator">/</span>data<span class="token operator">/</span>semantic_kitti<span class="token operator">/</span>logdir<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre><ul><li><p>KITTI_PREPROCESS保存的是未处理的数据</p></li><li><p>KITTI_ROOT保存的是处理后的数据</p></li><li><p>KITTI_LOG保存的是日志文件</p></li></ul><h3 id="kitti数据集标注解释"><a href="#kitti数据集标注解释" class="headerlink" title="kitti数据集标注解释"></a>kitti数据集标注解释</h3><p><a href="https://zhuanlan.zhihu.com/p/452672948">参考链接1</a></p><p><a href="https://blog.csdn.net/yangziluomu/article/details/78339575">参考链接2</a></p><pre class="line-numbers language-txt" data-language="txt"><code class="language-txt">1惯性导航系统（GPS / IMU）：OXTS RT 30031台激光雷达：Velodyne HDL-64E2台灰度相机，1.4百万像素：Point Grey Flea 2（FL2-14S3M-C）2个彩色摄像头，1.4百万像素：Point Grey Flea 2（FL2-14S3C-C）4个变焦镜头，4-8毫米：Edmund Optics NT59-917<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span></span></code></pre><p><img src="/pic/9dd8f665b331552caace5d45f362b48e.png"></p><h4 id="calib-txt-标定文件解读"><a href="#calib-txt-标定文件解读" class="headerlink" title="calib.txt 标定文件解读"></a>calib.txt 标定文件解读</h4><p>在calib文件中，有sequence 00-21序列，包括calib.txt 和 times.txt文件。<br>在sequence calib.txt 中，</p><pre class="line-numbers language-python" data-language="python"><code class="language-python">P0<span class="token punctuation">:</span> <span class="token number">7.188560000000e+02</span> <span class="token number">0.000000000000e+00</span> <span class="token number">6.071928000000e+02</span> <span class="token number">0.000000000000e+00</span> <span class="token number">0.000000000000e+00</span> <span class="token number">7.188560000000e+02</span> <span class="token number">1.852157000000e+02</span> <span class="token number">0.000000000000e+00</span> <span class="token number">0.000000000000e+00</span> <span class="token number">0.000000000000e+00</span> <span class="token number">1.000000000000e+00</span> <span class="token number">0.000000000000e+00</span>P1<span class="token punctuation">:</span> <span class="token number">7.188560000000e+02</span> <span class="token number">0.000000000000e+00</span> <span class="token number">6.071928000000e+02</span> <span class="token operator">-</span><span class="token number">3.861448000000e+02</span> <span class="token number">0.000000000000e+00</span> <span class="token number">7.188560000000e+02</span> <span class="token number">1.852157000000e+02</span> <span class="token number">0.000000000000e+00</span> <span class="token number">0.000000000000e+00</span> <span class="token number">0.000000000000e+00</span> <span class="token number">1.000000000000e+00</span> <span class="token number">0.000000000000e+00</span>P2<span class="token punctuation">:</span> <span class="token number">7.188560000000e+02</span> <span class="token number">0.000000000000e+00</span> <span class="token number">6.071928000000e+02</span> <span class="token number">4.538225000000e+01</span> <span class="token number">0.000000000000e+00</span> <span class="token number">7.188560000000e+02</span> <span class="token number">1.852157000000e+02</span> <span class="token operator">-</span><span class="token number">1.130887000000e-01</span> <span class="token number">0.000000000000e+00</span> <span class="token number">0.000000000000e+00</span> <span class="token number">1.000000000000e+00</span> <span class="token number">3.779761000000e-03</span>P3<span class="token punctuation">:</span> <span class="token number">7.188560000000e+02</span> <span class="token number">0.000000000000e+00</span> <span class="token number">6.071928000000e+02</span> <span class="token operator">-</span><span class="token number">3.372877000000e+02</span> <span class="token number">0.000000000000e+00</span> <span class="token number">7.188560000000e+02</span> <span class="token number">1.852157000000e+02</span> <span class="token number">2.369057000000e+00</span> <span class="token number">0.000000000000e+00</span> <span class="token number">0.000000000000e+00</span> <span class="token number">1.000000000000e+00</span> <span class="token number">4.915215000000e-03</span>Tr<span class="token punctuation">:</span> <span class="token number">4.276802385584e-04</span> <span class="token operator">-</span><span class="token number">9.999672484946e-01</span> <span class="token operator">-</span><span class="token number">8.084491683471e-03</span> <span class="token operator">-</span><span class="token number">1.198459927713e-02</span> <span class="token operator">-</span><span class="token number">7.210626507497e-03</span> <span class="token number">8.081198471645e-03</span> <span class="token operator">-</span><span class="token number">9.999413164504e-01</span> <span class="token operator">-</span><span class="token number">5.403984729748e-02</span> <span class="token number">9.999738645903e-01</span> <span class="token number">4.859485810390e-04</span> <span class="token operator">-</span><span class="token number">7.206933692422e-03</span> <span class="token operator">-</span><span class="token number">2.921968648686e-01</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>0,1,2,3 代表相机的编号，0表示左边灰度相机，1右边灰度相机，2左边彩色相机，3右边彩色相机。Tr表示将velodyne坐标系转换到左边相机系统坐标。</p><p>根据calib.txt相机投影矩阵可以得到相机内参。</p><p><img src="/pic/9b517ca2a52c3bcf686f3d2b598f234e.png"></p><p><img src="/pic/c7a26d5ae792b4f518aa79e835c820fa.png"></p><h2 id="网络训练"><a href="#网络训练" class="headerlink" title="网络训练"></a>网络训练</h2><pre class="line-numbers language-python" data-language="python"><code class="language-python">python monoscene<span class="token operator">/</span>scripts<span class="token operator">/</span>train_monoscene<span class="token punctuation">.</span>py \    dataset<span class="token operator">=</span>kitti \    enable_log<span class="token operator">=</span>true \    kitti_root<span class="token operator">=</span>$KITTI_ROOT \    kitti_preprocess_root<span class="token operator">=</span>$KITTI_PREPROCESS\    kitti_logdir<span class="token operator">=</span>$KITTI_LOG \    n_gpus<span class="token operator">=</span><span class="token number">3</span> batch_size<span class="token operator">=</span><span class="token number">3</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>服务器上的显存不够，训练不了</p><p>为了能够训练，可能采取的方法：</p><ul><li>更改2D解码器的特征尺寸</li><li>缩小输入图像：还需要调整投影矩阵</li><li>通过更改basemodel_name 和 num_features来使用较小的 2D 主干。</li><li>减少3D网络的特征维度</li><li>尝试先禁用或减小上下文的大小</li></ul><p><a href="https://github.com/astra-vision/MonoScene/issues/25">参考</a></p><h3 id="记录一下代码的构造"><a href="#记录一下代码的构造" class="headerlink" title="记录一下代码的构造"></a>记录一下代码的构造</h3><ul><li><p>2d unet结构的创建 编码器使用rwightman&#x2F;gen-efficientnet-pytorch下的tf_efficientnet_b7_ns作为backbone,解码器自己创建的上采样层</p></li><li><p>FLoSP结构将2d特征提升至三维的</p></li><li><p>3D UNets：2层 encoder-decoder 结构，用于提取 3d 特征</p></li><li><p>completion head：3D ASPP 结构和 softmax 层，用于处理 3D UNet 输出得到3d场景 completion 结果</p></li></ul><h4 id="介绍一下efficientnet"><a href="#介绍一下efficientnet" class="headerlink" title="介绍一下efficientnet"></a>介绍一下efficientnet</h4><p><a href="https://blog.csdn.net/qq_37541097/article/details/114434046">参考链接</a></p><p><img src="/pic/a221d02adcef4934cb9c8a3f909d0b52.png"></p><p>在之前的一些论文中，有的会通过增加网络的width即增加卷积核的个数（增加特征矩阵的channels）来提升网络的性能如图(b)所示，有的会通过增加网络的深度即使用更多的层结构来提升网络的性能如图(c)所示，有的会通过增加输入网络的分辨率来提升网络的性能如图(d)所示。而在本篇论文中会同时增加网络的width、网络的深度以及输入网络的分辨率来提升网络的性能如图(e)所示：</p><p><img src="/pic/a060b74b18a9239af2a8fb22663a2feb.png"></p><p><img src="/pic/2f6cb84d74f69886fa0d803d7702118a.png"></p><br/><p><strong>未完待续~</strong></p>]]></content>
      
      
      <categories>
          
          <category> DL </category>
          
      </categories>
      
      
        <tags>
            
            <tag> ssc </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>三维检测与占据预测PPT记录</title>
      <link href="/2023/11/16/3det-occ/"/>
      <url>/2023/11/16/3det-occ/</url>
      
        <content type="html"><![CDATA[<h1 id="三维检测与占据预测PPT记录"><a href="#三维检测与占据预测PPT记录" class="headerlink" title="三维检测与占据预测PPT记录"></a>三维检测与占据预测PPT记录</h1><p>主要对前期制作的三维目标检测和占据预测的PPT进行记录</p><blockquote><p>其中的三维目标检测主要从分类方法上展示不同的检测网络方法与结构，占据预测主要从BEV检测的两条主线开始讲起，再到特斯拉的占据预测网络，再到CVPR2023关于占据预测的模型，再到ICCV的占据预测的模型，以及最后的CVPR2023 占据预测比赛的前几名的解决方案。</p></blockquote><p>下面展示三维目标检测网络的PPT</p><iframe src="https://onedrive.live.com/embed?resid=E13F4D665106CFB4%212317&authkey=!AJirAp4km13Oq6Y&em=2" width="402" height="327" frameborder="0" scrolling="no"></iframe><p>接着展示占据预测的PPT</p><iframe src="https://onedrive.live.com/embed?resid=E13F4D665106CFB4%212316&authkey=!AIpr6wK938OwxXI&em=2" width="402" height="327" frameborder="0" scrolling="no"></iframe>]]></content>
      
      
      <categories>
          
          <category> DL </category>
          
      </categories>
      
      
        <tags>
            
            <tag> occ </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>PON:利用金字塔占用网络预测图像的语义地图表示</title>
      <link href="/2023/11/08/pon/"/>
      <url>/2023/11/08/pon/</url>
      
        <content type="html"><![CDATA[<h1 id="PON-Predicting-Semantic-Map-Representations-from-Images-using-Pyramid-Occupancy-Networks"><a href="#PON-Predicting-Semantic-Map-Representations-from-Images-using-Pyramid-Occupancy-Networks" class="headerlink" title="PON:Predicting Semantic Map Representations from Images using Pyramid Occupancy Networks"></a>PON:Predicting Semantic Map Representations from Images using Pyramid Occupancy Networks</h1><p>论文链接：<a href="https://arxiv.org/pdf/2003.13402.pdf">PON-pdf</a><br>代码链接：<a href="https://github.com/tom-roddick/mono-semantic-maps">PON-py</a></p><p>PON:利用金字塔占用网络预测图像的语义地图表示</p><p>提出了一个dense transformer（并非self attention的transformer， 只是MLP结构）的网络结构用于将2D图转换成BEV</p><p>我们的贡献如下:</p><ul><li>提出了一种新的密集变换层，它将基于图像的特征图映射到鸟瞰图空间。</li><li>设计了一个深度卷积神经网络架构，其中包括在多个图像尺度上运行的变压器金字塔，以从单眼图像预测准确的鸟瞰图。</li><li>我们在两个大规模自动驾驶数据集上评估我们的方法，并表明我们能够显着提高文献中领先作品的性能。<br>我们还定性地展示了如何使用贝叶斯语义占用网格框架来累积跨多个相机和时间步长的地图预测，以构建完整的场景模型。 该方法足够快，可用于实时应用程序，在单个 GeForce RTX 2080 Ti 显卡上每秒处理 23.2 帧</li></ul><p>一些参考链接</p><p><a href="https://blog.csdn.net/weixin_43889128/article/details/122301675?ops_request_misc=%257B%2522request%255Fid%2522%253A%2522169927967316800213076409%2522%252C%2522scm%2522%253A%252220140713.130102334..%2522%257D&request_id=169927967316800213076409&biz_id=0&utm_medium=distribute.pc_search_result.none-task-blog-2~all~sobaiduend~default-2-122301675-null-null.142%5Ev96%5Epc_search_result_base3&utm_term=Predicting%20Semantic%20Map%20Representations%20from%20Images%20using%20Pyramid%20Occupancy%20Networks&spm=1018.2226.3001.4187">翻译</a></p><p><a href="https://blog.csdn.net/Never__Say__No/article/details/120958739?ops_request_misc=%257B%2522request%255Fid%2522%253A%2522169927967316800213076409%2522%252C%2522scm%2522%253A%252220140713.130102334..%2522%257D&request_id=169927967316800213076409&biz_id=0&utm_medium=distribute.pc_search_result.none-task-blog-2~all~sobaiduend~default-4-120958739-null-null.142%5Ev96%5Epc_search_result_base3&utm_term=Predicting%20Semantic%20Map%20Representations%20from%20Images%20using%20Pyramid%20Occupancy%20Networks&spm=1018.2226.3001.4187">图像到BEV转换</a></p>]]></content>
      
      
      <categories>
          
          <category> DL </category>
          
          <category> bev </category>
          
      </categories>
      
      
        <tags>
            
            <tag> bev </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>语义场景补全（SSC）代码篇（二）</title>
      <link href="/2023/11/05/code-ssc2/"/>
      <url>/2023/11/05/code-ssc2/</url>
      
        <content type="html"><![CDATA[<h1 id="主要记录SSC中的理论与代码实现（第二章）"><a href="#主要记录SSC中的理论与代码实现（第二章）" class="headerlink" title="主要记录SSC中的理论与代码实现（第二章）"></a>主要记录SSC中的理论与代码实现（第二章）</h1><p>主要从开篇之作SSCNet（2017）开始讲起，然后按激光雷达点云输入（LMSCNet，2020；S3CNet，2021a；JS3C-Net，2021）,单双目以及两者融合的顺序</p><p>紧接上回，双目视觉的解决方案</p><h2 id="OccDepth"><a href="#OccDepth" class="headerlink" title="OccDepth"></a>OccDepth</h2><p>第一个只输入视觉的立体3D SSC方法。为了有效地利用深度信息，提出了一种立体软特征分配（Stereo-SFA）模块，通过隐式学习立体图像之间的相关性来更好地融合3D深度感知特征。占用感知深度（OAD）模块将知识提取与预先训练的深度模型明确地用于生成感知深度的3D特征。此外，为了更有效地验证立体输入场景，我们提出了基于TartanAir的SemanticTartanAir数据集。这是一个虚拟场景，可以使用真实的地面实况来更好地评估该方法的有效性。</p><p>代码链接如下: <a href="https://github.com/megvii-research/OccDepth">OccDepth: A Depth-aware Method for 3D Semantic Occupancy Network</a>, arXiv 2023.</p><p><img src="/pic/OccDepth.png"><br>3D SSC是通过桥接立体SFA模块来从立体图像中推断出来的，该模块用于将特征提升到3D空间，OAD模块用于增强深度预测，3D U-Net用于提取几何结构和语义。立体深度网络仅在训练中用于提供深度监督。</p><h3 id="代码介绍"><a href="#代码介绍" class="headerlink" title="代码介绍"></a>代码介绍</h3><p>代码方面类似monoscene，都使用了使用PyTorch Lightning，训练代码<a href="https://github.com/megvii-research/OccDepth/blob/main/occdepth/scripts/train.py">train.py</a>介绍如下：</p><ol><li><p><strong>导入模块</strong>: 脚本以各种导入语句开始，包括数据模块、模型、实用程序和配置。</p></li><li><p><strong>Hydra配置</strong>: <code>main</code> 函数被 <code>@hydra.main</code> 装饰，这表明可以使用Hydra进行配置，Hydra是一个强大的配置管理工具。配置路径根据 <code>config_path</code> 环境变量确定。</p></li><li><p><strong>主要函数</strong>: <code>main</code> 函数是脚本的入口点。它加载使用Hydra指定的配置。</p></li><li><p><strong>实验名称</strong>: 基于各种配置设置，创建了 <code>exp_name</code> 变量，用于命名实验。</p></li><li><p><strong>数据模块设置</strong>: 根据选择的数据集（例如 “kitti”、”NYU” 或 “tartanair”），实例化了相应的数据模块。数据模块处理数据加载和预处理。</p></li><li><p><strong>模型初始化</strong>: 创建了“OccDepth”模型的实例。模型接受各种参数，包括类别名称、类别权重和配置设置。</p></li><li><p><strong>日志记录和回调</strong>: 根据配置，脚本设置了使用TensorBoard的日志记录，并定义了各种回调，包括模型检查点和学习率监视器。</p></li><li><p><strong>从上次继续或从头开始训练</strong>: 脚本检查模型检查点文件（例如 “last.ckpt”）是否存在。如果存在，则继续从那个点训练。否则，从头开始训练模型。</p></li><li><p><strong>训练</strong>: 脚本初始化PyTorch Lightning Trainer，并调用 <code>fit</code> 方法，使用指定的数据模块来训练模型。训练过程由各种配置设置控制，包括GPU数量、梯度裁剪和其他超参数。</p></li><li><p><strong>随机种子初始化</strong>: 使用 <code>seed_everything</code> 设置随机种子。</p></li><li><p><strong>执行</strong>: 当运行脚本时，将调用主函数。</p></li></ol><p>总之，该脚本旨在训练”OccDepth”模型，可用于不同的数据集，并允许使用Hydra进行灵活的配置。它设置数据加载、模型和训练流程，并且如果有可用的检查点，可以从那个点继续训练。训练结果和日志将保存在配置中指定的目录中。</p><p><strong>接着看一下<a href="https://github.com/megvii-research/OccDepth/blob/main/occdepth/models/OccDepth.py">OccDepth.py</a>的模型</strong></p><p>“OccDepth” 是一个PyTorch模型，用于语义分割和深度预测的任务。这个模型主要由以下组件构成：</p><ol><li><p>**构造函数 <code>__init__</code>**：构造函数初始化模型的各种参数和组件。以下是一些重要的参数和配置项：</p><ul><li><code>class_names</code>：类别名称列表，用于分类问题。</li><li><code>class_weights</code>：类别权重，用于处理类别不平衡的情况。</li><li><code>class_weights_occ</code>：用于处理类别不平衡的另一组类别权重。</li><li><code>full_scene_size</code>：场景的完整尺寸。</li><li><code>project_res</code>：2D特征到3D特征的投影分辨率。</li><li><code>config</code>：模型的配置参数，包括超参数等。</li><li><code>infer_mode</code>：是否为推理模式，如果是，则不使用上下文先验（context prior）。</li></ul></li><li><p><strong>特征提取器</strong>：使用UNet结构进行特征提取。这些提取到的特征被用于语义分割。</p></li><li><p><strong>2D-3D投影层</strong>：这部分用于将2D特征映射到3D特征。包括两种投影方法：”flosp” 和 “flosp_depth”。</p><ul><li>“flosp” 模型对2D特征进行投影以生成3D场景特征。</li><li>“flosp_depth” 模型不仅对2D特征进行投影，还用深度信息进行额外的处理。</li></ul></li><li><p><strong>3D语义分割头部</strong>：这一部分负责将3D特征用于语义分割。具体的网络结构可能取决于不同的数据集（”NYU” 或 “kitti”）。</p></li><li><p><strong>2D语义分割头部</strong>：用于2D语义分割的头部网络。</p></li><li><p><strong>深度预测头部</strong>：根据需求，模型可以生成深度预测。</p></li><li><p><strong>训练、验证和测试步骤</strong>：这些步骤在不同的数据集和任务上运行，计算损失、评估性能，并记录指标。这包括分类损失、语义分割IoU、精度和召回。</p></li><li><p><strong>优化器和学习率调度器</strong>：在 <code>configure_optimizers</code> 方法中配置了优化器和学习率调度器。通常使用AdamW优化器和学习率衰减策略。</p></li><li><p><strong>模型的输入数据预处理</strong>：输入数据是图像，通过 <code>process_rgbs</code> 方法处理，生成特征图。</p></li><li><p><strong>模型的前向传播</strong>：通过 <code>forward</code> 方法执行模型的前向传播操作，包括特征提取、2D-3D投影、3D语义分割等。</p></li><li><p><strong>损失计算</strong>：根据任务类型和模型预测，计算不同的损失，包括分类损失、语义分割损失、深度损失等。</p></li><li><p><strong>评估指标</strong>：评估模型性能，包括IoU、精度、召回等。</p></li><li><p><strong>超参数配置</strong>：模型的超参数（如学习率、权重衰减等）在构造函数中进行了设置。</p></li><li><p><strong>导出模型和计算FLOPs和参数数量</strong>：通过条件选择，模型可以导出为ONNX格式，并计算模型的浮点运算数（FLOPs）和参数数量。</p></li></ol><p>总体而言，”OccDepth” 模型用于处理多视图数据的语义分割和深度预测任务，具有丰富的配置选项和评估指标，以满足不同数据集和任务的需求。这个模型的复杂性和功能强大，适用于一系列3D场景理解任务。</p><p><strong><a href="https://github.com/megvii-research/OccDepth/blob/main/occdepth/models/SFA.py">SFA.py</a>用于执行2D到3D的稀疏特征聚合。以下是该模块的功能和工作原理的简要说明：</strong></p><ul><li><p><code>SFA</code> 类继承自<code>nn.Module</code>，它包含了一个用于2D到3D投影的聚合过程。</p></li><li><p><code>__init__</code> 函数接受以下参数：</p><ul><li><code>scene_size</code>：3D场景的大小，通常表示为一个包含三个维度大小的元组或列表。</li><li><code>dataset</code>：表示使用的数据集的名称，例如”NYU”或”kitti”。</li><li><code>project_scale</code>：投影尺度，用于将2D特征映射到3D场景中。</li></ul></li><li><p><code>forward</code> 函数执行2D到3D的稀疏特征聚合操作：</p><ul><li>输入 <code>x2d</code> 是包含多个视图的2D特征的张量。</li><li><code>projected_pix</code> 是每个像素在3D场景中的投影坐标。</li><li><code>fov_mask</code> 是表示视野范围的掩码。</li></ul><p>  主要的工作步骤包括：</p><ol><li>对于每个视图，将2D特征映射到3D场景中。这是通过计算权重和聚合来实现的。</li><li>对不同视图之间的特征进行加权平均。</li><li>根据数据集类型（”NYU”或”kitti”）重塑3D场景特征的形状。</li></ol></li></ul><p>这个模块的关键思想是将多个2D视图的特征信息聚合到3D场景中，以增强3D场景的表示。这对于处理多视图或多帧输入的问题（如3D物体检测或重建）非常有用。模块的实现中包含了一系列的数学运算，包括加权平均和角度余弦相似度等。这有助于捕捉不同视图之间的相关性和信息融合。</p><p><a href="https://github.com/megvii-research/OccDepth/blob/main/occdepth/models/flosp_depth/flosp_depth.py">flosp_depth.py</a></p><p>这是一个名为 <code>FlospDepth</code> 的PyTorch模块，通常用于深度预测和点云处理。以下是该模块的功能和工作原理的简要说明：</p><ol><li><p>初始化和配置：</p><ul><li><code>__init__</code> 函数接受多个参数，包括场景边界、深度范围、输出通道数等，用于初始化模型的各个部分和配置。</li><li>可以选择不同的深度网络（<code>DepthNet</code>）配置。</li></ul></li><li><p>深度网络：</p><ul><li><code>DepthNet</code> 是一个用于预测深度图的子模块。</li><li>它接受输入特征图，相机内参矩阵等信息，并返回深度图。</li><li>深度图是在该模块中预测的，并且经过 softmax 处理。</li></ul></li><li><p>Voxel 特征聚合：</p><ul><li>从不同视角的深度图生成体素特征，这是通过采样视锥体积并将视锥体积中的深度信息投影到3D体素网格中实现的。</li><li>选择了不同的体素聚合方式，包括”mean”和”sum”，以聚合多个视角的信息。</li></ul></li><li><p>配置和参数：</p><ul><li>一些初始化参数，如体素大小、坐标、数量等，被存储在模块的缓冲区中，以便后续使用。</li><li>还有一些配置参数，如深度范围和体素网格大小等。</li></ul></li><li><p>推理模式和训练模式：</p><ul><li>模块支持两种模式，即推理模式和训练模式。</li><li>在推理模式中，模块可以接受缩放后的像素大小（<code>scaled_pixel_size</code>），以适应不同的尺度。</li><li>在训练模式中，模块接受相机内参矩阵、相机到世界坐标的变换矩阵等信息，用于生成体素网格。</li></ul></li></ol><p>这个模块的关键思想是从多个视角的深度图生成3D体素网格，然后通过合适的聚合方式将这些体素特征合并在一起。这对于许多3D场景理解任务，如点云分割、物体检测和语义分割，都是有用的。</p><h2 id="VoxFormer"><a href="#VoxFormer" class="headerlink" title="VoxFormer"></a>VoxFormer</h2><p>代码链接如下：<a href="https://github.com/NVlabs/VoxFormer">VoxFormer: a Cutting-edge Baseline for 3D Semantic Occupancy Prediction</a>, CVPR 2023</p><p>一种基于 Transformer 的语义场景完成框架，可以<strong>仅从 2D 图像输出完整的 3D 体积语义</strong>。我们的框架采用两阶段设计，从深度估计中一组稀疏的可见和占用体素查询开始，然后是从稀疏体素生成密集 3D 体素的致密化阶段。这种设计的一个关键思想是，2D 图像上的视觉特征仅对应于可见的场景结构，而不对应于被遮挡或空白的空间。因此，从可见结构的特征化和预测开始更为可靠。一旦我们获得了稀疏查询集，我们就应用屏蔽自动编码器设计，通过自注意力将信息传播到所有体素。</p><p><img src="/pic/VoxFormer.png"></p><p>VoxFormer的总体框架如上图所示。给定 RGB 图像，由 ResNet50 提取 2D 特征，并由现成的深度预测器估计深度。校正后的估计深度启用了与类别无关的查询建议阶段：将选择位于占用位置的查询来与图像特征进行可变形交叉注意。之后，将添加掩模标记以通过可变形自注意力来完成体素特征。精炼后的体素特征将被上采样并投影到输出空间以进行每体素语义分割。请注意，我们的框架支持单个或多个图像的输入。</p><p>VoxFormer由类不可知的查询建议（阶段-1）和类特定的语义分割（阶段-2）组成，其中阶段-1提出了一组稀疏的占用体素，阶段-2完成了从阶段-1给出的建议开始的场景表示。具体而言，阶段1具有轻量级的基于2D CNN的查询建议网络，该网络使用图像深度来重建场景几何结构。然后，它从整个视场上预定义的可学习体素查询中提出了一组稀疏的体素。</p><h3 id="代码分析"><a href="#代码分析" class="headerlink" title="代码分析"></a>代码分析</h3><p>第一阶段的查询建议网络(QPN)，根据深度决定要查询哪些体素:被占用的体素值得仔细关注，而空的体素可以从组中分离出来。给出二维RGB观测，首先基于深度估计得到场景的2.5D表示。然后，通过占用率预测获得三维查询位置，从而纠正图像深度不准确的问题。</p><p>代码<a href="https://github.com/NVlabs/VoxFormer/blob/main/projects/configs/voxformer/qpn.py">qpn.py</a>，基于mmdet3d</p><p>以下是对配置文件的更详细分析：</p><ol><li><p><code>_gamma_</code> 和 <code>_alpha_</code>：这些参数可能是用于模型的超参数调整，但配置文件中没有提供它们的具体用途。通常，<code>_gamma_</code> 和 <code>_alpha_</code> 可能会在训练过程中被动态地调整以提高模型性能。</p></li><li><p><code>_nsweep_</code>：指定了数据集中采样的扫描次数，这可能与数据集的采样策略和数据增强有关。在一些场景下，多次扫描可以提供更多的信息，例如在激光雷达数据中。</p></li><li><p><code>_depthmodel_</code>：这个参数指定了深度模型的名称或配置，定义了一个名为 MSNet3D 的三维立体匹配网络模型。这个模型主要用于深度估计和立体匹配任务。</p></li><li><p><code>point_cloud_range</code> 和 <code>voxel_size</code>：定义了点云的范围和体素的大小。这些参数用于将点云数据转化为体素表示，以便于模型的处理。<code>point_cloud_range</code> 指定了点云数据的范围，<code>voxel_size</code> 指定了体素的大小。</p></li><li><p><code>img_norm_cfg</code>：定义了图像的归一化配置，包括均值、标准差和是否将图像转化为RGB格式。这些配置通常用于对图像进行预处理，以便输入到深度学习模型中。</p></li><li><p><code>class_names</code>：包含目标类别的列表，用于指定模型需要检测或分类的物体类别。在这个配置中，包含了诸如’car’、’truck’、’pedestrian’等物体类别。</p></li><li><p><code>input_modality</code>：定义了数据输入模态，包括激光雷达、摄像机、雷达、地图和外部信息。这些模态可以用于训练和测试模型，根据任务需要选择合适的输入模态。</p></li><li><p><code>model</code>：指定了要训练的深度学习模型的配置。这个模型被命名为 <code>LMSCNet_SS</code>，并包括类别数、输入维度等信息。</p></li><li><p><code>train_cfg</code>：包含了训练配置，如点云的网格大小、体素大小、分配器配置等。这些配置参数影响了训练过程中的数据处理和损失计算。</p></li><li><p><code>dataset_type</code> 和 <code>data_root</code>：定义了数据集的类型和根目录，这将用于加载训练、验证和测试数据。</p></li><li><p><code>test_pipeline</code>：定义了测试数据的预处理管道，这里加载了多视角图像。预处理管道用于对输入数据进行处理以供模型使用。</p></li><li><p><code>data</code>：包含了数据集的设置，包括训练、验证和测试数据的类型、数据根目录、预处理根目录等。数据采样器也在这里配置。</p></li><li><p><code>optimizer</code> 和 <code>optimizer_config</code>：定义了优化器的类型、学习率、权重衰减等参数。<code>AdamW</code> 优化器用于模型的权重更新。</p></li><li><p><code>lr_config</code>：指定了学习率的调整策略，包括余弦退火、预热等。这些策略用于在训练过程中调整学习率。</p></li><li><p><code>total_epochs</code>：指定了总的训练轮数，模型将在这些轮数内进行训练。</p></li><li><p><code>evaluation</code>：定义了评估的间隔和评估的管道，用于在训练过程中定期评估模型的性能。</p></li><li><p><code>runner</code>：指定了训练器的类型和最大训练轮数，以及其他训练参数。</p></li><li><p><code>log_config</code>：包含了日志的配置，包括日志的记录间隔和日志类型。这些日志用于跟踪训练过程中的性能。</p></li><li><p><code>checkpoint_config</code>：定义了模型检查点的保存间隔，即模型在训练过程中的保存频率。</p></li></ol><p>分析一下_depthmodel_指定的MSNet3D和model指定的LMSCNet_SS模型</p><p>首先逐步分析代码<code>MSNet3D</code>的关键部分：</p><ol><li><p><code>hourglass3D</code> 类：这是一个用于定义三维卷积层的类，由多个 MobileV2_Residual_3D 模块组成。MobileV2_Residual_3D 模块用于构建深度神经网络的基本构建块。这个类定义了前向传播方法，通过堆叠卷积和反卷积层来构建一个”U”形网络。</p></li><li><p><code>MSNet3D</code> 类：这是主要的网络模型，包含了特征提取、特征匹配、立体匹配和深度估计等部分。它包括以下关键组件：</p><ul><li><code>feature_extraction</code>：特征提取模块。</li><li><code>dres0</code> 和 <code>dres1</code>：用于处理立体匹配代价体积的 MobileV2_Residual_3D 模块。</li><li><code>encoder_decoder1</code>、<code>encoder_decoder2</code> 和 <code>encoder_decoder3</code>：使用 <code>hourglass3D</code> 模块实现的编码器-解码器结构。</li><li><code>classif0</code>、<code>classif1</code>、<code>classif2</code> 和 <code>classif3</code>：用于执行立体匹配和深度估计的模块。</li></ul><p>在前向传播方法中，通过输入左视图和右视图的图像数据 <code>L</code> 和 <code>R</code>，首先提取特征，然后构建立体匹配代价体积。接下来，对代价体积进行多尺度编码器-解码器处理，最终产生深度估计。</p></li></ol><p>再好好分析一下<code>LMSCNet_SS</code>这个模型的主要组件和架构摘要：</p><ol><li><p>这个模型在<code>mmdet3d</code>框架中注册为自定义检测器。</p></li><li><p>它继承自<code>MVXTwoStageDetector</code>，这是<code>mmdet3d</code>框架中用于3D物体检测的基础类。</p></li><li><p>模型包括多个组件，包括编码器、解码器和分割头。它以3D占用网格数据作为输入，用于预测语义标签。</p></li><li><p>编码器通过多个2D卷积层处理输入数据，将特征图下采样到较小的尺寸。</p></li><li><p>解码器由一系列反卷积层组成，将特征图上采样到所需的输出比例。</p></li><li><p>分割头进一步处理特征图，以生成语义分割预测。</p></li><li><p>训练时计算损失，其中包括二元交叉熵损失（BCE）和其他特定于语义分割的损失。</p></li><li><p><code>forward</code> 方法用于处理训练和测试模式，训练时返回损失，测试时返回预测。</p></li><li><p>代码包括用于存储的二进制数据的打包和解包函数。</p></li><li><p>使用<code>auto_fp16</code>装饰器可以自动应用混合精度训练。</p></li></ol><p>第2阶段基于一种新颖的稀疏到密集类MAE架构，如图（a）所示。它首先通过允许所提出的体素关注图像观察来加强其特征化。接下来，未提出的体素将与可学习的掩码标记相关联，并且将通过自注意来处理全套体素，以完成每体素语义分割的场景表示。</p><p><img src="/pic/VoxFormer2.png"></p><p>请注意，这里提供了两个版本的VoxFormer，一个只以当前图像作为输入(<a href="https://github.com/NVlabs/VoxFormer/blob/main/projects/configs/voxformer/voxformer-S.py">VoxFormer- S</a>)，另一个以当前图像和前4个图像作为输入(<a href="https://github.com/NVlabs/VoxFormer/blob/main/projects/configs/voxformer/voxformer-T.py">VoxFormer- T</a>)。</p><p><strong>VoxFormer-S是一个用于3D物体检测的模型，该模型使用多通道LiDAR数据和图像数据进行训练。以下是对这个配置文件的主要部分进行的详细分析：</strong></p><ol><li><p><code>work_dir</code> 定义了模型训练期间的工作目录，训练日志和模型检查点将在此目录中保存。在这里，工作目录被设置为’result&#x2F;voxformer-S’。</p></li><li><p><code>_base_</code> 包含了其他配置文件的路径，这些文件会被合并到当前配置中。在这里，使用了默认的运行时配置文件’default_runtime.py’。</p></li><li><p><code>plugin</code> 和 <code>plugin_dir</code> 是用于配置MMDet3D插件的选项。<code>plugin</code> 设置为True，表示启用插件，而<code>plugin_dir</code> 指定了插件的目录。</p></li><li><p><code>_num_layers_cross_</code> 和 <code>_num_points_cross_</code> 分别定义了交叉变换器（cross_transformer）中的层数和点数。这些参数用于模型的 Transformer 层设置。</p></li><li><p><code>_dim_</code> 定义了模型的嵌入维度。在这里，它被设置为128。</p></li><li><p><code>_pos_dim_</code> 定义了位置编码的维度，通常是嵌入维度的一半。这里的值由_dim_除以2计算。</p></li><li><p><code>_ffn_dim_</code> 定义了Feedforward网络的维度，通常是嵌入维度的两倍。在这里，它被设置为256。</p></li><li><p><code>_num_levels_</code> 定义了输出级别的数量。这里设置为1，表示只有一个输出级别。</p></li><li><p><code>_labels_tag_</code> 和 <code>_num_cams_</code> 用于配置数据集标签和相机的数量。这些参数是与数据集相关的配置。</p></li><li><p><code>point_cloud_range</code> 和 <code>voxel_size</code> 分别定义了点云范围和体素大小。这些参数也是与数据集相关的配置。</p></li><li><p><code>_sem_scal_loss_</code> 和 <code>_geo_scal_loss_</code> 用于配置语义分割损失和几何损失的选项。这些参数控制是否启用语义分割损失和几何损失。</p></li><li><p><code>_depthmodel_</code> 是深度模型的选择。它在数据集中用于深度估计。</p></li><li><p><code>_nsweep_</code> 定义了查询建议网络的扫描次数。这是一个与数据集和模型训练相关的参数。</p></li><li><p><code>model</code> 部分定义了VoxFormer模型的结构，包括图像骨干网络、特征金字塔网络、以及交叉和自注意力变换器。模型结构的配置包括不同的组件和层，用于处理图像和点云数据。</p></li><li><p><code>dataset_type</code> 和 <code>data</code> 部分配置了数据集的类型、数据根目录以及训练、验证和测试数据集的设置。这些设置与数据集的加载和处理相关。</p></li><li><p><code>optimizer</code>、<code>optimizer_config</code> 和 <code>lr_config</code> 用于配置优化器、学习率衰减策略以及其他优化选项。这些设置用于模型训练的优化配置。</p></li><li><p><code>total_epochs</code> 定义了总的训练周期数。在这里，设置为20个周期。</p></li><li><p><code>evaluation</code> 部分配置了评估的间隔。这指定了多少个周期后进行一次模型评估。</p></li><li><p><code>runner</code> 部分定义了训练过程的运行器。这包括如何管理训练循环以及模型的保存和加载。</p></li><li><p><code>log_config</code> 部分配置了训练日志的记录方式。在这里，设置了文本记录和Tensorboard记录。</p></li><li><p><code>checkpoint_config</code> 用于配置模型检查点的保存间隔。在这里，设置为None，表示没有指定检查点的保存间隔。</p></li></ol><p><strong>VoxFormer- T一个用于3D物体检测的模型，结合了LiDAR点云数据和图像数据。以下是对这个配置文件的主要部分进行的详细分析：</strong></p><ol><li><p><code>work_dir</code> 定义了模型训练期间的工作目录，训练日志和模型检查点将在此目录中保存。在这里，工作目录被设置为’result&#x2F;voxformer-T’。</p></li><li><p><code>_base_</code> 包含了其他配置文件的路径，这些文件会被合并到当前配置中。在这里，使用了默认的运行时配置文件’default_runtime.py’。</p></li><li><p><code>plugin</code> 和 <code>plugin_dir</code> 是用于配置MMDet3D插件的选项。<code>plugin</code> 设置为True，表示启用插件，而<code>plugin_dir</code> 指定了插件的目录。</p></li><li><p><code>_num_layers_cross_</code> 和 <code>_num_points_cross_</code> 分别定义了交叉变换器（cross_transformer）中的层数和点数。这些参数用于模型的 Transformer 层设置。</p></li><li><p><code>_num_layers_self_</code> 和 <code>_num_points_self_</code> 分别定义了自注意力变换器（self_transformer）中的层数和点数。这些参数也用于模型的 Transformer 层设置。</p></li><li><p><code>_dim_</code> 定义了模型的嵌入维度。在这里，它被设置为128。</p></li><li><p><code>_pos_dim_</code> 定义了位置编码的维度，通常是嵌入维度的一半。这里的值由_dim_除以2计算。</p></li><li><p><code>_ffn_dim_</code> 定义了Feedforward网络的维度，通常是嵌入维度的两倍。在这里，它被设置为256。</p></li><li><p><code>_num_levels_</code> 定义了输出级别的数量。这里设置为1，表示只有一个输出级别。</p></li><li><p><code>_labels_tag_</code> 和 <code>_num_cams_</code> 用于配置数据集标签和相机的数量。这些参数是与数据集相关的配置。</p></li><li><p><code>_temporal_</code> 定义了用于数据集的时间轴信息。在这里，使用了一个包含-12、-9、-6和-3的列表，这表示了不同时间步长的输入数据。</p></li><li><p><code>point_cloud_range</code> 和 <code>voxel_size</code> 分别定义了点云范围和体素大小。这些参数也是与数据集相关的配置。</p></li><li><p><code>_sem_scal_loss_</code> 和 <code>_geo_scal_loss_</code> 用于配置语义分割损失和几何损失的选项。这些参数控制是否启用语义分割损失和几何损失。</p></li><li><p><code>_depthmodel_</code> 是深度模型的选择。它在数据集中用于深度估计。</p></li><li><p><code>_nsweep_</code> 定义了查询建议网络的扫描次数。这是一个与数据集和模型训练相关的参数。</p></li><li><p><code>model</code> 部分定义了VoxFormer-T模型的结构，包括图像骨干网络、特征金字塔网络、以及交叉和自注意力变换器。模型结构的配置包括不同的组件和层，用于处理图像和点云数据。</p></li><li><p><code>dataset_type</code> 和 <code>data</code> 部分配置了数据集的类型、数据根目录以及训练、验证和测试数据集的设置。这些设置与数据集的加载和处理相关。</p></li><li><p><code>optimizer</code>、<code>optimizer_config</code> 和 <code>lr_config</code> 用于配置优化器、学习率衰减策略以及其他优化选项。这些设置用于模型训练的优化配置。</p></li><li><p><code>total_epochs</code> 定义了总的训练周期数。在这里，设置为20个周期。</p></li><li><p><code>evaluation</code> 部分配置了评估的间隔。这指定了多少个周期后进行一次模型评估。</p></li><li><p><code>runner</code> 部分定义了训练过程的运行器。这包括如何管理训练循环以及模型的保存和加载。</p></li><li><p><code>log_config</code> 部分配置了训练日志的记录方式。在这里，设置了文本记录和Tensorboard记录。</p></li><li><p><code>checkpoint_config</code> 用于配置模型检查点的保存间隔。在这里，设置为None，表示没有指定检查点的保存间隔。</p></li></ol><p><strong>这两份代码之间的主要区别：</strong></p><ol><li><p><strong>时间轴差异</strong>:</p><ul><li><code>voxformer-S.py</code> 使用了一个空的时间轴，<code>_temporal_</code> 参数为空列表。</li><li><code>voxformer-T.py</code> 使用了包含多个时间步长的时间轴，<code>_temporal_</code> 参数设置为包含了[-12, -9, -6, -3]的列表。</li></ul></li><li><p><strong>相机数量差异</strong>:</p><ul><li><code>voxformer-S.py</code> 中的 <code>_num_cams_</code> 参数设置为1，表示只使用单个相机。</li><li><code>voxformer-T.py</code> 中的 <code>_num_cams_</code> 参数设置为5，表示使用5个相机。</li></ul></li><li><p><strong>输出级别的数量</strong>:</p><ul><li>两者中的 <code>model</code> 部分都设置了 <code>num_outs</code> 参数 <code>_num_levels_</code>，但两者都将其设置为1，表示只有一个输出级别。</li></ul></li><li><p>**<a href="https://github.com/zhangyp15/OccFormer">OccFormer: Dual-path Transformer for Vision-based 3D Semantic Occupancy Prediction</a>, ICCV 2023</p><ul><li><code>voxformer-S.py</code> 和 <code>voxformer-T.py</code> 都配置了相同的数据集类型 <code>dataset_type</code>，数据根目录 <code>data_root</code>，以及训练、验证和测试数据集的相关设置。</li></ul></li></ol><p>总的来说，主要的区别在于时间轴配置、相机数量以及数据集的数据加载。<code>voxformer-S.py</code> 使用了一个静态的时间轴和单个相机，而 <code>voxformer-T.py</code> 使用了多个时间步长和多个相机。这些差异可能是用于处理不同类型数据集或任务的配置。要选择合适的配置，需要考虑你的数据集和任务的特定需求。</p><h2 id="OccFormer"><a href="#OccFormer" class="headerlink" title="OccFormer"></a>OccFormer</h2><p>代码链接：<a href="https://github.com/zhangyp15/OccFormer">OccFormer: Dual-path Transformer for Vision-based 3D Semantic Occupancy Prediction</a>, ICCV 2023</p><p>自动驾驶的视觉感知经历了从鸟瞰图（BEV）表示到 3D 语义占用的转变。与BEV平面相比，3D语义占用进一步提供了沿垂直方向的结构信息。本文提出了 OccFormer，一种双路径transformer网络，可有效处理 3D 体积以进行语义占用预测。OccFormer 实现了对相机生成的 3D 体素特征的远程、动态且高效的编码。它是通过将繁重的 3D 处理分解为沿水平面的局部和全局transformer路径而获得的。对于占用解码器，我们通过提出保留池和类引导采样来适应 3D 语义占用的普通 Mask2Former，这显着减轻了稀疏性和类不平衡。实验结果表明，OccFormer 显着优于 SemanticKITTI 数据集上的语义场景完成和 nuScenes 数据集上的 LiDAR 语义分割的现有方法。</p><p><img src="/pic/OccFormer.png"></p><p>该Pipeline由用于提取多尺度2D特征的图像编码器、用于将2D特征提升到3D体积的图像到3D变换以及用于获得3D语义特征并预测3D语义占用的基于变换器的编码器-解码器组成。</p><h3 id="代码分析-1"><a href="#代码分析-1" class="headerlink" title="代码分析"></a>代码分析</h3><p><a href="https://github.com/zhangyp15/OccFormer/blob/main/projects/configs/occformer_kitti/occformer_kitti.py">occformer_kitti.py</a>这段代码是关于 <code>OccFormer</code> 模型的kitti数据集的配置，以下是对主要部分的详细分析：</p><ol><li><p><strong>基础配置</strong>:</p><ul><li><code>_base_</code> 参数包括两个配置文件，一个是 <code>custom_nus-3d.py</code>，另一个是 <code>default_runtime.py</code>。</li><li><code>sync_bn</code> 和 <code>plugin</code> 分别设置为 <code>True</code>，启用了同步批处理规范化（Batch Normalization）和插件（plugin）功能。</li></ul></li><li><p><strong>数据设置</strong>:</p><ul><li><code>class_names</code> 定义了类别的名称，包括一些物体类别（如汽车、自行车等）和一些特殊类别（如未标记、道路、人等），总共有20个类别。</li><li><code>point_cloud_range</code> 指定了点云的范围。</li><li><code>occ_size</code> 定义了3D体素的尺寸。</li><li><code>lss_downsample</code> 指定了生成3D体素（Local Semantic Segmentation, LSS）时的下采样比率。</li><li><code>voxel_size</code> 是通过计算 <code>point_cloud_range</code> 和 <code>occ_size</code> 得出的体素大小。</li></ul></li><li><p><strong>模型配置</strong>:</p><ul><li><code>model</code> 部分配置了 <code>OccFormer</code> 模型的架构，包括以下组件：<ul><li><code>img_backbone</code> 使用自定义的EfficientNet骨干网络。</li><li><code>img_neck</code> 使用SECONDFPN的颈部结构。</li><li><code>img_view_transformer</code> 使用 <code>ViewTransformerLiftSplatShootVoxel</code> 进行图像到点云的转换。</li><li><code>img_bev_encoder_backbone</code> 使用 <code>OccupancyEncoder</code> 进行BEV编码。</li><li><code>img_bev_encoder_neck</code> 使用多层Transformer解码器。</li><li><code>pts_bbox_head</code> 配置了 <code>Mask2FormerOccHead</code> 头部，用于形状转换、损失计算等。</li></ul></li></ul></li><li><p><strong>数据集配置</strong>:</p><ul><li>数据集类型 <code>dataset_type</code> 设置为 <code>CustomSemanticKITTILssDataset</code>。</li><li><code>data</code> 部分配置了数据加载和处理的设置，包括训练、验证和测试数据集。</li></ul></li><li><p><strong>优化器和学习率策略</strong>:</p><ul><li><code>optimizer</code> 部分配置了AdamW优化器，包括学习率、权重衰减等设置。</li><li><code>lr_config</code> 配置了学习率策略，采用阶段性的学习率衰减策略。</li></ul></li><li><p><strong>检查点和日志配置</strong>:</p><ul><li><code>checkpoint_config</code> 配置了检查点的保存策略。</li><li><code>runner</code> 部分定义了运行的类型和最大训练轮数。</li><li><code>evaluation</code> 部分配置了模型的评估策略。</li></ul></li></ol><p>总结，这段代码配置了 <code>OccFormer</code> 模型，包括图像和点云的处理、模型架构、数据加载、优化器、学习率策略和训练设置等。这个模型用于语义分割任务，能够将点云数据映射到3D体素空间，然后使用Transformer结构进行处理，最终输出语义分割结果。这是一个复杂的3D视觉模型，适用于点云数据的语义分割任务。</p><p><a href="https://github.com/zhangyp15/OccFormer/blob/main/projects/configs/occformer_nusc/occformer_nusc_r50_256x704.py">occformer_nusc_r50_256x704.py</a>这段代码是关于 <code>OccFormer</code> 模型的nuScenes数据集的配置，以下是对主要部分的详细分析：</p><ol><li><p><strong>基础配置</strong>:</p><ul><li><code>_base_</code> 参数包括两个配置文件：<code>custom_nus-3d.py</code> 和 <code>default_runtime.py</code>。</li><li><code>sync_bn</code> 和 <code>plugin</code> 分别设置为 <code>True</code>，启用了同步批处理规范化（Batch Normalization）和插件（plugin）功能。</li><li><code>plugin_dir</code> 指定插件的目录路径。</li><li><code>img_norm_cfg</code> 包含了图像的标准化配置，包括均值、标准差和是否转换为RGB格式。</li></ul></li><li><p><strong>类别和点云范围</strong>:</p><ul><li><code>class_names</code> 包含了物体类别的名称，共17个类别。</li><li><code>num_class</code> 表示类别的总数。</li><li><code>point_cloud_range</code> 定义了点云的范围。</li><li><code>occ_size</code> 指定了3D体素的尺寸。</li><li><code>lss_downsample</code> 是用于生成3D体素（Local Semantic Segmentation, LSS）时的下采样比率。</li><li><code>voxel_size</code> 是通过计算 <code>point_cloud_range</code> 和 <code>occ_size</code> 得出的体素大小。</li></ul></li><li><p><strong>数据配置</strong>:</p><ul><li><code>data_config</code> 包含了数据加载和处理的设置，包括相机信息、输入图像大小、数据增强参数等。</li><li><code>grid_config</code> 包含了3D卷积网格的配置，定义了xyz范围和体素大小等信息。</li></ul></li><li><p><strong>模型配置</strong>:</p><ul><li><code>model</code> 部分配置了 <code>OccupancyFormer</code> 模型，包括以下组件：<ul><li><code>img_backbone</code> 使用了ResNet-50骨干网络。</li><li><code>img_neck</code> 使用SECONDFPN的颈部结构。</li><li><code>img_view_transformer</code> 用于将图像信息转换为3D体素。</li><li><code>img_bev_encoder_backbone</code> 使用 <code>OccupancyEncoder</code> 进行BEV编码。</li><li><code>img_bev_encoder_neck</code> 使用多层Transformer解码器。</li><li><code>pts_bbox_head</code> 配置了 <code>Mask2FormerNuscOccHead</code> 头部，用于形状转换、损失计算等。</li></ul></li></ul></li><li><p><strong>数据集配置</strong>:</p><ul><li><code>dataset_type</code> 设置为 <code>CustomNuScenesOccLSSDataset</code>，指定了数据集类型。</li><li><code>data_root</code> 指定了数据集的根目录。</li><li><code>train_pipeline</code> 和 <code>test_pipeline</code> 分别配置了训练和测试数据的处理流程。</li></ul></li><li><p><strong>优化器和学习率策略</strong>:</p><ul><li><code>optimizer</code> 部分配置了AdamW优化器，包括学习率、权重衰减等设置。</li><li><code>optimizer_config</code> 包含了梯度剪切的设置。</li><li><code>lr_config</code> 配置了学习率策略，采用阶段性的学习率衰减策略。</li></ul></li><li><p><strong>检查点和日志配置</strong>:</p><ul><li><code>checkpoint_config</code> 配置了检查点的保存策略。</li><li><code>runner</code> 部分定义了运行的类型和最大训练轮数。</li><li><code>evaluation</code> 部分配置了模型评估的策略，包括评估间隔、评估指标等。</li></ul></li></ol><p><strong>这两段代码是针对不同的任务和数据集的配置文件，它们的主要区别在于以下几个方面：</strong></p><ol><li><p><strong>任务和数据集</strong>:</p><ul><li>第一个代码段是针对SemanticKITTI数据集的，主要用于3D语义分割任务。</li><li>第二个代码段是为了处理NuScenes数据集的，用于3D物体检测和语义分割任务。</li></ul></li><li><p><strong>类别和数据范围</strong>:</p><ul><li>第一个代码段中的 <code>class_names</code> 包含了SemanticKITTI数据集的类别，而第二个代码段的 <code>class_names</code> 包含了NuScenes数据集的类别，因此它们的类别列表是不同的。</li><li><code>point_cloud_range</code> 和 <code>occ_size</code> 也在两个代码段中有所不同，因为它们对应不同数据集的点云范围和3D体素尺寸。</li></ul></li><li><p><strong>数据处理和数据加载</strong>:</p><ul><li>两个代码段中的数据加载和处理流程是不同的。第一个代码段包含了用于加载SemanticKITTI数据的处理流程，而第二个代码段包含了用于加载NuScenes数据的处理流程。</li></ul></li><li><p><strong>模型配置</strong>:</p><ul><li>模型配置在两个代码段中也存在差异。虽然它们都使用了Transformer结构，但底层的图像骨干网络、特征提取过程、解码器结构等都可能有所不同。</li></ul></li><li><p><strong>数据路径和检查点路径</strong>:</p><ul><li>数据路径和检查点路径在两个配置中也不同。它们指定了数据集的根目录和用于预训练模型的检查点文件。</li></ul></li></ol><p>总的来说，这两段代码主要区别在于它们的应用领域、数据集和任务的不同。第一个代码段适用于SemanticKITTI数据集的3D语义分割任务，而第二个代码段则适用于NuScenes数据集的3D物体检测和语义分割任务。每个配置都经过仔细调整，以满足其特定的数据和任务需求。</p><h2 id="TPVFormer"><a href="#TPVFormer" class="headerlink" title="TPVFormer"></a>TPVFormer</h2><p>代码链接： <a href="https://github.com/wzzheng/TPVFormer">TPVFormer: An academic alternative to Tesla’s Occupancy Network</a>, CVPR2023</p><p>以视觉为中心的自动驾驶感知的现代方法广泛采用鸟瞰图（BEV）表示来描述 3D 场景。尽管它比体素表示效率更高，但它很难用单个平面描述场景的细粒度 3D 结构。为了解决这个问题，我们提出了一种三透视图（TPV）表示法，它与 BEV 一起提供了两个额外的垂直平面。我们通过对三个平面上的投影特征求和来对 3D 空间中的每个点进行建模。为了将图像特征提升到3D TPV空间，我们进一步提出了一种基于transformer的TPV编码器（TPVFormer）以有效地获得TPV特征。我们采用注意力机制来聚合每个 TPV 平面中每个查询对应的图像特征。实验表明，我们用稀疏监督训练的模型可以有效地预测所有体素的语义占用率。我们首次证明，在 nuScenes 上的 LiDAR 分割任务中，仅使用相机输入就可以实现与基于 LiDAR 的方法相当的性能。</p><p><img src="/pic/TPVFormer.png"></p><p>三维语义占用预测TPVFormer框架。我们采用一个图像骨干网来提取多摄像机图像的多尺度特征。然后通过交叉注意自适应提升二维特征到TPV空间，并利用交叉视图混合注意实现TPV平面之间的交互。为了预测三维空间中一个点的语义占用情况，我们在三个TPV平面上的投影特征的总和上应用一个轻量级预测头。</p><h3 id="代码分析-2"><a href="#代码分析-2" class="headerlink" title="代码分析"></a>代码分析</h3><p>这里我们只看TPVFormer 进行 3D 语义占用预测任务代码<a href="https://github.com/wzzheng/TPVFormer/blob/main/config/tpv04_occupancy.py">tpv04_occupancy.py</a></p><p>这段代码是一个配置文件，用于定义一个名为 “TPVFormer” 的模型，它的主要组成部分和参数如下：</p><ol><li><p><strong>基础配置</strong>:</p><ul><li><code>_base_</code> 中包含了用于数据集、优化器和学习率策略的配置文件路径。</li></ul></li><li><p><strong>数据集参数</strong>:</p><ul><li><code>dataset_params</code> 包含了数据集相关的参数，如数据版本、标签映射、数据空间的范围等。</li></ul></li><li><p><strong>模型类型</strong>:</p><ul><li><code>TPVFormer</code> 是所使用的模型类型。</li></ul></li><li><p><strong>TPV Aggregator</strong>:</p><ul><li><code>tpv_aggregator</code> 部分定义了一个名为 “TPVAggregator” 的模块，用于聚合TPV（Top-View Pillar）特征。</li><li>这个模块包括了输入和输出维度、TPV空间的尺寸、类别数等参数。</li></ul></li><li><p><strong>图像骨干网络</strong>:</p><ul><li><code>img_backbone</code> 定义了一个ResNet类型的图像骨干网络，用于从输入图像中提取特征。</li><li>这个网络的参数包括网络深度、输出特征层级、冻结的阶段、Batch Normalization等。</li></ul></li><li><p><strong>图像颈部（FPN）</strong>:</p><ul><li><code>img_neck</code> 定义了一个FPN类型的颈部网络，用于生成不同层级的特征金字塔。</li><li>这个网络将图像骨干网络的输出特征进行特征金字塔处理。</li></ul></li><li><p><strong>TPV Head</strong>:</p><ul><li><code>tpv_head</code> 定义了模型的头部，用于处理TPV特征。</li><li>这个部分包括了TPV空间的参数、特征维度、位置编码等。</li><li>它还包含了一个编码器，用于处理TPV特征。编码器中包括了多层的TPVFormerLayer，每一层包括了自注意力、跨视图注意力等组件。</li></ul></li></ol><p>这里我们主要看一下使用的<a href="https://github.com/wzzheng/TPVFormer/blob/main/tpvformer04/tpvformer.py">TPVFormer模型</a>的代码以及为 <a href="https://github.com/wzzheng/TPVFormer/blob/main/tpvformer04/tpv_aggregator.py">TPVAggregator</a>的模块，用于聚合TPV（Top-View Pillar）特征</p><p><strong>下面是对 TPVFormer 模型的代码的分析：</strong></p><ol><li><p>TPVFormer 继承了 <code>BaseModule</code>，这是一个基础的模型类，它是 mmcv（MMLab Computer Vision）和 mmseg（MMSegmentation）框架中的模型定义类。</p></li><li><p><code>__init__</code> 函数初始化 TPVFormer 模型，接受多个参数：</p><ul><li><code>use_grid_mask</code>: 控制是否使用 Grid Mask 数据增强。</li><li><code>img_backbone</code>: 图像骨干网络，用于提取图像特征。</li><li><code>img_neck</code>: 图像颈部网络，可选的，用于进一步处理图像特征。</li><li><code>tpv_head</code>: TPVFormer 头部网络，用于执行语义分割任务。</li><li><code>pretrained</code>: 预训练模型的配置。</li><li><code>tpv_aggregator</code>: 用于聚合 TPV 特征的头部网络。</li></ul></li><li><p>在 <code>__init__</code> 函数中，根据传入的参数初始化 TPVFormer 模型的各个组件，包括图像骨干网络、图像颈部网络、TPVFormer 头部网络以及 TPV 聚合器。如果提供了预训练模型，则将其配置传递给图像骨干网络。</p></li><li><p>TPVFormer 模型支持使用 Grid Mask 数据增强。Grid Mask 是一种数据增强方法，可以随机遮挡输入图像的一部分，从而增加模型的鲁棒性。</p></li><li><p><code>extract_img_feat</code> 函数用于提取图像特征，接受图像数据 <code>img</code> 作为输入。在函数内部，它首先对输入的图像进行形状变换，然后应用 Grid Mask 数据增强（如果启用），接着通过图像骨干网络提取图像特征，最后通过图像颈部网络进行进一步处理。</p></li><li><p><code>forward</code> 函数是 TPVFormer 模型的前向传播函数。它接受图像数据 <code>img</code> 和其他输入参数，调用 <code>extract_img_feat</code> 函数提取图像特征，然后将这些特征传递给 TPVFormer 头部网络 <code>tpv_head</code> 执行语义分割任务。最后，模型将输出结果传递给 TPV 聚合器 <code>tpv_aggregator</code> 进行进一步处理。</p></li></ol><p>总之，TPVFormer 是一个用于图像语义分割任务的模型，它包括图像骨干网络、图像颈部网络以及 TPVFormer 头部网络，可以处理输入的图像数据和点云数据，通过 TPV 聚合器聚合特征，以生成最终的语义分割结果。模型还支持 Grid Mask 数据增强以提高模型的鲁棒性。</p><p><strong>下面是对 TPVAggregator 模块的代码的分析：</strong></p><ol><li><p>TPVAggregator 继承了 <code>BaseModule</code>，这是 mmseg（MMSegmentation）框架中的头部模块类。</p></li><li><p><code>__init__</code> 函数初始化 TPVAggregator 模块，接受多个参数：</p><ul><li><code>tpv_h</code>, <code>tpv_w</code>, <code>tpv_z</code>: TPV 的高度、宽度和深度。</li><li><code>nbr_classes</code>: 类别数，即目标类别的数量。</li><li><code>in_dims</code>, <code>hidden_dims</code>, <code>out_dims</code>: 输入、隐藏和输出维度。</li><li><code>scale_h</code>, <code>scale_w</code>, <code>scale_z</code>: 高度、宽度和深度的缩放因子。</li><li><code>use_checkpoint</code>: 是否使用 PyTorch 的 checkpoint 功能。</li></ul></li><li><p>TPVAggregator 模块包括一个简单的神经网络，其中包含线性层（Linear）和 Softplus 激活函数用于从输入特征中提取特征，然后通过线性分类层进行目标分类。</p></li><li><p><code>forward</code> 函数用于执行前向传播操作，接受 TPV 特征列表 <code>tpv_list</code> 和点云数据 <code>points</code>（可选）。在函数内部，首先对 TPV 特征进行形状变换和插值操作，以匹配点云数据的尺寸和位置。然后，如果提供了点云数据，将点云数据映射到 TPV 特征上，执行点云与 TPV 特征的融合。最后，将融合后的特征传递给线性层和分类器进行分类，并返回分类结果。</p></li><li><p>如果未提供点云数据，则仅将 TPV 特征进行插值和融合，并执行分类。最终的分类结果以形状 <code>(batch_size, num_classes, scale_w * tpv_w, scale_h * tpv_h, scale_z * tpv_z)</code> 返回。</p></li></ol><p>总之，TPVAggregator 模块用于聚合 TPV 特征和点云数据，以执行语义分割任务。它包含线性层和分类器，用于将输入特征映射到目标类别的概率分布。这个模块的设计旨在结合点云数据和 TPV 特征，以提高语义分割的性能。</p>]]></content>
      
      
      <categories>
          
          <category> DL </category>
          
      </categories>
      
      
        <tags>
            
            <tag> ssc </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>OCC-VO:生成面向自动驾驶的基于3D占用栅格的视觉里程计稠密地图</title>
      <link href="/2023/11/04/occ-vo/"/>
      <url>/2023/11/04/occ-vo/</url>
      
        <content type="html"><![CDATA[<h1 id="OCC-VO-Dense-Mapping-via-3D-Occupancy-Based-Visual-Odometry-for-Autonomous-Driving"><a href="#OCC-VO-Dense-Mapping-via-3D-Occupancy-Based-Visual-Odometry-for-Autonomous-Driving" class="headerlink" title="OCC-VO: Dense Mapping via 3D Occupancy-Based Visual Odometry for Autonomous Driving"></a>OCC-VO: Dense Mapping via 3D Occupancy-Based Visual Odometry for Autonomous Driving</h1><p>论文链接：<a href="https://arxiv.org/pdf/2309.11011.pdf">OCC-VO-pdf</a><br>代码链接：<a href="https://github.com/USTCLH/OCC-VO">OCC-VO-py</a></p><p>生成面向自动驾驶的基于3D占用栅格的视觉里程计稠密地图</p><p>主要贡献：</p><ol><li><p><strong>OCC-VO框架的设计和开发：</strong> 作者设计并开发了OCC-VO框架，该框架接受环视相机图像作为输入，并生成密集语义地图，有助于增强场景理解，从而支持感知和导航等下游任务。</p></li><li><p><strong>3D语义占用预测模块：</strong> 在OCC-VO框架中，使用了名为TPV-Former的开源3D语义占用预测模块，用于将环视相机图像转化为3D语义占用栅格，实现对环境的语义理解。</p></li><li><p><strong>位姿估计和地图算法的定制：</strong> 为了解决3D语义占用地图的配准问题，作者设计了一种专为此任务定制的位姿估计和地图算法。该算法以GICP算法为基础，结合语义约束，以更好地对齐点云数据，特别适用于处理具有相似几何结构但不同语义的场景，如自动驾驶道路表面。</p></li><li><p><strong>全局语义地图的构建：</strong> 在地图创建阶段，借鉴了PFilter中的思想，消除了不可靠的点，从而创建了一个更稳健的全局语义地图。</p></li><li><p><strong>精准的地图和姿态估计：</strong> 最终的成果是经过精心调整的姿态估计和高度准确的地图，为感知和导航任务提供了可靠的基础。</p></li></ol><p>这些贡献共同构成了这项工作的核心，旨在提高环境理解和导航系统的性能，特别是在自动驾驶等领域中。</p><p>这段文本提供了关于OCC-VO框架的详细信息，包括作者、代码地址以及主要贡献。以下是以Markdown形式的总结：</p><h2 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h2><p>OCC-VO是一个新颖的框架，充分利用深度学习技术，将2D相机图像转化为3D语义占用，以解决自主系统中的视觉里程计（VO）挑战。它使用TPV-Former模块将环视相机图像转化为3D语义占用，经过特定设计的位姿估计和地图算法，包括语义标签滤波器、动态对象滤波器以及Voxel PFilter，生成密集的全局语义地图。在Occ3D-nuScenes数据集上的评估结果表明，OCC-VO相较于ORB-SLAM3取得了更高的成功率和轨迹准确性，成功率提高了20.6%，轨迹准确性提高了29.6%。</p><h2 id="主要贡献"><a href="#主要贡献" class="headerlink" title="主要贡献"></a>主要贡献</h2><ol><li><p>设计和开发OCC-VO框架，将环视相机图像转化为密集语义地图，用于增强场景理解和支持感知和导航任务。</p></li><li><p>使用TPV-Former模块，将环视相机图像转化为3D语义占用栅格。</p></li><li><p>定制位姿估计和地图算法，包括语义标签滤波器、动态对象滤波器，以及Voxel PFilter，以提高配准和全局地图一致性。</p></li><li><p>在Occ3D-nuScenes数据集上的评估，显示OCC-VO在自动驾驶场景中取得了较高的准确性和稳健性，尤其相较于ORB-SLAM3。</p></li></ol><p><img src="/pic/OCC-VO.png"></p><p>如上图所示，将同时被环绕视图摄像机捕获的6幅图像，由TPV-Former转换为3D语义占用。将得到的三维语义占用率作为点云，通过与全局语义图的配准估计出每一帧的姿态。具体来说，我们使用了GICP算法两次。首先，建立点之间的粗糙对应关系。在此之后，我们使用语义标签过滤器和动态对象过滤器来丢弃错误的匹配，从而细化第二个GICP应用程序的准确性。一旦确定了一个精确的姿势，我们利用Voxel PFilter将数据的框架合并到全局语义地图中，为全局一致性纠正TPV-Former对地图的推断中的错误。</p><h2 id="内容概述"><a href="#内容概述" class="headerlink" title="内容概述"></a>内容概述</h2><ul><li><p>系统概述：介绍了OCC-VO的工作流程，包括图像转化、姿态估计、过滤器应用，以及全局地图维护。</p></li><li><p>语义标签过滤器：介绍了基于语义标签的对象过滤方法，以提高配准准确性。有效地防止了各种物体或曲面之间的错误点匹配对优化解的影响。</p></li><li><p>动态对象过滤器：解释了如何处理动态对象，以平衡准确性和场景恶化。具体来说，使用语义标签将具有运动潜能的对象从三维语义占用和全局语义地图中分离出来。然后对每个对象进行点云聚类，这些聚类随后被视为统一的对象。利用第一个GICP的转换结果，我们比较了每个物体的位置。这让我们能够识别相对位移，<strong>并决定一个物体是否是动态的</strong>，是否应该被移除。利用每个输入的静态部分，进行了精细化配准，从而提高了姿态估计的精度。</p></li><li><p>Voxel PFilter：介绍了引入Voxel PFilter来维护全局地图的一致性，校正网络干扰噪音。</p></li><li><p>实验：使用Occ3D-nuScenes数据集评估OCC-VO，比较其性能和执行时间分析。</p></li></ul><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>OCC-VO是一个新颖的VO框架，结合3D语义占用栅格，用于自动驾驶场景中的密集地图生成。通过设计的滤波器，OCC-VO在自动驾驶环境中取得了更高的准确性和稳健性。未来的工作将包括将环路闭合检测等模块整合到OCC-VO中，以发展成为一个完整的SLAM系统。</p><p><strong><a href="https://mp.weixin.qq.com/s/q_YpZSS2mDoa2SNhUCAOTg">来源</a></strong></p><h2 id="详细阅读细节"><a href="#详细阅读细节" class="headerlink" title="详细阅读细节"></a>详细阅读细节</h2><p>挑战一：由于3D语义占用不同于场景结构的原始捕获，例如激光雷达扫描，因此出现了挑战。因此，使用这些数据来执行点云配准带来了几个问题。</p><ul><li>问题一：三维语义占用的粗分辨率导致地标位置估计的不确定性，进而影响配准的准确性。</li><li>问题二：由于神经网络模型的不完善，可能导致路标构造不准确甚至部分缺失</li><li>问题三：区分静止环境和动态对象非常重要，特别是在自动驾驶等应用中，因为它们的匹配可能导致姿态估计不太准确。</li></ul><p>解决一：</p><ul><li>方法一：设计了一种适合三维语义占用的姿态估计和映射算法。以Lidarbased SLAM中常用的GICP算法作为我们的基线。该算法通过特征匹配和迭代优化来实现点云的对齐。</li><li>方法二：在配准过程中引入语义约束，类似于ColorICP在几何结构相似但语义不同的情况下，例如自动驾驶环境中的路面，这些语义约束非常有效</li><li>方法三：实现了一个动态目标滤波器，以提高地图精度和姿态估计精度</li><li>方法四：在映射阶段，我们利用PFilter中的思想消除不可靠点，构建更健壮的全局语义映射。最终结果是一个微调的姿态估计和一个高度精确的地图</li></ul>]]></content>
      
      
      <categories>
          
          <category> DL </category>
          
          <category> slam </category>
          
      </categories>
      
      
        <tags>
            
            <tag> ssc </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>语义场景补全（SSC）代码篇（一）</title>
      <link href="/2023/11/03/code-ssc/"/>
      <url>/2023/11/03/code-ssc/</url>
      
        <content type="html"><![CDATA[<h1 id="主要记录SSC中的理论与代码实现（第一章）"><a href="#主要记录SSC中的理论与代码实现（第一章）" class="headerlink" title="主要记录SSC中的理论与代码实现（第一章）"></a>主要记录SSC中的理论与代码实现（第一章）</h1><p>主要从开篇之作SSCNet（2017）开始讲起，然后按激光雷达点云输入（LMSCNet，2020；S3CNet，2021a；JS3C-Net，2021）,单双目以及两者融合的顺序</p><h2 id="首先是开篇之作SSCNet"><a href="#首先是开篇之作SSCNet" class="headerlink" title="首先是开篇之作SSCNet"></a>首先是开篇之作SSCNet</h2><p>官方代码实在caffe框架上搭建的，第一个将语义分割和场景完成与 3D CNN 端到端结合的工作，从单视图深度地图观察生成一个完整的三维体素表示的体积占用和语义标签的场景，利用这两个任务的耦合特性，引入了语义场景完成网络(SSCNet)</p><p>代码链接： <a href="https://github.com/shurans/sscnet">SSCNet: Semantic Scene Completion from a Single Depth Image</a>, 2017</p><p><img src="/pic/SSCNet.png"></p><p>代码：<br>使用Caffe框架的网络定义txt文件,主要包括下面几个部分:</p><ul><li>数据层 (layer { name: “data” … })： 这是数据输入层，它从SUNCG数据集中加载数据。数据集包括深度信息（可能是三维数据）以及标签信息（seg_label）。还包括一些参数，如体素大小、裁剪大小、类别映射等。</li><li>卷积层 (layer { name: “conv1_1” … })： 这是一个卷积层，用于提取特征。它定义了卷积核的数量、大小、填充等参数。</li><li>ReLU 层 (layer { name: “relu1_1” … })： 这是激活函数层，通常用于引入非线性性。ReLU（Rectified Linear Unit）是一种常见的激活函数。</li><li>池化层 (layer { name: “pool2” … })： 这是一个池化层，用于降低特征图的分辨率，通常用于减少计算复杂度。</li><li>Eltwise 层 (layer { name: “res3_2” … })： 这是一个元素级操作层，通常用于残差网络（ResNet）中，用于连接不同层的输出。</li><li>全连接层 (layer { name: “fc12” … })： 这是一个全连接层，用于将卷积层的输出映射到最终的分类标签或预测。</li><li>SoftmaxWithLoss 层 (layer { name: “loss” … })： 这是损失层，通常与Softmax函数一起使用，用于计算损失函数，用于模型训练。</li></ul><h3 id="数据层配置"><a href="#数据层配置" class="headerlink" title="数据层配置"></a>数据层配置</h3><p>数据层在深度学习模型中是一个关键组成部分，用于加载和处理输入数据。在你提供的代码中，数据层的定义如下：</p><pre class="line-numbers language-protobuf" data-language="protobuf"><code class="language-protobuf">layer <span class="token punctuation">&#123;</span>  name<span class="token punctuation">:</span> <span class="token string">"data"</span>  type<span class="token punctuation">:</span> <span class="token string">"SuncgData"</span>  top<span class="token punctuation">:</span> <span class="token string">"data"</span>  top<span class="token punctuation">:</span> <span class="token string">"seg_label"</span>  top<span class="token punctuation">:</span> <span class="token string">"seg_weight"</span>  suncg_data_param <span class="token punctuation">&#123;</span>    file_data<span class="token punctuation">:</span> <span class="token string">"../../../../data/depthbin/SUNCGtrain_1_500"</span>    file_data<span class="token punctuation">:</span> <span class="token string">"../../../../data/depthbin/SUNCGtrain_501_1000"</span>    file_data<span class="token punctuation">:</span> <span class="token string">"../../../../data/depthbin/SUNCGtrain_1001_2000"</span>    file_data<span class="token punctuation">:</span> <span class="token string">"../../../../data/depthbin/SUNCGtrain_1001_3000"</span>    file_data<span class="token punctuation">:</span> <span class="token string">"../../../../data/depthbin/SUNCGtrain_3001_5000"</span>    file_data<span class="token punctuation">:</span> <span class="token string">"../../../../data/depthbin/SUNCGtrain_5001_7000"</span>    file_list<span class="token punctuation">:</span> <span class="token string">""</span>    vox_unit<span class="token punctuation">:</span> <span class="token number">0.02</span>    vox_margin<span class="token punctuation">:</span> <span class="token number">0.24</span>    vox_size<span class="token punctuation">:</span> <span class="token number">240</span>    vox_size<span class="token punctuation">:</span> <span class="token number">144</span>    vox_size<span class="token punctuation">:</span> <span class="token number">240</span>    crop_size<span class="token punctuation">:</span> <span class="token number">240</span>    crop_size<span class="token punctuation">:</span> <span class="token number">144</span>    crop_size<span class="token punctuation">:</span> <span class="token number">240</span>    label_size<span class="token punctuation">:</span> <span class="token number">60</span>    label_size<span class="token punctuation">:</span> <span class="token number">36</span>    label_size<span class="token punctuation">:</span> <span class="token number">60</span>    seg_classes<span class="token punctuation">:</span> <span class="token number">11</span>    shuffle<span class="token punctuation">:</span> <span class="token boolean">true</span>    occ_empty_only<span class="token punctuation">:</span> <span class="token boolean">true</span>    neg_obj_sample_ratio<span class="token punctuation">:</span> <span class="token number">2</span>    seg_class_map<span class="token punctuation">:</span> <span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span>    seg_class_weight<span class="token punctuation">:</span> <span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span>    occ_class_weight<span class="token punctuation">:</span> <span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span>    with_projection_tsdf<span class="token punctuation">:</span> <span class="token boolean">false</span>    batch_size<span class="token punctuation">:</span> <span class="token number">1</span>    tsdf_type<span class="token punctuation">:</span> <span class="token number">1</span>    data_type<span class="token punctuation">:</span> TSDF    surf_only<span class="token punctuation">:</span> <span class="token boolean">false</span>  <span class="token punctuation">&#125;</span><span class="token punctuation">&#125;</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>主要参数如下</p><ul><li><p><strong>name: “data”：</strong> 数据层的名称，用于在模型中引用该层。</p></li><li><p><strong>type: “SuncgData”：</strong> 数据层的类型，指定了如何处理数据。</p></li><li><p><strong>top: “data”, top: “seg_label”, top: “seg_weight”：</strong> 数据层的输出，模型中其他层可以引用这些输出。</p></li><li><p><strong>suncg_data_param：</strong> 数据层的参数配置块。</p></li><li><p><strong>file_data：</strong> 数据文件的路径，用于加载数据。这里看起来是从SUNCG数据集中加载深度信息。</p></li><li><p><strong>vox_unit, vox_margin, vox_size：</strong> 与体素（Voxel）的设置相关。<code>vox_unit</code>是体素的大小，<code>vox_margin</code>是体素的边缘大小，<code>vox_size</code>是体素在每个维度的数量。这些参数定义了体素网格的离散化空间。</p></li><li><p><strong>crop_size 和 label_size：</strong> 数据的裁剪大小和标签大小。<code>crop_size</code>表示输入数据的裁剪尺寸，<code>label_size</code>表示标签的大小。</p></li><li><p><strong>seg_classes：</strong> 指定了分类任务中的类别数量。</p></li><li><p><strong>shuffle：</strong> 一个布尔值，表示是否在加载数据时进行洗牌（打乱数据顺序）。</p></li><li><p><strong>occ_empty_only：</strong> 一个布尔值，可能表示只关注占用（occupied）和空闲（empty）之间的区别。</p></li><li><p><strong>neg_obj_sample_ratio：</strong> 负对象采样比例。</p></li><li><p><strong>seg_class_map, seg_class_weight, occ_class_weight：</strong> 用于定义不同类别的映射和权重。<code>seg_class_map</code>用于将一些类别映射到其他类别，<code>seg_class_weight</code>和<code>occ_class_weight</code>用于类别的权重设置。</p></li><li><p><strong>with_projection_tsdf 和 surf_only：</strong> 控制是否使用投影TSDF和是否仅考虑表面信息。投影TSDF通常用于三维重建，而<code>surf_only</code>可能用于指示仅关注表面的信息。</p></li><li><p><strong>batch_size：</strong> 训练时的批处理大小，影响每次模型权重更新时使用的样本数量。</p></li><li><p><strong>tsdf_type 和 data_type：</strong> 与数据类型相关的参数，例如TSDF数据类型。</p></li><li><p><strong>surf_only：</strong> 一个布尔值，可能表示是否仅考虑表面数据。</p></li></ul><p>这些参数共同定义了数据层的行为，确保数据被正确加载和预处理，以供深度学习模型使用。</p><h2 id="LMSCNet"><a href="#LMSCNet" class="headerlink" title="LMSCNet"></a>LMSCNet</h2><p>代码链接： <a href="https://github.com/astra-vision/LMSCNet">LMSCNet: Lightweight Multiscale 3D Semantic Completion</a>, 3DV 2020</p><p>一种从体素化稀疏三维激光雷达扫描中完成多尺度三维语义场景的新方法,使用具有全面多尺度跳跃连接的2D UNet主干来增强特征流，以及3D分割头</p><p><img src="/pic/LMSCNet.png"></p><p>首先看配置文件<a href="https://github.com/astra-vision/LMSCNet/blob/main/SSC_configs/examples/LMSCNet.yaml">LMSCNet.yaml</a>这段代码是一个用于训练深度学习模型的Python脚本，基于LMSCNet网络。以下是关于训练设置以及代码的关键部分的解释：</p><h3 id="训练设置"><a href="#训练设置" class="headerlink" title="训练设置"></a>训练设置</h3><ul><li><p><strong>数据加载器设置：</strong></p><ul><li><code>NUM_WORKERS: 4</code>：用于数据加载的工作线程数。</li></ul></li><li><p><strong>数据集设置：</strong></p><ul><li>数据增强：<ul><li><code>FLIPS: true</code>：表示是否进行数据增强，包括翻转等。</li></ul></li><li>模态设置：<ul><li><code>3D_LABEL: true</code>：表示是否使用3D标签数据。</li><li><code>3D_OCCLUDED: true</code>：表示是否使用3D遮挡数据。</li><li><code>3D_OCCUPANCY: true</code>：表示是否使用3D占用数据。</li></ul></li><li><code>ROOT_DIR: /datasets_local/datasets_lroldaoj/semantic_kitti_v1.0/</code>：数据集的根目录。</li><li><code>TYPE: SemanticKITTI</code>：数据集的类型，这里是SemanticKITTI。</li></ul></li><li><p><strong>模型设置：</strong></p><ul><li><code>TYPE: LMSCNet</code>：使用的模型类型为LMSCNet。</li></ul></li><li><p><strong>优化器设置：</strong></p><ul><li><code>BASE_LR: 0.001</code>：学习率的初始值。</li><li><code>BETA1: 0.9</code>：Adam优化器的beta1参数。</li><li><code>BETA2: 0.999</code>：Adam优化器的beta2参数。</li><li><code>MOMENTUM: NA</code>：动量参数（未设置）。</li><li><code>TYPE: Adam</code>：优化器的类型。</li><li><code>WEIGHT_DECAY: NA</code>：权重衰减参数（未设置）。</li></ul></li><li><p><strong>输出设置：</strong></p><ul><li><code>OUT_ROOT: ../SSC_out/</code>：输出结果的根目录。</li></ul></li><li><p><strong>调度器设置：</strong></p><ul><li><code>FREQUENCY: epoch</code>：学习率调度的频率为每个epoch。</li><li><code>LR_POWER: 0.98</code>：学习率调度的幂次方。</li><li><code>TYPE: power_iteration</code>：学习率调度类型。</li></ul></li><li><p><strong>状态设置：</strong></p><ul><li><code>RESUME: false</code>：是否从之前的检查点中恢复训练（未设置为恢复）。</li></ul></li><li><p><strong>训练设置：</strong></p><ul><li><code>BATCH_SIZE: 4</code>：每个批次的样本数量。</li><li><code>CHECKPOINT_PERIOD: 15</code>：保存检查点的周期。</li><li><code>EPOCHS: 80</code>：总共训练的轮数。</li><li><code>SUMMARY_PERIOD: 50</code>：汇总损失的周期。</li></ul></li><li><p><strong>验证设置：</strong></p><ul><li><code>BATCH_SIZE: 8</code>：验证时的批次大小。</li><li><code>SUMMARY_PERIOD: 20</code>：验证损失的周期。</li></ul></li></ul><h3 id="LMSCNet网络代码解释"><a href="#LMSCNet网络代码解释" class="headerlink" title="LMSCNet网络代码解释"></a>LMSCNet网络代码解释</h3><p><a href="https://github.com/astra-vision/LMSCNet/blob/main/LMSCNet/models/LMSCNet.py">LMSCNet.py</a></p><p>当分析这个神经网络模型时，我们可以将其分解为以下几个关键组件和部分。以下是对这些组件和部分的更详细解释：</p><h4 id="SegmentationHead（分割头部）"><a href="#SegmentationHead（分割头部）" class="headerlink" title="SegmentationHead（分割头部）"></a>SegmentationHead（分割头部）</h4><p><code>SegmentationHead</code> 类用于处理单一尺度的语义分割任务。它包括以下组件：</p><ul><li><p><strong>First Convolution (第一个卷积层)</strong>: <code>conv0</code> 定义了一个3D卷积层，用于将输入的特征从一个尺度（inplanes）转换到 <code>planes</code>。这是模型的初始特征处理步骤。</p></li><li><p><strong>ASPP Block (空间金字塔池化块)</strong>: <code>ASPP</code> 是”空间金字塔池化块”的缩写，它由多个卷积层和扩张卷积层（dilated convolution）组成。这些层用于捕捉不同感受野（receptive field）下的特征。这有助于模型更好地理解图像中的上下文信息。<code>conv1</code> 包括多个扩张卷积层，<code>bn1</code> 是相应的批归一化层，然后再通过 <code>conv2</code> 进行进一步处理。这些层通过 ReLU 激活函数进行激活。</p></li><li><p><strong>Convolution for Output (用于输出的卷积)</strong>: <code>conv_classes</code> 是用于生成最终语义分割预测的卷积层，其输出通道数等于目标类别数 <code>nbr_classes</code>。</p></li></ul><h4 id="LMSCNet（语义分割网络）"><a href="#LMSCNet（语义分割网络）" class="headerlink" title="LMSCNet（语义分割网络）"></a>LMSCNet（语义分割网络）</h4><p><code>LMSCNet</code> 类定义了整个语义分割网络，它的结构包括：</p><ul><li><p><strong>Encoder Blocks (编码块)</strong>: 这些块包括卷积层和激活函数，它们用于从输入数据提取特征。编码块分层堆叠，逐渐降低分辨率。其中 <code>Encoder_block1</code> 处理输入数据，然后 <code>Encoder_block2</code>、<code>Encoder_block3</code> 和 <code>Encoder_block4</code> 分别进行更多的特征提取。</p></li><li><p><strong>Output Scales (输出尺度)</strong>: 模型产生多个尺度的语义分割输出，包括1:8、1:4、1:2和1:1。每个输出尺度都有相应的卷积层和<code>SegmentationHead</code>。这些层用于生成不同分辨率下的语义分割预测。</p></li><li><p><strong>反卷积层 (Deconvolution Layers)</strong>: 这些层包括<code>deconv_1_8__1_2</code>、<code>deconv_1_8__1_1</code>、<code>deconv1_4</code>、<code>deconv1_2</code> 和 <code>deconv1_1</code>，用于上采样，将较低分辨率的输出转换为高分辨率。这些层用于与高分辨率的编码块特征进行连接。</p></li><li><p><strong>权重初始化和损失计算</strong>: 类中还包括了初始化权重的函数 <code>weights_initializer</code>，以及用于计算损失的函数 <code>compute_loss</code>。损失计算使用交叉熵损失，针对每个尺度的输出都会计算相应的损失。</p></li><li><p><strong>类别权重 (Class Weights)</strong>: 模型使用 <code>get_class_weights</code> 函数计算类别权重，以便处理不平衡的类别分布。</p></li><li><p><strong>其他辅助函数</strong>: 类还包括其他用于获取目标数据、设置训练尺度等的辅助函数。</p></li></ul><p>这个模型的核心思想是从低分辨率到高分辨率逐步提取特征，并生成多尺度的语义分割预测。模型的损失函数考虑了所有尺度的预测，以便综合多尺度信息来进行训练。这有助于提高语义分割任务的性能，特别是在处理不同尺度的对象时。模型的训练和验证过程通常需要提供训练数据、优化器和学习率调度器等组件。</p><h2 id="JS3CNet"><a href="#JS3CNet" class="headerlink" title="JS3CNet"></a>JS3CNet</h2><p>一种增强的联合单扫LiDAR点云语义分割方法，该方法利用学习的形状先验形式场景完成网络，即JS3C-Net</p><p>代码链接： <a href="https://github.com/yanx27/JS3C-Net">JS3CNet: Sparse Single Sweep LiDAR Point Cloud Segmentation via Learning Contextual Shape Priors from Scene Completion</a>, AAAI 2021</p><p><img src="/pic/JS3CNet.png" alt="在给定稀疏不完全单扫描点云的情况下，首先使用稀疏卷积U-Net对Fenc进行点特征编码。基于初始编码，MLP1用于生成形状嵌入（SE）FSE，该FSE与通过MLP2传递的初始编码一起流入MLP3，以生成用于点云语义分割的Fout。然后，来自SE的不完整细粒度点特征和来自语义场景完成（SSC）模块的完整体素特征流入点-体素交互（PVI）模块，以实现细化特征，最终在监督下输出完成体素。请注意，SSC和PVI模块可以在推理过程中丢弃。"></p><h3 id="代码分析"><a href="#代码分析" class="headerlink" title="代码分析"></a>代码分析</h3><p><a href="https://github.com/yanx27/JS3C-Net/blob/main/opt/JS3C_default_kitti.yaml">JS3C_default_kitti.yaml</a>这是一个JS3CNet网络的配置文件。配置文件用于定义训练和测试JS3CNet网络时的各种参数和设置。以下是关于配置文件的详细解释：</p><pre class="line-numbers language-yaml" data-language="yaml"><code class="language-yaml"><span class="token key atrule">GENERAL</span><span class="token punctuation">:</span>  <span class="token key atrule">task</span><span class="token punctuation">:</span> train  <span class="token key atrule">manual_seed</span><span class="token punctuation">:</span> <span class="token number">123</span>  <span class="token key atrule">dataset_dir</span><span class="token punctuation">:</span> /home/yxu/data/semantic_kitti/dataset/  <span class="token key atrule">debug</span><span class="token punctuation">:</span> <span class="token boolean important">False</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span></span></code></pre><ul><li><code>task</code>: 定义任务类型，这里设置为”train”表示进行训练。</li><li><code>manual_seed</code>: 随机数种子，用于确保实验的可复现性。</li><li><code>dataset_dir</code>: 数据集的目录路径。</li><li><code>debug</code>: 是否启用调试模式，设置为”False”表示不启用。</li></ul><pre class="line-numbers language-yaml" data-language="yaml"><code class="language-yaml"><span class="token key atrule">DATA</span><span class="token punctuation">:</span>  <span class="token key atrule">dataset</span><span class="token punctuation">:</span> SemanticKITTI  <span class="token key atrule">classes_seg</span><span class="token punctuation">:</span> <span class="token number">19</span>  <span class="token key atrule">classes_completion</span><span class="token punctuation">:</span> <span class="token number">20</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span></span></code></pre><ul><li><code>dataset</code>: 数据集的名称，这里使用SemanticKITTI数据集。</li><li><code>classes_seg</code>: 语义分割任务的类别数，这里设置为19。</li><li><code>classes_completion</code>: 完备性任务的类别数，这里设置为20。</li></ul><pre class="line-numbers language-yaml" data-language="yaml"><code class="language-yaml"><span class="token key atrule">Segmentation</span><span class="token punctuation">:</span>  <span class="token key atrule">model_name</span><span class="token punctuation">:</span> SubSparseConv  <span class="token key atrule">m</span><span class="token punctuation">:</span> <span class="token number">16</span>  <span class="token key atrule">block_residual</span><span class="token punctuation">:</span> <span class="token boolean important">False</span>  <span class="token key atrule">block_reps</span><span class="token punctuation">:</span> <span class="token number">1</span>  <span class="token key atrule">seg_groups</span><span class="token punctuation">:</span> <span class="token number">1</span>  <span class="token key atrule">use_coords</span><span class="token punctuation">:</span> <span class="token boolean important">False</span>  <span class="token key atrule">feature_dims</span><span class="token punctuation">:</span> <span class="token punctuation">[</span><span class="token number">16</span><span class="token punctuation">,</span><span class="token number">32</span><span class="token punctuation">,</span><span class="token number">48</span><span class="token punctuation">,</span><span class="token number">64</span><span class="token punctuation">,</span><span class="token number">80</span><span class="token punctuation">,</span><span class="token number">96</span><span class="token punctuation">,</span><span class="token number">112</span><span class="token punctuation">]</span>  <span class="token key atrule">input_channel</span><span class="token punctuation">:</span> <span class="token number">3</span>  <span class="token key atrule">scale</span><span class="token punctuation">:</span> <span class="token number">10</span>  <span class="token key atrule">full_scale</span><span class="token punctuation">:</span> <span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">2048</span><span class="token punctuation">]</span>  <span class="token key atrule">max_npoint</span><span class="token punctuation">:</span> <span class="token number">250000</span>  <span class="token key atrule">mode</span><span class="token punctuation">:</span> <span class="token number">4</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><ul><li><code>Segmentation</code>部分包含了与语义分割任务相关的参数设置。</li><li><code>model_name</code>: 语义分割模型的名称，这里使用SubSparseConv。</li><li><code>m</code>: SubSparseConv模型的参数，这里设置为16。</li><li><code>block_residual</code>: 是否使用块残差，设置为”False”表示不使用。</li><li><code>block_reps</code>: 块的重复次数，这里设置为1。</li><li><code>seg_groups</code>: 分割组的数量，这里设置为1。</li><li><code>use_coords</code>: 是否使用坐标信息，设置为”False”表示不使用。</li><li><code>feature_dims</code>: 特征维度的设置。</li><li><code>input_channel</code>: 输入通道的数量，这里设置为3。</li><li><code>scale</code>: 数据集的尺度。</li><li><code>full_scale</code>: 数据集的完全尺度范围。</li><li><code>max_npoint</code>: 最大点数，这里设置为250,000。</li><li><code>mode</code>: 模式设置，这里设置为4。</li></ul><pre class="line-numbers language-yaml" data-language="yaml"><code class="language-yaml"><span class="token key atrule">Completion</span><span class="token punctuation">:</span>  <span class="token key atrule">model_name</span><span class="token punctuation">:</span> SSCNet  <span class="token key atrule">m</span><span class="token punctuation">:</span> <span class="token number">32</span>  <span class="token key atrule">feeding</span><span class="token punctuation">:</span> both  <span class="token key atrule">no_fuse_feat</span><span class="token punctuation">:</span> <span class="token boolean important">False</span>  <span class="token key atrule">block_residual</span><span class="token punctuation">:</span> <span class="token boolean important">True</span>  <span class="token key atrule">block_reps</span><span class="token punctuation">:</span> <span class="token number">2</span>  <span class="token key atrule">use_coords</span><span class="token punctuation">:</span> <span class="token boolean important">False</span>  <span class="token key atrule">mode</span><span class="token punctuation">:</span> <span class="token number">0</span>  <span class="token key atrule">full_scale</span><span class="token punctuation">:</span> <span class="token punctuation">[</span><span class="token number">256</span><span class="token punctuation">,</span> <span class="token number">256</span><span class="token punctuation">,</span> <span class="token number">32</span><span class="token punctuation">]</span>  <span class="token key atrule">interaction</span><span class="token punctuation">:</span> <span class="token boolean important">True</span>  <span class="token key atrule">pooling_type</span><span class="token punctuation">:</span> mean  <span class="token key atrule">fuse_k</span><span class="token punctuation">:</span> <span class="token number">5</span>  <span class="token key atrule">point_cloud_range</span><span class="token punctuation">:</span> <span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">-25.6</span><span class="token punctuation">,</span> <span class="token number">-2</span><span class="token punctuation">,</span> <span class="token number">51.2</span><span class="token punctuation">,</span> <span class="token number">25.6</span><span class="token punctuation">,</span> <span class="token number">4.4</span><span class="token punctuation">]</span>  <span class="token key atrule">voxel_size</span><span class="token punctuation">:</span> <span class="token number">0.2</span>  <span class="token key atrule">search_k</span><span class="token punctuation">:</span> <span class="token number">8</span>  <span class="token key atrule">feat_relation</span><span class="token punctuation">:</span> <span class="token boolean important">False</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><ul><li><code>Completion</code>部分包含了与完备性任务相关的参数设置。</li><li><code>model_name</code>: 完备性模型的名称，这里使用SSCNet。</li><li><code>m</code>: SSCNet模型的参数，这里设置为32。</li><li><code>feeding</code>: 输入数据的类型，这里设置为”both”，表示同时输入特征和概率。</li><li><code>no_fuse_feat</code>: 是否不融合特征，设置为”False”表示融合特征。</li><li><code>block_residual</code>: 是否使用块残差，设置为”True”表示使用。</li><li><code>block_reps</code>: 块的重复次数，这里设置为2。</li><li><code>use_coords</code>: 是否使用坐标信息，设置为”False”表示不使用。</li><li><code>mode</code>: 模式设置，这里设置为0。</li><li><code>full_scale</code>: 完全尺度范围的设置。</li><li><code>interaction</code>: 是否启用交互，设置为”True”表示启用。</li><li><code>pooling_type</code>: 池化类型，这里设置为”mean”。</li><li><code>fuse_k</code>: 融合参数k的设置。</li><li><code>point_cloud_range</code>: 点云范围的设置。</li><li><code>voxel_size</code>: 体素大小的设置。</li><li><code>search_k</code>: 搜索参数k的设置。</li><li><code>feat_relation</code>: 特征关系的设置，这里设置为”False”。</li></ul><pre class="line-numbers language-yaml" data-language="yaml"><code class="language-yaml"><span class="token key atrule">TRAIN</span><span class="token punctuation">:</span>  <span class="token key atrule">epochs</span><span class="token punctuation">:</span> <span class="token number">100</span>  <span class="token key atrule">train_workers</span><span class="token punctuation">:</span> <span class="token number">10</span>  <span class="token key atrule">optim</span><span class="token punctuation">:</span> Adam  <span class="token key atrule">batch_size</span><span class="token punctuation">:</span> <span class="token number">2</span>  <span class="token key atrule">learning_rate</span><span class="token punctuation">:</span> <span class="token number">0.001</span>  <span class="token key atrule">lr_decay</span><span class="token punctuation">:</span> <span class="token number">0.7</span>  <span class="token key atrule">decay_step</span><span class="token punctuation">:</span> <span class="token number">10</span>  <span class="token key atrule">momentum</span><span class="token punctuation">:</span> <span class="token number">0.9</span>  <span class="token key atrule">weight_decay</span><span class="token punctuation">:</span> <span class="token number">0.0001</span>  <span class="token key atrule">save_freq</span><span class="token punctuation">:</span> <span class="token number">16</span>  <span class="token key atrule">uncertainty_loss</span><span class="token punctuation">:</span> <span class="token boolean important">True</span>  <span class="token key atrule">loss_weight</span><span class="token punctuation">:</span> <span class="token punctuation">[</span><span class="token number">1.0</span><span class="token punctuation">,</span> <span class="token number">0.8</span><span class="token punctuation">]</span>  <span class="token key atrule">pretrain_path</span><span class="token punctuation">:</span>  <span class="token key atrule">train_from</span><span class="token punctuation">:</span> <span class="token number">0</span>  <span class="token key atrule">seg_num_per_class</span><span class="token punctuation">:</span> <span class="token punctuation">[</span><span class="token number">55437630</span><span class="token punctuation">,</span> <span class="token number">320797</span><span class="token punctuation">,</span> <span class="token number">541736</span><span class="token punctuation">,</span> <span class="token number">2578735</span><span class="token punctuation">,</span> <span class="token number">3274484</span><span class="token punctuation">,</span> <span class="token number">552662</span><span class="token punctuation">,</span> <span class="token number">184064</span><span class="token punctuation">,</span> <span class="token number">78858</span><span class="token punctuation">,</span> <span class="token number">240942562</span><span class="token punctuation">,</span> <span class="token number">17294618</span><span class="token punctuation">,</span> <span class="token number">170599734</span><span class="token punctuation">,</span> <span class="token number">6369672</span><span class="token punctuation">,</span> <span class="token number">230413074</span><span class="token punctuation">,</span> <span class="token number">101130274</span><span class="token punctuation">,</span> <span class="token number">476491114</span><span class="token punctuation">,</span> <span class="token number">9833174</span><span class="token punctuation">,</span> <span class="token number">129609852</span><span class="token punctuation">,</span> <span class="token number">4506626</span><span class="token punctuation">,</span> <span class="token number">1168181</span><span class="token punctuation">]</span>  <span class="token key atrule">complt_num_per_class</span><span class="token punctuation">:</span> <span class="token punctuation">[</span><span class="token number">7632350044</span><span class="token punctuation">,</span> <span class="token number">15783539</span><span class="token punctuation">,</span>  <span class="token number">125136</span><span class="token punctuation">,</span> <span class="token number">118809</span><span class="token punctuation">,</span> <span class="token number">646799</span><span class="token punctuation">,</span> <span class="token number">821951</span><span class="token punctuation">,</span> <span class="token number">262978</span><span class="token punctuation">,</span> <span class="token number">283696</span><span class="token punctuation">,</span> <span class="token number">204750</span><span class="token punctuation">,</span> <span class="token number">61688703</span><span class="token punctuation">,</span> <span class="token number">4502961</span><span class="token punctuation">,</span> <span class="token number">44883650</span><span class="token punctuation">,</span> <span class="token number">2269923</span><span class="token punctuation">,</span> <span class="token number">56840218</span><span class="token punctuation">,</span> <span class="token number">15719652</span><span class="token punctuation">,</span> <span class="token number">158442623</span><span class="token punctuation">,</span> <span class="token number">2061623</span><span class="token punctuation">,</span> <span class="token number">36970522</span><span class="token punctuation">,</span> <span class="token number">1151988</span><span class="token punctuation">,</span> <span class="token number">334146</span><span class="token punctuation">]</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><ul><li><code>TRAIN</code>部分包含了训练相关的参数设置。</li><li><code>epochs</code>: 训练的轮数，这里设置为100。</li><li><code>train_workers</code>: 数据加载器的工作进程数，这里设置为10。</li><li><code>optim</code>: 优化器的选择，这里使用Adam。</li><li><code>batch_size</code>: 批量大小，这里设置为2。</li><li><code>learning_rate</code>: 学习率，这里设置为0.001。</li><li><code>lr_decay</code>: 学习率的衰减率，这里设置为0.7。</li><li><code>decay_step</code>: 学习率衰减的步数，这里设置为10。</li><li><code>momentum</code>: 优化器的动量，这里设置为0.9。</li><li><code>weight_decay</code>: 权重衰减，这里设置为0.0001。</li><li><code>save_freq</code>: 模型保存的频率，这里设置为16。</li><li><code>uncertainty_loss</code>: 是否启用不确定性损失，设置为”True”表示启用。</li><li><code>loss_weight</code>: 损失的权重，这里是一个列表，包括语义损失和完备性损失的权重。</li><li><code>pretrain_path</code>: 预训练模型的路径。</li><li><code>train_from</code>: 从哪一轮训练开始，这里设置为0。</li><li><code>seg_num_per_class</code>: 每个类别的语义分割样本数量。</li><li><code>complt_num_per_class</code>: 每个类别的完备性样本数量。</li></ul><p>这个配置文件包含了JS3CNet网络的各种参数和设置，用于训练和测试语义分割和完备性任务。</p><p><a href="https://github.com/yanx27/JS3C-Net/blob/main/train.py">J3SC_Net </a>是 JS3CNet 网络的核心部分，主要用于同时处理语义分割和点云完备性分割任务。它是一个继承自 PyTorch 的 <code>nn.Module</code> 的模型，下面将对其进行详细介绍：</p><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">class</span> <span class="token class-name">J3SC_Net</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> args<span class="token punctuation">)</span><span class="token punctuation">:</span>        <span class="token builtin">super</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>        self<span class="token punctuation">.</span>args <span class="token operator">=</span> args        self<span class="token punctuation">.</span>seg_head <span class="token operator">=</span> seg_model<span class="token punctuation">(</span>args<span class="token punctuation">)</span>        self<span class="token punctuation">.</span>complet_head <span class="token operator">=</span> complet_model<span class="token punctuation">(</span>args<span class="token punctuation">)</span>        self<span class="token punctuation">.</span>voxelpool <span class="token operator">=</span> model_utils<span class="token punctuation">.</span>VoxelPooling<span class="token punctuation">(</span>args<span class="token punctuation">)</span>        self<span class="token punctuation">.</span>seg_sigma <span class="token operator">=</span> nn<span class="token punctuation">.</span>Parameter<span class="token punctuation">(</span>torch<span class="token punctuation">.</span>Tensor<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">.</span>uniform_<span class="token punctuation">(</span><span class="token number">0.2</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">,</span> requires_grad<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>        self<span class="token punctuation">.</span>complet_sigma <span class="token operator">=</span> nn<span class="token punctuation">.</span>Parameter<span class="token punctuation">(</span>torch<span class="token punctuation">.</span>Tensor<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">.</span>uniform_<span class="token punctuation">(</span><span class="token number">0.2</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">,</span> requires_grad<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> x<span class="token punctuation">)</span><span class="token punctuation">:</span>        seg_inputs<span class="token punctuation">,</span> complet_inputs<span class="token punctuation">,</span> _ <span class="token operator">=</span> x        <span class="token comment"># 分割头部分</span>        seg_output<span class="token punctuation">,</span> feat <span class="token operator">=</span> self<span class="token punctuation">.</span>seg_head<span class="token punctuation">(</span>seg_inputs<span class="token punctuation">)</span>        <span class="token comment"># 完备性头部分</span>        coords <span class="token operator">=</span> complet_inputs<span class="token punctuation">[</span><span class="token string">'complet_coords'</span><span class="token punctuation">]</span>        coords <span class="token operator">=</span> coords<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span> <span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">]</span>        <span class="token keyword">if</span> args<span class="token punctuation">[</span><span class="token string">'DATA'</span><span class="token punctuation">]</span><span class="token punctuation">[</span><span class="token string">'dataset'</span><span class="token punctuation">]</span> <span class="token operator">==</span> <span class="token string">'SemanticKITTI'</span><span class="token punctuation">:</span>            coords<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">]</span> <span class="token operator">+=</span> <span class="token number">1</span>        <span class="token keyword">elif</span> args<span class="token punctuation">[</span><span class="token string">'DATA'</span><span class="token punctuation">]</span><span class="token punctuation">[</span><span class="token string">'dataset'</span><span class="token punctuation">]</span> <span class="token operator">==</span> <span class="token string">'SemanticPOSS'</span><span class="token punctuation">:</span>            coords<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">]</span><span class="token punctuation">[</span>coords<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">]</span> <span class="token operator">></span> <span class="token number">31</span><span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token number">31</span>        <span class="token keyword">if</span> args<span class="token punctuation">[</span><span class="token string">'Completion'</span><span class="token punctuation">]</span><span class="token punctuation">[</span><span class="token string">'feeding'</span><span class="token punctuation">]</span> <span class="token operator">==</span> <span class="token string">'both'</span><span class="token punctuation">:</span>            feeding <span class="token operator">=</span> torch<span class="token punctuation">.</span>cat<span class="token punctuation">(</span><span class="token punctuation">[</span>seg_output<span class="token punctuation">,</span> feat<span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span>        <span class="token keyword">elif</span> args<span class="token punctuation">[</span><span class="token string">'Completion'</span><span class="token punctuation">]</span><span class="token punctuation">[</span><span class="token string">'feeding'</span><span class="token punctuation">]</span> <span class="token operator">==</span> <span class="token string">'feat'</span><span class="token punctuation">:</span>            feeding <span class="token operator">=</span> feat        <span class="token keyword">else</span><span class="token punctuation">:</span>            feeding <span class="token operator">=</span> seg_output        <span class="token comment"># 使用Voxelpool模块进行特征池化操作</span>        features <span class="token operator">=</span> self<span class="token punctuation">.</span>voxelpool<span class="token punctuation">(</span>            invoxel_xyz<span class="token operator">=</span>complet_inputs<span class="token punctuation">[</span><span class="token string">'complet_invoxel_features'</span><span class="token punctuation">]</span><span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span> <span class="token punctuation">:</span><span class="token punctuation">,</span> <span class="token punctuation">:</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">,</span>            invoxel_map<span class="token operator">=</span>complet_inputs<span class="token punctuation">[</span><span class="token string">'complet_invoxel_features'</span><span class="token punctuation">]</span><span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span> <span class="token punctuation">:</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">.</span><span class="token builtin">long</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span>            src_feat<span class="token operator">=</span>feeding<span class="token punctuation">,</span>            voxel_center<span class="token operator">=</span>complet_inputs<span class="token punctuation">[</span><span class="token string">'voxel_centers'</span><span class="token punctuation">]</span>        <span class="token punctuation">)</span>        <span class="token keyword">if</span> self<span class="token punctuation">.</span>args<span class="token punctuation">[</span><span class="token string">'Completion'</span><span class="token punctuation">]</span><span class="token punctuation">[</span><span class="token string">'no_fuse_feat'</span><span class="token punctuation">]</span><span class="token punctuation">:</span>            features<span class="token punctuation">[</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token number">1</span>            features <span class="token operator">=</span> features<span class="token punctuation">.</span>detach<span class="token punctuation">(</span><span class="token punctuation">)</span>        <span class="token comment"># 创建SparseConvTensor，用于点云完备性分割</span>        batch_complet <span class="token operator">=</span> spconv<span class="token punctuation">.</span>SparseConvTensor<span class="token punctuation">(</span>            features<span class="token punctuation">.</span><span class="token builtin">float</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span> coords<span class="token punctuation">.</span><span class="token builtin">int</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span> args<span class="token punctuation">[</span><span class="token string">'Completion'</span><span class="token punctuation">]</span><span class="token punctuation">[</span><span class="token string">'full_scale'</span><span class="token punctuation">]</span><span class="token punctuation">,</span> args<span class="token punctuation">[</span><span class="token string">'TRAIN'</span><span class="token punctuation">]</span><span class="token punctuation">[</span><span class="token string">'batch_size'</span><span class="token punctuation">]</span>        <span class="token punctuation">)</span>        batch_complet <span class="token operator">=</span> dataset<span class="token punctuation">.</span>sparse_tensor_augmentation<span class="token punctuation">(</span>batch_complet<span class="token punctuation">,</span> complet_inputs<span class="token punctuation">[</span><span class="token string">'state'</span><span class="token punctuation">]</span><span class="token punctuation">)</span>        <span class="token comment"># 使用完备性头部分进行点云完备性分割</span>        complet_output <span class="token operator">=</span> self<span class="token punctuation">.</span>complet_head<span class="token punctuation">(</span>batch_complet<span class="token punctuation">)</span>        <span class="token keyword">return</span> seg_output<span class="token punctuation">,</span> complet_output<span class="token punctuation">,</span> <span class="token punctuation">[</span>self<span class="token punctuation">.</span>seg_sigma<span class="token punctuation">,</span> self<span class="token punctuation">.</span>complet_sigma<span class="token punctuation">]</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p><strong>主要组成部分</strong>：</p><ol><li><p><code>seg_head</code> 和 <code>complet_head</code>：这是分割头和完备性头的模型，它们通过 <code>args</code> 参数配置，用于执行分割任务和完备性任务。这些模型的初始化在 <code>__init__</code> 方法中进行。</p></li><li><p><code>voxelpool</code> 模块：用于对特征进行池化操作。根据输入参数，它将输入特征和相关的信息池化成新的特征。</p></li><li><p><code>seg_sigma</code> 和 <code>complet_sigma</code>：这是可学习的参数，分别表示语义分割和完备性分割任务的不确定性。模型在训练过程中会学习这些参数。</p></li></ol><p><strong>前向传播（<code>forward</code> 方法）</strong>：</p><p>在前向传播中，<code>J3SC_Net</code> 接受一个输入 <code>x</code>，其中 <code>x</code> 包含了分割和完备性任务的输入。首先，模型通过语义分割头 (<code>seg_head</code>) 处理分割任务的输入数据 <code>seg_inputs</code>，生成 <code>seg_output</code> 和 <code>feat</code>。</p><p>然后，对完备性任务的输入数据进行处理。首先调整坐标 <code>coords</code>，然后根据 <code>args</code> 的配置选择适当的输入数据 <code>feeding</code>。接下来，使用 <code>voxelpool</code> 模块对特征进行池化操作，生成 <code>features</code>。</p><p>最后，将生成的 <code>features</code> 传递给完备性头部分 (<code>complet_head</code>) 进行点云完备性分割。模型的输出包括语义分割结果 <code>seg_output</code> 和点云完备性分割结果 <code>complet_output</code>，以及模型学习的不确定性参数 <code>seg_sigma</code> 和 <code>complet_sigma</code>。</p><p><code>J3SC_Net</code> 模型将同时执行语义分割和点云完备性分割任务，使其成为 JS3CNet 网络的核心组件。这种多任务学习可以在处理点云数据时提高效率和性能。</p><p><code>SSCNet</code> 是一个用于点云完备性分割的神经网络模型，它是 JS3CNet 中完备性分割部分的模型。下面将详细介绍 <code>SSCNet</code> 模型的主要组成部分。</p><h4 id="SSCNet-Decoder"><a href="#SSCNet-Decoder" class="headerlink" title="SSCNet_Decoder"></a>SSCNet_Decoder</h4><p><code>SSCNet_Decoder</code> 是 <code>SSCNet</code> 的解码器部分。它主要负责将输入特征映射到点云的完备性分割结果。该解码器采用了类似 U-Net 架构的设计，分为不同的块（Block），其中包括卷积、批归一化和 ReLU 激活函数层。以下是 <code>SSCNet_Decoder</code> 中各个块的主要部分：</p><ul><li><p>Block 1：包括两个卷积层，每个卷积层后接批归一化和 ReLU 激活函数。该块的输出与一个残差块相加，并经过最大池化。</p></li><li><p>Block 2：与 Block 1 类似，包括两个卷积层，批归一化和 ReLU 激活函数。输出与残差块相加。</p></li><li><p>Block 3：包括两个卷积层，批归一化和 ReLU 激活函数。没有残差连接。</p></li><li><p>Block 4：包括两个卷积层，批归一化和 ReLU 激活函数。没有残差连接。</p></li><li><p>Block 5：包括两个卷积层，批归一化和 ReLU 激活函数。没有残差连接。</p></li><li><p>Prediction：该块用于生成最终的点云完备性分割结果。它包括两个卷积层，最终输出预测结果。</p></li></ul><p><code>SSCNet_Decoder</code> 的各个块逐渐提取并综合特征，最终生成点云完备性分割的预测结果。</p><h4 id="SSCNet"><a href="#SSCNet" class="headerlink" title="SSCNet"></a>SSCNet</h4><p><code>SSCNet</code> 是完备性分割网络的主要模型。它包含以下组件：</p><ul><li><p><code>Decoder</code>：前面介绍的 <code>SSCNet_Decoder</code>，用于点云完备性分割任务。它将输入的点云特征进行解码操作，最终生成点云完备性分割的预测结果。</p></li><li><p><code>upsample</code>：这是上采样模块，用于上采样点云完备性分割的结果，以便与语义分割结果相融合。</p></li><li><p><code>interaction_module</code>（可选）：如果配置中启用了交互模块（<code>args[&#39;Completion&#39;][&#39;interaction&#39;]</code> 为 <code>True</code>），则此组件用于执行点云之间的交互操作。这可以有助于改进点云完备性分割性能。</p></li></ul><p><code>SSCNet</code> 的前向传播过程首先将输入特征通过 <code>Decoder</code> 进行解码，然后根据配置选择是否执行交互操作，最后经过上采样模块生成最终的点云完备性分割结果。</p><p>这些组件一起构成了 <code>SSCNet</code> 模型，用于处理点云数据的完备性分割任务。</p><p><code>complet_head</code> 是用于点云语义完成任务的头部模块。它定义了一个 Unet 模型，用于处理 3D 点云数据。</p><h4 id="类-Unet"><a href="#类-Unet" class="headerlink" title="类 Unet"></a>类 <code>Unet</code></h4><p>这个类包含了 Unet 模型的构建和前向传播方法。以下是类 <code>Unet</code> 的详细分析：</p><ol><li><p><strong>构造方法 <code>__init__</code></strong>:</p><ul><li>这个方法接受一个配置参数 <code>config</code>，用于指定模型的各种参数和配置信息。</li><li><code>m</code> 是配置中指定的通道数。</li><li><code>input_dim</code> 是输入数据的维度，如果配置中指定了使用坐标信息 (<code>use_coords</code> 为 <code>True</code>)，则为 4，否则为 1。</li><li><code>sparseModel</code> 是一个 SparseConvNet 序列模型，它包含了 Unet 架构，包括编码器、解码器和跳跃连接部分。</li></ul></li><li><p><strong>前向传播方法 <code>forward</code></strong>:</p><ul><li>这个方法接受输入 <code>x</code>，其中 <code>x</code> 包含 <code>seg_coords</code> 和 <code>seg_features</code>。</li><li><code>x</code> 被传递给 <code>sparseModel</code>，这一步会进行 Unet 架构的前向传播。</li><li>如果在配置中启用了 <code>interaction</code>，则模型还会执行特征嵌入过程，以处理点云的交互信息。</li><li>最后，模型执行线性变换，将特征映射到类别预测空间，并返回预测结果以及特征信息。</li></ul></li></ol><h4 id="函数-Merge-tbl"><a href="#函数-Merge-tbl" class="headerlink" title="函数 Merge(tbl)"></a>函数 <code>Merge(tbl)</code></h4><p>这个函数是用于处理训练数据的工具函数，它接受一个包含多个样本数据的列表 <code>tbl</code>。</p><ol><li>函数通过迭代遍历每个样本数据，并从每个样本中提取出所需的数据，分别处理分割和完成任务的部分。</li><li>分割部分的数据包括坐标信息、标签和特征。</li><li>完成部分的数据包括点云坐标、输入数据、体素中心坐标、有效性信息、标签、统计信息以及点云体素特征。</li><li>这些数据被整理成适合输入到模型的格式，并以字典形式返回，包括 <code>seg_inputs</code> 和 <code>complet_inputs</code>。</li><li>还返回了一个包含样本文件名的列表，用于后续的分析和处理。</li></ol><p>总之，<code>complet_head</code> 中的代码定义了一个用于点云语义完成任务的 Unet 模型，同时提供了一个数据处理函数 <code>Merge(tbl)</code>，用于将原始数据整理成适合输入模型的格式。</p><p><code>model_utils.py</code> 包含了用于模型训练和实用工具的函数和类。下面是该文件中主要部分的详细分析：</p><ol><li><p><code>checkpoint_restore(model, exp_name, use_cuda=True, train_from=0)</code>:</p><ul><li>此函数用于从检查点文件中恢复模型的参数。</li><li><code>model</code> 是模型对象，<code>exp_name</code> 是实验名称，<code>use_cuda</code> 是是否使用 CUDA 运行模型，<code>train_from</code> 是指定的训练起始时期。</li><li>函数会查找以 “.pth” 结尾的模型检查点文件，找到最新的模型检查点并加载到模型中。</li><li>如果指定了 <code>train_from</code> 大于0，会从该时期开始训练。</li><li>函数返回模型训练的当前时期。</li></ul></li><li><p><code>VoxelPooling</code>:</p><ul><li>这是一个用于点云特征池化的自定义模块。</li><li>通过此模块，可以将点云体素汇聚为单一特征。</li><li>模块会对点云体素进行池化操作，并考虑了点云体素之间的关系。</li></ul></li><li><p><code>Loss</code>:</p><ul><li>这是一个自定义的损失函数模块，用于计算训练中的损失。</li><li>它计算分割损失和完成损失，支持带权重的损失计算。</li><li>如果 <code>uncertainty_loss</code> 被启用，还会计算损失的不确定性。</li></ul></li><li><p><code>interaction_module</code>:</p><ul><li>这是一个模块，用于模型中的点云交互。</li><li>它允许点云之间的特征交互，可以选择使用特征关系或直接的插值方法。</li><li>此模块通过模型的前向传播执行点云特征的交互操作。</li></ul></li><li><p><code>ResidualBlock</code>, <code>VGGBlock</code>, <code>UBlock</code>:</p><ul><li>这些模块用于构建模型的特定类型的块，如残差块、VGG 块和 U 块。</li><li>这些块用于构建点云处理模型的不同组件。</li></ul></li><li><p>其他工具函数:</p><ul><li>文件中还包括了其他用于文件读写、坐标转换、特征提取和其他辅助功能的函数。</li></ul></li></ol><p>总之，<code>model_utils.py</code> 包含了模型训练中的损失函数、模型恢复函数、自定义模块和工具函数，这些组成部分用于构建和训练点云处理模型。</p><h2 id="MonoScene"><a href="#MonoScene" class="headerlink" title="MonoScene"></a>MonoScene</h2><p>一种能在室内与室外场景均可使用的单目 SSC 方案：</p><ul><li>提出一种将 2D 特征投影到 3D 的方法： FLoSP </li><li>提出一种  3D Context Relation Prior （3D CRP） 提升网络的上下文意识</li><li>新的SSC损失函数，用于优化场景类亲和力（affinity）和局部锥体（frustum）比例</li></ul><p>代码链接： <a href="https://github.com/astra-vision/MonoScene">MonoScene: Monocular 3D Semantic Scene Completion</a>, CVPR 2022</p><p><img src="/pic/MonoScence.png"></p><p>Monoscene  网络流程</p><ul><li>输入 2d 图片，经过 2d unet 提取多层次的特征</li><li>Features Line of Sight Projection module (FLoSP) 用于将 2d 特征提升到 3d 位置上，增强信息流并实现2D-3D分离</li><li>3D Context Relation Prior （3D CRP） 用于增强长距离的上下文信息提取</li><li>loss 优化：Scene-Class Affinity Loss：提升类内和类间的场景方面度量；Frustum Proportion Loss：对齐局部截头体中的类分布，提供超越场景遮挡的监督信息</li></ul><p>网络结构</p><ul><li>2D unet：EfficientNetB7 用于提取图像特征</li><li>3D UNets：2层 encoder-decoder 结构，用于提取 3d 特征</li><li>completion head：3D ASPP 结构和 softmax 层，用于处理 3D UNet 输出得到3d场景 completion 结果</li></ul><h3 id="代码介绍"><a href="#代码介绍" class="headerlink" title="代码介绍"></a>代码介绍</h3><p><a href="https://github.com/astra-vision/MonoScene/blob/master/monoscene/models/monoscene.py">monoscene.py</a>使用PyTorch Lightning构建的一个名为<code>MonoScene</code>的模型类，用于单目深度估计和场景分割的任务。以下是该类的主要组件和功能的详细解释：</p><ol><li><p><strong>初始化方法</strong> (<code>__init__</code>)：这是类的构造方法，在创建<code>MonoScene</code>对象时被调用。它接受多个参数，包括类别数 (<code>n_classes</code>)、类别名称 (<code>class_names</code>)、特征数量 (<code>feature</code>)、类别权重 (<code>class_weights</code>) 等，用于初始化模型的各种参数和组件。其中的许多参数用于配置损失函数和训练过程中的各种选项。这里还创建了一个UNet 2D模型和多个FLoSP (Feature Learning from Single Point) 模型，用于3D场景估计。</p></li><li><p><strong>前向传播方法</strong> (<code>forward</code>)：<code>forward</code> 方法定义了如何计算模型的前向传播。它接受一个输入批次 (<code>batch</code>)，包括图像 (<code>img</code>)，并根据输入数据的尺寸进行前向传播计算。首先，通过UNet 2D模型 (<code>net_rgb</code>) 处理输入图像，然后根据尺度变化对图像进行3D投影，最终由UNet 3D模型 (<code>net_3d_decoder</code>) 进行3D场景估计。前向传播的结果保存在<code>out</code>变量中。</p></li><li><p><strong><code>step</code> 方法</strong>：<code>step</code> 方法执行了训练、验证和测试过程中的通用逻辑。它接受一个数据批次 (<code>batch</code>)、一个步骤类型 (<code>step_type</code>) 和一个度量指标 (<code>metric</code>)。在该方法中，对输入数据进行前向传播，计算并更新损失，同时也更新度量指标。这部分逻辑用于在训练、验证和测试时共享相同的操作。</p></li><li><p><strong>训练步骤</strong> (<code>training_step</code>)：<code>training_step</code> 方法是PyTorch Lightning中定义训练步骤的函数。它调用了<code>step</code> 方法，以及指定了步骤类型 (“train”) 和训练度量指标 (<code>self.train_metrics</code>)。</p></li><li><p><strong>验证步骤</strong> (<code>validation_step</code>) 和 <strong>验证结束方法</strong> (<code>validation_epoch_end</code>)：这两个方法用于在验证集上进行评估。<code>validation_step</code> 方法执行一个验证步骤，计算损失和更新验证度量指标 (<code>self.val_metrics</code>)。<code>validation_epoch_end</code> 方法在每个验证周期结束后，从度量指标中获取评估结果并进行记录。</p></li><li><p><strong>测试步骤</strong> (<code>test_step</code>) 和 <strong>测试结束方法</strong> (<code>test_epoch_end</code>)：这两个方法用于在测试集上进行评估，与验证过程类似。</p></li><li><p><strong>配置优化器</strong> (<code>configure_optimizers</code>)：在此方法中配置了模型的优化器和学习率调度器。根据数据集的不同（NYU或KITTI），选择了不同的优化器和学习率调度策略。</p></li><li><p><strong>指标记录和日志输出</strong>：在不同的步骤中，通过<code>self.log</code> 方法记录和输出训练、验证和测试的损失、IoU、精确度等度量指标。这些度量指标在每个epoch结束时会被重置，以便记录下一个epoch的结果。</p></li></ol><p>这段代码使用了PyTorch Lightning来规范化训练和评估流程，提供了清晰的组织结构，以及易于扩展和修改的接口，使其更容易适应不同的任务和数据集。根据您的需求，可以调整和扩展这个类来满足您的具体任务。</p><p>让我们逐行解释<code>MonoScene</code>类中的代码：</p><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">import</span> pytorch_lightning <span class="token keyword">as</span> pl<span class="token keyword">import</span> torch<span class="token keyword">import</span> torch<span class="token punctuation">.</span>nn <span class="token keyword">as</span> nn<span class="token keyword">from</span> monoscene<span class="token punctuation">.</span>models<span class="token punctuation">.</span>unet3d_nyu <span class="token keyword">import</span> UNet3D <span class="token keyword">as</span> UNet3DNYU<span class="token keyword">from</span> monoscene<span class="token punctuation">.</span>models<span class="token punctuation">.</span>unet3d_kitti <span class="token keyword">import</span> UNet3D <span class="token keyword">as</span> UNet3DKitti<span class="token keyword">from</span> monoscene<span class="token punctuation">.</span>loss<span class="token punctuation">.</span>sscMetrics <span class="token keyword">import</span> SSCMetrics<span class="token keyword">from</span> monoscene<span class="token punctuation">.</span>loss<span class="token punctuation">.</span>ssc_loss <span class="token keyword">import</span> sem_scal_loss<span class="token punctuation">,</span> CE_ssc_loss<span class="token punctuation">,</span> KL_sep<span class="token punctuation">,</span> geo_scal_loss<span class="token keyword">from</span> monoscene<span class="token punctuation">.</span>models<span class="token punctuation">.</span>flosp <span class="token keyword">import</span> FLoSP<span class="token keyword">from</span> monoscene<span class="token punctuation">.</span>loss<span class="token punctuation">.</span>CRP_loss <span class="token keyword">import</span> compute_super_CP_multilabel_loss<span class="token keyword">import</span> numpy <span class="token keyword">as</span> np<span class="token keyword">import</span> torch<span class="token punctuation">.</span>nn<span class="token punctuation">.</span>functional <span class="token keyword">as</span> F<span class="token keyword">from</span> monoscene<span class="token punctuation">.</span>models<span class="token punctuation">.</span>unet2d <span class="token keyword">import</span> UNet2D<span class="token keyword">from</span> torch<span class="token punctuation">.</span>optim<span class="token punctuation">.</span>lr_scheduler <span class="token keyword">import</span> MultiStepLR<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>这些行导入所需的Python库和模块，包括PyTorch、PyTorch Lightning、模型类、损失函数和其他依赖项。</p><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">class</span> <span class="token class-name">MonoScene</span><span class="token punctuation">(</span>pl<span class="token punctuation">.</span>LightningModule<span class="token punctuation">)</span><span class="token punctuation">:</span><span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>这一行定义了一个名为<code>MonoScene</code>的PyTorch Lightning模型类，它继承自<code>pl.LightningModule</code>，这是PyTorch Lightning中定义自定义模型的基类。</p><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>    self<span class="token punctuation">,</span>    n_classes<span class="token punctuation">,</span>    class_names<span class="token punctuation">,</span>    feature<span class="token punctuation">,</span>    class_weights<span class="token punctuation">,</span>    project_scale<span class="token punctuation">,</span>    full_scene_size<span class="token punctuation">,</span>    dataset<span class="token punctuation">,</span>    n_relations<span class="token operator">=</span><span class="token number">4</span><span class="token punctuation">,</span>    context_prior<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span>    fp_loss<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span>    project_res<span class="token operator">=</span><span class="token punctuation">[</span><span class="token punctuation">]</span><span class="token punctuation">,</span>    frustum_size<span class="token operator">=</span><span class="token number">4</span><span class="token punctuation">,</span>    relation_loss<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">,</span>    CE_ssc_loss<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span>    geo_scal_loss<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span>    sem_scal_loss<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span>    lr<span class="token operator">=</span><span class="token number">1e-4</span><span class="token punctuation">,</span>    weight_decay<span class="token operator">=</span><span class="token number">1e-4</span><span class="token punctuation">,</span><span class="token punctuation">)</span><span class="token punctuation">:</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>这是<code>MonoScene</code>类的构造方法 (<code>__init__</code>)。它接受许多参数，这些参数用于配置模型的不同方面，如类别数、类别名称、特征数量、损失权重、训练选项等。这些参数被用于初始化模型的各个组件和超参数。</p><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token builtin">super</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>__init<span class="token punctuation">(</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>这一行调用了基类的构造方法，即<code>pl.LightningModule</code>的构造方法。</p><pre class="line-numbers language-python" data-language="python"><code class="language-python">self<span class="token punctuation">.</span>project_res <span class="token operator">=</span> project_resself<span class="token punctuation">.</span>fp_loss <span class="token operator">=</span> fp_lossself<span class="token punctuation">.</span>dataset <span class="token operator">=</span> datasetself<span class="token punctuation">.</span>context_prior <span class="token operator">=</span> context_priorself<span class="token punctuation">.</span>frustum_size <span class="token operator">=</span> frustum_sizeself<span class="token punctuation">.</span>class_names <span class="token operator">=</span> class_namesself<span class="token punctuation">.</span>relation_loss <span class="token operator">=</span> relation_lossself<span class="token punctuation">.</span>CE_ssc_loss <span class="token operator">=</span> CE_ssc_lossself<span class="token punctuation">.</span>sem_scal_loss <span class="token operator">=</span> sem_scal_lossself<span class="token punctuation">.</span>geo_scal_loss <span class="token operator">=</span> geo_scal_lossself<span class="token punctuation">.</span>project_scale <span class="token operator">=</span> project_scaleself<span class="token punctuation">.</span>class_weights <span class="token operator">=</span> class_weightsself<span class="token punctuation">.</span>lr <span class="token operator">=</span> lrself<span class="token punctuation">.</span>weight_decay <span class="token operator">=</span> weight_decay<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>这些行将构造方法中传入的参数赋值给对象的属性，以便它们可以在整个类中使用。</p><pre class="line-numbers language-python" data-language="python"><code class="language-python">self<span class="token punctuation">.</span>projects <span class="token operator">=</span> <span class="token punctuation">&#123;</span><span class="token punctuation">&#125;</span>self<span class="token punctuation">.</span>scale_2ds <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">4</span><span class="token punctuation">,</span> <span class="token number">8</span><span class="token punctuation">]</span>  <span class="token comment"># 2D scales</span><span class="token keyword">for</span> scale_2d <span class="token keyword">in</span> self<span class="token punctuation">.</span>scale_2ds<span class="token punctuation">:</span>    self<span class="token punctuation">.</span>projects<span class="token punctuation">[</span><span class="token builtin">str</span><span class="token punctuation">(</span>scale_2d<span class="token punctuation">)</span><span class="token punctuation">]</span> <span class="token operator">=</span> FLoSP<span class="token punctuation">(</span>        full_scene_size<span class="token punctuation">,</span> project_scale<span class="token operator">=</span>self<span class="token punctuation">.</span>project_scale<span class="token punctuation">,</span> dataset<span class="token operator">=</span>self<span class="token punctuation">.</span>dataset    <span class="token punctuation">)</span>self<span class="token punctuation">.</span>projects <span class="token operator">=</span> nn<span class="token punctuation">.</span>ModuleDict<span class="token punctuation">(</span>self<span class="token punctuation">.</span>projects<span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>这一部分初始化了模型的<code>projects</code>属性，其中包含多个FLoSP模型，用于将2D特征投影到3D场景。不同的投影比例 (<code>scale_2d</code>) 在这里被迭代，为每个比例创建一个FLoSP模型，并将它们存储在<code>projects</code>字典中。</p><pre class="line-numbers language-python" data-language="python"><code class="language-python">self<span class="token punctuation">.</span>n_classes <span class="token operator">=</span> n_classes<span class="token keyword">if</span> self<span class="token punctuation">.</span>dataset <span class="token operator">==</span> <span class="token string">"NYU"</span><span class="token punctuation">:</span>    self<span class="token punctuation">.</span>net_3d_decoder <span class="token operator">=</span> UNet3DNYU<span class="token punctuation">(</span>        self<span class="token punctuation">.</span>n_classes<span class="token punctuation">,</span>        nn<span class="token punctuation">.</span>BatchNorm3d<span class="token punctuation">,</span>        n_relations<span class="token operator">=</span>n_relations<span class="token punctuation">,</span>        feature<span class="token operator">=</span>feature<span class="token punctuation">,</span>        full_scene_size<span class="token operator">=</span>full_scene_size<span class="token punctuation">,</span>        context_prior<span class="token operator">=</span>context_prior<span class="token punctuation">,</span>    <span class="token punctuation">)</span><span class="token keyword">elif</span> self<span class="token punctuation">.</span>dataset <span class="token operator">==</span> <span class="token string">"kitti"</span><span class="token punctuation">:</span>    self<span class="token punctuation">.</span>net_3d_decoder <span class="token operator">=</span> UNet3DKitti<span class="token punctuation">(</span>        self<span class="token punctuation">.</span>n_classes<span class="token punctuation">,</span>        nn<span class="token punctuation">.</span>BatchNorm3d<span class="token punctuation">,</span>        project_scale<span class="token operator">=</span>project_scale<span class="token punctuation">,</span>        feature<span class="token operator">=</span>feature<span class="token punctuation">,</span>        full_scene_size<span class="token operator">=</span>full_scene_size<span class="token punctuation">,</span>        context_prior<span class="token operator">=</span>context_prior<span class="token punctuation">,</span>    <span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>这一部分初始化了模型的3D解码器 (<code>net_3d_decoder</code>)，根据数据集的类型 (“NYU” 或 “kitti”) 选择不同的UNet 3D模型。该解码器将用于对3D场景进行估计。</p><pre class="line-numbers language-python" data-language="python"><code class="language-python">self<span class="token punctuation">.</span>net_rgb <span class="token operator">=</span> UNet2D<span class="token punctuation">.</span>build<span class="token punctuation">(</span>out_feature<span class="token operator">=</span>feature<span class="token punctuation">,</span> use_decoder<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>这一行创建了模型的2D UNet模型 (<code>net_rgb</code>)，该模型用于处理输入图像，其中的<code>feature</code>参数指定了模型的特征数量。</p><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token comment"># log hyperparameters</span>self<span class="token punctuation">.</span>save_hyperparameters<span class="token punctuation">(</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><p>这一行记录了模型的超参数，以便后续可以查看它们。</p><pre class="line-numbers language-python" data-language="python"><code class="language-python">self<span class="token punctuation">.</span>train_metrics <span class="token operator">=</span> SSCMetrics<span class="token punctuation">(</span>self<span class="token punctuation">.</span>n_classes<span class="token punctuation">)</span>self<span class="token punctuation">.</span>val_metrics <span class="token operator">=</span> SSCMetrics<span class="token punctuation">(</span>self<span class="token punctuation">.</span>n_classes<span class="token punctuation">)</span>self<span class="token punctuation">.</span>test_metrics <span class="token operator">=</span> SSCMetrics<span class="token punctuation">(</span>self<span class="token punctuation">.</span>n_classes<span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre><p>这些行初始化了训练、验证和测试度量指标 (<code>train_metrics</code>, <code>val_metrics</code>, <code>test_metrics</code>)，用于跟踪模型性能。</p><p>这只是构造方法的设置部分。整个<code>MonoScene</code>类的构造方法用于初始化模型的各个组件和超参数。在下面的部分中，将解释类中的其他方法。</p><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> batch<span class="token punctuation">)</span><span class="token punctuation">:</span>    img <span class="token operator">=</span> batch<span class="token punctuation">[</span><span class="token string">"img"</span><span class="token punctuation">]</span>    bs <span class="token operator">=</span> <span class="token builtin">len</span><span class="token punctuation">(</span>img<span class="token punctuation">)</span>    out <span class="token operator">=</span> <span class="token punctuation">&#123;</span><span class="token punctuation">&#125;</span>    x_rgb <span class="token operator">=</span> self<span class="token punctuation">.</span>net_rgb<span class="token punctuation">(</span>img<span class="token punctuation">)</span>    x3ds <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>    <span class="token keyword">for</span> i <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span>bs<span class="token punctuation">)</span><span class="token punctuation">:</span>        x3d <span class="token operator">=</span> <span class="token boolean">None</span>        <span class="token keyword">for</span> scale_2d <span class="token keyword">in</span> self<span class="token punctuation">.</span>project_res<span class="token punctuation">:</span>            <span class="token comment"># project features at each 2D scale to target 3D scale</span>            scale_2d <span class="token operator">=</span> <span class="token builtin">int</span><span class="token punctuation">(</span>scale_2d<span class="token punctuation">)</span>            projected_pix <span class="token operator">=</span> batch<span class="token punctuation">[</span><span class="token string">"projected_pix_&#123;&#125;"</span><span class="token punctuation">.</span><span class="token builtin">format</span><span class="token punctuation">(</span>self<span class="token punctuation">.</span>project_scale<span class="token punctuation">)</span><span class="token punctuation">]</span><span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token punctuation">.</span>cuda<span class="token punctuation">(</span><span class="token punctuation">)</span>            fov_mask <span class="token operator">=</span> batch<span class="token punctuation">[</span><span class="token string">"fov_mask_&#123;&#125;"</span><span class="token punctuation">.</span><span class="token builtin">format</span><span class="token punctuation">(</span>self<span class="token punctuation">.</span>project_scale<span class="token punctuation">)</span><span class="token punctuation">]</span><span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token punctuation">.</span>cuda<span class="token punctuation">(</span><span class="token punctuation">)</span>            <span class="token keyword">if</span> x3d <span class="token keyword">is</span> <span class="token boolean">None</span><span class="token punctuation">:</span>                x3d <span class="token operator">=</span> self<span class="token punctuation">.</span>projects<span class="token punctuation">[</span><span class="token builtin">str</span><span class="token punctuation">(</span>scale_2d<span class="token punctuation">)</span><span class="token punctuation">]</span><span class="token punctuation">(</span>                    x_rgb<span class="token punctuation">[</span><span class="token string">"1_"</span> <span class="token operator">+</span> <span class="token builtin">str</span><span class="token punctuation">(</span>scale_2d<span class="token punctuation">)</span><span class="token punctuation">]</span><span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token punctuation">,</span>                    projected_pix <span class="token operator">//</span> scale_2d<span class="token punctuation">,</span>                    fov_mask<span class="token punctuation">,</span>                <span class="token punctuation">)</span>            <span class="token keyword">else</span><span class="token punctuation">:</span>                x3d <span class="token operator">+=</span> self<span class="token punctuation">.</span>projects<span class="token punctuation">[</span><span class="token builtin">str</span><span class="token punctuation">(</span>scale_2d<span class="token punctuation">)</span><span class="token punctuation">]</span><span class="token punctuation">(</span>                    x_rgb<span class="token punctuation">[</span><span class="token string">"1_"</span> <span class="token operator">+</span> <span class="token builtin">str</span><span class="token punctuation">(</span>scale_2d<span class="token punctuation">)</span><span class="token punctuation">]</span><span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token punctuation">,</span>                    projected_pix <span class="token operator">//</span> scale_2d<span class="token punctuation">,</span>                    fov_mask<span class="token punctuation">,</span>                <span class="token punctuation">)</span>        x3ds<span class="token punctuation">.</span>append<span class="token punctuation">(</span>x3d<span class="token punctuation">)</span>    input_dict <span class="token operator">=</span> <span class="token punctuation">&#123;</span><span class="token string">"x3d"</span><span class="token punctuation">:</span> torch<span class="token punctuation">.</span>stack<span class="token punctuation">(</span>x3ds<span class="token punctuation">)</span><span class="token punctuation">&#125;</span>    out <span class="token operator">=</span> self<span class="token punctuation">.</span>net_3d_decoder<span class="token punctuation">(</span>input_dict<span class="token punctuation">)</span>    <span class="token keyword">return</span> out<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>这是模型的前向传播函数 <code>forward</code>。在这个函数中，输入图像 <code>img</code> 被传递给 2D UNet 模型 <code>net_rgb</code> 以提取特征。然后，特征将根据指定的投影比例 (<code>self.project_res</code>) 投影到3D场景中。对于每个输入样本，它会循环遍历不同的投影比例，并调用FLoSP模型 (<code>self.projects</code>) 来执行特征投影。最终，投影的特征将被传递给3D解码器 <code>net_3d_decoder</code> 进行3D场景估计。</p><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">def</span> <span class="token function">step</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> batch<span class="token punctuation">,</span> step_type<span class="token punctuation">,</span> metric<span class="token punctuation">)</span><span class="token punctuation">:</span>    bs <span class="token operator">=</span> <span class="token builtin">len</span><span class="token punctuation">(</span>batch<span class="token punctuation">[</span><span class="token string">"img"</span><span class="token punctuation">]</span><span class="token punctuation">)</span>    loss <span class="token operator">=</span> <span class="token number">0</span>    out_dict <span class="token operator">=</span> self<span class="token punctuation">(</span>batch<span class="token punctuation">)</span>    ssc_pred <span class="token operator">=</span> out_dict<span class="token punctuation">[</span><span class="token string">"ssc_logit"</span><span class="token punctuation">]</span>    target <span class="token operator">=</span> batch<span class="token punctuation">[</span><span class="token string">"target"</span><span class="token punctuation">]</span>    <span class="token keyword">if</span> self<span class="token punctuation">.</span>context_prior<span class="token punctuation">:</span>        P_logits <span class="token operator">=</span> out_dict<span class="token punctuation">[</span><span class="token string">"P_logits"</span><span class="token punctuation">]</span>        CP_mega_matrices <span class="token operator">=</span> batch<span class="token punctuation">[</span><span class="token string">"CP_mega_matrices"</span><span class="token punctuation">]</span>        <span class="token keyword">if</span> self<span class="token punctuation">.</span>relation_loss<span class="token punctuation">:</span>            loss_rel_ce <span class="token operator">=</span> compute_super_CP_multilabel_loss<span class="token punctuation">(</span>                P_logits<span class="token punctuation">,</span> CP_mega_matrices            <span class="token punctuation">)</span>            loss <span class="token operator">+=</span> loss_rel_ce            self<span class="token punctuation">.</span>log<span class="token punctuation">(</span>                step_type <span class="token operator">+</span> <span class="token string">"/loss_relation_ce_super"</span><span class="token punctuation">,</span>                loss_rel_ce<span class="token punctuation">.</span>detach<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span>                on_epoch<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span>                sync_dist<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span>            <span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>这个方法 <code>step</code> 用于执行训练、验证和测试步骤的公共逻辑。它接受批量数据 <code>batch</code>、步骤类型 <code>step_type</code>（如 “train”、”val”、”test”）和度量对象 <code>metric</code> 作为参数。在这个方法中，模型的前向传播被调用，并计算损失 <code>loss</code>。具体的损失计算依赖于模型的配置和目标。在这里，根据模型的配置，可以计算包括关系损失 (<code>loss_rel_ce</code>) 的不同损失项。如果启用了关系损失 (<code>self.relation_loss</code>)，则将计算关系损失并将其添加到总损失中。</p><pre class="line-numbers language-python" data-language="python"><code class="language-python">class_weight <span class="token operator">=</span> self<span class="token punctuation">.</span>class_weights<span class="token punctuation">.</span>type_as<span class="token punctuation">(</span>batch<span class="token punctuation">[</span><span class="token string">"img"</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token keyword">if</span> self<span class="token punctuation">.</span>CE_ssc_loss<span class="token punctuation">:</span>    loss_ssc <span class="token operator">=</span> CE_ssc_loss<span class="token punctuation">(</span>ssc_pred<span class="token punctuation">,</span> target<span class="token punctuation">,</span> class_weight<span class="token punctuation">)</span>    loss <span class="token operator">+=</span> loss_ssc    self<span class="token punctuation">.</span>log<span class="token punctuation">(</span>        step_type <span class="token operator">+</span> <span class="token string">"/loss_ssc"</span><span class="token punctuation">,</span>        loss_ssc<span class="token punctuation">.</span>detach<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span>        on_epoch<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span>        sync_dist<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span>    <span class="token punctuation">)</span><span class="token keyword">if</span> self<span class="token punctuation">.</span>sem_scal_loss<span class="token punctuation">:</span>    loss_sem_scal <span class="token operator">=</span> sem_scal_loss<span class="token punctuation">(</span>ssc_pred<span class="token punctuation">,</span> target<span class="token punctuation">)</span>    loss <span class="token operator">+=</span> loss_sem_scal    self<span class="token punctuation">.</span>log<span class="token punctuation">(</span>        step_type <span class="token operator">+</span> <span class="token string">"/loss_sem_scal"</span><span class="token punctuation">,</span>        loss_sem_scal<span class="token punctuation">.</span>detach<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span>        on_epoch<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span>        sync_dist<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span>    <span class="token punctuation">)</span><span class="token keyword">if</span> self<span class="token punctuation">.</span>geo_scal_loss<span class="token punctuation">:</span>    loss_geo_scal <span class="token operator">=</span> geo_scal_loss<span class="token punctuation">(</span>ssc_pred<span class="token punctuation">,</span> target<span class="token punctuation">)</span>    loss <span class="token operator">+=</span> loss_geo_scal    self<span class="token punctuation">.</span>log<span class="token punctuation">(</span>        step_type <span class="token operator">+</span> <span class="token string">"/loss_geo_scal"</span><span class="token punctuation">,</span>        loss_geo_scal<span class="token punctuation">.</span>detach<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span>        on_epoch<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span>        sync_dist<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span>    <span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>在这一部分中，根据模型的配置，计算了交叉熵损失 (<code>loss_ssc</code>)、语义分割损失 (<code>loss_sem_scal</code>) 和几何尺度损失 (<code>loss_geo_scal</code>)。这些损失用于对模型进行监督训练，以便它能够学习适应输入数据并进行3D场景估计。</p><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">if</span> self<span class="token punctuation">.</span>fp_loss <span class="token keyword">and</span> step_type <span class="token operator">!=</span> <span class="token string">"test"</span><span class="token punctuation">:</span>    frustums_masks <span class="token operator">=</span> torch<span class="token punctuation">.</span>stack<span class="token punctuation">(</span>batch<span class="token punctuation">[</span><span class="token string">"frustums_masks"</span><span class="token punctuation">]</span><span class="token punctuation">)</span>    frustums_class_dists <span class="token operator">=</span> torch<span class="token punctuation">.</span>stack<span class="token punctuation">(</span>        batch<span class="token punctuation">[</span><span class="token string">"frustums_class_dists"</span><span class="token punctuation">]</span>    <span class="token punctuation">)</span><span class="token punctuation">.</span><span class="token builtin">float</span><span class="token punctuation">(</span><span class="token punctuation">)</span>  <span class="token comment"># (bs, n_frustums, n_classes)</span>    n_frustums <span class="token operator">=</span> frustums_class_dists<span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span>    pred_prob <span class="token operator">=</span> F<span class="token punctuation">.</span>softmax<span class="token punctuation">(</span>ssc_pred<span class="token punctuation">,</span> dim<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span>    batch_cnt <span class="token operator">=</span> frustums_class_dists<span class="token punctuation">.</span><span class="token builtin">sum</span><span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">)</span>  <span class="token comment"># (n_frustums, n_classes)</span>    frustum_loss <span class="token operator">=</span> <span class="token number">0</span>    frustum_nonempty <span class="token operator">=</span> <span class="token number">0</span>    <span class="token keyword">for</span> frus <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span>n_frustums<span class="token punctuation">)</span><span class="token punctuation">:</span>        frustum_mask <span class="token operator">=</span> frustums_masks<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span> frus<span class="token punctuation">,</span> <span class="token punctuation">:</span><span class="token punctuation">,</span> <span class="token punctuation">:</span><span class="token punctuation">,</span> <span class="token punctuation">:</span><span class="token punctuation">]</span><span class="token punctuation">.</span>unsqueeze<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">.</span><span class="token builtin">float</span><span class="token punctuation">(</span><span class="token punctuation">)</span>        prob <span class="token operator">=</span> frustum_mask <span class="token operator">*</span> pred_prob  <span class="token comment"># bs, n_classes, H, W, D</span>        prob <span class="token operator">=</span> prob<span class="token punctuation">.</span>reshape<span class="token punctuation">(</span>bs<span class="token punctuation">,</span> self<span class="token punctuation">.</span>n_classes<span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">.</span>permute<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">)</span>        prob <span class="token operator">=</span> prob<span class="token punctuation">.</span>reshape<span class="token punctuation">(</span>self<span class="token punctuation">.</span>n_classes<span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span>        cum_prob <span class="token operator">=</span> prob<span class="token punctuation">.</span><span class="token builtin">sum</span><span class="token punctuation">(</span>dim<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span>  <span class="token comment"># n_classes</span>        total_cnt <span class="token operator">=</span> torch<span class="token punctuation">.</span><span class="token builtin">sum</span><span class="token punctuation">(</span>batch_cnt<span class="token punctuation">[</span>frus<span class="token punctuation">]</span><span class="token punctuation">)</span>        total_prob <span class="token operator">=</span> prob<span class="token punctuation">.</span><span class="token builtin">sum</span><span class="token punctuation">(</span><span class="token punctuation">)</span>        <span class="token keyword">if</span> total_prob <span class="token operator">></span> <span class="token number">0</span> <span class="token keyword">and</span> total_cnt <span class="token operator">></span> <span class="token number">0</span><span class="token punctuation">:</span>            frustum_target_proportion <span class="token operator">=</span> batch_cnt<span class="token punctuation">[</span>frus<span class="token punctuation">]</span> <span class="token operator">/</span> total_cnt            cum_prob <span class="token operator">=</span> cum_prob <span class="token operator">/</span> total_prob  <span class="token comment"># n_classes</span>            frustum_loss_i <span class="token operator">=</span> KL_sep<span class="token punctuation">(</span>cum_prob<span class="token punctuation">,</span> frustum_target_proportion<span class="token punctuation">)</span>            frustum_loss <span class="token operator">+=</span> frustum_loss_i            frustum_nonempty <span class="token operator">+=</span> <span class="token number">1</span>    frustum_loss <span class="token operator">=</span> frustum_loss <span class="token operator">/</span> frustum_nonempty    loss <span class="token operator">+=</span> frustum_loss    self<span class="token punctuation">.</span>log<span class="token punctuation">(</span>        step_type <span class="token operator">+</span> <span class="token string">"/loss_frustums"</span><span class="token punctuation">,</span>        frustum_loss<span class="token punctuation">.</span>detach<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span>        on_epoch<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span>        sync_dist<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span>    <span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>在这部分代码中，计算</p><p>了与模型输出的SSC预测和输入数据中的frustum masks 相关的损失项。如果 <code>self.fp_loss</code> 被设置为 <code>True</code> 并且步骤类型不是 “test”，则将计算frustum损失。这个损失考虑了模型对视场的理解，以及模型的预测与输入数据的关系。</p><p>最后，损失项被添加到总损失 <code>loss</code> 中，并使用 <code>self.log</code> 方法记录损失，以便在训练期间进行监控。</p><pre class="line-numbers language-python" data-language="python"><code class="language-python">y_true <span class="token operator">=</span> target<span class="token punctuation">.</span>cpu<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>numpy<span class="token punctuation">(</span><span class="token punctuation">)</span>y_pred <span class="token operator">=</span> ssc_pred<span class="token punctuation">.</span>detach<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>cpu<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>numpy<span class="token punctuation">(</span><span class="token punctuation">)</span>y_pred <span class="token operator">=</span> np<span class="token punctuation">.</span>argmax<span class="token punctuation">(</span>y_pred<span class="token punctuation">,</span> axis<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span>metric<span class="token punctuation">.</span>add_batch<span class="token punctuation">(</span>y_pred<span class="token punctuation">,</span> y_true<span class="token punctuation">)</span>self<span class="token punctuation">.</span>log<span class="token punctuation">(</span>step_type <span class="token operator">+</span> <span class="token string">"/loss"</span><span class="token punctuation">,</span> loss<span class="token punctuation">.</span>detach<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span> on_epoch<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span> sync_dist<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>在这里，真实标签和模型预测的标签用于计算模型性能指标，并将它们传递给 <code>metric</code> 对象。度量对象用于跟踪模型性能，并使用 <code>self.log</code> 方法记录损失。</p><p>这部分代码覆盖了<code>step</code> 方法中的主要逻辑，它是用于训练、验证和测试的公共代码。</p><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">def</span> <span class="token function">training_step</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> batch<span class="token punctuation">,</span> batch_idx<span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token keyword">return</span> self<span class="token punctuation">.</span>step<span class="token punctuation">(</span>batch<span class="token punctuation">,</span> <span class="token string">"train"</span><span class="token punctuation">,</span> self<span class="token punctuation">.</span>train_metrics<span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><p>这个方法是用于训练的步骤。它调用了 <code>step</code> 方法，并传递了相应的参数。在训练期间，它还返回损失以供 PyTorch Lightning 进行后续的处理。</p><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">def</span> <span class="token function">validation_step</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> batch<span class="token punctuation">,</span> batch_idx<span class="token punctuation">)</span><span class="token punctuation">:</span>    self<span class="token punctuation">.</span>step<span class="token punctuation">(</span>batch<span class="token punctuation">,</span> <span class="token string">"val"</span><span class="token punctuation">,</span> self<span class="token punctuation">.</span>val_metrics<span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><p>这个方法是验证步骤。它也调用了 <code>step</code> 方法，但不返回损失，而是将性能指标记录到验证度量对象 <code>val_metrics</code> 中。</p><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">def</span> <span class="token function">validation_epoch_end</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> outputs<span class="token punctuation">)</span><span class="token punctuation">:</span>    metric_list <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">(</span><span class="token string">"train"</span><span class="token punctuation">,</span> self<span class="token punctuation">.</span>train_metrics<span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token punctuation">(</span><span class="token string">"val"</span><span class="token punctuation">,</span> self<span class="token punctuation">.</span>val_metrics<span class="token punctuation">)</span><span class="token punctuation">]</span>    <span class="token keyword">for</span> prefix<span class="token punctuation">,</span> metric <span class="token keyword">in</span> metric_list<span class="token punctuation">:</span>        stats <span class="token operator">=</span> metric<span class="token punctuation">.</span>get_stats<span class="token punctuation">(</span><span class="token punctuation">)</span>        <span class="token keyword">for</span> i<span class="token punctuation">,</span> class_name <span class="token keyword">in</span> <span class="token builtin">enumerate</span><span class="token punctuation">(</span>self<span class="token punctuation">.</span>class_names<span class="token punctuation">)</span><span class="token punctuation">:</span>            self<span class="token punctuation">.</span>log<span class="token punctuation">(</span>                <span class="token string">"&#123;&#125;_SemIoU/&#123;&#125;"</span><span class="token punctuation">.</span><span class="token builtin">format</span><span class="token punctuation">(</span>prefix<span class="token punctuation">,</span> class_name<span class="token punctuation">)</span><span class="token punctuation">,</span>                stats<span class="token punctuation">[</span><span class="token string">"iou_ssc"</span><span class="token punctuation">]</span><span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token punctuation">,</span>                sync_dist<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span>            <span class="token punctuation">)</span>        self<span class="token punctuation">.</span>log<span class="token punctuation">(</span><span class="token string">"&#123;&#125;/mIoU"</span><span class="token punctuation">.</span><span class="token builtin">format</span><span class="token punctuation">(</span>prefix<span class="token punctuation">)</span><span class="token punctuation">,</span> stats<span class="token punctuation">[</span><span class="token string">"iou_ssc_mean"</span><span class="token punctuation">]</span><span class="token punctuation">,</span> sync_dist<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>        self<span class="token punctuation">.</span>log<span class="token punctuation">(</span><span class="token string">"&#123;&#125;/IoU"</span><span class="token punctuation">.</span><span class="token builtin">format</span><span class="token punctuation">(</span>prefix<span class="token punctuation">)</span><span class="token punctuation">,</span> stats<span class="token punctuation">[</span><span class="token string">"iou"</span><span class="token punctuation">]</span><span class="token punctuation">,</span> sync_dist<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>        self<span class="token punctuation">.</span>log<span class="token punctuation">(</span><span class="token string">"&#123;&#125;/Precision"</span><span class="token punctuation">.</span><span class="token builtin">format</span><span class="token punctuation">(</span>prefix<span class="token punctuation">)</span><span class="token punctuation">,</span> stats<span class="token punctuation">[</span><span class="token string">"recall"</span><span class="token punctuation">]</span><span class="token punctuation">,</span> sync_dist<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>        self<span class="token punctuation">.</span>log<span class="token punctuation">(</span><span class="token string">"&#123;&#125;/Recall"</span><span class="token punctuation">.</span><span class="token builtin">format</span><span class="token punctuation">(</span>prefix<span class="token punctuation">)</span><span class="token punctuation">,</span> stats<span class="token punctuation">[</span><span class="token string">"recall"</span><span class="token punctuation">]</span><span class="token punctuation">,</span> sync_dist<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>        metric<span class="token punctuation">.</span>reset<span class="token punctuation">(</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>这个方法在验证周期结束时被调用，用于总结验证阶段的性能。它计算各个类别的语义分割IoU、平均IoU、IoU和精确度，并使用 <code>self.log</code> 方法记录这些度量值。</p><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">def</span> <span class="token function">test_step</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> batch<span class="token punctuation">,</span> batch_idx<span class="token punctuation">)</span><span class="token punctuation">:</span>    self<span class="token punctuation">.</span>step<span class="token punctuation">(</span>batch<span class="token punctuation">,</span> <span class="token string">"test"</span><span class="token punctuation">,</span> self<span class="token punctuation">.</span>test_metrics<span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><p>这个方法是测试步骤，类似于验证步骤，它调用 <code>step</code> 方法并将性能指标记录到测试度量对象 <code>test_metrics</code> 中。</p><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">def</span> <span class="token function">test_epoch_end</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> outputs<span class="token punctuation">)</span><span class="token punctuation">:</span>    classes <span class="token operator">=</span> self<span class="token punctuation">.</span>class_names    metric_list <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">(</span><span class="token string">"test"</span><span class="token punctuation">,</span> self<span class="token punctuation">.</span>test_metrics<span class="token punctuation">)</span><span class="token punctuation">]</span>    <span class="token keyword">for</span> prefix<span class="token punctuation">,</span> metric <span class="token keyword">in</span> metric_list<span class="token punctuation">:</span>        <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">"&#123;&#125;======"</span><span class="token punctuation">.</span><span class="token builtin">format</span><span class="token punctuation">(</span>prefix<span class="token punctuation">)</span><span class="token punctuation">)</span>        stats <span class="token operator">=</span> metric<span class="token punctuation">.</span>get_stats<span class="token punctuation">(</span><span class="token punctuation">)</span>        <span class="token keyword">print</span><span class="token punctuation">(</span>            <span class="token string">"Precision=&#123;:.4f&#125;, Recall=&#123;:.4f&#125;, IoU=&#123;:.4f&#125;"</span><span class="token punctuation">.</span><span class="token builtin">format</span><span class="token punctuation">(</span>                stats<span class="token punctuation">[</span><span class="token string">"precision"</span><span class="token punctuation">]</span> <span class="token operator">*</span> <span class="token number">100</span><span class="token punctuation">,</span> stats<span class="token punctuation">[</span><span class="token string">"recall"</span><span class="token punctuation">]</span> <span class="token operator">*</span> <span class="token number">100</span><span class="token punctuation">,</span> stats<span class="token punctuation">[</span><span class="token string">"iou"</span><span class="token punctuation">]</span> <span class="token operator">*</span> <span class="token number">100</span>            <span class="token punctuation">)</span>        <span class="token punctuation">)</span>        <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">"class IoU: &#123;&#125;, "</span><span class="token punctuation">.</span><span class="token builtin">format</span><span class="token punctuation">(</span>classes<span class="token punctuation">)</span><span class="token punctuation">)</span>        <span class="token keyword">print</span><span class="token punctuation">(</span>            <span class="token string">" "</span><span class="token punctuation">.</span>join<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token string">"&#123;:.4f&#125;, "</span><span class="token punctuation">]</span> <span class="token operator">*</span> <span class="token builtin">len</span><span class="token punctuation">(</span>classes<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">.</span><span class="token builtin">format</span><span class="token punctuation">(</span>                <span class="token operator">*</span><span class="token punctuation">(</span>stats<span class="token punctuation">[</span><span class="token string">"iou_ssc"</span><span class="token punctuation">]</span> <span class="token operator">*</span> <span class="token number">100</span><span class="token punctuation">)</span><span class="token punctuation">.</span>tolist<span class="token punctuation">(</span><span class="token punctuation">)</span>            <span class="token punctuation">)</span>        <span class="token punctuation">)</span>        <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">"mIoU=&#123;:.4f&#125;"</span><span class="token punctuation">.</span><span class="token builtin">format</span><span class="token punctuation">(</span>stats<span class="token punctuation">[</span><span class="token string">"iou_ssc_mean"</span><span class="token punctuation">]</span> <span class="token operator">*</span> <span class="token number">100</span><span class="token punctuation">)</span><span class="token punctuation">)</span>        metric<span class="token punctuation">.</span>reset<span class="token punctuation">(</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>这个方法在测试周期结束时被调用，用于总结测试阶段的性能。它打印了精确度、召回率、IoU 和类别IoU的信息，并将这些信息显示在控制台上。</p><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">def</span> <span class="token function">configure_optimizers</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token keyword">if</span> self<span class="token punctuation">.</span>dataset <span class="token operator">==</span> <span class="token string">"NYU"</span><span class="token punctuation">:</span>        optimizer <span class="token operator">=</span> torch<span class="token punctuation">.</span>optim<span class="token punctuation">.</span>AdamW<span class="token punctuation">(</span>            self<span class="token punctuation">.</span>parameters<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span> lr<span class="token operator">=</span>self<span class="token punctuation">.</span>lr<span class="token punctuation">,</span> weight_decay<span class="token operator">=</span>self<span class="token punctuation">.</span>weight_decay        <span class="token punctuation">)</span>        scheduler <span class="token operator">=</span> MultiStepLR<span class="token punctuation">(</span>optimizer<span class="token punctuation">,</span> milestones<span class="token operator">=</span><span class="token punctuation">[</span><span class="token number">20</span><span class="token punctuation">]</span><span class="token punctuation">,</span> gamma<span class="token operator">=</span><span class="token number">0.1</span><span class="token punctuation">)</span>        <span class="token keyword">return</span> <span class="token punctuation">[</span>optimizer<span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token punctuation">[</span>scheduler<span class="token punctuation">]</span>    <span class="token keyword">elif</span> self<span class="token punctuation">.</span>dataset <span class="token operator">==</span> <span class="token string">"kitti"</span><span class="token punctuation">:</span>        optimizer <span class="token operator">=</span> torch<span class="token punctuation">.</span>optim<span class="token punctuation">.</span>AdamW<span class="token punctuation">(</span>            self<span class="token punctuation">.</span>parameters<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span> lr<span class="token operator">=</span>self<span class="token punctuation">.</span>lr<span class="token punctuation">,</span> weight_decay<span class="token operator">=</span>self<span class="token punctuation">.</span>weight_decay        <span class="token punctuation">)</span>        scheduler <span class="token operator">=</span> MultiStepLR<span class="token punctuation">(</span>optimizer<span class="token punctuation">,</span> milestones<span class="token operator">=</span><span class="token punctuation">[</span><span class="token number">20</span><span class="token punctuation">]</span><span class="token punctuation">,</span> gamma<span class="token operator">=</span><span class="token number">0.1</span><span class="token punctuation">)</span>        <span class="token keyword">return</span> <span class="token punctuation">[</span>optimizer<span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token punctuation">[</span>scheduler<span class="token punctuation">]</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>这个方法用于配置优化器和学习率调度器。根据数据集类型 (<code>self.dataset</code>)，它初始化一个AdamW优化器并将其与一个学习率调度器一起返回。调度器将在训练期间按照指定的里程碑来调整学习率。<br>这就是<code>MonoScene</code>类中的所有方法和逻辑的解释。该类代表了一个用于场景语义分割的PyTorch Lightning模型，它包括了许多用于定义前向传播、计算损失和跟踪性能的方法。</p><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">import</span> torch<span class="token keyword">import</span> torch<span class="token punctuation">.</span>nn <span class="token keyword">as</span> nn<span class="token keyword">class</span> <span class="token class-name">FLoSP</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> scene_size<span class="token punctuation">,</span> dataset<span class="token punctuation">,</span> project_scale<span class="token punctuation">)</span><span class="token punctuation">:</span>        <span class="token builtin">super</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>        self<span class="token punctuation">.</span>scene_size <span class="token operator">=</span> scene_size        self<span class="token punctuation">.</span>dataset <span class="token operator">=</span> dataset        self<span class="token punctuation">.</span>project_scale <span class="token operator">=</span> project_scale    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> x2d<span class="token punctuation">,</span> projected_pix<span class="token punctuation">,</span> fov_mask<span class="token punctuation">)</span><span class="token punctuation">:</span>        c<span class="token punctuation">,</span> h<span class="token punctuation">,</span> w <span class="token operator">=</span> x2d<span class="token punctuation">.</span>shape        src <span class="token operator">=</span> x2d<span class="token punctuation">.</span>view<span class="token punctuation">(</span>c<span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span>        zeros_vec <span class="token operator">=</span> torch<span class="token punctuation">.</span>zeros<span class="token punctuation">(</span>c<span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">.</span>type_as<span class="token punctuation">(</span>src<span class="token punctuation">)</span>        src <span class="token operator">=</span> torch<span class="token punctuation">.</span>cat<span class="token punctuation">(</span><span class="token punctuation">[</span>src<span class="token punctuation">,</span> zeros_vec<span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span>        pix_x<span class="token punctuation">,</span> pix_y <span class="token operator">=</span> projected_pix<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">,</span> projected_pix<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">]</span>        img_indices <span class="token operator">=</span> pix_y <span class="token operator">*</span> w <span class="token operator">+</span> pix_x        img_indices<span class="token punctuation">[</span><span class="token operator">~</span>fov_mask<span class="token punctuation">]</span> <span class="token operator">=</span> h <span class="token operator">*</span> w        img_indices <span class="token operator">=</span> img_indices<span class="token punctuation">.</span>expand<span class="token punctuation">(</span>c<span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">.</span><span class="token builtin">long</span><span class="token punctuation">(</span><span class="token punctuation">)</span>  <span class="token comment"># c, HWD</span>        src_feature <span class="token operator">=</span> torch<span class="token punctuation">.</span>gather<span class="token punctuation">(</span>src<span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> img_indices<span class="token punctuation">)</span>        <span class="token keyword">if</span> self<span class="token punctuation">.</span>dataset <span class="token operator">==</span> <span class="token string">"NYU"</span><span class="token punctuation">:</span>            x3d <span class="token operator">=</span> src_feature<span class="token punctuation">.</span>reshape<span class="token punctuation">(</span>                c<span class="token punctuation">,</span>                self<span class="token punctuation">.</span>scene_size<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span> <span class="token operator">//</span> self<span class="token punctuation">.</span>project_scale<span class="token punctuation">,</span>                self<span class="token punctuation">.</span>scene_size<span class="token punctuation">[</span><span class="token number">2</span><span class="token punctuation">]</span> <span class="token operator">//</span> self<span class="token punctuation">.</span>project_scale<span class="token punctuation">,</span>                self<span class="token punctuation">.</span>scene_size<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span> <span class="token operator">//</span> self<span class="token punctuation">.</span>project_scale<span class="token punctuation">,</span>            <span class="token punctuation">)</span>            x3d <span class="token operator">=</span> x3d<span class="token punctuation">.</span>permute<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">)</span>        <span class="token keyword">elif</span> self<span class="token punctuation">.</span>dataset <span class="token operator">==</span> <span class="token string">"kitti"</span><span class="token punctuation">:</span>            x3d <span class="token operator">=</span> src_feature<span class="token punctuation">.</span>reshape<span class="token punctuation">(</span>                c<span class="token punctuation">,</span>                self<span class="token punctuation">.</span>scene_size<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span> <span class="token operator">//</span> self<span class="token punctuation">.</span>project_scale<span class="token punctuation">,</span>                self<span class="token punctuation">.</span>scene_size<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span> <span class="token operator">//</span> self<span class="token punctuation">.</span>project_scale<span class="token punctuation">,</span>                self<span class="token punctuation">.</span>scene_size<span class="token punctuation">[</span><span class="token number">2</span><span class="token punctuation">]</span> <span class="token operator">//</span> self<span class="token punctuation">.</span>project_scale<span class="token punctuation">,</span>            <span class="token punctuation">)</span>        <span class="token keyword">return</span> x3d<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>这段代码定义了一个名为 <code>FLoSP</code> 的 PyTorch 模块，它用于执行特征投影操作。FLoSP 的作用是将一个 2D 图像中的特征投影到 3D 空间中。让我们逐行解释这个模块的功能和实现：</p><ol><li><p><code>class FLoSP(nn.Module):</code>：定义了一个名为 <code>FLoSP</code> 的 PyTorch 模块，它继承自 <code>nn.Module</code>。</p></li><li><p><code>def __init__(self, scene_size, dataset, project_scale):</code>：初始化方法，接受三个参数：</p><ul><li><code>scene_size</code>：表示 3D 场景的大小（宽度、高度、深度）。</li><li><code>dataset</code>：表示数据集的名称，可以是 “NYU” 或 “kitti”。</li><li><code>project_scale</code>：表示投影的尺度，通常是 2 的幂次方，用于将 2D 特征图中的像素投影到 3D 空间。</li></ul></li><li><p><code>self.scene_size = scene_size</code>：将输入的 <code>scene_size</code> 存储为对象属性，以便在模块的其他方法中使用。</p></li><li><p><code>self.dataset = dataset</code>：将输入的 <code>dataset</code> 存储为对象属性，以便在模块的其他方法中使用。</p></li><li><p><code>self.project_scale = project_scale</code>：将输入的 <code>project_scale</code> 存储为对象属性，以便在模块的其他方法中使用。</p></li><li><p><code>def forward(self, x2d, projected_pix, fov_mask):</code>：前向传播方法，用于执行特征投影操作。接受三个输入参数：</p><ul><li><code>x2d</code>：2D 特征图，是一个张量，其形状为 <code>(c, h, w)</code>，其中 <code>c</code> 表示通道数，<code>h</code> 和 <code>w</code> 表示高度和宽度。</li><li><code>projected_pix</code>：包含 2D 像素坐标的张量，其形状为 <code>(bs, 2)</code>，其中 <code>bs</code> 表示批处理大小。</li><li><code>fov_mask</code>：表示感兴趣区域的遮罩，是一个布尔值张量，形状与 <code>projected_pix</code> 相同。</li></ul></li><li><p><code>c, h, w = x2d.shape</code>：获取输入 2D 特征图 <code>x2d</code> 的通道数、高度和宽度。</p></li><li><p><code>src = x2d.view(c, -1)</code>：将 2D 特征图 <code>x2d</code> 重新形状为 <code>(c, h * w)</code>，即将每个像素的特征连接到一个向量中。</p></li><li><p><code>zeros_vec = torch.zeros(c, 1).type_as(src)</code>：创建一个与 <code>src</code> 相同数据类型的零向量，用于在特征向量后添加一个零值，以匹配 3D 坐标。</p></li><li><p><code>src = torch.cat([src, zeros_vec], 1)</code>：将零向量添加到特征向量的末尾，以将特征向量的维度从 <code>(c, h * w)</code> 扩展到 <code>(c, h * w + 1)</code>。</p></li><li><p><code>pix_x, pix_y = projected_pix[:, 0], projected_pix[:, 1]</code>：从 <code>projected_pix</code> 中提取 2D 像素坐标的 x 和 y 值。</p></li><li><p><code>img_indices = pix_y * w + pix_x</code>：计算像素坐标对应的在扁平化 2D 特征图中的索引。</p></li><li><p><code>img_indices[~fov_mask] = h * w</code>：将不在感兴趣区域内的像素坐标对应的索引设置为 2D 特征图的最大索引值（类似于超出图像范围的像素索引）。</p></li><li><p><code>img_indices = img_indices.expand(c, -1).long()</code>：将计算得到的索引扩展为 <code>(c, h * w)</code> 的长整型张量。</p></li><li><p><code>src_feature = torch.gather(src, 1, img_indices)</code>：使用计算得到的索引从 <code>src</code> 中聚合出感兴趣的像素特征。</p></li><li><p><code>if self.dataset == &quot;NYU&quot;:</code>：如果数据集是 “NYU”，则执行以下操作。否则，执行 “kitti” 分支。</p><p>a. <code>x3d = src_feature.reshape(c, self.scene_size[0] // self.project_scale, self.scene_size[2] // self.project_scale, self.scene_size[1] // self.project_scale)</code>：将特征投影到 3D 空间，根据输入的 <code>scene_size</code> 和 <code>project_scale</code> 进行重新形状。这一步是根据 NYU 数据集的特定情况进行的形状转换。</p><p>b. <code>x3d = x3d.permute(0, 1, 3, 2)</code>：对投影得到的 3D 特征进行维度的置换。</p></li><li><p><code>elif self.dataset == &quot;kitti&quot;:</code>：如果数据集是 “kitti”，则执行以下操作。</p><p>a. <code>x3d = src_feature.reshape(c, self.scene_size[0] // self.project_scale, self.scene_size[1] // self.project_scale, self.scene_size[2] // self.project_scale)</code>：将特征投影到 3D 空间，根据输入的 <code>scene_size</code> 和 <code>project_scale</code> 进行重新形状。这一步是根据 kitti 数据集的特定情况进行的形状转换。</p></li><li><p><code>return x3d</code>：返回投影到 3D 空间的特征张量 <code>x3d</code>。</p></li></ol><p>这个模块的主要功能是将 2D 特征图中的像素特征投影到 3D 空间中，以便后续在 3D 空间中进行深度学习任务。具体的投影方法和维度变换取决于数据集的类型（”NYU” 或 “kitti”）和投影尺度（<code>project_scale</code>）。这个模块通常用于处理语义分割和深度估计等任务。</p><p>unet3d_kitti.py是一个3D U-Net模型的定义，用于语义分割任务。以下是代码的逐行解释：</p><ol><li><p><code>UNet3D</code> 类继承自 <code>nn.Module</code>，用于定义3D U-Net模型。</p></li><li><p><code>__init__</code> 函数用于初始化模型，它接受一些参数，如类别数 <code>class_num</code>、标准化层 <code>norm_layer</code>、全尺寸 <code>full_scene_size</code>、特征数 <code>feature</code>、项目尺度 <code>project_scale</code>、上下文先验 <code>context_prior</code> 和 Batch Normalization 层的动量 <code>bn_momentum</code>。</p></li><li><p><code>size_l1</code>、<code>size_l2</code> 和 <code>size_l3</code> 分别表示3D U-Net的3个不同尺寸层。</p></li><li><p><code>dilations</code> 是用于处理图像的卷积核的膨胀率。</p></li><li><p><code>self.process_l1</code> 和 <code>self.process_l2</code> 定义了两个处理层，用于处理输入3D数据，包括一系列的卷积、标准化和下采样操作。</p></li><li><p><code>self.up_13_l2</code>、<code>self.up_12_l1</code> 和 <code>self.up_l1_lfull</code> 定义了上采样层，将3D数据上采样到不同尺寸的层。</p></li><li><p><code>self.ssc_head</code> 定义了用于预测语义分割的头部，包括一系列卷积和输出层。</p></li><li><p><code>self.context_prior</code> 用于确定是否使用上下文先验。如果 <code>context_prior</code> 为真，将定义 <code>self.CP_mega_voxels</code>，它是上下文先验处理的一部分。</p></li><li><p><code>forward</code> 函数接受输入数据的字典 <code>input_dict</code>，包括 <code>x3d</code>，表示3D数据。</p></li><li><p>通过一系列处理步骤，如 <code>self.process_l1</code> 和 <code>self.process_l2</code>，输入数据被处理和下采样到不同尺寸层。</p></li><li><p>如果存在上下文先验，将使用 <code>self.CP_mega_voxels</code> 处理数据。</p></li><li><p>使用上采样层 <code>self.up_13_l2</code> 和 <code>self.up_12_l1</code> 将数据上采样到不同尺寸的层。</p></li><li><p>最终，通过 <code>self.ssc_head</code> 进行语义分割预测，并将结果存储在 <code>res</code> 字典中。</p></li></ol><p>这个模型的核心部分是3D U-Net结构，它包含编码器和解码器部分，用于从3D输入数据中提取特征并生成语义分割结果。根据输入数据的尺寸和具体任务，该模型可以适应不同的上下文先验处理。</p><h2 id="StereoScene"><a href="#StereoScene" class="headerlink" title="StereoScene"></a>StereoScene</h2><p>StereoScene用于3D语义场景补全（SSC），它充分利用了轻量级相机输入而无需使用任何外部3D传感器。研究者的关键想法是利用立体匹配来解决几何模糊性问题。为了提高其在不匹配区域的鲁棒性，论文引入了鸟瞰图（BEV）表示法，以激发具有丰富上下文信息的未知区域预测能力。在立体匹配和BEV表示法的基础上，论文精心设计了一个相互交互聚合（MIA）模块，以充分发挥两者的作用，促进它们互补聚合</p><p>代码链接：<a href="https://github.com/Arlo0o/StereoScene">StereoScene: BEV-Assisted Stereo Matching Empowers 3D Semantic Scene Completion</a>, arXiv 2023.</p><p><img src="/pic/StereoScene.png"></p><p>整体的StereoScene框架如上图所示。论文遵循使用连续的2D和3 UNetsf作为backbones。给定输入的双目图像，我们使用2D UNet来提取多尺度特征。BEV潜在体积和立体几何体积分别由BEV构造器和立体构造器构造。为了充分利用这两个卷的互补潜力，提出了一个相互交互聚合模块来相互引导和聚合它们。</p><h3 id="代码解释"><a href="#代码解释" class="headerlink" title="代码解释"></a>代码解释</h3><p><a href="https://github.com/Arlo0o/StereoScene/blob/main/projects/configs/occupancy/semantickitti/stereoscene.py">stereoscene.py</a>代码是一个配置文件，它定义了一个基于 MMDetection3D 的三维计算机视觉模型的配置。以下是代码中各部分的解释：</p><ol><li><p><code>_base_</code>：这是导入配置的基础文件的地方，<code>custom_nus-3d.py</code> 和 <code>default_runtime.py</code> 包含了一些共享的配置选项。这些基础配置文件通常包括数据集设置、运行时设置等。</p></li><li><p><code>plugin</code> 和 <code>plugin_dir</code>：这两个参数用于启用和指定 MMDetection3D 插件。插件是额外的功能模块，可以扩展 MMDetection3D 的功能。</p></li><li><p><code>img_norm_cfg</code>：这是图像归一化配置，指定了均值和标准差，以便对图像进行归一化。这通常用于图像预处理。</p></li><li><p><code>class_names</code>：定义了数据集中的类别名称。在这个例子中，有 20 个类别，包括车辆、行人、道路、建筑物等。</p></li><li><p><code>point_cloud_range</code>、<code>occ_size</code> 和 <code>lss_downsample</code>：这些参数定义了点云数据的范围、体素的大小和降采样率。它们是用于处理点云数据的重要参数。</p></li><li><p><code>model</code>：这是定义模型的部分。它指定了模型的各个组件，包括图像骨干网络、头部网络、点云处理头部等。这里使用的是 <code>BEVDepthOccupancy</code> 模型，该模型在三维计算机视觉中执行深度估计和语义分割任务。</p></li><li><p><code>dataset_type</code> 和 <code>data_root</code>：这些参数定义了数据集的类型和数据集的根目录。在这个例子中，使用的是 <code>CustomSemanticKITTILssDataset</code> 数据集，数据位于 <code>./data/occupancy/semanticKITTI/RGB/</code>。</p></li><li><p><code>train_pipeline</code> 和 <code>test_pipeline</code>：这些参数定义了数据预处理和增强的步骤。它们包括图像加载、语义分割标签加载、深度图生成等。</p></li><li><p><code>input_modality</code>：定义了输入的模态，包括激光雷达、相机、雷达、地图等。在这个例子中，仅使用相机数据。</p></li><li><p><code>test_config</code> 和 <code>data</code>：这些参数定义了训练和测试数据加载的配置，包括数据预处理、数据集类型、类别、点云范围等。</p></li><li><p><code>optimizer</code> 和 <code>optimizer_config</code>：这些参数定义了优化器的类型、学习率和权重衰减等配置。</p></li><li><p><code>lr_config</code>：定义了学习率调度策略，这里使用的是阶段性学习率下降。</p></li><li><p><code>checkpoint_config</code>：用于定义模型检查点的保存和保留策略。</p></li><li><p><code>runner</code>：指定了训练的运行方式，这里使用的是按照 epoch 训练。</p></li><li><p><code>evaluation</code>：定义了模型评估的配置，包括评估间隔和保存最佳模型的规则。</p></li></ol><p>这个配置文件的目的是定义了一个用于深度估计和语义分割的三维计算机视觉模型，并指定了数据加载和训练配置。模型的结构和数据集的具体细节在其他文件中定义。这个配置文件会被传递给 MMDetection3D 的训练和测试脚本，以实际执行训练和测试任务。</p><p>这个配置文件中定义了一个名为<code>model</code>的模型，其结构和配置包括以下几个主要组件：</p><ol><li><p><code>type</code>: 这里指定了模型的类型为<code>BEVDepthOccupancy</code>，这是一个自定义的三维计算机视觉模型，用于深度估计和语义分割任务。</p></li><li><p><code>img_backbone</code>: 定义了图像骨干网络的配置，该网络用于提取图像特征。在这个配置中，使用了<code>CustomEfficientNet</code>骨干网络，具体配置包括网络的类型、预训练模型的路径等。</p></li><li><p><code>img_neck</code>: 这部分定义了图像特征的“颈部”或附加处理，用于进一步处理骨干网络提取的特征。这里使用了<code>SECONDFPN</code>结构，对特征进行了上采样和融合。</p></li><li><p><code>img_view_transformer</code>: 这是一个自定义的视图变换器，用于将图像特征映射到点云视图，包括视角的转换和体素化。</p></li><li><p><code>img_bev_encoder_backbone</code> 和 <code>img_bev_encoder_neck</code>: 定义了用于处理点云的背景信息的网络。<code>img_bev_encoder_backbone</code>用于提取点云的背景特征，<code>img_bev_encoder_neck</code>用于进一步处理这些特征。</p></li><li><p><code>pts_bbox_head</code>: 这是点云处理头部的配置，用于执行深度估计和语义分割任务。它包括输出通道数、语义分割类别数、点云范围等配置。</p></li><li><p><code>train_cfg</code> 和 <code>test_cfg</code>: 分别定义了模型的训练和测试配置，这些配置包括损失函数、评价指标等。</p></li></ol><p>这个<code>model</code>定义了整个三维计算机视觉模型的结构，包括图像骨干网络、点云处理头部等组件。模型将通过这些组件来提取和处理图像和点云数据，以执行深度估计和语义分割任务。配置文件中还包括数据加载和训练策略等，用于训练和测试这个模型。</p><p>这个<code>model</code>配置是在使用MMDetection框架时的一种标准格式。MMDetection使用Python的字典格式来组织模型的配置。下面是对这个<code>model</code>配置的格式解释：</p><ol><li><p><code>type</code>: 这个字段指定了要使用的模型的类型。在这个配置中，<code>type</code>的值是<code>BEVDepthOccupancy</code>，表示要使用名为<code>BEVDepthOccupancy</code>的自定义模型。</p></li><li><p><code>img_backbone</code>, <code>img_neck</code>, <code>img_view_transformer</code>, <code>img_bev_encoder_backbone</code>, <code>img_bev_encoder_neck</code>, <code>pts_bbox_head</code>: 这些字段用于配置模型中不同组件的设置。每个组件的配置包含了该组件的类型、参数、超参数等。</p></li><li><p><code>train_cfg</code> 和 <code>test_cfg</code>: 这些字段定义了模型的训练和测试配置，包括损失函数、评价指标等。这些配置用于指导训练和测试过程。</p></li></ol><p>整个<code>model</code>配置是一个嵌套的字典，用于描述模型的结构和超参数设置。在MMDetection框架中，模型的配置采用了类似于YAML格式的字典结构，以便灵活配置不同的模型和任务。这种配置方式使得用户可以根据自己的需求轻松地定制和修改模型的设置。</p><p><a href="https://github.com/Arlo0o/StereoScene/blob/main/projects/mmdet3d_plugin/occupancy/detectors/bevdepth_occupancy.py">bevdepth_occupancy.py</a>在上述代码中，<code>model</code> 的配置与类 <code>BEVDepthOccupancy</code> 相关，特别是与 <code>BEVDepthOccupancy</code> 类中的不同函数和组件有关。</p><p><code>BEVDepthOccupancy</code> 类是一个自定义的模型类，它继承自 <code>BEVDepth</code> 类。以下是与 <code>model</code> 配置中的组件相关的一些重要功能和配置项：</p><ol><li><p><code>image_encoder</code> 和 <code>bev_encoder</code> 函数：这些函数是模型的核心组件，分别用于处理图像信息和点云信息。<code>image_encoder</code> 用于处理输入的图像信息，而 <code>bev_encoder</code> 用于处理点云信息。这些函数将输入数据转换为特征表示。</p></li><li><p><code>forward_pts_train</code> 函数：这个函数在模型的训练过程中使用，计算了与点云相关的训练损失。这包括点云的目标检测和语义分割任务。</p></li><li><p><code>extract_img_feat</code> 函数：这个函数用于从输入的图像中提取特征。它将输入的图像分别送入 <code>image_encoder</code> 和 <code>img_view_transformer</code> 中，然后将它们的输出特征合并。</p></li><li><p><code>simple_test</code> 函数：这个函数用于在测试阶段生成模型的输出。它调用了 <code>extract_feat</code> 函数来提取特征，然后将这些特征传递给点云检测头部 <code>pts_bbox_head</code>，最后生成了测试结果。</p></li><li><p>其他与损失函数、数据预处理、评估等相关的函数：代码中还包括了与训练和测试相关的其他函数，用于处理损失函数的计算、数据的预处理以及评估模型性能等任务。</p></li></ol><p>总的来说，<code>model</code> 配置中的组件是 <code>BEVDepthOccupancy</code> 类中的一部分，用于定义模型的结构和训练&#x2F;测试过程中的操作。这些组件协同工作以实现点云目标检测和语义分割的任务。不同的组件负责处理不同类型的输入数据（图像、点云等）并生成相应的特征表示，最终用于模型的训练和测试。这种模块化的设计使得模型可以方便地扩展和配置，以适应不同的应用场景。</p><p><a href="https://github.com/Arlo0o/StereoScene/blob/main/projects/mmdet3d_plugin/occupancy/dense_heads/occhead.py">occhead.py</a>这是一个名为<code>OccHead</code>的模型头，主要用于3D语义分割任务。以下是对该头部的关键要点：</p><ol><li><p><strong>输入和输出</strong>:</p><ul><li>输入：<code>OccHead</code>头部的输入包括体素特征（<code>voxel_feats</code>）以及点云信息（<code>points</code>）。</li><li>输出：它的输出通常有两部分，一部分是用于体素级别的语义分割（<code>output_voxels</code>），另一部分是用于点云级别的语义分割（<code>output_points</code>）。</li></ul></li><li><p><strong>监督方式</strong>:</p><ul><li>该头部支持两种不同级别的监督方式：体素级别和点云级别。</li><li>体素级别监督是指根据体素特征生成语义分割结果，用于离散的体素数据。</li><li>点云级别监督是指根据点云信息生成语义分割结果，用于稠密点云数据。</li></ul></li><li><p><strong>损失函数</strong>:</p><ul><li>对于体素级别监督，支持交叉熵（<code>voxel_ce</code>）、Lovasz-Softmax（<code>voxel_lovasz</code>）、Voxel Semantic Scaling（<code>voxel_sem_scal</code>）、Voxel Geometric Scaling（<code>voxel_geo_scal</code>）、IoU损失（<code>voxel_dice</code>）等损失函数。</li><li>对于点云级别监督，支持交叉熵（<code>point_ce</code>）和Lovasz-Softmax（<code>point_lovasz</code>）损失函数。</li></ul></li><li><p><strong>学习策略</strong>:</p><ul><li>对于体素级别监督，通过卷积操作生成语义分割结果。</li><li>对于点云级别监督，采样相关的体素特征和图像特征，然后使用多层感知机（Mlp）处理这些特征生成点云级别的语义分割结果。</li></ul></li><li><p><strong>语义Kitti</strong>:</p><ul><li>该模型头部支持语义Kitti数据集，并提供了相关的损失函数和度量指标。</li></ul></li><li><p><strong>损失权重</strong>:</p><ul><li>您可以设置不同损失函数的权重，以控制它们对最终损失的贡献。</li></ul></li><li><p><strong>可选功能</strong>:</p><ul><li>该模型头部支持不同的可选功能，如软权重（<code>soft_weights</code>）和图像特征采样（<code>sampling_img_feats</code>）。</li></ul></li><li><p><strong>其他</strong>:</p><ul><li>还包括一些图像处理操作，如上采样和网格采样。</li></ul></li></ol><p>总之，<code>OccHead</code>是一个用于语义分割任务的多功能头部，支持体素级别和点云级别的监督，提供了多种损失函数和度量指标以用于不同的数据集和任务。这些选择和配置可以根据具体的任务需求进行调整和优化。</p>]]></content>
      
      
      <categories>
          
          <category> DL </category>
          
      </categories>
      
      
        <tags>
            
            <tag> ssc </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>语义场景补全（SSC）</title>
      <link href="/2023/10/29/ssc/"/>
      <url>/2023/10/29/ssc/</url>
      
        <content type="html"><![CDATA[<h1 id="语义场景补全（SSC）相关文章与数据集总结"><a href="#语义场景补全（SSC）相关文章与数据集总结" class="headerlink" title="语义场景补全（SSC）相关文章与数据集总结"></a>语义场景补全（SSC）相关文章与数据集总结</h1><p><em><strong>首先附上链接</strong></em></p><h2 id="SSC"><a href="#SSC" class="headerlink" title="SSC"></a>SSC</h2><ul><li><a href="https://github.com/Jiawei-Yao0812/NDCScene">NDC-Scene: Boost Monocular 3D Semantic Scene Completion in Normalized Device Coordinates Space</a>, ICCV 2023.</li><li><a href="https://arxiv.org/abs/2307.05873">OG: Equip vision occupancy with instance segmentation and visual grounding</a>, arXiv 2023.</li><li><a href="https://github.com/NVlabs/FB-BEV">FB-OCC: 3D Occupancy Prediction based on Forward-Backward View Transformation</a>, CVPRW 2023.</li><li><a href="https://github.com/hustvl/Symphonies">Symphonize 3D Semantic Scene Completion with Contextual Instance Queries</a>, arXiv 2023.</li><li><a href="https://arxiv.org/pdf/2305.16133.pdf">OVO: Open-Vocabulary Occupancy</a>, arXiv 2023.</li><li><a href="https://github.com/opendrivelab/occnet">OccNet: Scene as Occupancy</a>, ICCV 2023.</li><li><a href="https://astra-vision.github.io/SceneRF/">SceneRF: Self-Supervised Monocular 3D Scene Reconstruction with Radiance Fields</a>, ICCV 2023.</li><li><a href="https://fwmb.github.io/bts/">Behind the Scenes: Density Fields for Single View Reconstruction</a>, CVPR 2023.</li><li><a href="https://github.com/shurans/sscnet">Semantic Scene Completion from a Single Depth Image</a>, CVPR 2017</li><li><a href="https://github.com/astra-vision/LMSCNet">LMSCNet: Lightweight Multiscale 3D Semantic Completion</a>, 3DV 2020</li><li><a href="https://github.com/astra-vision/MonoScene">MonoScene: Monocular 3D Semantic Scene Completion</a>, CVPR 2022</li><li><a href="https://github.com/Arlo0o/StereoScene">StereoScene: BEV-Assisted Stereo Matching Empowers 3D Semantic Scene Completion</a>, arXiv 2023.</li><li><a href="https://github.com/megvii-research/OccDepth">OccDepth: A Depth-aware Method for 3D Semantic Occupancy Network</a>, arXiv 2023.</li><li><a href="https://github.com/NVlabs/VoxFormer">VoxFormer: a Cutting-edge Baseline for 3D Semantic Occupancy Prediction</a>, CVPR 2023</li><li><a href="https://github.com/wzzheng/TPVFormer">TPVFormer: An academic alternative to Tesla’s Occupancy Network</a>, CVPR2023</li><li><a href="https://github.com/zhangyp15/OccFormer">OccFormer: Dual-path Transformer for Vision-based 3D Semantic Occupancy Prediction</a>, ICCV 2023</li><li><a href="https://github.com/weiyithu/SurroundOcc">SurroundOcc: Multi-Camera 3D Occupancy Prediction for Autonomous Driving</a>, ICCV 2023</li><li><a href="https://ahayler.github.io/publications/s4c/">S4C: Self-Supervised Semantic Scene Completion with Neural Fields</a>, arXiv 2023</li><li><a href="https://arxiv.org/abs/2306.10013">PanoOcc: Unified Occupancy Representation for Camera-based 3D Panoptic Segmentation</a>, arXiv 2023.</li><li><a href="https://github.com/wzzheng/PointOcc">PointOcc: Cylindrical Tri-Perspective View for Point-based 3D Semantic Occupancy Prediction</a>, arXiv 2023.</li><li><a href="https://arxiv.org/abs/2309.09502">RenderOcc: Vision-Centric 3D Occupancy Prediction with 2D Rendering Supervision</a>, arXiv 2023.</li></ul><h2 id="相关-Dataset-x2F-Benchmark"><a href="#相关-Dataset-x2F-Benchmark" class="headerlink" title="相关 Dataset&#x2F;Benchmark"></a>相关 Dataset&#x2F;Benchmark</h2><ul><li><a href="https://arxiv.org/abs/2309.12708">PointSSC: A Cooperative Vehicle-Infrastructure Point Cloud Benchmark for Semantic Scene Completion</a>, arXiv 2023.</li><li><a href="https://github.com/Tsinghua-MARS-Lab/Occ3D">Occ3D: A Large-Scale 3D Occupancy Prediction Benchmark for Autonomous Driving</a>, arXiv 2023</li><li><a href="https://github.com/JeffWang987/OpenOccupancy">OpenOccupancy: A Large Scale Benchmark for Surrounding Semantic Occupancy Perception</a>, ICCV 2023</li><li><a href="https://github.com/ai4ce/Occ4cast/">Occ4cast: LiDAR-based 4D Occupancy Completion and Forecasting</a>, arXiv 2023.</li><li><a href="https://github.com/opendrivelab/occnet">OccNet: Scene as Occupancy</a>, ICCV 2023.</li><li><a href="https://github.com/ai4ce/SSCBench">SSCBench: A Large-Scale 3D Semantic Scene Completion Benchmark for Autonomous Driving</a>, arXiv 2023.</li></ul><p><em><strong>我们从SSCBench数据集开始介绍起</strong></em></p><h2 id="SSCBench-A-Large-Scale-3D-Semantic-Scene-Completion-Benchmark-for-Autonomous-Driving"><a href="#SSCBench-A-Large-Scale-3D-Semantic-Scene-Completion-Benchmark-for-Autonomous-Driving" class="headerlink" title="SSCBench: A Large-Scale 3D Semantic Scene Completion Benchmark for Autonomous Driving"></a>SSCBench: A Large-Scale 3D Semantic Scene Completion Benchmark for Autonomous Driving</h2><p>SSCBench提供的数据集格式与SemanticKITTI兼容，这是一个综合了 KITTI-360 、nuScenes和Waymo 中的场景的全面基准，总体而言，我们的SSCBench由三个子集组成，包括38562帧用于训练，15798帧用于验证，12553帧用于测试，总计66913帧（～67K），大大超过了上述SemanticKITTI的规模～7.7倍。SSCBench 便于在各种实际场景中轻松探索基于摄像头和LiDAR的SSC。</p><h3 id="相关工作"><a href="#相关工作" class="headerlink" title="相关工作"></a>相关工作</h3><ul><li><strong>单眼感知和3D语义场景完成</strong>。单眼相机的简单性、效率、可负担性和可访问性使单眼感知成为视觉和机器人界关注的焦点。SSCNet（2017）引入了单目3D语义场景完成（SSC）的概念，该概念旨在从单个深度图像重建和完成3D体积内的语义和几何结构。然而，由于缺乏室外数据集，他们只考虑有界的室内场景。Semantickitti（2019）构建了第一个基于KITTI的户外数据集，用于街景中的3D语义场景完成。现有的方法通常依赖于3D输入，如激光雷达点云（Lmscnet，2020；S3CNet，2021a；JS3C-Net，2021），而最近的基于单目视觉的解决方案也出现了（Monoscene，2022；Voxformer，2023），双目视觉的解决方案也出现了（OccDepth，2021）。然而，户外SSC的发展受到数据集缺乏的阻碍，SemanticKITTI（Behley et al.，2019）是唯一支持街景SSC的数据集。构建多样化的数据集对于释放SSC在自主系统中的全部潜力至关重要。</li><li><strong>街景中的点云分割</strong>。3D激光雷达分割旨在为点云分配逐点语义标签，在这一领域，源于PointNet++（2017）的基于点的方法在小型合成点云上表现良好。基于体素的方法通过最初通过笛卡尔坐标将3D空间划分为体素来处理点云。注意，3D激光雷达分割旨在基于原始激光雷达扫描对场景进行分类和理解，而3D语义场景完成包括在相机或激光雷达的输入下完成遮挡区域。</li><li><strong>自动驾驶数据集和基准</strong>。自动驾驶研究在高质量数据集上蓬勃发展，这些数据集是训练和评估感知、预测和规划算法的生命线。2012年，开创性的KITTI数据集引发了自动驾驶研究的一场革命，开启了包括物体检测、跟踪、映射和光学&#x2F;深度估计在内的多项任务。从那时起，研究界接受了这一挑战，产生了丰富的数据集。这些数据集通过应对多模式融合、多任务学习、恶劣天气、协同驾驶、重复驾驶，以及密集交通场景等。有几个有影响力且广泛使用的驾驶数据集，如KITTI-360（2022）、nuScenes（2020）和Waymo（2017）。它们提供了激光雷达和相机记录以及点云语义和边界注释，因此，我们可以通过聚合多个语义点云并利用3D框来处理动态对象，为SSC创建准确的地面实况标签。</li></ul><h3 id="介绍SSCNet"><a href="#介绍SSCNet" class="headerlink" title="介绍SSCNet"></a>介绍SSCNet</h3><p>本文的重点是语义场景补全，这是一个任务，从单视图深度地图观察生成一个完整的三维体素表示的体积占用和语义标签的场景。之前的工作分别考虑了场景补全和深度地图的语义标记。然而，我们注意到这两个问题是紧密联系在一起的。为了利用这两个任务的耦合特性，我们引入了语义场景完成网络(SSCNet)，<strong>这是一个端到端3D卷积网络，以单个深度图像作为输入，并同时输出相机视图截锥中所有体素的占用率和语义标签</strong>。我们的网络使用了一个基于扩张的3D上下文模块，以有效地扩展接受域，使3D上下文学习成为可能。为了训练我们的网络，我们构建了SUNCG——一个人工创建的大规模合成3D场景数据集，包含密集的体积标注。我们的实验表明，联合模型在语义场景完成任务方面优于单独处理每个任务的方法和替代方法。数据集、代码和经过训练的模型将在接受后在线提供。</p><p><img src="/pic/SSCNet.png"></p><p>语义场景补全网络（SSCNet），SSCNet [36] 是第一个将语义分割和场景完成与 3D CNN 端到端结合的工作。这是一种端到端的3D卷积网络，它以单个深度图像为输入，同时输出相机视图截头体中所有<em>体素的占用和语义标签</em>。网络使用基于扩张的3D上下文模块来有效地扩展感受野并实现3D上下文学习。</p><h5 id="数据编码（第3-1节）、网络架构（第3-2节）和训练数据生成（第4节）。"><a href="#数据编码（第3-1节）、网络架构（第3-2节）和训练数据生成（第4节）。" class="headerlink" title="数据编码（第3.1节）、网络架构（第3.2节）和训练数据生成（第4节）。"></a>数据编码（第3.1节）、网络架构（第3.2节）和训练数据生成（第4节）。</h5><ul><li><strong>数据编码</strong>。采用截断有符号距离函数（TSDF）对三维空间进行编码，其中每个体素都存储到其最近表面的距离值d，该值的符号表示体素是在自由空间中还是在遮挡空间中。对标准TSDF进行了以下修改：（1）消除视图相关性：选择计算到整个观测表面上任何地方最近点的距离。（2）消除空位中的强梯度,使用flipped TSDF(翻转的TSDF)</li><li><strong>网络架构</strong>。网络以高分辨率三维体积为输入，首先使用几个三维卷积层来学习局部几何表示。我们使用带有步长和池化层的卷积层，将分辨率降低到原始输入的四分之一。然后，我们使用基于膨胀的3D上下文模块来捕获更高级别的对象间上下文信息。之后，来自不同尺度的网络响应被连接并馈送到另外两个卷积层中，以聚合来自多个尺度的信息。最后，使用体素方向的softmax层来预测最终的体素标签。</li><li><strong>训练数据生成</strong>。SUNCG：一个大型合成场景数据集。SUNCG数据集包含45622个不同的场景，这些场景具有通过Planner5D平台手动创建的逼真的房间和家具布局[25]。有49884个有效楼层，包含404058个房间和5697217个对象实例，这些实例来自2644个覆盖84个类别的唯一对象网格。为了生成模拟典型图像捕获过程的合成深度图，我们使用一组简单的启发式方法来拾取相机视点。给定一个3D场景，我们从地板上间隔1米且不被物体占据的位置的统一网格开始。然后，我们根据NYU Depth v2数据集的分布选择相机姿势。1然后，我们使用Kinect的内部特性和分辨率渲染深度图。之后，我们使用一组简单的启发式方法来排除不好的观点。SUNCG数据集中的3D场景由有限数量的对象实例组成，我们通过首先对库中的每个单独对象进行体素化，然后根据每个场景配置和视点变换标签来加快体素化过程</li></ul><p><img src="/pic/SSCNet2.png"></p><p>上图显示<strong>表面(a)的不同编码</strong>。投影TSDF (b)是根据相机计算的，因此是视景相关的。准确的TSDF (c)具有较少的视图依赖性，但在沿着遮挡边界的空空间中显示出强烈的梯度(用灰色圈出)。相反，翻转TSDF (d)在近地表有最强的梯度。</p><p><img src="/pic/SSCNet2.png"></p><p>上图显示<strong>合成训练数据</strong>。我们收集了一个大规模的合成三维场景数据集来训练我们的网络。对于每个3D场景，我们选择一组摄像机位置，并生成成对的渲染深度图像和体积地面真实作为训练示例。</p><h3 id="介绍LMSCNet"><a href="#介绍LMSCNet" class="headerlink" title="介绍LMSCNet"></a>介绍LMSCNet</h3><p>一种从体素化稀疏三维激光雷达扫描中完成多尺度三维语义场景的新方法。与文献相反，我们使用具有全面多尺度跳跃连接的2D UNet主干来增强特征流，以及3D分割头。在SemanticKITTI基准测试中，与所有其他已发布的方法相比，我们的方法在语义完成方面表现相当，在占用完成方面表现更好，同时明显更轻、更快。</p><h4 id="方法"><a href="#方法" class="headerlink" title="方法"></a>方法</h4><p>我们解决密集 3D 语义完成的问题，其中的任务是为每个单独的体素分配语义标签。给定稀疏 3D 体素网格，目标是预测 3D 语义场景表示，其中每个体素被分配一个语义标签 C &#x3D; [c0, c1, … 。 。 , cN]，其中 N 是语义类别的数量，c0 代表自由体素。我们的架构称为 LMSCNet，如下图所示，使用轻量级 UNet 风格架构来预测多个尺度的 3D 语义完成，允许快速粗略推理，有利于移动机器人应用。我们主要沿高度轴使用 2D 卷积，而不是贪婪的 3D 卷积；类似于鸟瞰图。下面我们详细介绍我们的定制轻量级 2D&#x2F;3D 架构、多尺度重建和整体训练流程</p><p><img src="/pic/LMSCNet.png" alt="&quot;LMSCNet：轻量级多尺度语义完成网络。我们的管道使用具有 2D 主干卷积（蓝色）和 3D 分割头（灰色）的 UNet 架构来执行不同尺度的 3D 语义分割和完成，同时保持低复杂性。卷积参数显示为：（滤波器数量、内核大小和步幅）。请注意，我们有意降低 2D 特征维度并使用 Atrous 3D 卷积（来自 [26] 的 ASPP 块）来保持较低的推理复杂度。&quot;"></p><h3 id="介绍S3CNet"><a href="#介绍S3CNet" class="headerlink" title="介绍S3CNet"></a>介绍S3CNet</h3><p>主要贡献如下：（a）一种基于稀疏张量的神经网络架构，该架构能够有效地从稀疏的三维点云数据中学习特征，并联合解决耦合的场景完成和语义分割问题；（b） 一种新颖的几何感知三维张量分割损失；（c） 一种多视图融合和语义后处理策略，解决了远距离或遮挡区域和小尺寸对象的挑战。给定单个稀疏点云帧，我们的模型预测了一个密集的3D占用长方体，其中为每个体素单元分配了语义标签（如图1所示），生成了原始输入中不包含的3D环境的丰富信息，如激光雷达扫描之间的间隙、遮挡区域和未来场景。</p><p>利用逐点法向量作为几何特征编码来指导我们的模型根据对象的局部曲面凸性来填充间隙。我们还利用从球面范围图像计算的基于LiDAR的翻转截断有符号距离函数（fTSDF[5]）作为空间编码，来区分场景的自由空间、占用空间和遮挡空间。至于未来的场景，由于这些区域远离车辆，主要是道路或其他形式的地形，我们提出了稀疏语义场景完成网络的2D变体，以支持通过与鸟瞰图（BEV）语义图预测的多视图融合来构建3D场景。为了解决稀疏性，我们利用Minkowski引擎[6]，一个用于稀疏张量的自动微分库来构建我们的2D和3D语义场景完成网络。我们还采用了组合的几何启发语义分割损失来提高语义标签预测的准确性。</p><p><img src="/pic/S3CNet.png"><br>整个系统管道如图所示。从一次激光雷达扫描中，我们构建了两个稀疏张量，将场景封装为内存高效的2D和3D表示。每个张量通过其对应的语义场景完成网络，2D S3CNet或3D S3CNet，以在相应维度上语义地完成场景。我们提出了一种动态体素融合方法，用预测的2D语义BEV图进一步加密重建场景（详细讨论见第3.3节）。这抵消了对3D网络的显著内存需求——3D空间中的指数稀疏性增长使在一定范围内完成类变得困难。使用稀疏张量空间传播网络[30]，我们细化融合2D-3D预测的噪声区域中的语义标签。</p><h3 id="介绍JS3C-Net"><a href="#介绍JS3C-Net" class="headerlink" title="介绍JS3C-Net"></a>介绍JS3C-Net</h3><p>提出了一种增强的联合单扫LiDAR点云语义分割方法，该方法利用学习的形状先验形式场景完成网络，即JS3C-Net。具体来说，通过在LiDAR序列中合并数十个连续帧，在没有额外标注的情况下，实现了一个大的完整点云作为语义场景完成(SSC)任务的地面真相。利用这些标注优化的SSC可以捕获引人注意的形状先验，使不完整的输入完整到带有语义标签的可接受形状(Song et al. 2017)。因此，完全端到端的训练策略使得完成的形状先验在本质上有利于语义分割(SS)。进一步提出了一个设计良好的点体素交互(PVI)模块，用于SS和SSC任务之间的隐式相互知识融合。具体来说，通过PVI模块，利用逐点分割和逐体补全来维护粗全局结构和细粒度局部几何。更重要的是，我们设计的SSC和PVI模块是一次性的。为了实现这一点，JS3C-Net以级联的方式将SS和SSC结合在一起，这意味着它不会影响SS的信息流，同时在推理阶段丢弃SSC和PVI模块。因此，它可以避免生成完整的高分辨率密集体而带来额外的计算负担。</p><p><img src="/pic/JS3CNet.png"></p><p>JS3C Net的总体管道。在给定稀疏不完全单扫描点云的情况下，首先使用稀疏卷积U-Net对Fenc进行点特征编码。基于初始编码，MLP1用于生成形状嵌入（SE）FSE，该FSE与通过MLP2传递的初始编码一起流入MLP3，以生成用于点云语义分割的Fout。然后，来自SE的不完整细粒度点特征和来自语义场景完成（SSC）模块的完整体素特征流入点-体素交互（PVI）模块，以实现细化特征，最终在监督下输出完成体素。请注意，SSC和PVI模块可以在推理过程中丢弃。</p><p><img src="/pic/JS3CNet2.png"></p><p>上图 (a)部分显示了SSC模块的内部结构，该模块以分割网络中的语义概率为输入，通过多个卷积块和密集的上样本生成完整体积。(b)部分演示了PVI模块的一个二维实例，该实例利用数字“5”的粗全局结构的中心点，从原始点云中查询k个最近邻，然后应用基于图的聚合，通过细粒度的局部几何实现完整的“5”。</p><h3 id="MonoScene介绍"><a href="#MonoScene介绍" class="headerlink" title="MonoScene介绍"></a>MonoScene介绍</h3><p>一种能在室内与室外场景均可使用的单目 SSC 方案：</p><ul><li>提出一种将 2D 特征投影到 3D 的方法： FLoSP </li><li>提出一种  3D Context Relation Prior （3D CRP） 提升网络的上下文意识</li><li>新的SSC损失函数，用于优化场景类亲和力（affinity）和局部锥体（frustum）比例</li></ul><p><img src="/pic/MonoScence.png"></p><p>Monoscene  网络流程</p><ul><li>输入 2d 图片，经过 2d unet 提取多层次的特征</li><li>Features Line of Sight Projection module (FLoSP) 用于将 2d 特征提升到 3d 位置上，增强信息流并实现2D-3D分离</li><li>3D Context Relation Prior （3D CRP） 用于增强长距离的上下文信息提取</li><li>loss 优化：Scene-Class Affinity Loss：提升类内和类间的场景方面度量；Frustum Proportion Loss：对齐局部截头体中的类分布，提供超越场景遮挡的监督信息</li></ul><p>网络结构</p><ul><li>2D unet：EfficientNetB7 用于提取图像特征</li><li>3D UNets：2层 encoder-decoder 结构，用于提取 3d 特征</li><li>completion head：3D ASPP 结构和 softmax 层，用于处理 3D UNet 输出得到3d场景 completion 结果</li></ul><p><strong>Features Line of Sight Projection (FLoSP)</strong><br>从3维网络处理后者将为来自2维特征的集合提供线索。整个过程如下图所示，实际上假设已知相机内参，将3维体素中心投影到2维，从2维的解码器特征图采样得到对应特征，在所有尺度集合上进行重复，最终的3维特征图可用如下表示：</p><p><img src="/pic/MonoScence2.png"></p><h3 id="StereoScene介绍"><a href="#StereoScene介绍" class="headerlink" title="StereoScene介绍"></a>StereoScene介绍</h3><p><strong>3D语义场景补全（SSC）是一个需要从不完整观测中推断出密集3D场景的不适定问题。</strong>以往的方法要么明确地融合3D几何信息，要么依赖于从单目RGB图像中学习到的3D先验知识。然而，像LiDAR这样的3D传感器昂贵且具有侵入性，而单目相机由于固有的尺度模糊性而难以建模精确的几何信息。在这项工作中，研究者提出了StereoScene用于3D语义场景补全（SSC），它充分利用了轻量级相机输入而无需使用任何外部3D传感器。研究者的关键想法是利用<strong>立体匹配来解决几何模糊性问题</strong>。为了提高其在不匹配区域的鲁棒性，论文引入了鸟瞰图（BEV）表示法，以激发具有丰富上下文信息的未知区域预测能力。在立体匹配和BEV表示法的基础上，论文精心设计了一个<strong>相互交互聚合（MIA）</strong>模块，以充分发挥两者的作用，促进它们互补聚合。</p><p><img src="/pic/StereoScene.png"></p><p>整体的StereoScene框架如上图所示。论文遵循使用连续的2D和3 UNetsf作为backbones。给定输入的双目图像，我们使用2D UNet来提取多尺度特征。BEV潜在体积和立体几何体积分别由BEV构造器和立体构造器构造。为了充分利用这两个卷的互补潜力，提出了一个相互交互聚合模块来相互引导和聚合它们。</p><h3 id="介绍OccDepth"><a href="#介绍OccDepth" class="headerlink" title="介绍OccDepth"></a>介绍OccDepth</h3><p>第一个只输入视觉的立体3D SSC方法。为了有效地利用深度信息，提出了一种立体软特征分配（Stereo-SFA）模块，通过隐式学习立体图像之间的相关性来更好地融合3D深度感知特征。占用感知深度（OAD）模块将知识提取与预先训练的深度模型明确地用于生成感知深度的3D特征。此外，为了更有效地验证立体输入场景，我们提出了基于TartanAir的SemanticTartanAir数据集。这是一个虚拟场景，可以使用真实的地面实况来更好地评估该方法的有效性。</p><p><img src="/pic/OccDepth.png"><br>3D SSC是通过桥接立体SFA模块来从立体图像中推断出来的，该模块用于将特征提升到3D空间，OAD模块用于增强深度预测，3D U-Net用于提取几何结构和语义。立体深度网络仅在训练中用于提供深度监督。</p><h3 id="介绍VoxFormer"><a href="#介绍VoxFormer" class="headerlink" title="介绍VoxFormer"></a>介绍VoxFormer</h3><p>一种基于 Transformer 的语义场景完成框架，可以<strong>仅从 2D 图像输出完整的 3D 体积语义</strong>。我们的框架采用两阶段设计，从深度估计中一组稀疏的可见和占用体素查询开始，然后是从稀疏体素生成密集 3D 体素的致密化阶段。这种设计的一个关键思想是，2D 图像上的视觉特征仅对应于可见的场景结构，而不对应于被遮挡或空白的空间。因此，从可见结构的特征化和预测开始更为可靠。一旦我们获得了稀疏查询集，我们就应用屏蔽自动编码器设计，通过自注意力将信息传播到所有体素。</p><p>VoxFormer由类不可知的查询建议（阶段-1）和类特定的语义分割（阶段-2）组成，其中阶段-1提出了一组稀疏的占用体素，阶段-2完成了从阶段-1给出的建议开始的场景表示。具体而言，阶段1具有轻量级的基于2D CNN的查询建议网络，该网络使用图像深度来重建场景几何结构。然后，它从整个视场上预定义的可学习体素查询中提出了一组稀疏的体素。第2阶段基于一种新颖的稀疏到密集类MAE架构，如图（a）所示。它首先通过允许所提出的体素关注图像观察来加强其特征化。接下来，未提出的体素将与可学习的掩码标记相关联，并且将通过自注意来处理全套体素，以完成每体素语义分割的场景表示。</p><p><img src="/pic/VoxFormer2.png"></p><p>VoxFormer的总体框架如下图所示，给定RGB图像，通过ResNet50[61]提取2D特征，并通过现成的深度预测器估计深度。校正后的估计深度启用了类不可知查询建议阶段：将选择位于占用位置的查询，以与图像特征进行可变形的交叉关注。之后，将添加掩码令牌，用于通过可变形的自我注意来完成体素特征。细化的体素特征将被上采样并投影到输出空间，用于每个体素的语义分割。请注意，我们的框架支持单个或多个图像的输入。<br><img src="/pic/VoxFormer.png"></p><h3 id="介绍OccFormer"><a href="#介绍OccFormer" class="headerlink" title="介绍OccFormer"></a>介绍OccFormer</h3><p>提出了一种双路变压器网络OccFormer，该网络可以有效地处理三维体积进行语义占用预测。OccFormer实现了摄像机生成的3D体素特性的长程、动态和高效编码。它是通过将繁重的三维处理分解为局部和全局的变压器路径沿水平面得到的。在占位解码器中，我们将传统的Mask2Former用于3D语义占位，提出了保留池化和类引导采样，显著缓解了稀疏性和类的不平衡。</p><p><img src="/pic/OccFormer.png"></p><p>上图显示的为所提出的用于基于相机的3D语义占用预测的OccFormer的框架。该流水线由用于提取多尺度2D特征的图像编码器、用于将2D特征提升到3D体积的图像到3D变换以及用于获得3D语义特征并预测3D语义占用的基于变换器的编码器-解码器组成。</p><h4 id="对于编码器部分："><a href="#对于编码器部分：" class="headerlink" title="对于编码器部分："></a>对于编码器部分：</h4><p>我们提出了 dual-path transformer 模块，以释放 self-attention 的能力，同时限制了二次复杂性 (quadratic complexity)。</p><p>具体来说，</p><ul><li>local path 沿每个二维BEV切片运行，并使用共享窗口注意力 ( shared windowed attention) 来捕捉细粒度的细节。</li><li>global path 对 collapsed 的BEV特征进行处理，以获得场景级的理解。</li><li>最后，双路径的输出被自适应地融合以生成输出的三维特征体 3D feature volume 。<br>双路径设计明显打破了三维特征体的挑战性处理，我们证明了它比传统的三维卷积有明显优势。</li></ul><h4 id="对于解码器部分："><a href="#对于解码器部分：" class="headerlink" title="对于解码器部分："></a>对于解码器部分：</h4><p>我们是第一个将最先进的 Mask2Former[9] 方法用于三维语义占用预测的。</p><p>我们进一步提出使用最大池化而不是默认的双线性来计算 attention的 masked regions，这可以更好地保留小类。此外，我们还提出了 class-guided 的采样方法，以捕获前景区域，从而进行更有效的优化。</p><h3 id="介绍S4CNet"><a href="#介绍S4CNet" class="headerlink" title="介绍S4CNet"></a>介绍S4CNet</h3><p><img src="/pic/S4C.png"></p><p>S4C是第一种完全自我监督的语义场景完成（SSC）方法，该方法仅基于图像数据（和相机姿势）进行训练。我们的方法从单个图像中预测体积场景表示，即使在遮挡区域中也能捕获几何和语义信息。与以前的SSC方法相比，我们不需要任何3D地面实况信息，允许使用仅图像的数据集进行训练。尽管缺乏基本事实数据，但我们的方法与性能差异很小的监督方法相比具有竞争力。</p><p><img src="/pic/S4C2.png"></p><p>上图显示网络概述a） 根据输入图像II，编码器-解码器网络预测描述图像的截头体中的语义场的像素对齐特征图F。像素ui的特征fui对从光学中心通过像素投射的光线上的语义和占用分布进行编码。b） 语义场允许通过体积渲染来渲染新颖的视图及其相应的语义分割。将3D点xi投影到输入图像中，并因此将F投影到采样fui中。结合xi的位置编码，两个MLP分别对点σi和语义标签li的密度进行解码。用于新颖视图合成的颜色ci是通过颜色采样从其他图像中获得的。c） 为了获得最佳效果，我们要求训练视图覆盖尽可能多的场景表面。因此，我们从随机的未来时间步长中采样侧视图，这些时间步长观察在输入帧中被遮挡的场景区域。</p><h4 id="语义多视图一致性训练"><a href="#语义多视图一致性训练" class="headerlink" title="语义多视图一致性训练"></a>语义多视图一致性训练</h4><p>SSC的所有现有方法都依赖于3D地面实况数据进行训练。这些数据通常是从带注释的激光雷达扫描中获得的，这是非常困难和昂贵的。与3D数据相比，具有语义标签的图像是大量可用的。我们建议利用这些可用的2D数据来训练用于3D语义场景完成的神经网络。为了使我们的方法尽可能通用，我们使用从预先训练的语义分割网络生成的伪语义标签。这使我们能够以完全自我监督的方式，仅从姿势图像中训练我们的架构，而不需要2D或3D地面实况数据。</p><p><strong>训练数据和目标</strong>：该方法的目标是进行语义场景完成，包括对3D场景的重建和分配语义标签。与传统方法不同，传统方法依赖于昂贵的从LiDAR扫描中获得的3D真实数据，这种方法旨在利用具有语义标签的2D图像数据的丰富性。作者建议使用从预训练的语义分割网络生成的伪语义标签来训练SSC的神经网络。这种方法允许在不需要2D或3D真实数据的情况下进行全面的自我监督训练。</p><p><strong>数据来源</strong>：在自动驾驶场景中，汽车上装有多个摄像头，包括前置摄像头和侧置摄像头。该方法在多个时间步的视频序列中使用这些摄像头拍摄的多个构图图像进行训练。</p><p><strong>训练目标</strong>：从这些帧中随机选择一部分作为新视图合成的重建目标。神经网络经过训练，可以根据其他帧的语义场和颜色样本来重建颜色信息和语义标签。</p><p><strong>训练信号</strong>：训练信号是通过测量伪语义真值语义掩模和重建图像之间的差异生成的。这种差异用作训练期间的损失函数。</p><p><strong>战略视图选择</strong>：为了确保有效的训练，选择训练视图是经过策略性考虑的。选择了带有与输入图像的随机偏移的侧置摄像视图。这些视图对于捕捉输入图像中被遮挡的区域提供了重要线索，为场景完成提供了重要线索。</p><p><strong>重建块</strong>：只从不同帧中随机选择的块进行颜色（ˆPi,k）和语义标签（ˆSi）的重建。这种方法旨在进行高效的训练，同时确保神经网络学习了一般场景几何和特定对象的语义。</p><p><strong>损失函数</strong>：进行SSC的训练涉及使用语义和光度重建损失的组合。光度损失有助于捕捉一般场景几何，而语义损失对于区分对象和学习粗略几何非常重要，并且引导模型学习物体周围更清晰的边缘。</p><p><img src="/pic/S4C3.png"></p><p>上图显示的是在SSCBench-KITTI-360预测体素网格。对占用图的定性评价表明，该方法能够准确地重建和标注场景。特别是与MonoScene等其他基于图像的方法相比，S4C能够恢复图像1中右侧车道等细节。S4C产生的体素占用显示出比基于激光雷达的训练更少的洞，后者再现了在地面上发现的洞。</p><h3 id="与Occ3D，OpenOccupancy以及Occ4cast对比。"><a href="#与Occ3D，OpenOccupancy以及Occ4cast对比。" class="headerlink" title="与Occ3D，OpenOccupancy以及Occ4cast对比。"></a>与Occ3D，OpenOccupancy以及Occ4cast对比。</h3><p> 我们将SSCBench与同时进行的相关工作Occ3D进行了比较（Tian et al.，2023）。差异在于：</p><ul><li>（a）设置：Occ3D使用周围视图图像作为输入，并且只考虑相机可见的3D体素的重建。SSCBench考虑了一个更具挑战性但更有意义的设置（也是一个公认的设置）：如何仅使用单眼视觉输入在可见和遮挡区域重建和完成3D语义。这项任务需要对时间信息和三维几何关系进行推理，以摆脱有限的视野；</li><li>（b） 规模：SSCBench提供了比Occ3D更多的数据集，由于单目驾驶记录丰富，计划增加更多数据集；</li><li>（c） 可访问性：我们继承了先驱KITTI广泛使用的设置，从而使SSCBench更容易被社区访问；</li><li>（d） 全面性：我们将SSC方法与单目、三目和点云输入进行比较，并为跨领域泛化测试提供统一的标签。另一个相关的基准，OpenOccupancy（Wang et al.，2023），也表现出类似的差异，特别是它只使用了nuScenes数据集（Caesar et al.，2020），这导致了多样性的限制。</li></ul><p>Occ4cast提出了一个新的LiDAR感知任务：将占用补全和预测Occupancy Completion and Forecasting（OCF）统一到一个框架中<br>主要贡献总结如下：</p><ul><li>我们提出了OCF任务，该任务要求从稀疏的3D输入中获得时空密集的4D感知。</li><li>我们利用公共自动驾驶数据生成了一个名为OCFBench的大规模数据集。</li><li>我们提出了基线方法来处理OCF任务，并在我们的数据集上提供了详细的基准。</li></ul><h3 id="数据集介绍："><a href="#数据集介绍：" class="headerlink" title="数据集介绍："></a>数据集介绍：</h3><ul><li>SemanticKITTI：大规模的 LiDAR 点云标注数据集 SemanticKITTI，标注 28 类语义，共 22 个 sequences，43000 scans，不仅支持3D语义分割，而且是第一个户外SSC基准。一个明显的局限性是它在生成真值时遗漏了动态物体，导致了标签的不准确，产生痕迹。其次，它受到规模有限和缺乏多样化地理覆盖范围的限制，数据收集仅限于一个城市。</li><li>SSCBench-KITTI-360。KITTI-360（Liao et al.，2022）在著名的KITTI数据集的基础上，引入了一个丰富的数据收集框架，具有不同的传感器模态和全景视点（一个透视立体相机和一对鱼眼相机），并提供了全面的注释，包括一致的2D和3D语义实例标签以及3D边界基元。密集和连贯的标签不仅支持分割和检测等既定任务，还支持语义SLAM（Bowman et al.，2017）和新视图合成（Zhang et al.，2023a）等新应用。虽然KITTI-360包括基于点云的语义场景完成，但SSC的流行方法仍然以体素化表示为中心（Roldao等人，2022），这在机器人技术中表现出更广泛的适用性。我们利用开源训练和验证集，我们构建了由9个长序列组成的SSCBench-KITTI-360。为了减少冗余，我们按照SemanticKITTI SSC基准，每5帧采样一次。训练集包括来自场景00、02-05、07和10的8487帧，而验证集包括来自情景06的1812帧。测试集包括来自场景09的2566帧。数据集总共包含12865（~13K）帧，比SemanticKITTI的规模高出约1.5倍。</li><li>SSCBench-nuScenes。与KITTI的前置摄像头不同，nuScenes（Caesar et al.，2020）捕捉到了自我车辆周围的360度全景。它提供了各种各样的多模式传感数据，包括在波士顿和新加坡收集的相机图像、激光雷达点云和雷达数据。nuScenes为复杂的城市驾驶场景提供了细致的注释，包括不同的天气条件、施工区域和不同的照明。全景nuScenes（Fong et al.，2022）用语义和实例标签扩展了原始nuScene数据集。凭借全面的指标和评估协议，nuScenes在自动驾驶研究中得到了广泛应用（Gu et al.，2023；胡等人，2023，李等人，2021；Huang等人，2023.）。nuScenes数据集由1K 20秒的场景组成，其中仅为训练和验证集提供标签，共850个场景。从可用的850个场景中，我们分配了500个场景用于训练，200个场景用于验证，150个场景用于测试。这种分布导致20064帧用于训练，8050帧用于验证，5949帧用于测试，总计34078帧（～34K）。这个规模大约是SemanticKITTI的四倍。由于“nuScenes”仅为频率为2Hz的关键帧提供注释，因此在SSCBench“nuScene”中没有下采样。</li><li>SSC Bench Waymo。Waymo数据集（Sun et al.，2020）收集自美国各地，提供了大规模的多模式传感器记录。Waymo提供了5台相机，其组合水平视场为～230度，略小于nuScenes。数据是在多个城市的不同条件下采集的，包括旧金山、凤凰城和山景城，确保了每个城市的广泛地理覆盖。它包括1000个用于训练和验证的场景，以及150个用于测试的场景，每个场景的时间跨度为20秒。评论为了构建SSCBench Waymo，我们利用开源训练和验证场景，并将它们重新分配到500、298和202个场景中，分别用于训练、验证和测试。为了减少基准测试的冗余和训练时间，我们将原始数据的样本减少了10倍。这种下采样产生了10011帧的训练集、5936帧的验证集和4038帧的测试集，总计19985帧（～20K）。</li></ul><h3 id="构建Pipeline"><a href="#构建Pipeline" class="headerlink" title="构建Pipeline"></a>构建Pipeline</h3><ul><li><strong>先决条件</strong>。为了建立SSCBench，基于激光雷达或基于相机的SSC需要具有多模式记录的驾驶数据集。数据集应包括顺序收集的3D激光雷达点云，具有用于完成几何图形的精确传感器姿态，用于理解语义场景的逐点语义注释，以及用于处理动态实例的3D边界注释。</li><li><strong>点云聚合</strong>。为了生成完整的表示，我们的方法包括在车辆前方的定义区域内叠加一组广泛的激光扫描。在像nuScenes和Waymo这样的短序列中，我们利用未来的扫描和相应区域的测量来创建密集的语义点云。在像KITTI-360这样具有多个循环闭包的长序列中，除了时间邻域之外，我们还合并了所有空间相邻点云。先进的SLAM系统（Bailey&amp;Durrant-White，2006）提供了精确的传感器姿态，极大地促进了静态环境中点云的聚集。对于动态对象，我们通过同步来避免时空管道。我们使用实例标签将动态对象转换为当前帧内的空间对齐。</li><li><strong>聚合点云的Voxeization</strong>。体素化是将连续的3D空间离散为由称为体素的体积元素组成的规则网格结构，使非结构化数据能够转换为可由卷积神经网络（CNNs）或视觉变换器（ViTs）有效处理的结构化格式。Voxelization引入了空间分辨率和内存消耗之间的权衡，并为3D感知提供了灵活和可扩展的表示（Maturana&amp;Scherer，2015；周和图泽尔，2018；李等人，2023）。为了便于集成，SSCBench遵循SemanticKITTI的设置，体积向前延伸51.2米，每侧延伸25.6米，高度为6.4米。体素分辨率为0.2m，产生256×256×32的体素体积。每个体素的标签由其内标记点的多数投票决定，而如果不存在点，则相应地标记空体素。</li><li><strong>排除未知Voxels</strong>。如果没有无处不在的场景感知，捕捉完整的3D户外动态场景几乎是不可能的。虽然可以利用空间或先验知识推理，但我们的意图是通过最小化这些步骤产生的误差来确保基本事实的保真度。因此，在训练和评估过程中，我们只考虑来自所有视点的可见和探测体素。具体来说，我们首先从不同的角度使用光线跟踪来识别和去除物体内或墙后的遮挡体素。此外，在具有稀疏感测的数据集中，其中许多体素仍然没有被标记，我们在训练和评估期间去除这些未知体素，以增强基本事实标签的可靠性</li></ul><p>讨论与分析</p><ul><li>点云密度的影响。我们的实验阐明了激光雷达输入密度对模型性能的影响。在SSCBench nuScenes数据集中，其特征是相对稀疏的激光雷达输入（32个通道），基于相机的方法在几何度量上优于基于激光雷达的方法。然而，在SSCBench Waymo数据集中，得益于密集的激光雷达输入（64个通道，5个激光雷达），基于激光雷达的方法大大优于基于相机的方法。基于激光雷达的方法对输入的敏感性变得明显，在密集输入中观察到优势，而在稀疏输入中则观察到显著的性能下降。这突出了未来研究开发强大的基于激光雷达的方法的必要性，该方法可以在利用效益的同时减轻退化。</li><li>单眼与三眼。表3显示了具有单目和三目输入的TPVFormer的性能。而三眼设置提供了更宽的视野，有助于提高IoU的整体性能（36.78→ 39.06）和mIoU（10.91→ 13.70），仅使用一台相机就能获得优异的结果仍然是一个引人注目的学术挑战。开发能够将模型的性能与全景图相匹配的单目方法仍然具有重要的研究价值，因为它们具有内存高效、计算高效和易于部署的特点。</li><li>与SemanticKITTI的比较。当将我们在SSCBench上的实验结果与SemanticKITTI的实验结果进行比较时，我们观察到了显著的差异（Behley et al.，2019）（有关更多细节，我们请读者参阅VoxFormer（Li et al.，2023））。虽然VoxFormer在IoU和mIoU等指标上在SemanticKITTI上表现出色，但它面临着SSCBench数据集多样性的挑战。这一挑战主要源于其深度估计模块无法超越SemanticKITTI进行推广。此外，LMSCNet在SemanticKITTI上通常表现出优于SSCNet的几何性能，而在SSCBench上则表现出相反的趋势。这些差异强调了两个要点。首先，他们强调了SSCBench的重要性，它为全面评估提供了多样化和苛刻的现实世界场景。其次，他们强调了在各种环境中保持高性能的稳健方法的必要性。</li></ul><h3 id="结论"><a href="#结论" class="headerlink" title="结论"></a>结论</h3><ul><li>限制和未来工作。SSCBench仅包含符合SSC问题惯例的3D数据。这限制了对具有时间维度的4D方法的评估。未来的工作将旨在扩大SSCBench，以包括时间信息。</li><li>总结。在本文中，我们介绍了SSCBench，这是一个由不同街景组成的大型基准，旨在促进稳健和可推广的语义场景完成模型的开发。通过细致的策划和全面的基准测试，我们发现了现有方法的瓶颈，并为未来的研究方向提供了宝贵的见解。我们的目标是让SSCBench刺激3D语义场景完成的进步，最终增强下一代自主系统的感知能力。</li></ul>]]></content>
      
      
      <categories>
          
          <category> DL </category>
          
      </categories>
      
      
        <tags>
            
            <tag> ssc </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>ubuntu学习记录</title>
      <link href="/2023/05/15/zhiling/"/>
      <url>/2023/05/15/zhiling/</url>
      
        <content type="html"><![CDATA[<p>&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;节点&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;<br><a href="https://subscribe.pronetworklink.com/api/v1/client/e5cea80c08c2367003269501e9cbea19">https://subscribe.pronetworklink.com/api/v1/client/e5cea80c08c2367003269501e9cbea19</a></p><p>&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;orbslam&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;</p><h1 id="运行摄像头，发布话题"><a href="#运行摄像头，发布话题" class="headerlink" title="运行摄像头，发布话题"></a>运行摄像头，发布话题</h1><p>roslaunch usb_cam usb_cam-test.launch</p><h1 id="新终端：开启ORBSLAM2"><a href="#新终端：开启ORBSLAM2" class="headerlink" title="新终端：开启ORBSLAM2"></a>新终端：开启ORBSLAM2</h1><p>rosrun ORB_SLAM2 Mono .&#x2F;Vocabulary&#x2F;ORBvoc.txt .&#x2F;Examples&#x2F;ROS&#x2F;ORB_SLAM2&#x2F;Asus.yaml</p><p>pcl_viewer的使用:pcl_viewer a.pcd</p><p>&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;vscode使用&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;</p><p>vscode查看函数列表Ctrl + Shift + O</p><p>markdown ctrl-shift-vOpen preview(打开新窗口预览该文件)</p><p>ctrl+c+c </p><p>pkg-config –modversion opencv</p><p>g++ -std&#x3D;c++11 test.cpp <code>pkg-config --libs --cflags opencv</code> -o result</p><p>sudo gedit ~&#x2F;.bashrc</p><p>source ~&#x2F;.bashrc</p><p>echo $ROS_PACKAGE_PATH</p><p>问题：liunx系统打开vscode之后代码和终端字体有的单词间距很奇怪，修改字体大小和间距后无效，修改了字体完成。</p><p>操作：打开设置，输入Editor:Font Family，</p><p>修改终端字体：在Terminal › Integrated: Font Family下输入monospace，</p><p>修改编辑器字体：在Editor:Font Family输入monospac</p><p>&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;服务器&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;</p><p>ssh -p 1938 <a href="mailto:&#99;&#104;&#101;&#110;&#104;&#97;&#105;&#121;&#97;&#x6e;&#103;&#x40;&#x31;&#48;&#x2e;&#x36;&#x39;&#x2e;&#52;&#55;&#46;&#56;&#x32;">&#99;&#104;&#101;&#110;&#104;&#97;&#105;&#121;&#97;&#x6e;&#103;&#x40;&#x31;&#48;&#x2e;&#x36;&#x39;&#x2e;&#52;&#55;&#46;&#56;&#x32;</a></p><p>nvidia-smi</p><p>gpustat -i 1</p><p>-i <a href="http://pypi.douban.com/simple/">http://pypi.douban.com/simple/</a> –trusted-host pypi.douban.com</p><p>vim ~&#x2F;.bashrc</p><p>source ~&#x2F;.bashrc</p><p>&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;cuda&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;</p><p>export PATH&#x3D;”$PATH:&#x2F;usr&#x2F;local&#x2F;cuda&#x2F;bin”<br>export LD_LIBRARY_PATH&#x3D;”$LD_LIBRARY_PATH:&#x2F;usr&#x2F;local&#x2F;cuda&#x2F;lib64&#x2F;“<br>export LIBRARY_PATH&#x3D;”$LIBRARY_PATH:&#x2F;usr&#x2F;local&#x2F;cuda&#x2F;lib64”<br>export CUDA_HOME&#x3D;$CUDA_HOME:”&#x2F;usr&#x2F;local&#x2F;cuda”</p><p>chmod 777 file</p><p>&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;docker&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;</p><p>查看容器运行现状:docker ps。<br>停止容器:docker stop 容器id，<br>查看容器标准输出：docker logs<br>获取镜像：docker pull 命令来载入 ubuntu 镜像<br>启动容器：docker run -it ubuntu &#x2F;bin&#x2F;bash<br>查看容器：docker ps -a<br>退出容器：exit或着CTRL+D<br>启动已停止的容器：docker start 容器id<br>后台运行：-d指定容器运行模式，如：docker run -itd ubuntu-test ubuntu &#x2F;bin&#x2F;bash<br>注：-d参数默认不会进入容器<br>进入容器：docker attach容器id（从容器最初会导致容器停止）以及docker exec容器id(推荐使用，从容器退出不会导致容器停止)<br>导出容器：docker export 如：docker export 容器id  &gt;  ubuntu.tar<br>导入容器：docker import  如：cat docker&#x2F;ubuntu.tar | docker import - test&#x2F;ubuntu:v1<br>将快照文件 ubuntu.tar 导入到镜像 test&#x2F;ubuntu:v1:<br>删除容器：docker rm -f 容器id<br>删除所有终止状态的容器：docker container prune</p><p>sudo docker ps -a</p><p>sudo docker container ls -a</p><p>docker stop 容器id</p><p>sudo docker start 容器id          # 启动容器</p><p>sudo docker attach 容器id  进入容器正在执行的终端</p><p>sudo docker stats</p><p>sudo docker images</p><p>&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;conda源&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;</p><p>channels:</p><ul><li>defaults<br>show_channel_urls: true<br>channel_alias: <a href="https://mirrors.bfsu.edu.cn/anaconda">https://mirrors.bfsu.edu.cn/anaconda</a><br>default_channels:</li><li><a href="https://mirrors.bfsu.edu.cn/anaconda/pkgs/main">https://mirrors.bfsu.edu.cn/anaconda/pkgs/main</a></li><li><a href="https://mirrors.bfsu.edu.cn/anaconda/pkgs/free">https://mirrors.bfsu.edu.cn/anaconda/pkgs/free</a></li><li><a href="https://mirrors.bfsu.edu.cn/anaconda/pkgs/r">https://mirrors.bfsu.edu.cn/anaconda/pkgs/r</a></li><li><a href="https://mirrors.bfsu.edu.cn/anaconda/pkgs/pro">https://mirrors.bfsu.edu.cn/anaconda/pkgs/pro</a></li><li><a href="https://mirrors.bfsu.edu.cn/anaconda/pkgs/msys2">https://mirrors.bfsu.edu.cn/anaconda/pkgs/msys2</a><br>custom_channels:<br>  conda-forge: <a href="https://mirrors.bfsu.edu.cn/anaconda/cloud">https://mirrors.bfsu.edu.cn/anaconda/cloud</a><br>  msys2: <a href="https://mirrors.bfsu.edu.cn/anaconda/cloud">https://mirrors.bfsu.edu.cn/anaconda/cloud</a><br>  bioconda: <a href="https://mirrors.bfsu.edu.cn/anaconda/cloud">https://mirrors.bfsu.edu.cn/anaconda/cloud</a><br>  menpo: <a href="https://mirrors.bfsu.edu.cn/anaconda/cloud">https://mirrors.bfsu.edu.cn/anaconda/cloud</a><br>  pytorch: <a href="https://mirrors.bfsu.edu.cn/anaconda/cloud">https://mirrors.bfsu.edu.cn/anaconda/cloud</a><br>  simpleitk: <a href="https://mirrors.bfsu.edu.cn/anaconda/cloud">https://mirrors.bfsu.edu.cn/anaconda/cloud</a></li></ul><p>channels:</p><ul><li>defaults<br>show_channel_urls: true<br>channel_alias: <a href="https://mirrors.tuna.tsinghua.edu.cn/anaconda">https://mirrors.tuna.tsinghua.edu.cn/anaconda</a><br>default_channels:</li><li><a href="https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/main">https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/main</a></li><li><a href="https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/free">https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/free</a></li><li><a href="https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/r">https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/r</a></li><li><a href="https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/pro">https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/pro</a></li><li><a href="https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/msys2">https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/msys2</a><br>custom_channels:<br>  conda-forge: <a href="https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud">https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud</a><br>  msys2: <a href="https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud">https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud</a><br>  bioconda: <a href="https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud">https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud</a><br>  menpo: <a href="https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud">https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud</a><br>  pytorch: <a href="https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud">https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud</a><br>  simpleitk: <a href="https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud">https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud</a></li></ul><p>&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;opencv安装&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;</p><p>cmake -D CMAKE_BUILD_TYPE&#x3D;RELEASE <br>      -D CMAKE_INSTALL_PREFIX&#x3D;&#x2F;usr&#x2F;local&#x2F;opencv_2.4.11 <br>      -D WITH_CUDA&#x3D;OFF <br>      -D WITH_OPENGL&#x3D;OFF <br>      -D WITH_OPENCL&#x3D;OFF <br>      -D BUILD_JPEG&#x3D;OFF <br>      -D BUILD_PNG&#x3D;OFF <br>      -D BUILD_JASPER&#x3D;OFF <br>      -DBUILD_OPENEXR&#x3D;OFF <br>      -D BUILD_TIFF&#x3D;OFF <br>      -D BUILD_ZLIB&#x3D;OFF <br>      -D WITH_FFMPEG&#x3D;OFF <br>    ..</p><p>cmake -D CMAKE_BUILD_TYPE&#x3D;RELEASE -D CMAKE_INSTALL_PREFIX&#x3D;&#x2F;usr&#x2F;local&#x2F;opencv2411 -D CUDA_GENERATION&#x3D;Kepler -D WITH_TBB&#x3D;ON -D BUILD_NEW_PYTHON_SUPPORT&#x3D;ON -D WITH_V4L&#x3D;ON -D INSTALL_C_EXAMPLES&#x3D;ON -D INSTALL_PYTHON_EXAMPLES&#x3D;ON -D BUILD_EXAMPLES&#x3D;ON -D WITH_QT&#x3D;OFF -D WITH_OPENGL&#x3D;ON -D BUILD_TIFF&#x3D;ON ..&#x2F;local&#x2F;opencv2411 ..</p><p>sudo cmake -D CMAKE_BUILD_TYPE&#x3D;RELEASE \ -D CMAKE_INSTALL_PREFIX&#x3D;&#x2F;usr&#x2F;local \ -D OPENCV_EXTRA_MODULES_PATH&#x3D;~&#x2F;opencv_contrib-3.4.3&#x2F;modules -D INSTALL_PYTHON_EXAMPLES&#x3D;ON \ -D INSTALL_C_EXAMPLES&#x3D;ON -D OPENCV_ENABLE_NONFREE:BOOL&#x3D;ON -D BUILD_opencv_world:BOOL&#x3D;ON -D BUILD_EXAMPLES&#x3D;ON .. </p><p>—————————-ROS—————————————</p><p>rostopic list</p><p>rostopic type &#x2F;tianbot_mini&#x2F;odom    输出：类型</p><p>rosmsg show 类型    输出：类型包含的结构</p><p>rosrun  turtlesim turtlesim_node 运行节点</p><p>rosrun turtlesim turtle_teleop_key 键盘控制运动</p><p>查看当前所有的topic：rostopic list<br>查看某个topic的输出：rostopic echo [topic_name]<br>查看某个topic的发布频率：rostopic hz [topic_name]<br>查看某个topic的数据格式：rostopic echo [topic_name]&#x2F;encoding</p><p>&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;slam-yolov5&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;</p><p>.&#x2F;Examples&#x2F;RGB-D&#x2F;rgbd_tum  Vocabulary&#x2F;ORBvoc.txt Examples&#x2F;RGB-D&#x2F;TUM3.yaml &#x2F;home&#x2F;chy&#x2F;dataset&#x2F;rgbd_dataset_freiburg3_walking_halfsphere &#x2F;home&#x2F;chy&#x2F;dataset&#x2F;rgbd_dataset_freiburg3_walking_halfsphere&#x2F;associations.txt   </p><p>&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;segment-anything&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;</p><p>python scripts&#x2F;amg.py –input &#x2F;home&#x2F;chy&#x2F;code_chy&#x2F;segment-anything&#x2F;99.jpg –output &#x2F;home&#x2F;chy&#x2F;code_chy&#x2F;segment-anything&#x2F;outs –model-type vit_b –checkpoint &#x2F;home&#x2F;chy&#x2F;code_chy&#x2F;segment-anything&#x2F;sam_vit_b_01ec64.pth</p><p>&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;解压&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;<br>tgz格式解压到当前文件夹：<br>tar -zxvf xxx.tar.gz<br>zip格式解压到当前文件夹：<br>unzip xxx.zip<br>tar.xz格式解压<br>首先：xz -d xxx.tar.xz 解压得到tar文件 ;<br>其次：tar -xvf xxx.tar得到完整解压文件。</p><p>&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;git&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;</p><p>在git的repo中，可能会有子项目的代码，也就是”git中的git”</p><p> –recursive是递归的意思，不仅会git clone当前项目中的代码，也会clone项目中子项目的代码。</p><p><em><strong>未完待续</strong></em></p>]]></content>
      
      
      <categories>
          
          <category> linux </category>
          
      </categories>
      
      
        <tags>
            
            <tag> linux </tag>
            
            <tag> ubuntu </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>docker学习记录</title>
      <link href="/2023/05/07/docker1/"/>
      <url>/2023/05/07/docker1/</url>
      
        <content type="html"><![CDATA[<h1 id="docker-的基本使用"><a href="#docker-的基本使用" class="headerlink" title="docker 的基本使用"></a>docker 的基本使用</h1><h2 id="docker镜像命令和容器命令"><a href="#docker镜像命令和容器命令" class="headerlink" title="docker镜像命令和容器命令"></a>docker镜像命令和容器命令</h2><p><img src="/pic/e59c326db4bece61c8e5916822302408.png" alt="uTools_1683184235571.png"></p><p><img src="/pic/1bed533c0452fc72ba67462e37e5d850.png" alt="uTools_1683184257569.png"></p><br/><h2 id="x3D-x3D-docker-作业练习-x3D-x3D"><a href="#x3D-x3D-docker-作业练习-x3D-x3D" class="headerlink" title="&#x3D;&#x3D;docker 作业练习&#x3D;&#x3D;"></a><em><strong>&#x3D;&#x3D;docker 作业练习&#x3D;&#x3D;</strong></em></h2><blockquote><p><strong>&#x3D;&#x3D;Docker 安装 Nginx&#x3D;&#x3D;</strong></p></blockquote><pre class="line-numbers language-powershell" data-language="powershell"><code class="language-powershell"><span class="token comment"># -d后台运行</span><span class="token comment">#--name   给容器起名字</span><span class="token comment"># -p 宿主机端口：容器内部端口</span>chy@ocean:~$ docker run <span class="token operator">-</span>d <span class="token operator">--</span>name nginx01 <span class="token operator">-</span>p 8000:80 nginxc83728c3f1d2d20cba7571a6fd08c506cb5bc14b1fbb9385b8fca0e687615e4dchy@ocean:~$ docker <span class="token function">ps</span>CONTAINER ID   IMAGE     COMMAND                   CREATED         STATUS         PORTS                                   NAMESc83728c3f1d2   nginx     <span class="token string">"/docker-entrypoint.…"</span>   6 seconds ago   Up 5 seconds   0<span class="token punctuation">.</span>0<span class="token punctuation">.</span>0<span class="token punctuation">.</span>0:8000->80/tcp<span class="token punctuation">,</span> :::8000->80/tcp   nginx01chy@ocean:~$ curl localhost:8000&lt;<span class="token operator">!</span>DOCTYPE html>&lt;html>&lt;head>&lt;title>Welcome to nginx!&lt;<span class="token operator">/</span>title>&lt;style>html <span class="token punctuation">&#123;</span> color-scheme: light dark<span class="token punctuation">;</span> <span class="token punctuation">&#125;</span>body <span class="token punctuation">&#123;</span> width: 35em<span class="token punctuation">;</span> margin: 0 auto<span class="token punctuation">;</span>font-family: Tahoma<span class="token punctuation">,</span> Verdana<span class="token punctuation">,</span> Arial<span class="token punctuation">,</span> sans-serif<span class="token punctuation">;</span> <span class="token punctuation">&#125;</span>&lt;<span class="token operator">/</span>style>&lt;<span class="token operator">/</span>head>&lt;body>&lt;h1>Welcome to nginx!&lt;<span class="token operator">/</span>h1><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p><img src="/pic/27c501e8902ecff60cf6e1b31b56641e.png" alt="截图"></p><blockquote><p><strong>&#x3D;&#x3D;作业：docker 来装一个tomcat&#x3D;&#x3D;</strong></p></blockquote><pre class="line-numbers language-powershell" data-language="powershell"><code class="language-powershell">docker run <span class="token operator">-</span>it <span class="token operator">--</span><span class="token function">rm</span> tomcat:9<span class="token punctuation">.</span>0<span class="token comment"># --rm 一般用来测试，用完就删 </span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre><br/><blockquote><p>作业三：部署es+kibana</p></blockquote><pre class="line-numbers language-powershell" data-language="powershell"><code class="language-powershell"><span class="token comment"># es暴露端口多</span><span class="token comment">#es 十分的耗内存</span><span class="token comment">#es的数据一般挂在到安全目录！挂载</span><span class="token comment"># --net somenetwork  网络配置</span>docker stats查看docker所用内存<span class="token operator">-</span>e环境配置修改docker run <span class="token operator">-</span>d <span class="token operator">--</span>name elasticsearch <span class="token operator">-</span>p 9200:9200 <span class="token operator">-</span>p 9300:9300 <span class="token operator">-</span>e <span class="token string">"discovery.type=single-node"</span> elasticsearch:7<span class="token punctuation">.</span>6<span class="token punctuation">.</span>2chy@ocean:~$ curl localhost:9200<span class="token punctuation">&#123;</span>  <span class="token string">"name"</span> : <span class="token string">"c41cc3475bc3"</span><span class="token punctuation">,</span>  <span class="token string">"cluster_name"</span> : <span class="token string">"docker-cluster"</span><span class="token punctuation">,</span>  <span class="token string">"cluster_uuid"</span> : <span class="token string">"bhypk5Q-RvC4x_FOMoFF-g"</span><span class="token punctuation">,</span>  <span class="token string">"version"</span> : <span class="token punctuation">&#123;</span>    <span class="token string">"number"</span> : <span class="token string">"7.6.2"</span><span class="token punctuation">,</span>    <span class="token string">"build_flavor"</span> : <span class="token string">"default"</span><span class="token punctuation">,</span>    <span class="token string">"build_type"</span> : <span class="token string">"docker"</span><span class="token punctuation">,</span>    <span class="token string">"build_hash"</span> : <span class="token string">"ef48eb35cf30adf4db14086e8aabd07ef6fb113f"</span><span class="token punctuation">,</span>    <span class="token string">"build_date"</span> : <span class="token string">"2020-03-26T06:34:37.794943Z"</span><span class="token punctuation">,</span>    <span class="token string">"build_snapshot"</span> : false<span class="token punctuation">,</span>    <span class="token string">"lucene_version"</span> : <span class="token string">"8.4.0"</span><span class="token punctuation">,</span>    <span class="token string">"minimum_wire_compatibility_version"</span> : <span class="token string">"6.8.0"</span><span class="token punctuation">,</span>    <span class="token string">"minimum_index_compatibility_version"</span> : <span class="token string">"6.0.0-beta1"</span>  <span class="token punctuation">&#125;</span><span class="token punctuation">,</span>  <span class="token string">"tagline"</span> : <span class="token string">"You Know, for Search"</span><span class="token punctuation">&#125;</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p><strong>&#x3D;&#x3D;可视化&#x3D;&#x3D;</strong></p><ul><li>portainer(先用这个)</li></ul><pre class="line-numbers language-powershell" data-language="powershell"><code class="language-powershell">docker run <span class="token operator">-</span>d <span class="token operator">-</span>p 8080:9000\<span class="token operator">--</span>restart=always <span class="token operator">-</span>v <span class="token operator">/</span><span class="token keyword">var</span><span class="token operator">/</span>run/docker<span class="token punctuation">.</span>sock:<span class="token operator">/</span><span class="token keyword">var</span><span class="token operator">/</span>run/docker<span class="token punctuation">.</span>sock <span class="token operator">--</span>privileged=true portainer/portainer<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre><br/><p><strong>&#x3D;&#x3D;什么是portainer&#x3D;&#x3D;</strong></p><p>Docker 图像化界面管理工具，提供一个后台面板供我们操作</p><p>访问测试：<a href="http://192.168.1.40:8088/">http://192.168.1.40:8088</a></p><br/><h2 id="docker-镜像"><a href="#docker-镜像" class="headerlink" title="docker  镜像"></a>docker  镜像</h2><p><img src="/pic/c2cccd377124402c404b71f338287944.png" alt="截图"></p><p><strong>&#x3D;&#x3D;commit镜像&#x3D;&#x3D;</strong></p><br/><pre class="line-numbers language-powershell" data-language="powershell"><code class="language-powershell">docker commit 提交容器成为一个新的副本docker commit <span class="token operator">-</span>m=提交的描述信息  <span class="token operator">-</span>a=“作者 ” 容器 id 目标镜像名:<span class="token namespace">[TAG]</span>docker commit <span class="token operator">-</span>a=<span class="token string">"chy"</span> <span class="token operator">-</span>m=<span class="token string">"add webapps application"</span> 8f9660706542 tomcat_chy:1<span class="token punctuation">.</span>0<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span></span></code></pre><h2 id="x3D-x3D-容器数据卷-x3D-x3D"><a href="#x3D-x3D-容器数据卷-x3D-x3D" class="headerlink" title="&#x3D;&#x3D;容器数据卷&#x3D;&#x3D;"></a><strong>&#x3D;&#x3D;容器数据卷&#x3D;&#x3D;</strong></h2><h3 id="什么是容器数据卷"><a href="#什么是容器数据卷" class="headerlink" title="什么是容器数据卷"></a>什么是容器数据卷</h3><p><img src="/pic/2eb6ed69ff73865ee2e352c59cfa3263.png" alt="截图"></p><p><strong>总结一句话:容器的持久化和同步操作！容器间也可以进行数据共享的！</strong></p><br/><h3 id="使用数据卷"><a href="#使用数据卷" class="headerlink" title="使用数据卷"></a>使用数据卷</h3><br/><blockquote><p>方式一：直接使用命令来挂载 -v</p><p>docker run -it -v  主机目录：容器内目录    类似于-p</p><p> docker run -it -v &#x2F;home&#x2F;chy&#x2F;docker_ubuntu18_home:&#x2F;home ubuntu:18.04 &#x2F;bin&#x2F;bash</p></blockquote><pre class="line-numbers language-c_cpp" data-language="c_cpp"><code class="language-c_cpp">&quot;Mounts&quot;: [            &#123;                &quot;Type&quot;: &quot;bind&quot;,                &quot;Source&quot;: &quot;&#x2F;home&#x2F;chy&#x2F;docker_ubuntu18_home&quot;,                &quot;Destination&quot;: &quot;&#x2F;home&quot;,                &quot;Mode&quot;: &quot;&quot;,                &quot;RW&quot;: true,                &quot;Propagation&quot;: &quot;rprivate&quot;            &#125;        ],<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><br/><pre class="line-numbers language-powershell" data-language="powershell"><code class="language-powershell">chy@ocean:~<span class="token operator">/</span>docker_ubuntu18_home$ vim ceshi<span class="token punctuation">.</span>py <span class="token punctuation">[</span>2<span class="token punctuation">]</span><span class="token operator">+</span>  已停止               vim ceshi<span class="token punctuation">.</span>pychy@ocean:~<span class="token operator">/</span>docker_ubuntu18_home$ docker <span class="token function">ps</span>CONTAINER ID   IMAGE                 COMMAND        CREATED       STATUS       PORTS                                       NAMESd11b0f59e8b1   portainer/portainer   <span class="token string">"/portainer"</span>   5 hours ago   Up 5 hours   0<span class="token punctuation">.</span>0<span class="token punctuation">.</span>0<span class="token punctuation">.</span>0:8088->9000/tcp<span class="token punctuation">,</span> :::8088->9000/tcp   elated_davincichy@ocean:~<span class="token operator">/</span>docker_ubuntu18_home$ docker <span class="token function">ps</span> <span class="token operator">-</span>aCONTAINER ID   IMAGE                 COMMAND                   CREATED          STATUS                      PORTS                                       NAMESd591bed7171e   ubuntu:18<span class="token punctuation">.</span>04          <span class="token string">"/bin/bash"</span>               22 minutes ago   Exited <span class="token punctuation">(</span>0<span class="token punctuation">)</span> 21 minutes ago                                               wizardly_spenced11b0f59e8b1   portainer/portainer   <span class="token string">"/portainer"</span>              5 hours ago      Up 5 hours                  0<span class="token punctuation">.</span>0<span class="token punctuation">.</span>0<span class="token punctuation">.</span>0:8088->9000/tcp<span class="token punctuation">,</span> :::8088->9000/tcp   elated_davincic83728c3f1d2   nginx                 <span class="token string">"/docker-entrypoint.…"</span>   6 hours ago      Exited <span class="token punctuation">(</span>0<span class="token punctuation">)</span> 6 hours ago                                                  nginx01ff3cf19a0b79   ubuntu:18<span class="token punctuation">.</span>04          <span class="token string">"/bin/bash"</span>               7 hours ago      Exited <span class="token punctuation">(</span>127<span class="token punctuation">)</span> 7 hours ago                                                lucid_chatterjeee19f4782e1c5   ubuntu:18<span class="token punctuation">.</span>04          <span class="token string">"bash"</span>                    7 hours ago      Exited <span class="token punctuation">(</span>130<span class="token punctuation">)</span> 7 hours ago                                                pedantic_paninic29921e3edcf   composetest_web       <span class="token string">"flask run"</span>               2 weeks ago      Exited <span class="token punctuation">(</span>137<span class="token punctuation">)</span> 2 weeks ago                                                composetest-web-10a446f730c1f   redis:alpine          <span class="token string">"docker-entrypoint.s…"</span>   2 weeks ago      Exited <span class="token punctuation">(</span>0<span class="token punctuation">)</span> 6 days ago                                                   composetest-redis-1chy@ocean:~<span class="token operator">/</span>docker_ubuntu18_home$ docker <span class="token function">start</span> d591bed7171ed591bed7171echy@ocean:~<span class="token operator">/</span>docker_ubuntu18_home$ docker <span class="token function">ps</span> CONTAINER ID   IMAGE                 COMMAND        CREATED          STATUS         PORTS                                       NAMESd591bed7171e   ubuntu:18<span class="token punctuation">.</span>04          <span class="token string">"/bin/bash"</span>    22 minutes ago   Up 3 seconds                                               wizardly_spenced11b0f59e8b1   portainer/portainer   <span class="token string">"/portainer"</span>   5 hours ago      Up 5 hours     0<span class="token punctuation">.</span>0<span class="token punctuation">.</span>0<span class="token punctuation">.</span>0:8088->9000/tcp<span class="token punctuation">,</span> :::8088->9000/tcp   elated_davincichy@ocean:~<span class="token operator">/</span>docker_ubuntu18_home$ docker attach d591bed7171eroot@d591bed7171e:<span class="token operator">/</span><span class="token comment"># ls</span>bin  boot  dev  etc  home  lib  lib64  media  mnt  opt  proc  root  run  sbin  srv  sys  tmp  usr  <span class="token keyword">var</span>root@d591bed7171e:<span class="token operator">/</span><span class="token comment"># cd home/</span>root@d591bed7171e:<span class="token operator">/</span>home<span class="token comment"># ls</span>ceshi<span class="token punctuation">.</span>pyroot@d591bed7171e:<span class="token operator">/</span>home<span class="token comment"># cat ceshi.py </span>hello<span class="token punctuation">,</span> linux updata<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h3 id="好处：我们以后修改只需要在本地修改即可，容器内会自动同步"><a href="#好处：我们以后修改只需要在本地修改即可，容器内会自动同步" class="headerlink" title="好处：我们以后修改只需要在本地修改即可，容器内会自动同步"></a>好处：我们以后修改只需要在本地修改即可，容器内会自动同步</h3><br/><h3 id="实战：安装MySQL"><a href="#实战：安装MySQL" class="headerlink" title="实战：安装MySQL"></a>实战：安装MySQL</h3><p>思考：MySQL的数据持久化的问题</p><pre class="line-numbers language-powershell" data-language="powershell"><code class="language-powershell"><span class="token comment"># 官方的运行，有配置密码</span>docker run <span class="token operator">--</span>name some-mysql <span class="token operator">-</span>e MYSQL_ROOT_PASSWORD=my-secret-pw <span class="token operator">-</span>d mysql:tag<span class="token operator">-</span>e 环境配置<span class="token comment">#本主机上测试</span>docker run <span class="token operator">-</span>d <span class="token operator">-</span>p 3310:3306 <span class="token operator">-</span>v <span class="token operator">/</span>home/chy/docker_volume/mysql/conf:<span class="token operator">/</span>etc/mysql/conf<span class="token punctuation">.</span>d <span class="token operator">-</span>v <span class="token operator">/</span>home/chy/docker_volume/mysql/<span class="token keyword">data</span>:<span class="token operator">/</span><span class="token keyword">var</span><span class="token operator">/</span>lib/mysql <span class="token operator">-</span>e MYSQL_ROOT_PASSWOR=chy <span class="token operator">--</span>name mysql_chy mysql:5<span class="token punctuation">.</span>7<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><br/><h3 id="具名和匿名挂载"><a href="#具名和匿名挂载" class="headerlink" title="具名和匿名挂载"></a>具名和匿名挂载</h3><pre class="line-numbers language-powershell" data-language="powershell"><code class="language-powershell"><span class="token comment"># 匿名挂载</span><span class="token operator">-</span>v 容器内路径docker run <span class="token operator">-</span>d <span class="token operator">-</span>P <span class="token operator">--</span>name nginx <span class="token operator">-</span>v <span class="token operator">/</span>etc/nginx nginx<span class="token comment"># 查看所有volume 的情况</span>chy@ocean:~$ docker volume <span class="token function">ls</span>DRIVER    VOLUME NAMElocal     2198da5e7d86ba5a3de24d1aa71ab791bcb22e94e9f84e3d2f1fef73dc54d6b7local     a361a22ba8ef6ffffd78f233128054bd668b3e5315adffab8cfa79b5921ea720local     ba8715be0d242c0a4d004e38640b10c5dd5833f021920f8bd6fc0e830e012ebb<span class="token comment"># 这里的乱码号就是匿名容器名</span>chy@ocean:~$ docker run <span class="token operator">-</span>d <span class="token operator">-</span>P <span class="token operator">--</span>name nginx01 <span class="token operator">-</span>v juming-nginx:<span class="token operator">/</span>etc/nginx nginxa221eae136c83f19fa5f87944efa8eb1e9ea7841b8b982c0b2a943626be6842dchy@ocean:~$ docker volume <span class="token function">ls</span>DRIVER    VOLUME NAMElocal     2198da5e7d86ba5a3de24d1aa71ab791bcb22e94e9f84e3d2f1fef73dc54d6b7local     a361a22ba8ef6ffffd78f233128054bd668b3e5315adffab8cfa79b5921ea720local     ba8715be0d242c0a4d004e38640b10c5dd5833f021920f8bd6fc0e830e012ebblocal     juming-nginx<span class="token comment">#这里的juming-nginx就是具名挂载</span><span class="token comment"># 通过 -v 卷名：容器内路径</span><span class="token comment"># 查看一下这个卷</span>chy@ocean:~$ docker volume inspect juming-nginx<span class="token punctuation">[</span>    <span class="token punctuation">&#123;</span>        <span class="token string">"CreatedAt"</span>: <span class="token string">"2023-05-05T10:03:11+08:00"</span><span class="token punctuation">,</span>        <span class="token string">"Driver"</span>: <span class="token string">"local"</span><span class="token punctuation">,</span>        <span class="token string">"Labels"</span>: null<span class="token punctuation">,</span>        <span class="token string">"Mountpoint"</span>: <span class="token string">"/var/lib/docker/volumes/juming-nginx/_data"</span><span class="token punctuation">,</span>        <span class="token string">"Name"</span>: <span class="token string">"juming-nginx"</span><span class="token punctuation">,</span>        <span class="token string">"Options"</span>: null<span class="token punctuation">,</span>        <span class="token string">"Scope"</span>: <span class="token string">"local"</span>    <span class="token punctuation">&#125;</span><span class="token punctuation">]</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>挂载位置在&#x2F;var&#x2F;lib&#x2F;docker&#x2F;volumes&#x2F;XXXX&#x2F;_data内</p><br/><pre class="line-numbers language-powershell" data-language="powershell"><code class="language-powershell"><span class="token comment">#如何确定是具名挂载还是匿名挂载，还是指定路径挂载！</span><span class="token operator">-</span>V容器内路径  <span class="token comment">#匿名挂载</span><span class="token operator">-</span>V卷名：容器内路径  <span class="token comment">#具名挂载</span><span class="token operator">-</span>V/宿主机路径：：容器内路径   <span class="token comment">#指定路径挂载！</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span></span></code></pre><p>&#x3D;&#x3D;扩展&#x3D;&#x3D;</p><pre class="line-numbers language-powershell" data-language="powershell"><code class="language-powershell"><span class="token comment">#通过-V容器内路径：rorw改变读写权限</span>ro    readonly   <span class="token comment">#只读</span>rw   readwrite  <span class="token comment">#可读可写</span><span class="token comment">#一旦这个了设置了容器权限，容器对我们挂载出来的内容就有限定了！</span>docker run <span class="token operator">-</span>d <span class="token operator">-</span>P-<span class="token operator">-</span>name nginx02 <span class="token operator">-</span>v juming-nginx:<span class="token operator">/</span>etc/nginx:ro nginxdocker run <span class="token operator">-</span>d <span class="token operator">-</span>P <span class="token operator">--</span>name nginx02 <span class="token operator">-</span>v juming-nginx:<span class="token operator">/</span>etc/nginx:rw nginx<span class="token comment">#ro只要看到ro就说明这个路径只能通过宿主机来操作，容器内部是无法操作</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><br/><h2 id="x3D-x3D-Docker-File-x3D-x3D"><a href="#x3D-x3D-Docker-File-x3D-x3D" class="headerlink" title="&#x3D;&#x3D;Docker File&#x3D;&#x3D;"></a><strong>&#x3D;&#x3D;Docker File&#x3D;&#x3D;</strong></h2><h3 id="dockerfile介绍"><a href="#dockerfile介绍" class="headerlink" title="dockerfile介绍"></a>dockerfile介绍</h3><br/><p>dockerfile是用来构建dokcer镜像的文件！命令参数脚本I</p><h3 id="构建步骤："><a href="#构建步骤：" class="headerlink" title="构建步骤："></a>构建步骤：</h3><p>1、编写一个dockerfile文件<br>2、docker build构建成为一个镜像<br>3、docker run运行镜像<br>4、docker push发布镜像(DockerHub、阿里云镜像仓库！)</p><p><img src="/pic/4a105decfbf4a49ac1c660091b87418e.png" alt="截图"></p><p><img src="/pic/f49f57bbfe2c62d2a5c54b898d94ce24.png" alt="截图"></p><br/><p><strong>dockerfile是面向开发的，我们以后要发布项目，做镜像，就需要编写dockerfile文件，这个文件十分简单！</strong></p><p>&#x3D;&#x3D;Docker镜像逐渐成为企业交付的标准，必须要掌握！&#x3D;&#x3D;</p><br/><p><strong>步骤：开发，部署，运维。。。缺一不可</strong></p><ul><li>DockerFile:构建文件，定义了一切的步骤，源代码</li><li>Dockerlmages:通过DockerFile构建生成的镜像，最终发布和运行的产品！</li><li>Docker容器：容器就是镜像运行起来提供服务器</li></ul><br/><p><img src="/pic/83ae3813ab76ef7f521bbbb61aa89f30.png" alt="截图"></p><p><img src="/pic/fe5789bef1d222aaed7c55d0d3a99709.png" alt="截图"></p><pre class="line-numbers language-powershell" data-language="powershell"><code class="language-powershell"><span class="token keyword">FROM</span>    <span class="token comment">#基础镜镜像，一切从这里开始构建</span>MAINTAINER    <span class="token comment">#镜像是谁写的，姓名+邮箱</span>RUN   <span class="token comment">#镜像构建的时候需要运行的命令</span>ADD   <span class="token comment">#步骤：tomcat镜像，这个tomcat压缩包！添加内容</span>WORKDIR       <span class="token comment">#镜像的工作目录</span>VOLUME      <span class="token comment">#挂载的目录</span>EXPOST    <span class="token comment">#保留端口配置</span>ENTRYPOINT  <span class="token comment">#指定这个容器启动的时候要运行的命令，可以追加命令</span>ONBUILD   <span class="token comment">#当构建一个被继承DockerFile这个时候就会运行ONBUILD的指令。触发指令。</span><span class="token function">COPY</span>    <span class="token comment">#类似ADD，将我们文件拷贝到镜像中</span>ENV   <span class="token comment">#构建的时候设置环境变量！</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><br/><br/><br/><br/><br/><br/><br/><br/><br/><p>Dockerfile就是用来构建docker镜像的构建文件！命令脚本！先体验一下！<br>通过这个脚本可以生成镜像，镜像是一层一层的，脚本一个个的命令，每个命令都是一层！</p><pre class="line-numbers language-powershell" data-language="powershell"><code class="language-powershell"><span class="token comment">#创建一个dockerfi1e文件，名字可以随机建议Dockerfi1e</span><span class="token comment">#文件中的内容指令（大写）参数</span><span class="token keyword">FROM</span> ubuntu:18<span class="token punctuation">.</span>04VOLUME <span class="token punctuation">[</span><span class="token string">"volume01"</span><span class="token punctuation">,</span><span class="token string">"volume02"</span><span class="token punctuation">]</span><span class="token comment">#匿名挂载，未来使用特别多</span>CMD <span class="token function">echo</span> <span class="token string">"-------end-------"</span>CMD <span class="token operator">/</span>bin/bash<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p><img src="/pic/9082fb7a18429cafcfa617e90b675cd1.png" alt="截图"></p><br/><pre class="line-numbers language-powershell" data-language="powershell"><code class="language-powershell"><span class="token string">"Mounts"</span>: <span class="token punctuation">[</span>    <span class="token punctuation">&#123;</span>        <span class="token string">"Type"</span>: <span class="token string">"volume"</span><span class="token punctuation">,</span>        <span class="token string">"Name"</span>: <span class="token string">"22c524e180d59166fb95b5012971cfe1e5566bca1ceb5ecabd26fbe20d6371c0"</span><span class="token punctuation">,</span>        <span class="token string">"Source"</span>: <span class="token string">"/var/lib/docker/volumes/22c524e180d59166fb95b5012971cfe1e5566bca1ceb5ecabd26fbe20d6371c0/_data"</span><span class="token punctuation">,</span>        <span class="token string">"Destination"</span>: <span class="token string">"volume01"</span><span class="token punctuation">,</span>        <span class="token string">"Driver"</span>: <span class="token string">"local"</span><span class="token punctuation">,</span>        <span class="token string">"Mode"</span>: <span class="token string">""</span><span class="token punctuation">,</span>        <span class="token string">"RW"</span>: true<span class="token punctuation">,</span>        <span class="token string">"Propagation"</span>: <span class="token string">""</span>    <span class="token punctuation">&#125;</span><span class="token punctuation">,</span>    <span class="token punctuation">&#123;</span>        <span class="token string">"Type"</span>: <span class="token string">"volume"</span><span class="token punctuation">,</span>        <span class="token string">"Name"</span>: <span class="token string">"20a0e49e357f110e9452991d152ec1a8ac34d48a8c77b0fed84a1d8232f6282b"</span><span class="token punctuation">,</span>        <span class="token string">"Source"</span>: <span class="token string">"/var/lib/docker/volumes/20a0e49e357f110e9452991d152ec1a8ac34d48a8c77b0fed84a1d8232f6282b/_data"</span><span class="token punctuation">,</span>        <span class="token string">"Destination"</span>: <span class="token string">"volume02"</span><span class="token punctuation">,</span>        <span class="token string">"Driver"</span>: <span class="token string">"local"</span><span class="token punctuation">,</span>        <span class="token string">"Mode"</span>: <span class="token string">""</span><span class="token punctuation">,</span>        <span class="token string">"RW"</span>: true<span class="token punctuation">,</span>        <span class="token string">"Propagation"</span>: <span class="token string">""</span>    <span class="token punctuation">&#125;</span><span class="token punctuation">]</span><span class="token punctuation">,</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>测试一下刚才的文件是否同步出去了！</p><br/><p>这种方式我们未来使用的十分多，因为我们通常会构建自己的镜像！</p><br/><p>假设构建镜像时候没有挂载卷，要手动镜像挂载V卷名：容器内路径！</p><br/><h3 id="数据卷挂载"><a href="#数据卷挂载" class="headerlink" title="数据卷挂载"></a>数据卷挂载</h3><p>多个容器实现数据同步</p><p><img src="/pic/eb4af3f4a09cdc70786469482529743b.png" alt="截图"></p><pre class="line-numbers language-powershell" data-language="powershell"><code class="language-powershell">chy@ocean:~<span class="token operator">/</span>docker_volume$ docker run <span class="token operator">-</span>it <span class="token operator">--</span>name chy_ubuntu02 <span class="token operator">--</span>volumes-<span class="token keyword">from</span> chy_ubuntu01 chy_ubuntu:1<span class="token punctuation">.</span>0  <span class="token operator">/</span>bin/bashroot@bd9d9eddee71:<span class="token operator">/</span><span class="token comment"># ls</span>bin  boot  dev  etc  home  lib  lib64  media  mnt  opt  proc  root  run  sbin  srv  sys  tmp  usr  <span class="token keyword">var</span>  volume01  volume02root@bd9d9eddee71:<span class="token operator">/</span><span class="token comment"># cd volume01</span>root@bd9d9eddee71:<span class="token operator">/</span>volume01<span class="token comment"># ls</span>docker01<span class="token punctuation">.</span>txtroot@bd9d9eddee71:<span class="token operator">/</span>volume01<span class="token comment"># touch docker02.txt</span>root@bd9d9eddee71:<span class="token operator">/</span>volume01<span class="token comment"># ls</span>docker01<span class="token punctuation">.</span>txt  docker02<span class="token punctuation">.</span>txtroot@bd9d9eddee71:<span class="token operator">/</span>volume01<span class="token comment"># ls</span>docker01<span class="token punctuation">.</span>txt  docker02<span class="token punctuation">.</span>txt  docker03<span class="token punctuation">.</span>txtroot@bd9d9eddee71:<span class="token operator">/</span>volume01<span class="token comment"># </span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><pre class="line-numbers language-powershell" data-language="powershell"><code class="language-powershell">chy@ocean:~$ docker  run <span class="token operator">-</span>it <span class="token operator">--</span>name chy_ubuntu03 <span class="token operator">--</span>volumes-<span class="token keyword">from</span> chy_ubuntu02 chy_ubuntu:1<span class="token punctuation">.</span>0 <span class="token operator">/</span>bin/bashroot@0872aa038cfb:<span class="token operator">/</span><span class="token comment"># ls</span>bin   dev  home  lib64  mnt  proc  run   srv  tmp  <span class="token keyword">var</span>       volume02boot  etc  lib   media  opt  root  sbin  sys  usr  volume01root@0872aa038cfb:<span class="token operator">/</span><span class="token comment"># cd volume01</span>root@0872aa038cfb:<span class="token operator">/</span>volume01<span class="token comment"># ls</span>docker01<span class="token punctuation">.</span>txt  docker02<span class="token punctuation">.</span>txtroot@0872aa038cfb:<span class="token operator">/</span>volume01<span class="token comment"># touch docker03.txt</span>root@0872aa038cfb:<span class="token operator">/</span>volume01<span class="token comment"># ls</span>docker01<span class="token punctuation">.</span>txt  docker02<span class="token punctuation">.</span>txt  docker03<span class="token punctuation">.</span>txtroot@0872aa038cfb:<span class="token operator">/</span>volume01<span class="token comment"># </span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>&#x3D;&#x3D;主要使用的是 –volumes-from 实现容器间的数据共享 &#x3D;&#x3D;</p><p><strong>结论：</strong></p><p>容器之间配置信息的传递，数据卷容器的生命周期一直持续到没有容器使用为止。<br>但是一旦你持久化到了本地，这个时候，本地的数据是不会删除的！</p><h3 id="docker-compose常用命令"><a href="#docker-compose常用命令" class="headerlink" title="docker-compose常用命令"></a>docker-compose常用命令</h3><br/><pre class="line-numbers language-powershell" data-language="powershell"><code class="language-powershell">docker-compose up <span class="token operator">-</span>d nginx                     构建建启动nignx容器docker-compose exec nginx bash            登录到nginx容器中docker-compose down                              删除所有nginx容器<span class="token punctuation">,</span>镜像docker-compose <span class="token function">ps</span>                                   显示所有容器docker-compose restart nginx                   重新启动nginx容器docker-compose run <span class="token operator">--</span>no-deps <span class="token operator">--</span><span class="token function">rm</span> php-fpm php <span class="token operator">-</span>v  在php-fpm中不启动关联容器，并容器执行php <span class="token operator">-</span>v 执行完成后删除容器docker-compose build nginx                     构建镜像 。        docker-compose build <span class="token operator">--</span>no-cache nginx   不带缓存的构建。docker-compose logs  nginx                     查看nginx的日志 docker-compose logs <span class="token operator">-</span>f nginx                   查看nginx的实时日志docker-compose config  <span class="token operator">-</span>q                        验证（docker-compose<span class="token punctuation">.</span>yml）文件配置，当配置正确时，不输出任何内容，当文件配置错误，输出错误信息。 docker-compose events <span class="token operator">--</span>json nginx       以json的形式输出nginx的docker日志docker-compose pause nginx                 暂停nignx容器docker-compose unpause nginx             恢复ningx容器docker-compose <span class="token function">rm</span> nginx                       删除容器（删除前必须关闭容器）docker-compose stop nginx                    停止nignx容器docker-compose <span class="token function">start</span> nginx                    启动nignx容器<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h2 id="Docker-小结"><a href="#Docker-小结" class="headerlink" title="Docker 小结"></a>Docker 小结</h2><p><img src="/pic/82819ea9bbc4474aba79ad5353b2f288.png" alt="截图"></p><br/><h2 id="x3D-x3D-Docker-网络（未来安排，集成部署）-x3D-x3D"><a href="#x3D-x3D-Docker-网络（未来安排，集成部署）-x3D-x3D" class="headerlink" title="&#x3D;&#x3D;Docker 网络（未来安排，集成部署）&#x3D;&#x3D;"></a><strong>&#x3D;&#x3D;Docker 网络（未来安排，集成部署）&#x3D;&#x3D;</strong></h2><h3 id="理解Docker0"><a href="#理解Docker0" class="headerlink" title="理解Docker0"></a>理解Docker0</h3><p>清空所有环境</p><blockquote><p> 测试</p></blockquote><pre class="line-numbers language-powershell" data-language="powershell"><code class="language-powershell">1: lo: &lt;LOOPBACK<span class="token punctuation">,</span>UP<span class="token punctuation">,</span>LOWER_UP> mtu 65536 qdisc noqueue state UNKNOWN <span class="token function">group</span> default qlen 1000    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00    inet 127<span class="token punctuation">.</span>0<span class="token punctuation">.</span>0<span class="token punctuation">.</span>1/8 scope host lo<span class="token comment">#本机回环地址</span>       valid_lft forever preferred_lft forever    inet6 ::1/128 scope host        valid_lft forever preferred_lft forever2: enp5s0: &lt;BROADCAST<span class="token punctuation">,</span>MULTICAST<span class="token punctuation">,</span>UP<span class="token punctuation">,</span>LOWER_UP> mtu 1500 qdisc fq_codel state UP <span class="token function">group</span> default qlen 1000    link/ether c8:7f:54:57:48:62 brd ff:ff:ff:ff:ff:ff    inet 192<span class="token punctuation">.</span>168<span class="token punctuation">.</span>1<span class="token punctuation">.</span>40/24 brd 192<span class="token punctuation">.</span>168<span class="token punctuation">.</span>1<span class="token punctuation">.</span>255 scope global dynamic noprefixroute enp5s0       valid_lft 5419sec preferred_lft 5419sec<span class="token comment">#阿里云内网地址</span>    inet6 fe80::f8aa:d5c6:44bb:4a07/64 scope link noprefixroute        valid_lft forever preferred_lft forever       4: docker0: &lt;BROADCAST<span class="token punctuation">,</span>MULTICAST<span class="token punctuation">,</span>UP<span class="token punctuation">,</span>LOWER_UP> mtu 1500 qdisc noqueue state UP <span class="token function">group</span> default     link/ether 02:42:87:11:cb:17 brd ff:ff:ff:ff:ff:ff    inet 172<span class="token punctuation">.</span>17<span class="token punctuation">.</span>0<span class="token punctuation">.</span>1/16 brd 172<span class="token punctuation">.</span>17<span class="token punctuation">.</span>255<span class="token punctuation">.</span>255 scope global docker0       valid_lft forever preferred_lft forever<span class="token comment">#docker地址</span>    inet6 fe80::42:87ff:fe11:cb17/64 scope link        valid_lft forever preferred_lft forever<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>三个网络</p><br/><pre class="line-numbers language-powershell" data-language="powershell"><code class="language-powershell">root@e75dea24bd59:~<span class="token operator">/</span>catkin_ws<span class="token comment"># ifconfig</span>eth0: flags=4163&lt;UP<span class="token punctuation">,</span>BROADCAST<span class="token punctuation">,</span>RUNNING<span class="token punctuation">,</span>MULTICAST>  mtu 1500        inet 172<span class="token punctuation">.</span>17<span class="token punctuation">.</span>0<span class="token punctuation">.</span>4  netmask 255<span class="token punctuation">.</span>255<span class="token punctuation">.</span>0<span class="token punctuation">.</span>0  broadcast 172<span class="token punctuation">.</span>17<span class="token punctuation">.</span>255<span class="token punctuation">.</span>255        ether 02:42:<span class="token function">ac</span>:11:00:04  txqueuelen 0  <span class="token punctuation">(</span>Ethernet<span class="token punctuation">)</span>        RX packets 35  bytes 4999 <span class="token punctuation">(</span>4<span class="token punctuation">.</span>9 KB<span class="token punctuation">)</span>        RX errors 0  dropped 0  overruns 0  frame 0        TX packets 0  bytes 0 <span class="token punctuation">(</span>0<span class="token punctuation">.</span>0 B<span class="token punctuation">)</span>        TX errors 0  dropped 0 overruns 0  carrier 0  collisions 0lo: flags=73&lt;UP<span class="token punctuation">,</span>LOOPBACK<span class="token punctuation">,</span>RUNNING>  mtu 65536        inet 127<span class="token punctuation">.</span>0<span class="token punctuation">.</span>0<span class="token punctuation">.</span>1  netmask 255<span class="token punctuation">.</span>0<span class="token punctuation">.</span>0<span class="token punctuation">.</span>0        loop  txqueuelen 1000  <span class="token punctuation">(</span>Local Loopback<span class="token punctuation">)</span>        RX packets 0  bytes 0 <span class="token punctuation">(</span>0<span class="token punctuation">.</span>0 B<span class="token punctuation">)</span>        RX errors 0  dropped 0  overruns 0  frame 0        TX packets 0  bytes 0 <span class="token punctuation">(</span>0<span class="token punctuation">.</span>0 B<span class="token punctuation">)</span>        TX errors 0  dropped 0 overruns 0  carrier 0  collisions 0root@e75dea24bd59:~<span class="token operator">/</span>catkin_ws<span class="token comment"># read escape sequence</span>chy@ocean:~$ docker <span class="token function">ps</span> CONTAINER ID   IMAGE                 COMMAND                   CREATED        STATUS          PORTS                                       NAMESe75dea24bd59   4483fce64730          <span class="token string">"/entrypoint-melodic…"</span>   46 hours ago   Up 2 minutes                                                elastic_brownd11b0f59e8b1   portainer/portainer   <span class="token string">"/portainer"</span>              2 days ago     Up 11 minutes   0<span class="token punctuation">.</span>0<span class="token punctuation">.</span>0<span class="token punctuation">.</span>0:8088->9000/tcp<span class="token punctuation">,</span> :::8088->9000/tcp   elated_davincichy@ocean:~$ ping 172<span class="token punctuation">.</span>17<span class="token punctuation">.</span>0<span class="token punctuation">.</span>4PING 172<span class="token punctuation">.</span>17<span class="token punctuation">.</span>0<span class="token punctuation">.</span>4 <span class="token punctuation">(</span>172<span class="token punctuation">.</span>17<span class="token punctuation">.</span>0<span class="token punctuation">.</span>4<span class="token punctuation">)</span> 56<span class="token punctuation">(</span>84<span class="token punctuation">)</span> bytes of <span class="token keyword">data</span><span class="token punctuation">.</span>64 bytes <span class="token keyword">from</span> 172<span class="token punctuation">.</span>17<span class="token punctuation">.</span>0<span class="token punctuation">.</span>4: icmp_seq=1 ttl=64 time=0<span class="token punctuation">.</span>120 ms64 bytes <span class="token keyword">from</span> 172<span class="token punctuation">.</span>17<span class="token punctuation">.</span>0<span class="token punctuation">.</span>4: icmp_seq=2 ttl=64 time=0<span class="token punctuation">.</span>078 ms64 bytes <span class="token keyword">from</span> 172<span class="token punctuation">.</span>17<span class="token punctuation">.</span>0<span class="token punctuation">.</span>4: icmp_seq=3 ttl=64 time=0<span class="token punctuation">.</span>086 ms<span class="token comment">#Linux可以ping通docker容器</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><blockquote><p>原理</p></blockquote><br/><p>1、我们每启动一个docker容器，docker就会给docker容器分配一个ip,我们只要安装了docker,就会有一个网卡docker0<br>桥接模式，使用的技术是evth-pair技术！</p><br/><pre class="line-numbers language-powershell" data-language="powershell"><code class="language-powershell"><span class="token comment">#我们发现这个容器带来网卡，都是一对对的</span><span class="token comment"># evth-pair就是一对的虚拟设备接口，他们都是成对出现的，一段连着协议，一段彼此相连</span><span class="token comment">#正因为有这个特性，evth-pair充当一个桥梁，连接各种虚拟网络设备的</span><span class="token comment">#Openstac,Docker容器之间的连接，OVS的连接，都是使用evth-pair技术</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span></span></code></pre><p><img src="/pic/9bffa4485e6f6dbfc14beeb7a8fd9dd1.png" alt="截图"></p><br/><p>结论：tomcat01和tomcat(02是公用的一个路由器，docker0.<br>所有的容器不指定网络的情况下，都是docker0路由的，docker:会给我们的容器分配一个默认的可用IP<br>255.255.0.1&#x2F;16域局域网<br>0000000.000000.000000.000000<br>255.255.255.255</p><blockquote><p>结论</p></blockquote><br/><p>Docker 使用的是Linux的桥接，宿主机中是IGDocker容器的网桥 docker0</p><br/><br/><p><img src="/pic/09cd8423a8f4a1175f54bfa430c0ac5a.png" alt="截图"></p><p>Docker 中所哟的网络接口都是虚拟的，虚拟的转发效率高（内网传递文件）</p><br/><p>只要容器删除，对应网桥一对也没了</p><br/><h3 id="自定义网络"><a href="#自定义网络" class="headerlink" title="自定义网络"></a>自定义网络</h3><br/><blockquote><p> 查看所有的docker网络</p></blockquote><pre class="line-numbers language-powershell" data-language="powershell"><code class="language-powershell">chy@ocean:~$ docker network <span class="token function">ls</span>NETWORK ID     NAME                  DRIVER    SCOPE0e0cf6ea89e8   bridge                bridge    local2a0cbafc7650   composetest_default   bridge    local9a4f385414eb   host                  host      local7772e34bde04   none                  null      local<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h3 id="网络模式"><a href="#网络模式" class="headerlink" title="网络模式"></a>网络模式</h3><p>bridge:桥接docker(默认，自己创建也是使用桥接模式)</p><p>none:不配置网络</p><p>host:和宿主机共享网络</p><p>container:容器网络连通！（用的少！局限很大）</p><br/><pre class="line-numbers language-powershell" data-language="powershell"><code class="language-powershell">Usage:  docker network COMMANDManage networksCommands:  connect     Connect a container to a network  create      Create a network  disconnect  Disconnect a container <span class="token keyword">from</span> a network  inspect     Display detailed information on one or more networks  <span class="token function">ls</span>          List networks  prune       Remove all unused networks  <span class="token function">rm</span>          Remove one or more networksRun <span class="token string">'docker network COMMAND --help'</span> <span class="token keyword">for</span> more information on a command<span class="token punctuation">.</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><pre class="line-numbers language-powershell" data-language="powershell"><code class="language-powershell"><span class="token comment">#docker0特点：默认，域名不能访问，--1ink可以打通连接！</span><span class="token comment">#我们可以自定义一个网络！</span><span class="token comment">#--driver bridge</span><span class="token comment">#--subnet192.168.0.0/16</span><span class="token comment">#--gateway192.168.0.1</span><span class="token namespace">[root@kuangshen /]</span>docker network create <span class="token operator">--</span>driver bridge <span class="token operator">--</span>subnet 192<span class="token punctuation">.</span>168<span class="token punctuation">.</span>0<span class="token punctuation">.</span>0/16 <span class="token operator">--</span>gateway 192<span class="token punctuation">.</span>168<span class="token punctuation">.</span>0<span class="token punctuation">.</span>1myneteb21272b3a35 ceaba11b4aa5bbff131c3fb09c4790f0852ed4540707438db052<span class="token namespace">[root@kuangshen /]</span>docker network 1s<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>]]></content>
      
      
      <categories>
          
          <category> docker </category>
          
      </categories>
      
      
        <tags>
            
            <tag> docker </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>c++核心编程</title>
      <link href="/2023/04/11/c-base2/"/>
      <url>/2023/04/11/c-base2/</url>
      
        <content type="html"><![CDATA[<h1 id="内存分区模型"><a href="#内存分区模型" class="headerlink" title="内存分区模型"></a>内存分区模型</h1><p>c++程序执行时，将内存大方向分为4个区域：</p><ul><li>代码区：存放函数体的<strong>二进制代码</strong>，由操作系统进行管理的</li><li>全局区：存放全局变量和静态变量以及常量</li><li>栈区：由编译器自动分配释放，存放函数的参数值，局部变量等</li><li>堆区：由程序员分配和释放，若程序员不释放，程序结束时由操作系统回收</li></ul><p>内存四区的意义：</p><p><em><strong>不同区域存放的数据，赋予不同声明周期给我们更大的灵活编程</strong></em></p><h2 id="程序运行前"><a href="#程序运行前" class="headerlink" title="程序运行前"></a>程序运行前</h2><p>在程序编译后，生成了exe可执行程序，<strong>未执行该程序前</strong>分为两个区域</p><p><strong>代码区：</strong></p><ul><li>存放CPU执行的机器指令</li><li>代码区是共享的，共享的目的是对于频繁被执行的程序，只需要在内存中有一份代码即可</li><li>代码区是只读的，使其只读的原因是防止程序意外的修改了它的指令</li></ul><p><strong>全局区：</strong></p><ul><li><strong>全局变量</strong>和<strong>静态变量</strong>存放在此</li><li>全局区还包含**常量区，字符串常量和其他常量(const修饰的全局变量)**也存在于此</li><li>该区域的数据在程序执行结束后由<strong>操作系统</strong>释放</li></ul><blockquote><p>注意：局部变量和const修饰的局部变量不在全局区里</p></blockquote><p>总结：</p><ul><li>c++中程序运行前分为全局区和代码区</li><li>代码区特点是共享和只读</li><li>全局区中存放全局变量、静态变量、常量 </li><li>常量区存放const修饰的全局常量和字符串常量</li></ul><p>示例</p><pre class="line-numbers language-c++" data-language="c++"><code class="language-c++">#include&lt;iostream&gt;using namespace std;&#x2F;&#x2F;全局变量int g_a&#x3D;10;int g_b&#x3D;20;const int c_g_a&#x3D;10;const int c_g_b&#x3D;20;int main()&#123;    &#x2F;&#x2F;全局区    &#x2F;&#x2F;全局变量、静态变量，常量    &#x2F;&#x2F;创建普通局部变量    int a&#x3D;10;    int b &#x3D;20;    cout&lt;&lt;&quot;局部变量a的地址:&quot;&lt;&lt;(long long)&amp;a&lt;&lt;endl;    cout&lt;&lt;&quot;局部变量b的地址:&quot;&lt;&lt;(long long)&amp;b&lt;&lt;endl;&#x2F;&#x2F;     局部变量a的地址:140723942533960&#x2F;&#x2F; 局部变量b的地址:140723942533964    cout&lt;&lt;&quot;全局变量g_a的地址:&quot;&lt;&lt;(long long)&amp;g_a&lt;&lt;endl;    cout&lt;&lt;&quot;全局变量g_b的地址:&quot;&lt;&lt;(long long)&amp;g_b&lt;&lt;endl;&#x2F;&#x2F;     全局变量g_a的地址:94603780857872&#x2F;&#x2F; 全局变量g_b的地址:94603780857876    &#x2F;&#x2F;静态变量    static int s_a&#x3D;10;    static int s_b&#x3D;20;    cout&lt;&lt;&quot;静态变量s_a的地址:&quot;&lt;&lt;(long long)&amp;s_a&lt;&lt;endl;    cout&lt;&lt;&quot;静态变量s_b的地址:&quot;&lt;&lt;(long long)&amp;s_b&lt;&lt;endl;&#x2F;&#x2F;     静态变量s_a的地址:94603780857880&#x2F;&#x2F; 静态变量s_b的地址:94603780857884    &#x2F;&#x2F;常量    &#x2F;&#x2F;字符串常量    cout&lt;&lt;&quot;字符串常量的地址：&quot;&lt;&lt;(long long)&amp;&quot;hello world&quot;&lt;&lt;endl;&#x2F;&#x2F; 字符串常量的地址：94603778756100    &#x2F;&#x2F;const修饰的变量：const修饰的全局变量以及const修饰的局部变量    cout&lt;&lt;&quot;全局变量c_g_a的地址:&quot;&lt;&lt;(long long)&amp;c_g_a&lt;&lt;endl;    cout&lt;&lt;&quot;全局变量c_g_b的地址:&quot;&lt;&lt;(long long)&amp;c_g_b&lt;&lt;endl;&#x2F;&#x2F;     全局变量c_g_a的地址:94603778755912&#x2F;&#x2F; 全局变量c_g_b的地址:94603778755916    const int c_l_a&#x3D;10;    const int c_l_b&#x3D;20;    cout&lt;&lt;&quot;局部变量c_l_a的地址:&quot;&lt;&lt;(long long)&amp;c_l_a&lt;&lt;endl;    cout&lt;&lt;&quot;局部变量c_l_b的地址:&quot;&lt;&lt;(long long)&amp;c_l_a&lt;&lt;endl;&#x2F;&#x2F;     局部变量c_l_a的地址:140723942533968&#x2F;&#x2F; 局部变量c_l_b的地址:140723942533968    return 0;&#125;<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h2 id="程序运行后"><a href="#程序运行后" class="headerlink" title="程序运行后"></a>程序运行后</h2><p><strong>栈区：</strong></p><ul><li>由编译器自动释放，存放函数的参数值，局部变量等</li></ul><blockquote><p>注意事项：不要返回局部变量的地址，栈区开辟的数据由编译器自动释放</p></blockquote><p>示例</p><pre class="line-numbers language-c++" data-language="c++"><code class="language-c++">#include&lt;iostream&gt;using namespace std;&#x2F;&#x2F;栈区数据的注意事项 ----不要返回局部变量的地址&#x2F;&#x2F;栈区的数据由编译器管理开辟与释放int* func(int b)&#123;&#x2F;&#x2F;形参数据也会放在栈区    b&#x3D;100;    int a &#x3D;10;&#x2F;&#x2F;局部变量存放栈区，栈区数据在函数执行完后自动释放    return &amp;a;&#125;int main()&#123;    int *p&#x3D;func(1);    cout&lt;&lt;*p&lt;&lt;endl;    system(&quot;read -p &#39;Press Enter to continue...&#39; var&quot;);    return 0;&#125;<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p><strong>堆区：</strong></p><p>由程序员分配释放，若程序员不释放，程序结束时由系统回收</p><p>在c++中主要利用new在堆区开辟内存</p><p>示例</p><pre class="line-numbers language-c++" data-language="c++"><code class="language-c++">#include&lt;iostream&gt;using namespace std;int *func()&#123;    &#x2F;&#x2F;利用new关键字可以将数据开辟到堆区    &#x2F;&#x2F;指针本质也是局部变量，放在栈上。指针保存的数据放在堆区    int *p&#x3D;new int(10);    return p;&#125;int main()&#123;    &#x2F;&#x2F;在堆区开辟数据    int *p&#x3D;func();    cout&lt;&lt;*p&lt;&lt;endl;    system(&quot;read -p &#39;Press Enter to continue...&#39; var&quot;);    return 0;&#125;<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h2 id="new操作符"><a href="#new操作符" class="headerlink" title="new操作符"></a>new操作符</h2><p>c++中利用new操作符在堆区开辟数据</p><p>堆区开辟的数据由程序员手动开辟与释放，释放利用操作符delect</p><p>语法：new 数据类型</p><p>利用new创建的数据，会返回该数据对应的类型的指针</p><h1 id="引用"><a href="#引用" class="headerlink" title="引用"></a>引用</h1><h2 id="引用的基本能使用"><a href="#引用的基本能使用" class="headerlink" title="引用的基本能使用"></a>引用的基本能使用</h2><p>作用：给变量起别名</p><p>语法：数据类型 &amp;别名&#x3D;原名</p><p>示例：</p><pre class="line-numbers language-c++" data-language="c++"><code class="language-c++">#include&lt;iostream&gt;using namespace std;int main()&#123;    int a&#x3D;10;    int &amp;b&#x3D;a;    b&#x3D;20;    cout&lt;&lt;a&lt;&lt;endl;    &#x2F;&#x2F;20<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p><a href="https://blog.csdn.net/qq_58325487/article/details/124691945">c++引用入门</a></p><h2 id="引用的注意事项"><a href="#引用的注意事项" class="headerlink" title="引用的注意事项"></a>引用的注意事项</h2><ul><li>引用必须初始化</li><li>引用初始化后，不可以改变</li></ul><p>示例：</p><pre class="line-numbers language-c++" data-language="c++"><code class="language-c++">#include&lt;iostream&gt;using namespace std;int main()&#123;    int a&#x3D;10;    &#x2F;&#x2F;int&amp; b; &#x2F;&#x2F;错误，必须要初始化    int &amp;b&#x3D;a;    int c&#x3D;20;    b&#x3D;c;&#x2F;&#x2F;赋值操作不是更改引用    cout&lt;&lt;a&lt;&lt;endl;    &#x2F;&#x2F;20&#125;<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h2 id="y引用做函数参数"><a href="#y引用做函数参数" class="headerlink" title="y引用做函数参数"></a>y引用做函数参数</h2><p>作用：函数传参时，可以利用引用的技术让形参修饰实参</p><p>有点：可以简化指针修改实参</p><p>示例：</p><pre class="line-numbers language-c++" data-language="c++"><code class="language-c++"><span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre>]]></content>
      
      
      <categories>
          
          <category> c++ </category>
          
      </categories>
      
      
        <tags>
            
            <tag> c++ </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>linux环境下c++实现通讯录系统</title>
      <link href="/2023/04/11/c-base/"/>
      <url>/2023/04/11/c-base/</url>
      
        <content type="html"><![CDATA[<h1 id="通讯录系统"><a href="#通讯录系统" class="headerlink" title="通讯录系统"></a>通讯录系统</h1><p>该系统具有下面7种操作：</p><hr><p>***** 1、添加联系人*****</p><p>***** 2、显示联系人*****</p><p>***** 3、删除联系人*****</p><p>***** 4、查找联系人*****</p><p>***** 5、修改联系人*****</p><p>***** 6、清空联系人*****</p><p>***** 0、退出通讯录 ****</p><hr><p>源代码如下：</p><pre class="line-numbers language-c++" data-language="c++"><code class="language-c++">&#x2F;*封装函数显示界面，如void showMenu()*&#x2F;#include&lt;iostream&gt;#include &lt;unistd.h&gt;#include &lt;string&gt;using namespace std;#define MAX 1000&#x2F;&#x2F;联系人结构体struct Person&#123;    string m_Name;    int m_Sex;    int m_Age;    string m_Phone;    string m_Addr;&#125;;&#x2F;&#x2F;通讯录结构体struct Addressbooks&#123;    Person personArray [MAX];    int m_Size;&#x2F;&#x2F;通讯录中人员个数&#125;;&#x2F;&#x2F;1.添加联系人void addPerson(Addressbooks *abs)&#123;        if(abs-&gt;m_Size&#x3D;&#x3D;MAX)&#123;            cout&lt;&lt;&quot;通讯录已满，无法添加&quot;&lt;&lt; endl;            return;        &#125;else&#123;            string name;            cout&lt;&lt;&quot;请输入姓名：&quot;&lt;&lt;endl;            cin&gt;&gt;name;            abs-&gt;personArray[abs-&gt;m_Size].m_Name&#x3D;name;            cout&lt;&lt;&quot;1---男&quot;&lt;&lt;endl;            cout&lt;&lt;&quot;2---女&quot;&lt;&lt;endl;            int sex&#x3D;0;            while(true)&#123;                cin&gt;&gt;sex;                    if(sex&#x3D;&#x3D;1||sex&#x3D;&#x3D;2)&#123;                        abs-&gt;personArray[abs-&gt;m_Size].m_Sex&#x3D;sex;                        break;         &#125;            cout&lt;&lt;&quot;输入有误，请重新输入&quot;&lt;&lt;endl;        &#125;        cout&lt;&lt;&quot;请输入年龄&quot;&lt;&lt;endl;        int age &#x3D;0;        cin&gt;&gt;age;        abs-&gt;personArray[abs-&gt;m_Size].m_Age&#x3D;age;        cout&lt;&lt;&quot;请输入电话号码&quot;&lt;&lt;endl;        string iphone;        cin&gt;&gt;iphone;        abs-&gt;personArray[abs-&gt;m_Size].m_Phone&#x3D;iphone;        cout&lt;&lt;&quot;请输入家庭住址&quot;&lt;&lt;endl;        string address;        cin&gt;&gt;address;        abs-&gt;personArray[abs-&gt;m_Size].m_Addr&#x3D;address;        abs-&gt;m_Size++;        cout&lt;&lt;&quot;添加成功&quot;&lt;&lt;endl;        &#125;                        &#x2F;&#x2F;linux按任意键继续命令        system(&quot;read -p &#39;Press Enter to continue...&#39; var&quot;);        system(&quot;clear&quot;);&#125;&#x2F;&#x2F;2.示所有的联系人void showPerson(Addressbooks *abs)&#123;    if(abs-&gt;m_Size&#x3D;&#x3D;0)&#123;        cout&lt;&lt;&quot;当前记录为空&quot;&lt;&lt;endl;    &#125;else&#123;        for(int i&#x3D;0;i&lt;abs-&gt;m_Size;i++)&#123;            cout&lt;&lt;&quot;姓名：&quot;&lt;&lt;abs-&gt;personArray[i].m_Name&lt;&lt;&quot;\t&quot;;            cout&lt;&lt;&quot;性别：&quot;&lt;&lt;(abs-&gt;personArray[i].m_Sex &#x3D;&#x3D;1 ?&quot;男&quot;:&quot;女&quot;)&lt;&lt;&quot;\t&quot;;            cout&lt;&lt;&quot;年龄：&quot;&lt;&lt;abs-&gt;personArray[i].m_Age&lt;&lt;&quot;\t&quot;;            cout&lt;&lt;&quot;电话：&quot;&lt;&lt;abs-&gt;personArray[i].m_Phone&lt;&lt;&quot;\t&quot;;            cout&lt;&lt;&quot;地址：&quot;&lt;&lt;abs-&gt;personArray[i].m_Addr&lt;&lt;endl;        &#125;    &#125;                &#x2F;&#x2F;linux按任意键继续命令            system(&quot;read -p &#39;Press Enter to continue...&#39; var&quot;);            system(&quot;clear&quot;);&#x2F;&#x2F;清屏&#125;    &#x2F;&#x2F;检测联系人是否存在，如果存在返回数组的具体位置，不存在返回-1int isExist(Addressbooks *abs,string name)&#123;        for(int i&#x3D;0;i&lt;abs-&gt;m_Size;i++)&#123;            if(abs-&gt;personArray[i].m_Name&#x3D;&#x3D;name)&#123;                    return i;            &#125;        &#125;        return -1;&#125;&#x2F;&#x2F;3.删除指定联系人void deletePerson(Addressbooks *abs)&#123;    cout&lt;&lt;&quot;请输入您要删除的联系人&quot;&lt;&lt;endl;    string name;    cin&gt;&gt;name;    int ret&#x3D;isExist(abs,name);    if(ret&#x3D;&#x3D;-1)&#123;        cout&lt;&lt;&quot;查无此人&quot;&lt;&lt;endl;    &#125;else&#123;            for(int i&#x3D;ret;i&lt;abs-&gt;m_Size;i++)&#123;                &#x2F;&#x2F;数据迁移                abs-&gt;personArray[i]&#x3D;abs-&gt;personArray[i+1];            &#125;            abs-&gt;m_Size--;            cout&lt;&lt;&quot;删除成功&quot;&lt;&lt;endl;    &#125;                &#x2F;&#x2F;linux按任意键继续命令            system(&quot;read -p &#39;Press Enter to continue...&#39; var&quot;);            system(&quot;clear&quot;);&#x2F;&#x2F;清屏&#125;&#x2F;&#x2F;4.查找联系人信息void findPerson(Addressbooks *abs)&#123;    cout&lt;&lt;&quot;请输入您要查找的联系人&quot;&lt;&lt;endl;    string name;    cin&gt;&gt;name;    int ret &#x3D;isExist(abs,name);    if(ret!&#x3D;-1)&#123;            cout&lt;&lt;&quot;姓名：&quot;&lt;&lt;abs-&gt;personArray[ret].m_Name&lt;&lt;&quot;\t&quot;;            cout&lt;&lt;&quot;性别：&quot;&lt;&lt;(abs-&gt;personArray[ret].m_Sex &#x3D;&#x3D;1 ?&quot;男&quot;:&quot;女&quot;)&lt;&lt;&quot;\t&quot;;            cout&lt;&lt;&quot;年龄：&quot;&lt;&lt;abs-&gt;personArray[ret].m_Age&lt;&lt;&quot;\t&quot;;            cout&lt;&lt;&quot;电话：&quot;&lt;&lt;abs-&gt;personArray[ret].m_Phone&lt;&lt;&quot;\t&quot;;            cout&lt;&lt;&quot;地址：&quot;&lt;&lt;abs-&gt;personArray[ret].m_Addr&lt;&lt;endl;    &#125;else&#123;        cout&lt;&lt;&quot;查无此人&quot;&lt;&lt;endl;    &#125;                &#x2F;&#x2F;linux按任意键继续命令            system(&quot;read -p &#39;Press Enter to continue...&#39; var&quot;);            system(&quot;clear&quot;);&#x2F;&#x2F;清屏&#125;&#x2F;&#x2F;5.修改联系人void modifyPerson(Addressbooks* abs)&#123;    cout&lt;&lt;&quot;请输入你要修改的联系人&quot;&lt;&lt;endl;    string name;    cin&gt;&gt;name;    int ret &#x3D; isExist(abs,name);    if(ret!&#x3D;-1)&#123;        string name;        cout&lt;&lt;&quot;请输入姓名：&quot;&lt;&lt;endl;        cin&gt;&gt;name;        abs-&gt;personArray[ret].m_Name&#x3D;name;        cout&lt;&lt;&quot;请输入年龄：&quot;&lt;&lt;endl;        cout&lt;&lt;&quot;1---男&quot;&lt;&lt;endl;        cout&lt;&lt;&quot;2---女&quot;&lt;&lt;endl;        int sex&#x3D;0;        while(true)&#123;            cin&gt;&gt;sex;            if(sex&#x3D;&#x3D;1||sex&#x3D;&#x3D;2)&#123;                abs-&gt;personArray[ret].m_Sex&#x3D;sex;                break;            &#125;            cout&lt;&lt;&quot;输入有误，请重新输入&quot;&lt;&lt;endl;        &#125;        cout&lt;&lt;&quot;请输入年龄：&quot;&lt;&lt;endl;        int age&#x3D;0;        cin&gt;&gt;age;        abs-&gt;personArray[ret].m_Age&#x3D;age;        cout&lt;&lt;&quot;请输入联系电话：&quot;&lt;&lt;endl;        string iphone;        cin&gt;&gt;iphone;        abs-&gt;personArray[ret].m_Phone&#x3D;iphone;        cout&lt;&lt;&quot;请输入家庭住址：&quot;&lt;&lt;endl;        string address;        cin&gt;&gt;address;        abs-&gt;personArray[ret].m_Addr&#x3D;address;        cout&lt;&lt;&quot;修改成功&quot;&lt;&lt;endl;    &#125;else&#123;        cout&lt;&lt;&quot;查无此人&quot;&lt;&lt;endl;    &#125;                    &#x2F;&#x2F;linux按任意键继续命令            system(&quot;read -p &#39;Press Enter to continue...&#39; var&quot;);            system(&quot;clear&quot;);&#x2F;&#x2F;清屏&#125;&#x2F;&#x2F;6.清空联系人void cleanPerson(Addressbooks *abs)&#123;    abs-&gt;m_Size&#x3D;0;    cout&lt;&lt;&quot;通讯录已清空&quot;&lt;&lt;endl;    system(&quot;read -p &#39;Press Enter to continue...&#39; var&quot;);    system(&quot;clear&quot;);&#x2F;&#x2F;清屏&#125;void showMenu()&#123;    cout&lt;&lt;&quot;*************************&quot;&lt;&lt;endl;    cout&lt;&lt;&quot;***** 1、添加联系人*****&quot; &lt;&lt;endl;    cout&lt;&lt;&quot;***** 2、显示联系人*****&quot;&lt;&lt;endl;    cout&lt;&lt;&quot;***** 3、删除联系人*****&quot;&lt;&lt;endl;    cout&lt;&lt;&quot;***** 4、查找联系人*****&quot;&lt;&lt;endl;    cout&lt;&lt;&quot;***** 5、修改联系人*****&quot;&lt;&lt;endl;    cout&lt;&lt;&quot;***** 6、清空联系人*****&quot;&lt;&lt;endl;    cout&lt;&lt;&quot;***** 0、退出通讯录 ****&quot;&lt;&lt;endl;    cout&lt;&lt;&quot;*************************&quot;&lt;&lt;endl;&#125;int main()&#123;    Addressbooks abs;    abs.m_Size&#x3D;0;    int select &#x3D;0;    &#x2F;&#x2F;菜单调用    while(true)&#123;        showMenu();        cin &gt;&gt;select;        switch (select)&#123;            case 1:     &#x2F;&#x2F;添加联系人            addPerson(&amp;abs);                break;            case 2:     &#x2F;&#x2F;显示联系人                showPerson(&amp;abs);                break;            case 3: &#x2F;&#x2F;删除联系人                deletePerson(&amp;abs);            &#x2F;&#x2F; &#123;            &#x2F;&#x2F;     cout&lt;&lt;&quot; 请输入要删除联系人的姓名&quot;&lt;&lt;endl;            &#x2F;&#x2F;     string name;            &#x2F;&#x2F;     cin&gt;&gt;name;            &#x2F;&#x2F;     cout&lt;&lt;(isExist(&amp;abs,name)&#x3D;&#x3D;-1?&quot;查无此人&quot;:&quot;查有此人&quot;)&lt;&lt;endl;            &#x2F;&#x2F; &#125;                break;            case 4:&#x2F;&#x2F;查找联系人                findPerson(&amp;abs);                break;            case 5:&#x2F;&#x2F;修改联系人                modifyPerson(&amp;abs);                break;            case 6: &#x2F;&#x2F;清空联系人                cleanPerson(&amp;abs);                break;            case 0:                cout&lt;&lt;&quot;欢迎下次使用&quot;&lt;&lt;endl;                &#x2F;&#x2F;linux按任意键继续命令                system(&quot;read -p &#39;Press Enter to continue...&#39; var&quot;);                return 0;                break;             default:                break;        &#125;    &#125;&#x2F;&#x2F;linux按任意键继续命令    system(&quot;read -p &#39;Press Enter to continue...&#39; var&quot;);    return 0;&#125;<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>]]></content>
      
      
      <categories>
          
          <category> c++ </category>
          
      </categories>
      
      
        <tags>
            
            <tag> c++ </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>slam14讲源代码的记录（后九讲）</title>
      <link href="/2023/04/08/slam14-3/"/>
      <url>/2023/04/08/slam14-3/</url>
      
        <content type="html"><![CDATA[<h2 id="第六讲"><a href="#第六讲" class="headerlink" title="第六讲"></a>第六讲</h2><p>本讲只要讲解最小二乘法的含义以及处理方式，如高斯牛顿（GN）、列文伯格-马夸尔特法(L-M)等下降法策略</p><h3 id="在ch6-x2F-gaussNewton-cpp中，手写GN法，最小二乘估计曲线参数"><a href="#在ch6-x2F-gaussNewton-cpp中，手写GN法，最小二乘估计曲线参数" class="headerlink" title="在ch6&#x2F;gaussNewton.cpp中，手写GN法，最小二乘估计曲线参数"></a>在ch6&#x2F;gaussNewton.cpp中，手写GN法，最小二乘估计曲线参数</h3><pre class="line-numbers language-c++" data-language="c++"><code class="language-c++">&#x2F;&#x2F;求解误差项    for (int i &#x3D; 0; i &lt; N; i++) &#123;  double xi &#x3D; x_data[i], yi &#x3D; y_data[i];  &#x2F;&#x2F; 第i个数据点  double error &#x3D; yi - exp(ae * xi * xi + be * xi + ce);  Vector3d J; &#x2F;&#x2F; 雅可比矩阵  J[0] &#x3D; -xi * xi * exp(ae * xi * xi + be * xi + ce);  &#x2F;&#x2F; de&#x2F;da  J[1] &#x3D; -xi * exp(ae * xi * xi + be * xi + ce);  &#x2F;&#x2F; de&#x2F;db  J[2] &#x3D; -exp(ae * xi * xi + be * xi + ce);  &#x2F;&#x2F; de&#x2F;dc  H +&#x3D; inv_sigma * inv_sigma * J * J.transpose();  b +&#x3D; -inv_sigma * inv_sigma * error * J;  cost +&#x3D; error * error;&#125;&#x2F;&#x2F; 求解线性方程 Hx&#x3D;b&#x2F;&#x2F;对于正定矩阵，可以使用cholesky分解来解方程Vector3d dx &#x3D; H.ldlt().solve(b);&#x2F;&#x2F;也可以使用QR分解Vector3d dx &#x3D; H.colPivHouseholderQr().solve(b);<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h3 id="ch6-x2F-ceresCurveFitting-cpp"><a href="#ch6-x2F-ceresCurveFitting-cpp" class="headerlink" title="ch6&#x2F;ceresCurveFitting.cpp"></a>ch6&#x2F;ceresCurveFitting.cpp</h3><p>使用ceres拟合曲线</p><p><img src="/pic/%E9%80%89%E5%8C%BA_131.png" alt="Ceres简介"></p><pre class="line-numbers language-c++" data-language="c++"><code class="language-c++">&#x2F;&#x2F; 代价函数的计算模型,结构体struct CURVE_FITTING_COST &#123;  CURVE_FITTING_COST(double x, double y) : _x(x), _y(y) &#123;&#125;&#x2F;&#x2F;构造函数初始化方式，给_x赋值x,_y赋值y  &#x2F;&#x2F; 残差的计算  template&lt;typename T&gt;&#x2F;&#x2F;函数模板  bool operator()(          &#x2F;&#x2F;括号运算符重载    const T *const abc, &#x2F;&#x2F; 模型参数，有3维    T *residual) const &#123;    residual[0] &#x3D; T(_y) - ceres::exp(abc[0] * T(_x) * T(_x) + abc[1] * T(_x) + abc[2]); &#x2F;&#x2F; y-exp(ax^2+bx+c)    return true;  &#125;  const double _x, _y;    &#x2F;&#x2F; x,y数据&#125;;  &#x2F;&#x2F; 构建最小二乘问题  ceres::Problem problem;  for (int i &#x3D; 0; i &lt; N; i++) &#123;    problem.AddResidualBlock(     &#x2F;&#x2F; 向问题中添加误差项      &#x2F;&#x2F; 使用自动求导，模板参数：误差类型，输出维度，输入维度，维数要与前面struct中一致      new ceres::AutoDiffCostFunction&lt;CURVE_FITTING_COST, 1, 3&gt;(        new CURVE_FITTING_COST(x_data[i], y_data[i])      ),      nullptr,            &#x2F;&#x2F; 核函数，这里不使用，为空      abc                 &#x2F;&#x2F; 待估计参数    );  &#125;  &#x2F;&#x2F; 配置求解器  ceres::Solver::Options options;     &#x2F;&#x2F; 这里有很多配置项可以填  options.linear_solver_type &#x3D; ceres::DENSE_NORMAL_CHOLESKY;  &#x2F;&#x2F; 增量方程如何求解  options.minimizer_progress_to_stdout &#x3D; true;   &#x2F;&#x2F; 输出到cout  ceres::Solver::Summary summary;                &#x2F;&#x2F; 优化信息  chrono::steady_clock::time_point t1 &#x3D; chrono::steady_clock::now();  ceres::Solve(options, &amp;problem, &amp;summary);  &#x2F;&#x2F; 开始优化  chrono::steady_clock::time_point t2 &#x3D; chrono::steady_clock::now();  chrono::duration&lt;double&gt; time_used &#x3D; chrono::duration_cast&lt;chrono::duration&lt;double&gt;&gt;(t2 - t1);  cout &lt;&lt; &quot;solve time cost &#x3D; &quot; &lt;&lt; time_used.count() &lt;&lt; &quot; seconds. &quot; &lt;&lt; endl;<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p><a href="http://www.ceres-solver.org/tutorial.html">ceres官方教程</a></p><h3 id="ch6-x2F-g2oCurveFitting-cpp"><a href="#ch6-x2F-g2oCurveFitting-cpp" class="headerlink" title="ch6&#x2F;g2oCurveFitting.cpp"></a>ch6&#x2F;g2oCurveFitting.cpp</h3><p>待更新</p><h2 id="第七讲"><a href="#第七讲" class="headerlink" title="第七讲"></a>第七讲</h2><p>视觉里程计1,vo,特征提取与匹配，对极几何，PnP，ICP,三角化,BA,SVD,直接法，光流法，光度误差</p><h3 id="ch7-x2F-orb-cv-cpp"><a href="#ch7-x2F-orb-cv-cpp" class="headerlink" title="ch7&#x2F;orb_cv.cpp"></a>ch7&#x2F;orb_cv.cpp</h3><p><a href="https://zhuanlan.zhihu.com/p/345482379">ORB特征匹配、手写ORB特征代码详解</a></p><pre class="line-numbers language-c++" data-language="c++"><code class="language-c++">#include &lt;iostream&gt;#include &lt;opencv2&#x2F;core&#x2F;core.hpp&gt;#include &lt;opencv2&#x2F;features2d&#x2F;features2d.hpp&gt;#include &lt;opencv2&#x2F;highgui&#x2F;highgui.hpp&gt;#include &lt;chrono&gt;using namespace std;using namespace cv;int main(int argc, char **argv) &#123;  if (argc !&#x3D; 3) &#123;    cout &lt;&lt; &quot;usage: feature_extraction img1 img2&quot; &lt;&lt; endl;    return 1;  &#125;  &#x2F;&#x2F;-- 读取图像  Mat img_1 &#x3D; imread(argv[1], CV_LOAD_IMAGE_COLOR);  Mat img_2 &#x3D; imread(argv[2], CV_LOAD_IMAGE_COLOR);  assert(img_1.data !&#x3D; nullptr &amp;&amp; img_2.data !&#x3D; nullptr);  &#x2F;&#x2F;-- 初始化  std::vector&lt;KeyPoint&gt; keypoints_1, keypoints_2;  Mat descriptors_1, descriptors_2;  Ptr&lt;FeatureDetector&gt; detector &#x3D; ORB::create();        &#x2F;&#x2F;ORB  Ptr&lt;DescriptorExtractor&gt; descriptor &#x3D; ORB::create();  Ptr&lt;DescriptorMatcher&gt; matcher &#x3D; DescriptorMatcher::create(&quot;BruteForce-Hamming&quot;);  &#x2F;&#x2F;-- 第一步:检测 Oriented FAST 角点位置  chrono::steady_clock::time_point t1 &#x3D; chrono::steady_clock::now();  detector-&gt;detect(img_1, keypoints_1);  detector-&gt;detect(img_2, keypoints_2);  &#x2F;&#x2F;-- 第二步:根据角点位置计算 BRIEF 描述子  descriptor-&gt;compute(img_1, keypoints_1, descriptors_1);  descriptor-&gt;compute(img_2, keypoints_2, descriptors_2);  chrono::steady_clock::time_point t2 &#x3D; chrono::steady_clock::now();  chrono::duration&lt;double&gt; time_used &#x3D; chrono::duration_cast&lt;chrono::duration&lt;double&gt;&gt;(t2 - t1);  cout &lt;&lt; &quot;extract ORB cost &#x3D; &quot; &lt;&lt; time_used.count() &lt;&lt; &quot; seconds. &quot; &lt;&lt; endl;  Mat outimg1,outimg2;  drawKeypoints(img_1, keypoints_1, outimg1, Scalar::all(-1), DrawMatchesFlags::DEFAULT);  drawKeypoints(img_2, keypoints_2, outimg2, Scalar::all(-1), DrawMatchesFlags::DEFAULT);  imshow(&quot;pic1_ORB features&quot;, outimg1);  imshow(&quot;pic2_ORB features&quot;, outimg2);  &#x2F;&#x2F;-- 第三步:对两幅图像中的BRIEF描述子进行匹配，使用 Hamming 距离  vector&lt;DMatch&gt; matches;  t1 &#x3D; chrono::steady_clock::now();  matcher-&gt;match(descriptors_1, descriptors_2, matches);  t2 &#x3D; chrono::steady_clock::now();  time_used &#x3D; chrono::duration_cast&lt;chrono::duration&lt;double&gt;&gt;(t2 - t1);  cout &lt;&lt; &quot;match ORB cost &#x3D; &quot; &lt;&lt; time_used.count() &lt;&lt; &quot; seconds. &quot; &lt;&lt; endl;  &#x2F;&#x2F;-- 第四步:匹配点对筛选  &#x2F;&#x2F; 计算最小距离和最大距离  auto min_max &#x3D; minmax_element(matches.begin(), matches.end(),                                [](const DMatch &amp;m1, const DMatch &amp;m2) &#123; return m1.distance &lt; m2.distance; &#125;);  double min_dist &#x3D; min_max.first-&gt;distance;  double max_dist &#x3D; min_max.second-&gt;distance;  printf(&quot;-- Max dist : %f \n&quot;, max_dist);  printf(&quot;-- Min dist : %f \n&quot;, min_dist);  &#x2F;&#x2F;当描述子之间的距离大于两倍的最小距离时,即认为匹配有误.但有时候最小距离会非常小,设置一个经验值30作为下限.  std::vector&lt;DMatch&gt; good_matches;  for (int i &#x3D; 0; i &lt; descriptors_1.rows; i++) &#123;    if (matches[i].distance &lt;&#x3D; max(2 * min_dist, 30.0)) &#123;      good_matches.push_back(matches[i]);    &#125;  &#125;  &#x2F;&#x2F;-- 第五步:绘制匹配结果  Mat img_match;  Mat img_goodmatch;  drawMatches(img_1, keypoints_1, img_2, keypoints_2, matches, img_match);  drawMatches(img_1, keypoints_1, img_2, keypoints_2, good_matches, img_goodmatch);  imshow(&quot;all matches&quot;, img_match);  imshow(&quot;good matches&quot;, img_goodmatch);  waitKey(0);  return 0;&#125;<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p><a href="https://vimsky.com/examples/usage/cpp-algorithm-minmax_element-function-01.html">C++ Algorithm minmax_element()用法及代码示例</a></p><p><img src="/pic/%E9%80%89%E5%8C%BA_133.png" alt="两图特征点"></p><p><img src="/pic/%E9%80%89%E5%8C%BA_134.png" alt="匹配情况"></p><h3 id="ch7-x2F-orb-self-cpp"><a href="#ch7-x2F-orb-self-cpp" class="headerlink" title="ch7&#x2F;orb_self.cpp"></a>ch7&#x2F;orb_self.cpp</h3><p>手写ORB特征</p><p><strong>(1)FAST角点检测</strong></p><p>opencv库函数：利用ORB特征检测器detector里的detect函数。</p><p>手写：利用改进的FAST算法，增加了中心像素和围绕该像素的圆的像素之间的强度差阈值以及非最大值抑制。</p><p>其中的FAST（）函数结构如下：</p><pre class="line-numbers language-C++" data-language="C++"><code class="language-C++">CV_EXPORTS void FAST( InputArray image, CV_OUT std::vector&lt;KeyPoint&gt;&amp; keypoints,                      int threshold, bool nonmaxSuppression&#x3D;true );&#x2F;*image： 检测的灰度图像keypoints: 在图像上检测到的关键点threshold: 中心像素和围绕该像素的圆的像素之间的强度差阈值nonmaxSuppression: 参数非最大值抑制,默认为真，对检测到的角点应用非最大值抑制。*&#x2F;<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p><strong>（2）描述子计算</strong></p><p>opencv库函数：构建ORB特征描述器descriptor里的compute函数</p><p>手写：自定义函数ComputeORB()来计算描述子，在这个函数里面首先会排除一些靠近边缘的特征点。排除的坏点的判断方式如下：</p><ul><li><p>当kp.pt.x &lt; half_boundary时，边长为boundary的图像块将在-x轴上超出图像。</p></li><li><p>当kp.pt.y &lt; half_boundary时，边长为boundary的图像块将在-y轴上超出图像。</p></li><li><p>当kp.pt.x &gt;&#x3D; img.cols - half_boundary（kp.pt.x&gt;&#x3D; 640-16）时，边长为boundary(32)的图像块将在+x轴上超出图像。</p></li><li><p>当kp.pt.y &gt;&#x3D; img.rows - half_boundary（kp.pt.y&gt;&#x3D; 480-16）时，边长为boundary(32)的图像块将在+y轴上超出图像。</p></li></ul><p>同时，在手写代码中，按照灰度质心法定义了图像块的矩和质心，最后求出了特征点的角度。</p><p>在求描述子时，事先准备了256*4个数据集，这些数据集表示以关键点为中心，[-13,12]的范围内,随机选点对p,q。选取两个点p,q，这两个点的坐标从数据集选取，然后乘上之前求的角度再加上关键点，以此找到关键点附近的两个随机像素，然后比较像素值。最终形成描述子。</p><p><strong>（3）BRIEF描述子匹配函数</strong></p><p>opencv下：利用自带的match函数，比较两副图像的描述子的汉明距离，并从小到大排序在matches容器中，然后在容器中挑选好的描述子，这些描述子满足描述子之间的距离小于两倍的最小距离和经验阈值的最小值，因为最小距离可能是0；</p><p>手写：描述子是采用256位二进制描述，对应到8个32位的unsigned int 数据，并利用SSE指令集计算每个unsigned int变量中1的个数，从而计算汉明距离。手写的暴力匹配代码中输入三个参数，分别是第一副和第二副图像的描述子，和存放输出匹配对的容器；这里的暴力匹配的思路为：取第一副图片中的一个描述子，分别计算与第二副图片每个描述子的汉明距离，然后选取最近的距离以及所对应的匹配对，然后多次选取图片1中的描述子重复上述操作，分别找到最短距离和相应的匹配对。最后再将比较得到的最小距离与设定的经验阈值作比较，如果小于经验阈值则保留并输出该匹配对。</p><p>具体代码如下：</p><pre class="line-numbers language-C++" data-language="C++"><code class="language-C++"> &#x2F;&#x2F; compute the descriptor&#x2F;&#x2F;(1)计算角点的方向；(2)计算描述子。void ComputeORB(const cv::Mat &amp;img, vector&lt;cv::KeyPoint&gt; &amp;keypoints, vector&lt;DescType&gt; &amp;descriptors) &#123;  const int half_patch_size &#x3D; 8; &#x2F;&#x2F;计算特征点方向时，选取的图像块，16*16  const int half_boundary &#x3D; 16;&#x2F;&#x2F;计算描述子时在32*32的图像块中选点  int bad_points &#x3D; 0; &#x2F;&#x2F;计算描述子时，在32*32的区域块选择两个点比较，所选择的点超出图像范围的。出现这种情况下的FAST角点的数目。    &#x2F;&#x2F;遍历所有FAST角点  for (auto &amp;kp: keypoints)   &#123;    &#x2F;&#x2F;超出图像边界的角点的描述子设为空    if (kp.pt.x &lt; half_boundary || kp.pt.y &lt; half_boundary ||        kp.pt.x &gt;&#x3D; img.cols - half_boundary || kp.pt.y &gt;&#x3D; img.rows - half_boundary) &#123;      &#x2F;&#x2F; outside      bad_points++; &#x2F;&#x2F;bad_points的描述子设为空      descriptors.push_back(&#123;&#125;);      continue;    &#125;    &#x2F;&#x2F;计算16*16图像块的灰度质心    &#x2F;&#x2F;可参照下面的图片帮助理解    float m01 &#x3D; 0, m10 &#x3D; 0;&#x2F;&#x2F;图像块的矩 视觉slam十四讲中p157    for (int dx &#x3D; -half_patch_size; dx &lt; half_patch_size; ++dx)     &#123;      for (int dy &#x3D; -half_patch_size; dy &lt; half_patch_size; ++dy)       &#123;        uchar pixel &#x3D; img.at&lt;uchar&gt;(kp.pt.y + dy, kp.pt.x + dx);        m10 +&#x3D; dx * pixel; &#x2F;&#x2F;pixel表示灰度值 视觉slam十四讲中p157最上面的公式        m01 +&#x3D; dy * pixel;&#x2F;&#x2F;pixel表示灰度值 视觉slam十四讲中p157最上面的公式      &#125;    &#125;     &#x2F;&#x2F; angle should be arc tan(m01&#x2F;m10);参照下面第三章图片帮助理解    float m_sqrt &#x3D; sqrt(m01 * m01 + m10 * m10) + 1e-18; &#x2F;&#x2F; avoid divide by zero  1e-18避免了m_sqrt的值为0（图像块全黑）将m01 * m01 + m10 * m10进行开根    float sin_theta &#x3D; m01 &#x2F; m_sqrt;&#x2F;&#x2F;sin_theta &#x3D; m01 &#x2F; 根号下（m01 * m01 + m10 * m10））    float cos_theta &#x3D; m10 &#x2F; m_sqrt;&#x2F;&#x2F;cos_theta &#x3D; m10 &#x2F; 根号下（m01 * m01 + m10 * m10））    &#x2F;&#x2F;因为tan_theta &#x3D; m01&#x2F;m10 即为tan_theta &#x3D; sin_theta &#x2F; cos_theta &#x3D; [m01 &#x2F; 根号下（m01 * m01 + m10 * m10] &#x2F; [m10 &#x2F; 根号下（m01 * m01 + m10 * m10]    &#x2F;&#x2F;目的是求出特征点的方向  视觉slam十四讲中p157第三个公式    &#x2F;&#x2F; compute the angle of this point    DescType desc(8, 0); &#x2F;&#x2F;8个元素，它们的值初始化为0        for (int i &#x3D; 0; i &lt; 8; i++) &#123;      uint32_t d &#x3D; 0;      for (int k &#x3D; 0; k &lt; 32; k++)       &#123;        int idx_pq &#x3D; i * 32 + k;&#x2F;&#x2F;idx_pq表示二进制描述子中的第几位        cv::Point2f p(ORB_pattern[idx_pq * 4], ORB_pattern[idx_pq * 4 + 1]);        cv::Point2f q(ORB_pattern[idx_pq * 4 + 2], ORB_pattern[idx_pq * 4 + 3]);         &#x2F;&#x2F; rotate with theta        &#x2F;&#x2F;p,q绕原点旋转theta得到pp,qq        cv::Point2f pp &#x3D; cv::Point2f(cos_theta * p.x - sin_theta * p.y, sin_theta * p.x + cos_theta * p.y)                         + kp.pt;        cv::Point2f qq &#x3D; cv::Point2f(cos_theta * q.x - sin_theta * q.y, sin_theta * q.x + cos_theta * q.y)                         + kp.pt;        if (img.at&lt;uchar&gt;(pp.y, pp.x) &lt; img.at&lt;uchar&gt;(qq.y, qq.x)) &#123;          d |&#x3D; 1 &lt;&lt; k;        &#125;      &#125;      desc[i] &#x3D; d;    &#125;    descriptors.push_back(desc);&#x2F;&#x2F;desc表示该Oriented_FAST角点的描述子  &#125;   cout &lt;&lt; &quot;bad&#x2F;total: &quot; &lt;&lt; bad_points &lt;&lt; &quot;&#x2F;&quot; &lt;&lt; keypoints.size() &lt;&lt; endl;&#125; &#x2F;&#x2F; brute-force matchingvoid BfMatch(const vector&lt;DescType&gt; &amp;desc1, const vector&lt;DescType&gt; &amp;desc2, vector&lt;cv::DMatch&gt; &amp;matches) &#123;  const int d_max &#x3D; 40;&#x2F;&#x2F;描述子之间的距离小于这个值，才被认为是正确匹配   for (size_t i1 &#x3D; 0; i1 &lt; desc1.size(); ++i1)  &#x2F;&#x2F;size_t相当于int，便于代码移植  &#123;    if (desc1[i1].empty()) continue;    cv::DMatch m&#123;i1, 0, 256&#125;; &#x2F;&#x2F;定义了一个匹配对m    for (size_t i2 &#x3D; 0; i2 &lt; desc2.size(); ++i2)&#x2F;&#x2F;计算描述子desc1[i1]和描述子desc2[i2]的距离，即不同位数的数目     &#123;      if (desc2[i2].empty()) continue;      int distance &#x3D; 0;      for (int k &#x3D; 0; k &lt; 8; k++) &#123;        distance +&#x3D; _mm_popcnt_u32(desc1[i1][k] ^ desc2[i2][k]);      &#125;      if (distance &lt; d_max &amp;&amp; distance &lt; m.distance) &#123;        m.distance &#x3D; distance;        m.trainIdx &#x3D; i2;      &#125;    &#125;    if (m.distance &lt; d_max) &#123;      matches.push_back(m);    &#125;  &#125;<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><blockquote><p>参考<a href="https://blog.csdn.net/weixin_53660567/article/details/121095677">视觉SLAM十四讲CH7代码解析及课后习题详解</a></p></blockquote><h3 id="ch7-x2F-pose-estimation-2d2d-cpp"><a href="#ch7-x2F-pose-estimation-2d2d-cpp" class="headerlink" title="ch7&#x2F;pose_estimation_2d2d.cpp"></a>ch7&#x2F;pose_estimation_2d2d.cpp</h3><p>本程序演示了如何使用2D-2D的特征匹配估计相机运动,对极约束求解相机运动</p><pre class="line-numbers language-c++" data-language="c++"><code class="language-c++">&#x2F;&#x2F;&#x2F;&#x2F;#include &lt;iostream&gt;#include &lt;opencv2&#x2F;core&#x2F;core.hpp&gt;#include &lt;opencv2&#x2F;features2d&#x2F;features2d.hpp&gt;#include &lt;opencv2&#x2F;highgui&#x2F;highgui.hpp&gt;#include &lt;opencv2&#x2F;calib3d&#x2F;calib3d.hpp&gt;using namespace std;using namespace cv;&#x2F;&#x2F;声明特征匹配函数void find_feature_matches(const Mat &amp;img1,const Mat &amp;img2,                          std::vector&lt;KeyPoint&gt; &amp;keypoints_1,                          std::vector&lt;KeyPoint&gt; &amp;keypoints_2,                          std::vector&lt;DMatch&gt; &amp;matches);&#x2F;&#x2F;声明位姿估计函数void pose_estimated_2d2d( std::vector&lt;KeyPoint&gt; keypoints_1,                          std::vector&lt;KeyPoint&gt; keypoints_2,                          std::vector&lt;DMatch&gt; matches,                          Mat &amp;R, Mat &amp;t);&#x2F;&#x2F;声明 像素坐标转归一化坐标 p99页 公式5.5 变换一下就好啦！&#x2F;&#x2F;这里的函数我改了一下，书上的只是得到二维点，主程序中，还把二维点转换为三维点，我觉得多此一举，一个函数实现不就好了么&#x2F;&#x2F;下面是我实现的坐标变换函数 返回的是Mat类型，其实照片也是矩阵Mat pixel2cam(const Point2d &amp;p,const Mat &amp;K);&#x2F;&#x2F;p是像素坐标，K是相机内参矩阵int main(int argc,char** argv)&#123;    &#x2F;&#x2F;运行可执行文件+图片1的路径+图片2的路径    if(argc!&#x3D;3)    &#123;        cout&lt;&lt;&quot;usage: .&#x2F;pose_estimated_2d2d img1 img2&quot;&lt;&lt;endl;        return 1;    &#125;    &#x2F;*argv[1]&#x3D;&quot;&#x2F;home&#x2F;nnz&#x2F;data&#x2F;slam_pratice&#x2F;pratice_pose_estimated&#x2F;1.png&quot;;    argv[2]&#x3D;&quot;&#x2F;home&#x2F;nnz&#x2F;data&#x2F;slam_pratice&#x2F;pratice_pose_estimated&#x2F;2.png&quot;;*&#x2F;    &#x2F;&#x2F;读取图片    Mat img1&#x3D;imread(argv[1],CV_LOAD_IMAGE_COLOR);&#x2F;&#x2F;彩色图    Mat img2&#x3D;imread(argv[2],CV_LOAD_IMAGE_COLOR);    &#x2F;&#x2F;定义要用到的变量啊，比如 keypoints_1,keypoints_2,matches    vector&lt;KeyPoint&gt; keypoints_1, keypoints_2;    vector&lt;DMatch&gt; matches;    &#x2F;&#x2F;计算两个keypoints的matches    find_feature_matches(img1,img2,keypoints_1,keypoints_2,matches);    &#x2F;&#x2F;这样就得到matche啦！    cout &lt;&lt; &quot;一共找到了&quot; &lt;&lt; matches.size() &lt;&lt; &quot;组匹配点&quot; &lt;&lt; endl;    &#x2F;&#x2F;重点来了    &#x2F;&#x2F;估计两组图片之间的运动    Mat R,t;&#x2F;&#x2F;定义一下旋转和平移    pose_estimated_2d2d(keypoints_1,keypoints_2,matches,R,t);    &#x2F;&#x2F;这样就算出R ,t啦    &#x2F;&#x2F;当然我们还需要验证一下对极约束 准不准了    &#x2F;&#x2F;验证 E&#x3D;t^R*scale;    &#x2F;&#x2F;t_x是t的反对称矩阵    Mat t_x &#x3D;            (Mat_&lt;double&gt;(3, 3) &lt;&lt; 0, -t.at&lt;double&gt;(2, 0), t.at&lt;double&gt;(1, 0),                    t.at&lt;double&gt;(2, 0), 0, -t.at&lt;double&gt;(0, 0),                    -t.at&lt;double&gt;(1, 0), t.at&lt;double&gt;(0, 0), 0);    cout &lt;&lt; &quot;t^R&#x3D;&quot; &lt;&lt; endl &lt;&lt; t_x * R &lt;&lt; endl;    &#x2F;&#x2F;验证对极约束 对应P167 页公式7.10 x2TEx1&#x3D;0  E&#x3D;t^*R    &#x2F;&#x2F;定义内参矩阵    Mat K &#x3D; (Mat_&lt;double&gt;(3, 3) &lt;&lt; 520.9, 0, 325.1,                                                 0, 521.0,249.7,                                                 0, 0, 1);    for(DMatch m: matches)    &#123;        Mat x1&#x3D;pixel2cam(keypoints_1[m.queryIdx].pt,K);        Mat x2&#x3D;pixel2cam(keypoints_2[m.queryIdx].pt,K);        Mat d&#x3D;x2.t()*t_x*R*x1;&#x2F;&#x2F;若d很趋近于零，则说明没啥问题        cout &lt;&lt; &quot;epipolar constraint &#x3D; &quot; &lt;&lt; d &lt;&lt; endl;    &#125;    return 0;&#125;&#x2F;&#x2F;最复杂的地方来咯&#x2F;&#x2F;函数的实现&#x2F;&#x2F;匹配函数void find_feature_matches(const Mat &amp;img1,const Mat &amp;img2,                          std::vector&lt;KeyPoint&gt; &amp;keypoints_1,                          std::vector&lt;KeyPoint&gt; &amp;keypoints_2,                          std::vector&lt;DMatch&gt; &amp;matches)&#123;    &#x2F;&#x2F;先初始化，创建咱们要用到的对象    &#x2F;&#x2F;定义两个关键点对应的描述子，同时创建检测keypoints的检测器    Mat descriptors_1, descriptors_2;    vector&lt;DMatch&gt; match;&#x2F;&#x2F;暂时存放匹配点,因为后面还要进行筛选    Ptr&lt;FeatureDetector&gt; detector&#x3D;ORB::create();&#x2F;&#x2F;keypoints检测器    Ptr&lt;DescriptorExtractor&gt; descriptor &#x3D;ORB::create();&#x2F;&#x2F;描述子提取器    Ptr&lt;DescriptorMatcher&gt; matcher &#x3D;DescriptorMatcher::create(            &quot;BruteForce-Hamming&quot;);&#x2F;&#x2F;描述子匹配器（方法：暴力匹配）    &#x2F;&#x2F;step1:找到角点    detector-&gt;detect(img1,keypoints_1);&#x2F;&#x2F;得到图1的关键点（keypoints_1）    detector-&gt;detect(img2,keypoints_2);&#x2F;&#x2F;得到图2的关键点（keypoints_2）    &#x2F;&#x2F;step2:计算关键点所对应的描述子    descriptor-&gt;compute(img1,keypoints_1,descriptors_1);&#x2F;&#x2F;得到descriptors_1    descriptor-&gt;compute(img2,keypoints_2,descriptors_2);&#x2F;&#x2F;得到descriptors_2    &#x2F;&#x2F;step3:进行暴力匹配    matcher-&gt;match(descriptors_1,descriptors_2,match);    &#x2F;&#x2F;step4:对match进行筛选，得到好的匹配点，把好的匹配点放在matches中    &#x2F;&#x2F;先定义两个变量，一个是最大距离，一个是最小距离    double min_dist&#x3D;1000, max_dist&#x3D;0;    &#x2F;&#x2F;找出所有匹配之间的最小距离和最大距离, 即是最相似的和最不相似的两组点之间的距离    for(int i&#x3D;0;i&lt;descriptors_1.rows;i++) &#x2F;&#x2F;描述子本质是由 0,1 组成的向量    &#123;        double dist &#x3D;match[i].distance;        &#x2F;&#x2F;还记得orb_cv中如何找最大距离和最远距离的吗，那里面的程序是用下面的函数实现的，下面的函数得到的是pair first 里面是最小距离，second里面是最大距离        &#x2F;&#x2F; minmax_element(matches.begin(),matched.end,[](const DMatch &amp;m1,const DMatch &amp;m2)&#123;return m1.distance&lt;m2.distance;&#125;);        &#x2F;&#x2F;本程序用下面的if语句得到距离        if (dist &lt; min_dist) min_dist &#x3D; dist;        if (dist &gt; max_dist) max_dist &#x3D; dist;    &#125;    printf(&quot;-- Max dist : %f \n&quot;, max_dist);    printf(&quot;-- Min dist : %f \n&quot;, min_dist);    &#x2F;&#x2F;当描述子之间的距离大于两倍的最小距离时,即认为匹配有误.    &#x2F;&#x2F; 但有时候最小距离会非常小,设置一个经验值30作为下限.    for(int i&#x3D;0;i&lt;descriptors_1.rows;i++)    &#123;        if(match[i].distance&lt;&#x3D;max(2*min_dist,30.0))        &#123;            matches.push_back(match[i]);        &#125;    &#125;&#125;&#x2F;&#x2F;像素到归一化坐标Mat pixel2cam(const Point2d &amp;p,const Mat &amp;K)&#x2F;&#x2F;p是像素坐标，K是相机内参矩阵&#123;    Mat t;    t&#x3D;(Mat_&lt;double&gt;(3,1)&lt;&lt;(p.x - K.at&lt;double&gt;(0, 2)) &#x2F; K.at&lt;double&gt;(0, 0),            (p.y - K.at&lt;double&gt;(1, 2)) &#x2F; K.at&lt;double&gt;(1, 1),1);    return t;&#125;&#x2F;&#x2F;实现位姿估计函数void pose_estimated_2d2d( std::vector&lt;KeyPoint&gt; keypoints_1,                          std::vector&lt;KeyPoint&gt; keypoints_2,                          std::vector&lt;DMatch&gt; matches,                          Mat &amp;R, Mat &amp;t)&#123;    &#x2F;&#x2F; 相机内参来源于 TUM Freiburg2    Mat K &#x3D; (Mat_&lt;double&gt;(3, 3) &lt;&lt; 520.9, 0, 325.1,                                                 0, 521.0, 249.7,                                                 0, 0, 1);    &#x2F;&#x2F;咱们要把关键点的像素点拿出来 ,定义两个容器接受两个图关键点的像素位置    vector&lt;Point2d&gt; points1;    vector&lt;Point2d&gt; points2;    for(int i&#x3D;0;i&lt;(int)matches.size();i++)    &#123;        &#x2F;&#x2F;queryIdx是图1中匹配的关键点的对应编号        &#x2F;&#x2F;trainIdx是图2中匹配的关键点的对应编号        &#x2F;&#x2F;pt可以把关键点的像素位置取出来        points1.push_back(keypoints_1[matches[i].queryIdx].pt);        points2.push_back(keypoints_2[matches[i].trainIdx].pt);    &#125;    &#x2F;&#x2F;-- 计算基础矩阵    Mat fundamental_matrix;    fundamental_matrix &#x3D; findFundamentalMat(points1, points2, CV_FM_8POINT);    cout &lt;&lt; &quot;fundamental_matrix is &quot; &lt;&lt; endl &lt;&lt; fundamental_matrix &lt;&lt; endl;    &#x2F;&#x2F;计算本质矩阵 E    &#x2F;&#x2F;把cx ,cy放进一个向量里面 &#x3D;相机的光心    Point2d principal_point(325.1, 249.7);    double focal_length&#x3D;521;&#x2F;&#x2F;相机的焦距    &#x2F;&#x2F;之所以取上面的principal_point、focal_length是因为计算本质矩阵的函数要用    &#x2F;&#x2F;得到本质矩阵essential_matrix    Mat essential_matrix&#x3D;findEssentialMat(points1,points2,focal_length,principal_point);    cout&lt;&lt;&quot; essential_matrix &#x3D;\n&quot;&lt;&lt; essential_matrix &lt;&lt;endl;    &#x2F;&#x2F;-- 计算单应矩阵 homography_matrix    &#x2F;&#x2F;-- 但是本例中场景不是平面，单应矩阵意义不大    Mat homography_matrix;    homography_matrix &#x3D; findHomography(points1, points2, RANSAC, 3);    cout &lt;&lt; &quot;homography_matrix is &quot; &lt;&lt; endl &lt;&lt; homography_matrix &lt;&lt; endl;    &#x2F;&#x2F;通过本质矩阵恢复咱们的 R  t    recoverPose(essential_matrix,points1,points2,R,t,focal_length,principal_point);    &#x2F;&#x2F;输出咱们的 R t    cout&lt;&lt;&quot; 得到图1到图2 的位姿变换:\n &quot;&lt;&lt;endl;    cout&lt;&lt;&quot;R&#x3D; \n&quot;&lt;&lt; R &lt;&lt;endl;    cout&lt;&lt;&quot;t&#x3D; \n&quot;&lt;&lt; t &lt;&lt;endl;&#125;<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p><a href="https://blog.csdn.net/joun772/article/details/109466153?utm_medium=distribute.pc_relevant.none-task-blog-2~default~baidujs_baidulandingword~default-12-109466153-blog-116243451.235%5Ev28%5Epc_relevant_recovery_v2&spm=1001.2101.3001.4242.7&utm_relevant_index=15">SLAM十四讲-ch7(2)-位姿估计(包含2d-2d、3d-2d、3d-3d、以及三角化实现代码的注释)</a></p><h3 id="ch7-x2F-triangulation-cpp"><a href="#ch7-x2F-triangulation-cpp" class="headerlink" title="ch7&#x2F;triangulation.cpp"></a>ch7&#x2F;triangulation.cpp</h3><p>在上面的2d2d位姿估计的基础上，利用三角化来获得特征匹配点的深度信息(通过画图，验证三维点与特征点的重投影关系)</p><pre class="line-numbers language-c++" data-language="c++"><code class="language-c++">&#x2F;&#x2F;&#x2F;&#x2F; Created by wenbo on 2020&#x2F;11&#x2F;3.&#x2F;&#x2F;&#x2F;&#x2F;该程序在pose_estimated_2d2d的基础上加上三角化，以求得匹配的特征点在世界下的三维点&#x2F;&#x2F;&#x2F;&#x2F;#include &lt;iostream&gt;#include &lt;opencv2&#x2F;opencv.hpp&gt;&#x2F;&#x2F; #include &quot;extra.h&quot; &#x2F;&#x2F; used in opencv2using namespace std;using namespace cv;&#x2F;&#x2F;声明特征匹配函数void find_feature_matches(const Mat &amp;img1,const Mat &amp;img2,                          std::vector&lt;KeyPoint&gt; &amp;keypoints_1,                          std::vector&lt;KeyPoint&gt; &amp;keypoints_2,                          std::vector&lt;DMatch&gt; &amp;matches);&#x2F;&#x2F;声明位姿估计函数void pose_estimated_2d2d( std::vector&lt;KeyPoint&gt; &amp;keypoints_1,                          std::vector&lt;KeyPoint&gt; &amp;keypoints_2,                          std::vector&lt;DMatch&gt; &amp;matches,                          Mat &amp;R, Mat &amp;t);&#x2F;&#x2F;声明 像素坐标转归一化坐标 p99页 公式5.5 变换一下就好啦！&#x2F;&#x2F;这里的函数我改了一下，书上的只是得到二维点，主程序中，还把二维点转换为三维点，我觉得多此一举，一个函数实现不就好了么&#x2F;&#x2F;下面是我实现的坐标变换函数 返回的是Mat类型，其实照片也是矩阵Point2f pixel2cam(const Point2d &amp;p, const Mat &amp;K);&#x2F;&#x2F;p是像素坐标，K是相机内参矩阵&#x2F;&#x2F;声明三角化函数void triangulation(        const vector&lt;KeyPoint&gt; &amp;keypoint_1,        const vector&lt;KeyPoint&gt; &amp;keypoint_2,        const std::vector&lt;DMatch&gt; &amp;matches,        const Mat &amp;R, const Mat &amp;t,        vector&lt;Point3d&gt; &amp;points);&#x2F;&#x2F;&#x2F; 作图用inline cv::Scalar get_color(float depth) &#123;    float up_th &#x3D; 50, low_th &#x3D; 10, th_range &#x3D; up_th - low_th;    if (depth &gt; up_th) depth &#x3D; up_th;    if (depth &lt; low_th) depth &#x3D; low_th;    return cv::Scalar(255 * depth &#x2F; th_range, 0, 255 * (1 - depth &#x2F; th_range));&#125;int main(int argc,char** argv)&#123;    &#x2F;&#x2F;运行可执行文件+图片1的路径+图片2的路径    if(argc!&#x3D;3)    &#123;        cout&lt;&lt;&quot;usage: .&#x2F;triangulation img1 img2&quot;&lt;&lt;endl;        return 1;    &#125;    &#x2F;&#x2F;读取图片    Mat img1&#x3D;imread(argv[1],CV_LOAD_IMAGE_COLOR);&#x2F;&#x2F;彩色图    Mat img2&#x3D;imread(argv[2],CV_LOAD_IMAGE_COLOR);    &#x2F;&#x2F;定义要用到的变量啊，比如 keypoints_1,keypoints_2,matches    vector&lt;KeyPoint&gt; keypoints_1, keypoints_2;    vector&lt;DMatch&gt; matches;    &#x2F;&#x2F;计算两个keypoints的matches    find_feature_matches(img1,img2,keypoints_1,keypoints_2,matches);    &#x2F;&#x2F;这样就得到matches啦！    cout &lt;&lt; &quot;一共找到了&quot; &lt;&lt; matches.size() &lt;&lt; &quot;组匹配点&quot; &lt;&lt; endl;    &#x2F;&#x2F;重点来了    &#x2F;&#x2F;估计两组图片之间的运动    Mat R,t;&#x2F;&#x2F;定义一下旋转和平移    pose_estimated_2d2d(keypoints_1,keypoints_2,matches,R,t);    &#x2F;&#x2F;这样就算出R ,t啦    &#x2F;&#x2F;三角化    &#x2F;&#x2F;定义一个容器 points 用来存放特征匹配点在世界坐标系下的3d点    vector&lt;Point3d&gt; points;    triangulation(keypoints_1,keypoints_2,matches,R,t,points);    &#x2F;&#x2F;得到三维点    &#x2F;&#x2F;验证三维点与特征点的重投影关系    Mat K &#x3D; (Mat_&lt;double&gt;(3, 3) &lt;&lt; 520.9, 0, 325.1,                                                 0, 521.0, 249.7,                                                 0, 0, 1);    Mat img_plot1&#x3D;img1.clone();    Mat img_plot2&#x3D;img2.clone();    &#x2F;&#x2F;利用循环找到图1和图2特征点在图上的位置，并圈出来    for(int i&#x3D;0;i&lt;matches.size();i++)    &#123;        &#x2F;&#x2F;先画图1中特征点        &#x2F;&#x2F;在这里，为什么从一个世界坐标系下的3d点，就可以得到，图1相机坐标下的深度点呢？        &#x2F;&#x2F;我觉得是因为 图1的位姿: R是单位矩阵，t为0（在三角化函数中有写到） 所以可以把图1的相机坐标看成是世界坐标        float  depth1&#x3D;points[i].z;&#x2F;&#x2F;取出图1各个特征点的深度信息        cout&lt;&lt;&quot;depth: &quot;&lt;&lt;depth1&lt;&lt;endl;        Point2d pt1_cam&#x3D;pixel2cam(keypoints_1[matches[i].queryIdx].pt,K);        cv::circle(img_plot1, keypoints_1[matches[i].queryIdx].pt, 2, get_color(depth1), 2);        &#x2F;&#x2F;画图2        &#x2F;&#x2F;得到图2坐标系下的3d点，得到图2的深度信息        Mat pt2_trans&#x3D;R*(Mat_&lt;double&gt;(3, 1) &lt;&lt;points[i].x,points[i].y,points[i].z)+t;        float depth2 &#x3D; pt2_trans.at&lt;double&gt;(2, 0);        cv::circle(img_plot2, keypoints_2[matches[i].trainIdx].pt, 2, get_color(depth2), 2);    &#125;&#x2F;&#x2F;画图    cv::imshow(&quot;img 1&quot;, img_plot1);    cv::imshow(&quot;img 2&quot;, img_plot2);    cv::waitKey();    return 0;&#125;void triangulation(        const vector&lt;KeyPoint&gt; &amp;keypoint_1,        const vector&lt;KeyPoint&gt; &amp;keypoint_2,        const std::vector&lt;DMatch&gt; &amp;matches,        const Mat &amp;R, const Mat &amp;t,        vector&lt;Point3d&gt; &amp;points)&#123;    &#x2F;&#x2F;定义图1在世界坐标系下的位姿    Mat T1 &#x3D; (Mat_&lt;float&gt;(3, 4) &lt;&lt;                                1, 0, 0, 0,                                0, 1, 0, 0,                                0, 0, 1, 0);    &#x2F;&#x2F;定义图2在世界坐标系下的位姿    Mat T2 &#x3D; (Mat_&lt;float&gt;(3, 4) &lt;&lt;                                R.at&lt;double&gt;(0, 0), R.at&lt;double&gt;(0, 1), R.at&lt;double&gt;(0, 2), t.at&lt;double&gt;(0, 0),            R.at&lt;double&gt;(1, 0), R.at&lt;double&gt;(1, 1), R.at&lt;double&gt;(1, 2), t.at&lt;double&gt;(1, 0),            R.at&lt;double&gt;(2, 0), R.at&lt;double&gt;(2, 1), R.at&lt;double&gt;(2, 2), t.at&lt;double&gt;(2, 0)    );    Mat K &#x3D; (Mat_&lt;double&gt;(3, 3) &lt;&lt; 520.9, 0, 325.1, 0, 521.0, 249.7, 0, 0, 1);    &#x2F;&#x2F;容器 pts_1、pts_2分别存放图1和图2中特征点对应的自己相机归一化坐标中的 x与 y    vector&lt;Point2f&gt; pts_1,pts_2;    for(DMatch m:matches)&#x2F;&#x2F;这样的遍历写起来比较快    &#123;        &#x2F;&#x2F;将像素坐标变为相机下的归一化坐标        pts_1.push_back(pixel2cam(keypoint_1[m.queryIdx].pt,K));        pts_2.push_back(pixel2cam(keypoint_2[m.trainIdx].pt,K));    &#125;    Mat pts_4d;    cv::triangulatePoints(T1,T2,pts_1,pts_2,pts_4d);    &#x2F;*    传入两个图像对应相机的变化矩阵，各自相机坐标系下归一化相机坐标，    输出的3D坐标是齐次坐标，共四个维度，因此需要将前三个维度除以第四个维度以得到非齐次坐标xyz    *&#x2F;    &#x2F;&#x2F;转换为非齐次坐标    for(int i&#x3D;0;i&lt;pts_4d.cols;i++)&#x2F;&#x2F;遍历所有的点，列数表述点的数量    &#123;        &#x2F;&#x2F;定义x来接收每一个三维点        Mat x&#x3D;pts_4d.col(i); &#x2F;&#x2F;x为4x1维度        x&#x2F;&#x3D;x.at&lt;float&gt;(3,0);&#x2F;&#x2F;归一化        Point3d p(x.at&lt;float&gt;(0, 0),                  x.at&lt;float&gt;(1, 0),                  x.at&lt;float&gt;(2, 0));        points.push_back(p);&#x2F;&#x2F;将图1测得的目标相对相机实际位置（Xc,Yc,Zc）存入points    &#125;&#125;<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p><img src="/pic/%E9%80%89%E5%8C%BA_135.png"></p><p><a href="https://www.codenong.com/cs105088833/">对极约束和三角测量</a></p><h3 id="ch7-x2F-pose-estimation-3d2d-cpp"><a href="#ch7-x2F-pose-estimation-3d2d-cpp" class="headerlink" title="ch7&#x2F;pose_estimation_3d2d.cpp"></a>ch7&#x2F;pose_estimation_3d2d.cpp</h3><p>本程序使用Opencv的EPnP求解pnp问题，并手写了一个高斯牛顿法的PnP,然后调用g2o来求解</p><pre class="line-numbers language-c++" data-language="c++"><code class="language-c++">&#x2F;&#x2F;&#x2F;&#x2F; Created by nnz on 2020&#x2F;11&#x2F;5.&#x2F;&#x2F;&#x2F;&#x2F;&#x2F;&#x2F; Created by nnz on 2020&#x2F;11&#x2F;4.&#x2F;&#x2F;#include &lt;iostream&gt;#include &lt;opencv2&#x2F;core&#x2F;core.hpp&gt;#include &lt;opencv2&#x2F;features2d&#x2F;features2d.hpp&gt;#include &lt;opencv2&#x2F;highgui&#x2F;highgui.hpp&gt;#include &lt;opencv2&#x2F;calib3d&#x2F;calib3d.hpp&gt;#include &lt;Eigen&#x2F;Core&gt;#include &lt;g2o&#x2F;core&#x2F;base_vertex.h&gt;#include &lt;g2o&#x2F;core&#x2F;base_unary_edge.h&gt;#include &lt;g2o&#x2F;core&#x2F;sparse_optimizer.h&gt;#include &lt;g2o&#x2F;core&#x2F;block_solver.h&gt;#include &lt;g2o&#x2F;core&#x2F;solver.h&gt;#include &lt;g2o&#x2F;core&#x2F;optimization_algorithm_gauss_newton.h&gt;#include &lt;g2o&#x2F;solvers&#x2F;dense&#x2F;linear_solver_dense.h&gt;#include &lt;sophus&#x2F;se3.hpp&gt;#include &lt;chrono&gt;&#x2F;&#x2F;该程序用了三种方法实现位姿估计&#x2F;&#x2F;第一种，调用cv的函数pnp求解 R ,t&#x2F;&#x2F;第二种，手写高斯牛顿进行位姿优化&#x2F;&#x2F;第三种，利用g2o进行位姿优化using namespace std;using namespace cv;typedef vector&lt;Eigen::Vector2d, Eigen::aligned_allocator&lt;Eigen::Vector2d&gt;&gt; VecVector2d;&#x2F;&#x2F;VecVector2d可以定义存放二维向量的容器typedef vector&lt;Eigen::Vector3d, Eigen::aligned_allocator&lt;Eigen::Vector3d&gt;&gt; VecVector3d;&#x2F;&#x2F;VecVector3d可以定义存放三维向量的容器void find_feature_matches(        const Mat &amp;img_1, const Mat &amp;img_2,        std::vector&lt;KeyPoint&gt; &amp;keypoints_1,        std::vector&lt;KeyPoint&gt; &amp;keypoints_2,        std::vector&lt;DMatch&gt; &amp;matches);&#x2F;&#x2F; 像素坐标转相机归一化坐标Point2d pixel2cam(const Point2d &amp;p, const Mat &amp;K);&#x2F;&#x2F; BA by gauss-newton 手写高斯牛顿进行位姿优化void bundleAdjustmentGaussNewton(        const VecVector3d &amp;points_3d,        const VecVector2d &amp;points_2d,        const Mat &amp;K,        Sophus::SE3d &amp;pose);&#x2F;&#x2F;利用g2o优化posevoid bundleAdjustmentG2O(        const VecVector3d &amp;points_3d,        const VecVector2d &amp;points_2d,        const Mat &amp;K,        Sophus::SE3d &amp;pose);int main(int argc ,char** argv)&#123;    &#x2F;&#x2F;读取图片    if (argc !&#x3D; 5) &#123;        cout &lt;&lt; &quot;usage: pose_estimation_3d2d img1 img2 depth1 depth2&quot; &lt;&lt; endl;        return 1;    &#125;    &#x2F;&#x2F;读取图片    Mat img_1&#x3D;imread(argv[1],CV_LOAD_IMAGE_COLOR);&#x2F;&#x2F;读取彩色图    Mat img_2&#x3D;imread(argv[2],CV_LOAD_IMAGE_COLOR);    assert(img_1.data &amp;&amp; img_2.data &amp;&amp; &quot;Can Not load images!&quot;);&#x2F;&#x2F;若读取的图片没有内容，就终止程序    vector&lt;KeyPoint&gt; keypoints_1, keypoints_2;    vector&lt;DMatch&gt; matches;    find_feature_matches(img_1,img_2,keypoints_1,keypoints_2,matches);&#x2F;&#x2F;得到两个图片的特征匹配点    cout &lt;&lt; &quot;一共找到了&quot; &lt;&lt; matches.size() &lt;&lt; &quot;组匹配点&quot; &lt;&lt; endl;    &#x2F;&#x2F;建立3d点，把深度图信息读进来，构造三维点    Mat d1 &#x3D; imread(argv[3], CV_LOAD_IMAGE_UNCHANGED);       &#x2F;&#x2F; 深度图为16位无符号数，单通道图像    Mat K &#x3D; (Mat_&lt;double&gt;(3, 3) &lt;&lt; 520.9, 0, 325.1, 0, 521.0, 249.7, 0, 0, 1);    vector&lt;Point3d&gt; pts_3d;&#x2F;&#x2F;创建容器pts_3d存放3d点（图1对应的特征点的相机坐标下的3d点）    vector&lt;Point2d&gt; pts_2d;&#x2F;&#x2F;创建容器pts_2d存放图2的特征点    for(DMatch m:matches)    &#123;        &#x2F;&#x2F;把对应的图1的特征点的深度信息拿出来        ushort d &#x3D; d1.ptr&lt;unsigned short&gt;(int(keypoints_1[m.queryIdx].pt.y))[int(keypoints_1[m.queryIdx].pt.x)];        if(d&#x3D;&#x3D;0) &#x2F;&#x2F;深度有问题            continue;        float dd&#x3D;d&#x2F;5000.0;&#x2F;&#x2F;用dd存放换算过尺度的深度信息        Point2d p1&#x3D;pixel2cam(keypoints_1[m.queryIdx].pt,K);&#x2F;&#x2F;p1里面放的是图1特征点在相机坐标下的归一化坐标（只包含 x,y）        pts_3d.push_back(Point3d(p1.x*dd,p1.y*dd,dd));&#x2F;&#x2F;得到图1特征点在相机坐标下的3d坐标        pts_2d.push_back(keypoints_2[m.trainIdx].pt);&#x2F;&#x2F;得到图2特张点的像素坐标    &#125;    cout&lt;&lt;&quot;3d-2d pairs:&quot;&lt;&lt; pts_3d.size() &lt;&lt;endl;&#x2F;&#x2F;3d-2d配对个数得用pts_3d的size    cout&lt;&lt;&quot;使用cv求解 位姿&quot;&lt;&lt;endl;    cout&lt;&lt;&quot;***********************************opencv***********************************&quot;&lt;&lt;endl;    chrono::steady_clock::time_point t1 &#x3D; chrono::steady_clock::now();    Mat r, t;    &#x2F;&#x2F;Mat()这个参数指的是畸变系数向量？    solvePnP(pts_3d, pts_2d, K, Mat(), r, t, false); &#x2F;&#x2F; 调用OpenCV 的 PnP 求解，可选择EPNP，DLS等方法    Mat R;    cv::Rodrigues(r,R);&#x2F;&#x2F;r是旋转向量，利用cv的Rodrigues()函数将旋转向量转换为旋转矩阵    chrono::steady_clock::time_point t2 &#x3D; chrono::steady_clock::now();    chrono::duration&lt;double&gt; time_used &#x3D; chrono::duration_cast&lt;chrono::duration&lt;double&gt;&gt;(t2 - t1);    cout &lt;&lt; &quot;solve pnp in opencv cost time: &quot; &lt;&lt; time_used.count() &lt;&lt; &quot; seconds.&quot; &lt;&lt; endl;    cout &lt;&lt; &quot;R&#x3D;&quot; &lt;&lt; endl &lt;&lt; R &lt;&lt; endl;    cout &lt;&lt; &quot;t&#x3D;&quot; &lt;&lt; endl &lt;&lt; t &lt;&lt; endl;    cout&lt;&lt;&quot;***********************************opencv***********************************&quot;&lt;&lt;endl;    cout&lt;&lt;&quot;手写高斯牛顿优化位姿&quot;&lt;&lt;endl;    cout&lt;&lt;&quot;***********************************手写高斯牛顿***********************************&quot;&lt;&lt;endl;    VecVector3d pts_3d_eigen;&#x2F;&#x2F;存放3d点（图1对应的特征点的相机坐标下的3d点）    VecVector2d pts_2d_eigen;&#x2F;&#x2F;存放图2的特征点    for(size_t i&#x3D;0;i&lt;pts_3d.size();i++)&#x2F;&#x2F;size_t    &#123;        pts_3d_eigen.push_back(Eigen::Vector3d(pts_3d[i].x,pts_3d[i].y,pts_3d[i].z));        pts_2d_eigen.push_back(Eigen::Vector2d(pts_2d[i].x,pts_2d[i].y));    &#125;    Sophus::SE3d pose_gn;&#x2F;&#x2F;位姿（李群）    t1 &#x3D; chrono::steady_clock::now();    bundleAdjustmentGaussNewton(pts_3d_eigen, pts_2d_eigen, K, pose_gn);    t2 &#x3D; chrono::steady_clock::now();    time_used &#x3D; chrono::duration_cast&lt;chrono::duration&lt;double&gt;&gt;(t2 - t1);    cout &lt;&lt; &quot;solve pnp by gauss newton cost time: &quot; &lt;&lt; time_used.count() &lt;&lt; &quot; seconds.&quot; &lt;&lt; endl;    cout&lt;&lt;&quot;R &#x3D; \n&quot;&lt;&lt;pose_gn.rotationMatrix()&lt;&lt;endl;    cout&lt;&lt;&quot;t &#x3D; &quot;&lt;&lt;pose_gn.translation().transpose()&lt;&lt;endl;    cout&lt;&lt;&quot;***********************************手写高斯牛顿***********************************&quot;&lt;&lt;endl;    cout&lt;&lt;&quot;g2o优化位姿&quot;&lt;&lt;endl;    cout &lt;&lt; &quot;***********************************g2o***********************************&quot; &lt;&lt; endl;    Sophus::SE3d pose_g2o;    t1 &#x3D; chrono::steady_clock::now();    bundleAdjustmentG2O(pts_3d_eigen, pts_2d_eigen, K, pose_g2o);    t2 &#x3D; chrono::steady_clock::now();    time_used &#x3D; chrono::duration_cast&lt;chrono::duration&lt;double&gt;&gt;(t2 - t1);    cout &lt;&lt; &quot;solve pnp by g2o cost time: &quot; &lt;&lt; time_used.count() &lt;&lt; &quot; seconds.&quot; &lt;&lt; endl;    cout &lt;&lt; &quot;***********************************g2o***********************************&quot; &lt;&lt; endl;    return 0;&#125;&#x2F;&#x2F;实现特征匹配void find_feature_matches(const Mat &amp;img_1, const Mat &amp;img_2,                          std::vector&lt;KeyPoint&gt; &amp;keypoints_1,                          std::vector&lt;KeyPoint&gt; &amp;keypoints_2,                          std::vector&lt;DMatch&gt; &amp;matches) &#123;    &#x2F;&#x2F;-- 初始化    Mat descriptors_1, descriptors_2;    &#x2F;&#x2F; used in OpenCV3    Ptr&lt;FeatureDetector&gt; detector &#x3D; ORB::create();    Ptr&lt;DescriptorExtractor&gt; descriptor &#x3D; ORB::create();    &#x2F;&#x2F; use this if you are in OpenCV2    &#x2F;&#x2F; Ptr&lt;FeatureDetector&gt; detector &#x3D; FeatureDetector::create ( &quot;ORB&quot; );    &#x2F;&#x2F; Ptr&lt;DescriptorExtractor&gt; descriptor &#x3D; DescriptorExtractor::create ( &quot;ORB&quot; );    Ptr&lt;DescriptorMatcher&gt; matcher &#x3D; DescriptorMatcher::create(&quot;BruteForce-Hamming&quot;);    &#x2F;&#x2F;-- 第一步:检测 Oriented FAST 角点位置    detector-&gt;detect(img_1, keypoints_1);    detector-&gt;detect(img_2, keypoints_2);    &#x2F;&#x2F;-- 第二步:根据角点位置计算 BRIEF 描述子    descriptor-&gt;compute(img_1, keypoints_1, descriptors_1);    descriptor-&gt;compute(img_2, keypoints_2, descriptors_2);    &#x2F;&#x2F;-- 第三步:对两幅图像中的BRIEF描述子进行匹配，使用 Hamming 距离    vector&lt;DMatch&gt; match;    &#x2F;&#x2F; BFMatcher matcher ( NORM_HAMMING );    matcher-&gt;match(descriptors_1, descriptors_2, match);    &#x2F;&#x2F;-- 第四步:匹配点对筛选    double min_dist &#x3D; 10000, max_dist &#x3D; 0;    &#x2F;&#x2F;找出所有匹配之间的最小距离和最大距离, 即是最相似的和最不相似的两组点之间的距离    for (int i &#x3D; 0; i &lt; descriptors_1.rows; i++) &#123;        double dist &#x3D; match[i].distance;        if (dist &lt; min_dist) min_dist &#x3D; dist;        if (dist &gt; max_dist) max_dist &#x3D; dist;    &#125;    printf(&quot;-- Max dist : %f \n&quot;, max_dist);    printf(&quot;-- Min dist : %f \n&quot;, min_dist);    &#x2F;&#x2F;当描述子之间的距离大于两倍的最小距离时,即认为匹配有误.但有时候最小距离会非常小,设置一个经验值30作为下限.    for (int i &#x3D; 0; i &lt; descriptors_1.rows; i++) &#123;        if (match[i].distance &lt;&#x3D; max(2 * min_dist, 30.0)) &#123;            matches.push_back(match[i]);        &#125;    &#125;&#125;&#x2F;&#x2F;实现像素坐标到相机坐标的转换（求出来的只是包含相机坐标下的x,y的二维点）Point2d pixel2cam(const Point2d &amp;p, const Mat &amp;K) &#123;    return Point2d            (                    (p.x - K.at&lt;double&gt;(0, 2)) &#x2F; K.at&lt;double&gt;(0, 0),                    (p.y - K.at&lt;double&gt;(1, 2)) &#x2F; K.at&lt;double&gt;(1, 1)            );&#125;&#x2F;&#x2F;手写高斯牛顿void bundleAdjustmentGaussNewton(        const VecVector3d &amp;points_3d,        const VecVector2d &amp;points_2d,        const Mat &amp;K,        Sophus::SE3d &amp;pose)&#123;    typedef Eigen::Matrix&lt;double,6,1&gt; Vector6d;    const int iters&#x3D;10;&#x2F;&#x2F;迭代次数    double cost&#x3D;0,lastcost&#x3D;0;&#x2F;&#x2F;代价函数（目标函数）    &#x2F;&#x2F;拿出内参    double fx &#x3D; K.at&lt;double&gt;(0, 0);    double fy &#x3D; K.at&lt;double&gt;(1, 1);    double cx &#x3D; K.at&lt;double&gt;(0, 2);    double cy &#x3D; K.at&lt;double&gt;(1, 2);    &#x2F;&#x2F;进入迭代    for (int iter &#x3D; 0; iter &lt;iters ; iter++)    &#123;        Eigen::Matrix&lt;double,6,6&gt; H &#x3D; Eigen::Matrix&lt;double,6,6&gt;::Zero();&#x2F;&#x2F;初始化H矩阵        Vector6d b &#x3D; Vector6d::Zero();&#x2F;&#x2F;对b矩阵初始化        cost &#x3D; 0;        &#x2F;&#x2F; 遍历所有的特征点  计算cost        for(int i&#x3D;0;i&lt;points_3d.size();i++)        &#123;            Eigen::Vector3d pc&#x3D;pose*points_3d[i];&#x2F;&#x2F;利用待优化的pose得到图2的相机坐标下的3d点            double inv_z&#x3D;1.0&#x2F;pc[2];&#x2F;&#x2F;得到图2的相机坐标下的3d点的z的倒数，也就是1&#x2F;z            double inv_z2 &#x3D; inv_z * inv_z;&#x2F;&#x2F;(1&#x2F;z)^2            &#x2F;&#x2F;定义投影            Eigen::Vector2d proj(fx * pc[0] &#x2F; pc[2] + cx, fy * pc[1] &#x2F; pc[2] + cy);            &#x2F;&#x2F;定义误差            Eigen::Vector2d e&#x3D;points_2d[i]-proj;            cost +&#x3D; e.squaredNorm();&#x2F;&#x2F;cost&#x3D;e*e            &#x2F;&#x2F;定义雅克比矩阵J            Eigen::Matrix&lt;double, 2, 6&gt; J;            J &lt;&lt; -fx * inv_z,                    0,                    fx * pc[0] * inv_z2,                    fx * pc[0] * pc[1] * inv_z2,                    -fx - fx * pc[0] * pc[0] * inv_z2,                    fx * pc[1] * inv_z,                    0,                    -fy * inv_z,                    fy * pc[1] * inv_z2,                    fy + fy * pc[1] * pc[1] * inv_z2,                    -fy * pc[0] * pc[1] * inv_z2,                    -fy * pc[0] * inv_z;            H +&#x3D; J.transpose() * J;            b +&#x3D; -J.transpose() * e;        &#125;        &#x2F;&#x2F;出了这个内循环，表述结束一次迭代的计算，接下来，要求pose了        Vector6d dx;&#x2F;&#x2F;P129页 公式6.33 计算增量方程 Hdx&#x3D;b        dx &#x3D; H.ldlt().solve(b);&#x2F;&#x2F;算出增量dx        &#x2F;&#x2F;判断dx这个数是否有效        if (isnan(dx[0]))        &#123;            cout &lt;&lt; &quot;result is nan!&quot; &lt;&lt; endl;            break;        &#125;        &#x2F;&#x2F;如果我们进行了迭代，且最后的cost&gt;&#x3D;lastcost的话，那就表明满足要求了，可以停止迭代了        if (iter &gt; 0 &amp;&amp; cost &gt;&#x3D; lastcost)        &#123;            &#x2F;&#x2F; cost increase, update is not good            cout &lt;&lt; &quot;cost: &quot; &lt;&lt; cost &lt;&lt; &quot;, last cost: &quot; &lt;&lt; lastcost &lt;&lt; endl;            break;        &#125;        &#x2F;&#x2F;优化pose 也就是用dx更新pose        pose&#x3D;Sophus::SE3d::exp(dx) * pose;&#x2F;&#x2F;dx是李代数，要转换为李群        lastcost&#x3D;cost;        cout &lt;&lt; &quot;iteration &quot; &lt;&lt; iter &lt;&lt; &quot; cost&#x3D;&quot;&lt;&lt; std::setprecision(12) &lt;&lt; cost &lt;&lt; endl;        &#x2F;&#x2F;std::setprecision(12)浮点数控制位数为12位        &#x2F;&#x2F;如果误差特别小了，也结束迭代        if (dx.norm() &lt; 1e-6)        &#123;            &#x2F;&#x2F; converge            break;        &#125;    &#125;    cout&lt;&lt;&quot;pose by g-n \n&quot;&lt;&lt;pose.matrix()&lt;&lt;endl;&#125;&#x2F;&#x2F;对于用g2o来进行优化的话，首先要定义顶点和边的模板&#x2F;&#x2F;顶点，也就是咱们要优化的pose 用李代数表示它 6维class Vertexpose: public g2o::BaseVertex&lt;6,Sophus::SE3d&gt;L&#123;public:    EIGEN_MAKE_ALIGNED_OPERATOR_NEW;&#x2F;&#x2F;必须写，我也不知道为什么    &#x2F;&#x2F;重载setToOriginImpl函数 这个应该就是把刚开的待优化的pose放进去    virtual void setToOriginImpl() override    &#123;        _estimate &#x3D; Sophus::SE3d();    &#125;    &#x2F;&#x2F;重载oplusImpl函数，用来更新pose（待优化的系数）    virtual void oplusImpl(const double *update) override    &#123;        Eigen::Matrix&lt;double,6,1&gt; update_eigen;&#x2F;&#x2F;更新的量，就是增量呗，dx        update_eigen &lt;&lt; update[0], update[1], update[2], update[3], update[4], update[5];        _estimate&#x3D;Sophus::SE3d::exp(update_eigen)* _estimate;&#x2F;&#x2F;更新pose 李代数要转换为李群，这样才可以左乘    &#125;    &#x2F;&#x2F;存盘 读盘 ：留空    virtual bool read(istream &amp;in) override &#123;&#125;    virtual bool write(ostream &amp;out) const override &#123;&#125;&#125;;&#x2F;&#x2F;定义边模板 边也就是误差，二维 并且把顶点也放进去class EdgeProjection : public g2o::BaseUnaryEdge&lt;2,Eigen::Vector2d,Vertexpose&gt;&#123;public:    EIGEN_MAKE_ALIGNED_OPERATOR_NEW;&#x2F;&#x2F;必须写，我也不知道为什么    &#x2F;&#x2F;有参构造，初始化 图1中的3d点 以及相机内参K    EdgeProjection(const Eigen::Vector3d &amp;pos, const Eigen::Matrix3d &amp;K) : _pos3d(pos),_K(K) &#123;&#125;    &#x2F;&#x2F;计算误差    virtual void computeError() override    &#123;        const Vertexpose *v&#x3D;static_cast&lt;const Vertexpose *&gt;(_vertices[0]);        Sophus::SE3d T&#x3D;v-&gt;estimate();        Eigen::Vector3d pos_pixel &#x3D; _K * (T * _pos3d);&#x2F;&#x2F;T * _pos3d是图2的相机坐标下的3d点        pos_pixel &#x2F;&#x3D; pos_pixel[2];&#x2F;&#x2F;得到了像素坐标的齐次形式        _error &#x3D; _measurement - pos_pixel.head&lt;2&gt;();    &#125;    &#x2F;&#x2F;计算雅克比矩阵    virtual void linearizeOplus() override    &#123;        const Vertexpose *v &#x3D; static_cast&lt;Vertexpose *&gt; (_vertices[0]);        Sophus::SE3d T &#x3D; v-&gt;estimate();        Eigen::Vector3d pos_cam&#x3D;T*_pos3d;&#x2F;&#x2F;图2的相机坐标下的3d点        double fx &#x3D; _K(0, 0);        double fy &#x3D; _K(1, 1);        double cx &#x3D; _K(0, 2);        double cy &#x3D; _K(1, 2);        double X &#x3D; pos_cam[0];        double Y &#x3D; pos_cam[1];        double Z &#x3D; pos_cam[2];        double Z2 &#x3D; Z * Z;        &#x2F;&#x2F;雅克比矩阵见 书 p187 公式7.46        _jacobianOplusXi                &lt;&lt; -fx &#x2F; Z, 0, fx * X &#x2F; Z2, fx * X * Y &#x2F; Z2, -fx - fx * X * X &#x2F; Z2, fx * Y &#x2F; Z,                0, -fy &#x2F; Z, fy * Y &#x2F; (Z * Z), fy + fy * Y * Y &#x2F; Z2, -fy * X * Y &#x2F; Z2, -fy * X &#x2F; Z;    &#125;    &#x2F;&#x2F;存盘 读盘 ：留空    virtual bool read(istream &amp;in) override &#123;&#125;    virtual bool write(ostream &amp;out) const override &#123;&#125;private:    Eigen::Vector3d _pos3d;    Eigen::Matrix3d _K;&#125;;&#x2F;&#x2F;利用g2o优化posevoid bundleAdjustmentG2O(        const VecVector3d &amp;points_3d,        const VecVector2d &amp;points_2d,        const Mat &amp;K,        Sophus::SE3d &amp;pose)&#123;    &#x2F;&#x2F; 构建图优化，先设定g2o    typedef g2o::BlockSolver&lt;g2o::BlockSolverTraits&lt;6, 3&gt;&gt; BlockSolverType;  &#x2F;&#x2F;  优化系数pose is 6, 数据点 landmark is 3    typedef g2o::LinearSolverDense&lt;BlockSolverType::PoseMatrixType&gt; LinearSolverType; &#x2F;&#x2F; 线性求解器类型    &#x2F;&#x2F; 梯度下降方法，可以从GN, LM, DogLeg 中选    auto solver &#x3D; new g2o::OptimizationAlgorithmGaussNewton(            g2o::make_unique&lt;BlockSolverType&gt;                    (g2o::make_unique&lt;LinearSolverType&gt;()));&#x2F;&#x2F;把设定的类型都放进求解器    g2o::SparseOptimizer optimizer;     &#x2F;&#x2F; 图模型    optimizer.setAlgorithm(solver);   &#x2F;&#x2F; 设置求解器 算法g-n    optimizer.setVerbose(true);       &#x2F;&#x2F; 打开调试输出    &#x2F;&#x2F;加入顶点    Vertexpose *v&#x3D;new Vertexpose();    v-&gt;setEstimate(Sophus::SE3d());    v-&gt;setId(0);    optimizer.addVertex(v);    &#x2F;&#x2F;K    &#x2F;&#x2F; K    Eigen::Matrix3d K_eigen;    K_eigen &lt;&lt;            K.at&lt;double&gt;(0, 0), K.at&lt;double&gt;(0, 1), K.at&lt;double&gt;(0, 2),            K.at&lt;double&gt;(1, 0), K.at&lt;double&gt;(1, 1), K.at&lt;double&gt;(1, 2),            K.at&lt;double&gt;(2, 0), K.at&lt;double&gt;(2, 1), K.at&lt;double&gt;(2, 2);    &#x2F;&#x2F;加入边    int index&#x3D;1;    for(size_t i&#x3D;0;i&lt;points_2d.size();++i)    &#123;        &#x2F;&#x2F;遍历 把3d点和像素点拿出来        auto p2d &#x3D; points_2d[i];        auto p3d &#x3D; points_3d[i];        EdgeProjection *edge &#x3D; new EdgeProjection(p3d, K_eigen);&#x2F;&#x2F;有参构造        edge-&gt;setId(index);        edge-&gt;setVertex(0,v);        edge-&gt;setMeasurement(p2d);&#x2F;&#x2F;设置观测值，其实就是图2 里的匹配特征点的像素位置        edge-&gt;setInformation(Eigen::Matrix2d::Identity());&#x2F;&#x2F;信息矩阵是二维方阵，因为误差是二维        optimizer.addEdge(edge);&#x2F;&#x2F;加入边        index++;&#x2F;&#x2F;边的编号++    &#125;    chrono::steady_clock::time_point t1 &#x3D; chrono::steady_clock::now();    optimizer.setVerbose(true);    optimizer.initializeOptimization();&#x2F;&#x2F;开始  初始化    optimizer.optimize(10);&#x2F;&#x2F;迭代次数    chrono::steady_clock::time_point t2 &#x3D; chrono::steady_clock::now();    chrono::duration&lt;double&gt; time_used &#x3D; chrono::duration_cast&lt;chrono::duration&lt;double&gt;&gt;(t2 - t1);    cout &lt;&lt; &quot;optimization costs time: &quot; &lt;&lt; time_used.count() &lt;&lt; &quot; seconds.&quot; &lt;&lt; endl;    cout &lt;&lt; &quot;pose estimated by g2o &#x3D;\n&quot; &lt;&lt; v-&gt;estimate().matrix() &lt;&lt; endl;    pose &#x3D; v-&gt;estimate();&#125;<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p><a href="https://blog.csdn.net/joun772/article/details/109466153?utm_medium=distribute.pc_relevant.none-task-blog-2~default~baidujs_baidulandingword~default-12-109466153-blog-116243451.235%5Ev28%5Epc_relevant_recovery_v2&spm=1001.2101.3001.4242.7&utm_relevant_index=15">SLAM十四讲-ch7(2)-位姿估计(包含2d-2d、3d-2d、3d-3d、以及三角化实现代码的注释)</a></p><h3 id="ch7-x2F-pose-estimation-3d3d-cpp"><a href="#ch7-x2F-pose-estimation-3d3d-cpp" class="headerlink" title="ch7&#x2F;pose_estimation_3d3d.cpp"></a>ch7&#x2F;pose_estimation_3d3d.cpp</h3><pre class="line-numbers language-c++" data-language="c++"><code class="language-c++">&#x2F;&#x2F;&#x2F;&#x2F; Created by nnz on 2020&#x2F;11&#x2F;5.&#x2F;&#x2F;#include &lt;iostream&gt;#include &lt;opencv2&#x2F;core&#x2F;core.hpp&gt;#include &lt;opencv2&#x2F;features2d&#x2F;features2d.hpp&gt;#include &lt;opencv2&#x2F;highgui&#x2F;highgui.hpp&gt;#include &lt;opencv2&#x2F;calib3d&#x2F;calib3d.hpp&gt;#include &lt;Eigen&#x2F;Core&gt;#include &lt;g2o&#x2F;core&#x2F;base_vertex.h&gt;#include &lt;g2o&#x2F;core&#x2F;base_unary_edge.h&gt;#include &lt;g2o&#x2F;core&#x2F;sparse_optimizer.h&gt;#include &lt;g2o&#x2F;core&#x2F;block_solver.h&gt;#include &lt;g2o&#x2F;core&#x2F;solver.h&gt;#include &lt;g2o&#x2F;core&#x2F;optimization_algorithm_gauss_newton.h&gt;#include &lt;g2o&#x2F;solvers&#x2F;dense&#x2F;linear_solver_dense.h&gt;#include &lt;sophus&#x2F;se3.hpp&gt;#include &lt;chrono&gt;using namespace std;using namespace cv;void find_feature_matches(        const Mat &amp;img_1, const Mat &amp;img_2,        std::vector&lt;KeyPoint&gt; &amp;keypoints_1,        std::vector&lt;KeyPoint&gt; &amp;keypoints_2,        std::vector&lt;DMatch&gt; &amp;matches);&#x2F;&#x2F; 像素坐标转相机归一化坐标Point2d pixel2cam(const Point2d &amp;p, const Mat &amp;K);void pose_estimation_3d3d(        const vector&lt;Point3f&gt; &amp;pts1,        const vector&lt;Point3f&gt; &amp;pts2,        Mat &amp;R, Mat &amp;t);&#x2F;&#x2F;void bundleAdjustment(        const vector&lt;Point3f&gt; &amp;pts1,        const vector&lt;Point3f&gt; &amp;pts2,        Mat &amp;R, Mat &amp;t);int main(int argc,char** argv)&#123;    if(argc!&#x3D;5)    &#123;        cout&lt;&lt;&quot; usage: pose_estimation_3d3d img1 img2 depth1 depth2 &quot;&lt;&lt;endl;        return 1;    &#125;    &#x2F;&#x2F;读取图片    Mat img_1&#x3D;imread(argv[1],CV_LOAD_IMAGE_COLOR);&#x2F;&#x2F;读取彩色图    Mat img_2&#x3D;imread(argv[2],CV_LOAD_IMAGE_COLOR);    vector&lt;KeyPoint&gt; keypoints_1, keypoints_2;&#x2F;&#x2F;容器keypoints_1, keypoints_2分别存放图1和图2的特征点    vector&lt;DMatch&gt; matches;    find_feature_matches(img_1, img_2, keypoints_1, keypoints_2, matches);&#x2F;&#x2F;得到图1与图2的特征匹配点    cout &lt;&lt; &quot;一共找到了&quot; &lt;&lt; matches.size() &lt;&lt; &quot;组匹配点&quot; &lt;&lt; endl;    &#x2F;&#x2F;接下来的是建立3d点 利用深度图可以获取深度信息    &#x2F;&#x2F;depth1是图1对应的深度图 depth2是图2对应的深度图    Mat depth1 &#x3D; imread(argv[3], CV_LOAD_IMAGE_UNCHANGED);       &#x2F;&#x2F; 深度图为16位无符号数，单通道图像    Mat depth2 &#x3D; imread(argv[4], CV_LOAD_IMAGE_UNCHANGED);    &#x2F;&#x2F;内参矩阵    Mat K &#x3D;(Mat_&lt;double&gt;(3, 3) &lt;&lt; 520.9, 0, 325.1, 0, 521.0, 249.7, 0, 0, 1);    vector&lt;Point3f&gt; pts1, pts2;    for(DMatch m:matches)    &#123;        &#x2F;&#x2F;先把两图特征匹配点对应的深度拿出来        ushort d1&#x3D;depth1.ptr&lt;unsigned short &gt;(int(keypoints_1[m.queryIdx].pt.y))[int(keypoints_1[m.queryIdx].pt.x)];        ushort d2&#x3D;depth2.ptr&lt;unsigned short &gt;(int(keypoints_2[m.trainIdx].pt.y))[int(keypoints_2[m.trainIdx].pt.x)];        if(d1&#x3D;&#x3D;0 || d2&#x3D;&#x3D;0)&#x2F;&#x2F;深度无效            continue;        Point2d p1&#x3D;pixel2cam(keypoints_1[m.queryIdx].pt,K);&#x2F;&#x2F;得到图1的特征匹配点在其相机坐标下的x ,y        Point2d p2&#x3D;pixel2cam(keypoints_2[m.trainIdx].pt,K);&#x2F;&#x2F;得到图2的特征匹配点在其相机坐标下的x ,y        &#x2F;&#x2F;对深度进行尺度变化得到真正的深度        float dd1 &#x3D; float(d1) &#x2F; 5000.0;        float dd2 &#x3D; float(d2) &#x2F; 5000.0;        &#x2F;&#x2F;容器 pts_1与pts_2分别存放 图1中的特征匹配点其相机坐标下的3d点 和 图2中的特征匹配点其相机坐标下的3d点        pts1.push_back(Point3f(p1.x * dd1, p1.y * dd1, dd1));        pts2.push_back(Point3f(p2.x * dd2, p2.y * dd2, dd2));    &#125;    &#x2F;&#x2F;这样就可以得到 3d-3d的匹配点    cout &lt;&lt; &quot;3d-3d pairs: &quot; &lt;&lt; pts1.size() &lt;&lt; endl;    Mat R, t;    cout&lt;&lt;&quot; ************************************ICP 3d-3d by SVD**************************************** &quot;&lt;&lt;endl;    pose_estimation_3d3d(pts1, pts2, R, t);    cout &lt;&lt; &quot;ICP via SVD results: &quot; &lt;&lt; endl;    cout &lt;&lt; &quot;R &#x3D; &quot; &lt;&lt; R &lt;&lt; endl;    cout &lt;&lt; &quot;t &#x3D; &quot; &lt;&lt; t &lt;&lt; endl;    cout &lt;&lt; &quot;R^T &#x3D; &quot; &lt;&lt; R.t() &lt;&lt; endl;    cout &lt;&lt; &quot;t^T &#x3D; &quot; &lt;&lt; -R.t() * t &lt;&lt; endl;    cout&lt;&lt;&quot; ************************************ICP 3d-3d by SVD**************************************** &quot;&lt;&lt;endl;    cout&lt;&lt;endl;    cout&lt;&lt;&quot; ************************************ICP 3d-3d by g2o**************************************** &quot;&lt;&lt;endl;    bundleAdjustment(pts1, pts2, R, t);    cout&lt;&lt;&quot;R&#x3D; \n&quot;&lt;&lt;R&lt;&lt;endl;    cout&lt;&lt;&quot;t &#x3D; &quot;&lt;&lt; t.t() &lt;&lt;endl;    cout&lt;&lt;&quot;验证 p2 &#x3D; R*P1 +t &quot;&lt;&lt;endl;    for (int i &#x3D; 0; i &lt; 5; i++) &#123;        cout &lt;&lt; &quot;p1 &#x3D; &quot; &lt;&lt; pts1[i] &lt;&lt; endl;        cout &lt;&lt; &quot;p2 &#x3D; &quot; &lt;&lt; pts2[i] &lt;&lt; endl;        cout &lt;&lt; &quot;(R*p1+t) &#x3D; &quot; &lt;&lt;             R * (Mat_&lt;double&gt;(3, 1) &lt;&lt; pts1[i].x, pts1[i].y, pts1[i].z) + t             &lt;&lt; endl;        cout &lt;&lt; endl;    &#125;    cout&lt;&lt;&quot; ************************************ICP 3d-3d by g2o**************************************** &quot;&lt;&lt;endl;    return 0;&#125;void find_feature_matches(const Mat &amp;img_1, const Mat &amp;img_2,                          std::vector&lt;KeyPoint&gt; &amp;keypoints_1,                          std::vector&lt;KeyPoint&gt; &amp;keypoints_2,                          std::vector&lt;DMatch&gt; &amp;matches) &#123;    &#x2F;&#x2F;-- 初始化    Mat descriptors_1, descriptors_2;    &#x2F;&#x2F; used in OpenCV3    Ptr&lt;FeatureDetector&gt; detector &#x3D; ORB::create();    Ptr&lt;DescriptorExtractor&gt; descriptor &#x3D; ORB::create();    &#x2F;&#x2F; use this if you are in OpenCV2    &#x2F;&#x2F; Ptr&lt;FeatureDetector&gt; detector &#x3D; FeatureDetector::create ( &quot;ORB&quot; );    &#x2F;&#x2F; Ptr&lt;DescriptorExtractor&gt; descriptor &#x3D; DescriptorExtractor::create ( &quot;ORB&quot; );    Ptr&lt;DescriptorMatcher&gt; matcher &#x3D; DescriptorMatcher::create(&quot;BruteForce-Hamming&quot;);    &#x2F;&#x2F;-- 第一步:检测 Oriented FAST 角点位置    detector-&gt;detect(img_1, keypoints_1);    detector-&gt;detect(img_2, keypoints_2);    &#x2F;&#x2F;-- 第二步:根据角点位置计算 BRIEF 描述子    descriptor-&gt;compute(img_1, keypoints_1, descriptors_1);    descriptor-&gt;compute(img_2, keypoints_2, descriptors_2);    &#x2F;&#x2F;-- 第三步:对两幅图像中的BRIEF描述子进行匹配，使用 Hamming 距离    vector&lt;DMatch&gt; match;    &#x2F;&#x2F; BFMatcher matcher ( NORM_HAMMING );    matcher-&gt;match(descriptors_1, descriptors_2, match);    &#x2F;&#x2F;-- 第四步:匹配点对筛选    double min_dist &#x3D; 10000, max_dist &#x3D; 0;    &#x2F;&#x2F;找出所有匹配之间的最小距离和最大距离, 即是最相似的和最不相似的两组点之间的距离    for (int i &#x3D; 0; i &lt; descriptors_1.rows; i++) &#123;        double dist &#x3D; match[i].distance;        if (dist &lt; min_dist) min_dist &#x3D; dist;        if (dist &gt; max_dist) max_dist &#x3D; dist;    &#125;    printf(&quot;-- Max dist : %f \n&quot;, max_dist);    printf(&quot;-- Min dist : %f \n&quot;, min_dist);    &#x2F;&#x2F;当描述子之间的距离大于两倍的最小距离时,即认为匹配有误.但有时候最小距离会非常小,设置一个经验值30作为下限.    for (int i &#x3D; 0; i &lt; descriptors_1.rows; i++) &#123;        if (match[i].distance &lt;&#x3D; max(2 * min_dist, 30.0)) &#123;            matches.push_back(match[i]);        &#125;    &#125;&#125;&#x2F;&#x2F;实现像素坐标到相机坐标的转换（求出来的只是包含相机坐标下的x,y的二维点）Point2d pixel2cam(const Point2d &amp;p, const Mat &amp;K) &#123;    return Point2d            (                    (p.x - K.at&lt;double&gt;(0, 2)) &#x2F; K.at&lt;double&gt;(0, 0),                    (p.y - K.at&lt;double&gt;(1, 2)) &#x2F; K.at&lt;double&gt;(1, 1)            );&#125;&#x2F;&#x2F;参考书上的p197页void pose_estimation_3d3d(        const vector&lt;Point3f&gt; &amp;pts1,        const vector&lt;Point3f&gt; &amp;pts2,        Mat &amp;R, Mat &amp;t)&#123;    int N&#x3D;pts1.size();&#x2F;&#x2F;匹配的3d点个数    Point3f p1,p2;&#x2F;&#x2F;质心    for(int i&#x3D;0;i&lt;N;i++)    &#123;        p1+&#x3D;pts1[i];        p2+&#x3D;pts2[i];    &#125;    p1 &#x3D; Point3f(Vec3f(p1)&#x2F;N);&#x2F;&#x2F;得到质心    p2 &#x3D; Point3f(Vec3f(p2) &#x2F; N);    vector&lt;Point3f&gt; q1(N),q2(N);    for(int i&#x3D;0;i&lt;N;i++)    &#123;        &#x2F;&#x2F;去质心        q1[i]&#x3D;pts1[i]-p1;        q2[i]&#x3D;pts2[i]-p2;    &#125;    &#x2F;&#x2F;计算 W+&#x3D;q1*q2^T(求和)    Eigen::Matrix3d W&#x3D;Eigen::Matrix3d::Zero();&#x2F;&#x2F;初始化    for(int i&#x3D;0;i&lt;N;i++)    &#123;        W+&#x3D; Eigen::Vector3d (q1[i].x,q1[i].y,q1[i].z)*(Eigen::Vector3d (q2[i].x,q2[i].y,q2[i].z).transpose());    &#125;    cout&lt;&lt;&quot;W &#x3D; &quot;&lt;&lt;endl&lt;&lt;W&lt;&lt;endl;    &#x2F;&#x2F;利用svd分解 W&#x3D;U*sigema*V    Eigen::JacobiSVD&lt;Eigen::Matrix3d&gt; svd(W,Eigen::ComputeFullU | Eigen::ComputeFullV);    Eigen::Matrix3d U&#x3D;svd.matrixU();&#x2F;&#x2F;得到U矩阵    Eigen::Matrix3d V&#x3D;svd.matrixV();&#x2F;&#x2F;得到V矩阵    cout &lt;&lt; &quot;U&#x3D;&quot; &lt;&lt; U &lt;&lt; endl;    cout &lt;&lt; &quot;V&#x3D;&quot; &lt;&lt; V &lt;&lt; endl;    Eigen::Matrix3d R_&#x3D;U*(V.transpose());    if (R_.determinant() &lt; 0)&#x2F;&#x2F;若旋转矩阵R_的行列式&lt;0 则取负号    &#123;        R_ &#x3D; -R_;    &#125;    Eigen::Vector3d t_&#x3D;Eigen::Vector3d (p1.x,p1.y,p1.z)-R_*Eigen::Vector3d (p2.x,p2.y,p2.z);&#x2F;&#x2F;得到平移向量    &#x2F;&#x2F;把 Eigen形式的 r 和 t_ 转换为CV 中的Mat格式    R &#x3D; (Mat_&lt;double&gt;(3, 3) &lt;&lt;                            R_(0, 0), R_(0, 1), R_(0, 2),            R_(1, 0), R_(1, 1), R_(1, 2),            R_(2, 0), R_(2, 1), R_(2, 2)    );    t &#x3D; (Mat_&lt;double&gt;(3, 1) &lt;&lt; t_(0, 0), t_(1, 0), t_(2, 0));&#125;&#x2F;&#x2F;对于用g2o来进行优化的话，首先要定义顶点和边的模板&#x2F;&#x2F;顶点，也就是咱们要优化的pose 用李代数表示它 6维class Vertexpose: public g2o::BaseVertex&lt;6,Sophus::SE3d&gt;&#123;public:    EIGEN_MAKE_ALIGNED_OPERATOR_NEW;&#x2F;&#x2F;必须写，我也不知道为什么    &#x2F;&#x2F;重载setToOriginImpl函数 这个应该就是把刚开的待优化的pose放进去    virtual void setToOriginImpl() override    &#123;        _estimate &#x3D; Sophus::SE3d();    &#125;    &#x2F;&#x2F;重载oplusImpl函数，用来更新pose（待优化的系数）    virtual void oplusImpl(const double *update) override    &#123;        Eigen::Matrix&lt;double,6,1&gt; update_eigen;&#x2F;&#x2F;更新的量，就是增量呗，dx        update_eigen &lt;&lt; update[0], update[1], update[2], update[3], update[4], update[5];        _estimate&#x3D;Sophus::SE3d::exp(update_eigen)* _estimate;&#x2F;&#x2F;更新pose 李代数要转换为李群，这样才可以左乘    &#125;    &#x2F;&#x2F;存盘 读盘 ：留空    virtual bool read(istream &amp;in) override &#123;&#125;    virtual bool write(ostream &amp;out) const override &#123;&#125;&#125;;&#x2F;&#x2F;定义边class EdgeProjectXYZRGBD: public g2o::BaseUnaryEdge&lt;3,Eigen::Vector3d,Vertexpose&gt;&#123;public:    EdgeProjectXYZRGBD(const Eigen::Vector3d &amp;point) : _point(point) &#123;&#125;&#x2F;&#x2F;赋值这个是图1坐标下的3d点    &#x2F;&#x2F;计算误差    virtual void computeError() override    &#123;        const Vertexpose *v&#x3D;static_cast&lt;const Vertexpose *&gt;(_vertices[0]);&#x2F;&#x2F;顶点v        _error &#x3D; _measurement - v-&gt;estimate() * _point;    &#125;    &#x2F;&#x2F;计算雅克比    virtual void linearizeOplus() override    &#123;        const Vertexpose *v&#x3D;static_cast&lt;const Vertexpose *&gt;(_vertices[0]);&#x2F;&#x2F;顶点v        Sophus::SE3d T&#x3D;v-&gt;estimate();&#x2F;&#x2F;把顶点的待优化系数拿出来        Eigen::Vector3d xyz_trans&#x3D;T*_point;&#x2F;&#x2F;变换到图2下的坐标点        &#x2F;&#x2F;下面的雅克比没看懂        _jacobianOplusXi.block&lt;3, 3&gt;(0, 0) &#x3D; -Eigen::Matrix3d::Identity();        _jacobianOplusXi.block&lt;3, 3&gt;(0, 3) &#x3D; Sophus::SO3d::hat(xyz_trans);    &#125;    bool read(istream &amp;in) &#123;&#125;    bool write(ostream &amp;out) const &#123;&#125;protected:    Eigen::Vector3d _point;&#125;;&#x2F;&#x2F;利用g2ovoid bundleAdjustment(const vector&lt;Point3f&gt; &amp;pts1,                      const vector&lt;Point3f&gt; &amp;pts2,                      Mat &amp;R, Mat &amp;t)&#123;&#x2F;&#x2F; 构建图优化，先设定g2o    typedef g2o::BlockSolver&lt;g2o::BlockSolverTraits&lt;6, 3&gt;&gt; BlockSolverType;  &#x2F;&#x2F;  优化系数pose is 6, 数据点 landmark is 3    typedef g2o::LinearSolverDense&lt;BlockSolverType::PoseMatrixType&gt; LinearSolverType; &#x2F;&#x2F; 线性求解器类型    &#x2F;&#x2F; 梯度下降方法，可以从GN, LM, DogLeg 中选    auto solver &#x3D; new g2o::OptimizationAlgorithmGaussNewton(            g2o::make_unique&lt;BlockSolverType&gt;                    (g2o::make_unique&lt;LinearSolverType&gt;()));&#x2F;&#x2F;把设定的类型都放进求解器    g2o::SparseOptimizer optimizer;     &#x2F;&#x2F; 图模型    optimizer.setAlgorithm(solver);   &#x2F;&#x2F; 设置求解器 算法g-n    optimizer.setVerbose(true);       &#x2F;&#x2F; 打开调试输出    &#x2F;&#x2F;加入顶点    Vertexpose *v&#x3D;new Vertexpose();    v-&gt;setEstimate(Sophus::SE3d());    v-&gt;setId(0);    optimizer.addVertex(v);    &#x2F;&#x2F;加入边    int index&#x3D;1;    for(size_t i&#x3D;0;i&lt;pts1.size();i++)    &#123;        EdgeProjectXYZRGBD *edge &#x3D; new EdgeProjectXYZRGBD(Eigen::Vector3d(pts1[i].x,pts1[i].y,pts1[i].z));        edge-&gt;setId(index);&#x2F;&#x2F;边的编号        edge-&gt;setVertex(0,v);&#x2F;&#x2F;设置顶点  顶点编号        edge-&gt;setMeasurement(Eigen::Vector3d(pts2[i].x,pts2[i].y,pts2[i].z));        edge-&gt;setInformation(Eigen::Matrix3d::Identity());&#x2F;&#x2F;set信息矩阵为单位矩阵        optimizer.addEdge(edge);&#x2F;&#x2F;加入边        index++;    &#125;    chrono::steady_clock::time_point t1 &#x3D; chrono::steady_clock::now();    optimizer.initializeOptimization();&#x2F;&#x2F;开始    optimizer.optimize(10);&#x2F;&#x2F;迭代次数    chrono::steady_clock::time_point t2 &#x3D; chrono::steady_clock::now();    chrono::duration&lt;double&gt; time_used &#x3D; chrono::duration_cast&lt;chrono::duration&lt;double&gt;&gt;(t2 - t1);    cout &lt;&lt; &quot;optimization costs time: &quot; &lt;&lt; time_used.count() &lt;&lt; &quot; seconds.&quot; &lt;&lt; endl;    cout &lt;&lt; endl &lt;&lt; &quot;after optimization:&quot; &lt;&lt; endl;    cout &lt;&lt; &quot;T&#x3D;\n&quot; &lt;&lt; v-&gt;estimate().matrix() &lt;&lt; endl;    &#x2F;&#x2F; 把位姿转换为Mat类型    Eigen::Matrix3d R_ &#x3D; v-&gt;estimate().rotationMatrix();    Eigen::Vector3d t_ &#x3D; v-&gt;estimate().translation();    R &#x3D; (Mat_&lt;double&gt;(3, 3) &lt;&lt;                            R_(0, 0), R_(0, 1), R_(0, 2),            R_(1, 0), R_(1, 1), R_(1, 2),            R_(2, 0), R_(2, 1), R_(2, 2)    );    t &#x3D; (Mat_&lt;double&gt;(3, 1) &lt;&lt; t_(0, 0), t_(1, 0), t_(2, 0));&#125;<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p><a href="https://blog.csdn.net/joun772/article/details/109466153?utm_medium=distribute.pc_relevant.none-task-blog-2~default~baidujs_baidulandingword~default-12-109466153-blog-116243451.235%5Ev28%5Epc_relevant_recovery_v2&spm=1001.2101.3001.4242.7&utm_relevant_index=15">SLAM十四讲-ch7(2)-位姿估计(包含2d-2d、3d-2d、3d-3d、以及三角化实现代码的注释)</a></p><h2 id="第八讲"><a href="#第八讲" class="headerlink" title="第八讲"></a>第八讲</h2><p>直接法是vo的另一个主要的分支，它与特征点法有很大的不同，理解光流法跟踪特征点的原理，理解直接法估计相机位姿，实现多层直接法的计算</p><h3 id="ch8-x2F-optical-flow-cpp"><a href="#ch8-x2F-optical-flow-cpp" class="headerlink" title="ch8&#x2F;optical_flow.cpp"></a>ch8&#x2F;optical_flow.cpp</h3><p>光流是一种描述像素随时间在图像之间运动的方法：</p><p>光流法有两个假设：（1）灰度不变假设：同一个空间点的像素灰度值，在各个图像中是固定不变的；（2）假设某个窗口内的像素具有相同的运动</p><p>一、本讲的代码使用了三种方法来追踪图像上的特征点</p><ul><li>第一种：使用OpenCV中的LK光流；</li><li>第二种：用高斯牛顿实现光流：单层光流；</li><li>第三种：用高斯牛顿实现光流：多层光流。</li></ul><p>其中高斯牛顿法，即最小化灰度误差估计最优的像素偏移。在具体函数实现中（即calculateOpticalFlow)，求解这样一个问题：</p><p><img src="/pic/%E9%80%89%E5%8C%BA_136.png"></p><p><strong>Opencv中的LK光流</strong>：使用  cv::calculateOpticalFlowPyrLK函数：</p><ul><li>提供前后两张图像及对应的特征点，即可得到追踪后的点，以及各点的状态、误差;</li><li>根据status变量是否为1来确定对应的点是否被正确追踪到。</li><li>cv::calcOpticalFlowPyrLK(img1,img2,pt1,pt2,status,error);</li></ul><p><strong>单层光流</strong>：用高斯牛顿法实现光流，光流也可以看成一个优化问题，通过最小化灰度误差估计最优的像素偏移</p><p><strong>多层光流</strong>：因为单层光流在相机运动较快的情况下，容易达到一个局部极小值，因此引入图像金字塔。当原始图像的像素运动较大时，在金字塔顶层看来，运动仍然是一个小的运动范围</p><p>二、代码注释</p><pre class="line-numbers language-c++" data-language="c++"><code class="language-c++">#include&lt;opencv2&#x2F;opencv.hpp&gt;#include&lt;string&gt;#include&lt;chrono&gt;#include&lt;Eigen&#x2F;Core&gt;#include&lt;Eigen&#x2F;Dense&gt;using namespace std;using namespace cv;string file_1&#x3D;&quot;.&#x2F;LK1.png&quot;;   &#x2F;&#x2F;第一张图像的路径,可能需要写成绝对路径string file_2&#x3D;&quot;.&#x2F;LK2.png&quot;;  &#x2F;&#x2F;第二张图像的路径  &#x2F;&#x2F;使用高斯牛顿法实现光流&#x2F;&#x2F;定义一个光流追踪类class OpticalFlowTracker&#123; public:    OpticalFlowTracker(    &#x2F;&#x2F;带参构造函数，并初始化      const Mat &amp;img1_,      const Mat &amp;img2_,      const vector&lt;KeyPoint&gt;&amp;kp1_,      vector&lt;KeyPoint&gt;&amp;kp2_,      vector&lt;bool&gt;&amp;success_,      bool inverse_&#x3D;true,bool has_initial_&#x3D;false):      img1(img1_),img2(img2_),kp1(kp1_),kp2(kp2_),success(success_),inverse(inverse_),      has_initial(has_initial_) &#123;&#125;            &#x2F;&#x2F;计算光流的函数      void calculateOpticalFlow(const Range &amp;range);  &#x2F;&#x2F;range是一个区间，应该看作一个窗口        private:    const Mat &amp;img1;    const Mat &amp;img2;    const vector&lt;KeyPoint&gt; &amp;kp1;    vector&lt;KeyPoint&gt; &amp;kp2;    vector&lt;bool&gt; &amp;success;    bool inverse&#x3D;true;    bool has_initial&#x3D;false;  &#125;;  &#x2F;&#x2F;单层光流的函数声明  void OpticalFlowSingleLevel(    const Mat &amp;img1,    const Mat &amp;img2,    const vector&lt;KeyPoint&gt;&amp;kp1,    vector&lt;KeyPoint&gt;&amp;kp2,    vector&lt;bool&gt;&amp;success,    bool inverse&#x3D;false,    bool has_initial_guess&#x3D;false  );  &#x2F;&#x2F;多层光流的函数声明  void OpticalFlowMultiLevel(    const Mat &amp;img1,    const Mat &amp;img2,    const vector&lt;KeyPoint&gt;&amp;kp1,    vector&lt;KeyPoint&gt;&amp;kp2,    vector&lt;bool&gt; &amp;success,    bool inverse&#x3D;false       );    &#x2F;&#x2F;从图像中获取一个灰度值  &#x2F;&#x2F;采用双线性内插法，来估计一个点的像素：  &#x2F;&#x2F;f(x,y)&#x3D;f(0,0)(1-x)(1-y)+f(1,0)x(1-y)+f(0,1)(1-x)y+f(1,1)xy  inline float GetPixelValue(const cv::Mat &amp;img,float x,float y)  &#123;    &#x2F;&#x2F;边缘检测    if(x&lt;0)      x&#x3D;0;    if(y&lt;0)      y&#x3D;0;    if(x&gt;&#x3D;img.cols)      x&#x3D;img.cols-1;    if(y&gt;&#x3D;img.rows)      y&#x3D;img.rows-1;    uchar *data&#x3D;&amp;img.data[int(y)*img.step+int(x)];  &#x2F;&#x2F;img.step:表示图像矩阵中每行包含的字节数;int(x)将x转换为int类型        float xx&#x3D;x-floor(x);   &#x2F;&#x2F;floor(x)函数：向下取整函数，即返回一个不大于x的最大整数    float yy&#x3D;y-floor(y);        return float(      (1-xx)*(1-yy)*data[0]+      xx*(1-yy)*data[1]+      (1-xx)*yy*data[img.step]+      xx*yy*data[img.step+1]      );  &#125;    &#x2F;&#x2F;主函数  int main(int argc,char**argv)  &#123;    Mat img1&#x3D;imread(file_1,0);  &#x2F;&#x2F;以灰度读取图像，重点    Mat img2&#x3D;imread(file_2,0);    &#x2F;&#x2F;特征点检测    vector&lt;KeyPoint&gt;kp1;  &#x2F;&#x2F;关键点 存放在容器kp1中    Ptr&lt;GFTTDetector&gt; detector&#x3D;GFTTDetector::create(500,0.01,20); &#x2F;&#x2F;通过GFTTD来获取角点，参数：最大角点数目500;角点可以接受的最小特征值0.01;角点之间的最小距离20    detector-&gt;detect(img1,kp1);&#x2F;&#x2F;类似于ORB特征点的提取过程        &#x2F;&#x2F;接下来实现在第二张图像中追踪这些角点，即追踪 kp1    &#x2F;&#x2F;第一种方法：单层光流    vector&lt;KeyPoint&gt;kp2_single;    vector&lt;bool&gt;success_single;    OpticalFlowSingleLevel(img1,img2,kp1,kp2_single,success_single);        &#x2F;&#x2F;第二种方法：多层光流    vector&lt;KeyPoint&gt;kp2_multi;    vector&lt;bool&gt;success_multi;    chrono::steady_clock::time_point t1&#x3D;chrono::steady_clock::now();    OpticalFlowMultiLevel(img1,img2,kp1,kp2_multi,success_multi,true);    chrono::steady_clock::time_point t2&#x3D;chrono::steady_clock::now();    auto time_used&#x3D;chrono::duration_cast&lt;chrono::duration&lt;double&gt;&gt;(t2-t1);     &#x2F;&#x2F;输出使用高斯牛顿法所花费的时间    cout &lt;&lt; &quot;optical flow by gauss-newton: &quot; &lt;&lt; time_used.count() &lt;&lt; endl;        &#x2F;&#x2F;使用OpenCV中的LK光流    vector&lt;Point2f&gt;pt1,pt2;    for(auto &amp;kp:kp1)     &#x2F;&#x2F;kp1中存放的是第一张图像中的角点，通过遍历，将kp1存放在pt1中      pt1.push_back(kp.pt);    vector&lt;uchar&gt;status;    vector&lt;float&gt;error;    t1&#x3D;chrono::steady_clock::now();    &#x2F;&#x2F;调用cv::calculateOpticalFlowPyrLK函数：    &#x2F;&#x2F;提供前后两张图像及对应的特征点，即可得到追踪后的点，以及各点的状态、误差;    &#x2F;&#x2F;根据status变量是否为1来确定对应的点是否被正确追踪到。    cv::calcOpticalFlowPyrLK(img1,img2,pt1,pt2,status,error);    t2&#x3D;chrono::steady_clock::now();    time_used&#x3D;chrono::duration_cast&lt;chrono::duration&lt;double&gt;&gt;(t2-t1);    cout &lt;&lt; &quot;optical flow by opencv: &quot; &lt;&lt; time_used.count() &lt;&lt; endl;  &#x2F;&#x2F;输出使用opencv所花费的时间        &#x2F;&#x2F;下面一部分代码实现绘图的功能    &#x2F;&#x2F;第一张图像：单层光流的效果图     Mat img2_single;     &#x2F;&#x2F;将输入图像从一个空间转换到另一个色彩空间     cv::cvtColor(img2,img2_single,CV_GRAY2BGR); &#x2F;&#x2F;cvtColor(）函数实现的功能：将img2灰度图转换成彩色图img2_single输出     for(int i&#x3D;0;i&lt;kp2_single.size();i++)     &#123;       if(success_single[i])   &#x2F;&#x2F;判断是否追踪成功       &#123; &#x2F;&#x2F;circle():画圆：参数：源图像，画圆的圆心坐标，圆的半径，圆的颜色，线条的粗细程度 &#x2F;&#x2F;kp2_single[i].pt：用来取第i个角点的坐标；Scalar(0,250,0)：设置颜色，遵循B G R ，所以此图中为绿色 cv::circle(img2_single,kp2_single[i].pt,2,cv::Scalar(0,250,0),2); &#x2F;&#x2F;line():绘制直线：参数：要画的线所在的图像，直线起点，直线终点，直线的颜色（绿色） cv::line(img2_single,kp1[i].pt,kp2_single[i].pt,cv::Scalar(0,250,0));       &#125;     &#125;          &#x2F;&#x2F;第二张图像：多层光流的效果图     Mat img2_multi;     cv::cvtColor(img2,img2_multi,CV_GRAY2BGR);      for(int i&#x3D;0;i&lt;kp2_multi.size();i++)     &#123;       if(success_multi[i])          &#123; cv::circle(img2_multi,kp2_multi[i].pt,2,cv::Scalar(250,0,0),2);  cv::line(img2_multi,kp1[i].pt,kp2_multi[i].pt,cv::Scalar(250,0,0));       &#125;     &#125;          &#x2F;&#x2F;第三张图像：使用OpenCV中的LK光流     Mat img2_CV;     cv::cvtColor(img2,img2_CV,CV_GRAY2BGR);      for(int i&#x3D;0;i&lt;pt2.size();i++)     &#123;       if(status[i])          &#123;  cv::circle(img2_CV,pt2[i],2,cv::Scalar(0,0,250),2);  cv::line(img2_CV,pt1[i],pt2[i],cv::Scalar(0,0,250));       &#125;     &#125;          &#x2F;&#x2F;     cv::imshow(&quot;tracked single level&quot;,img2_single);     cv::imshow(&quot;tracked multi level&quot;,img2_multi);     cv::imshow(&quot;tracked by opencv&quot;,img2_CV);     cv::waitKey(0);     return 0;  &#125;<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p><em><strong>具体功能实现如下：</strong></em></p><pre class="line-numbers language-c++" data-language="c++"><code class="language-c++"> &#x2F;&#x2F;接下来这一部分：具体函数的实现 &#x2F;&#x2F;第一个：单层光流函数的实现 void OpticalFlowSingleLevel(   const Mat &amp;img1,   const Mat &amp;img2,   const vector&lt;KeyPoint&gt;&amp;kp1,   vector&lt;KeyPoint&gt;&amp;kp2,   vector&lt;bool&gt;&amp;success,   bool inverse,   bool has_initial) &#123;   &#x2F;&#x2F;resize()函数：调整图像的大小；size（）函数：获取kp1的长度   &#x2F;&#x2F;初始化   kp2.resize(kp1.size());   success.resize(kp1.size());   &#x2F;&#x2F;是否追踪成功的标志      OpticalFlowTracker tracker(img1,img2,kp1,kp2,success,inverse,has_initial);  &#x2F;&#x2F;创建类的对象tracker   &#x2F;&#x2F;调用parallel_for_并行调用OpticalFlowTracker::calculateOpticalFlow，该函数计算指定范围内特征点的光流   &#x2F;&#x2F;range():从指定的第一个值开始，并在到达指定的第二个值后终止   parallel_for_(Range(0,kp1.size()),std::bind(&amp;OpticalFlowTracker::calculateOpticalFlow,&amp;tracker,placeholders::_1)); &#125;  &#x2F;&#x2F;类外实现成员函数 void OpticalFlowTracker::calculateOpticalFlow(const Range &amp;range) &#123;   &#x2F;&#x2F;定义参数   int half_patch_size&#x3D;4;  &#x2F;&#x2F;窗口的大小8×8   int iterations&#x3D;10;  &#x2F;&#x2F;每个角点迭代10次   for(size_t i&#x3D;range.start;i&lt;range.end;i++)   &#123;     auto kp&#x3D;kp1[i];   &#x2F;&#x2F;将第一张图像中的第i个关键点kp1[i]存放在 kp 中     double dx&#x3D;0,dy&#x3D;0; &#x2F;&#x2F;初始化     if(has_initial)     &#123;dx&#x3D;kp2[i].pt.x-kp.pt.x;   &#x2F;&#x2F;第i个点在第二张图像中的位置与第一张图像中的位置的差值dy&#x3D;kp2[i].pt.y-kp.pt.y;     &#125;          double cost&#x3D;0,lastCost&#x3D;0;     bool succ&#x3D;true;          &#x2F;&#x2F;高斯牛顿方程     &#x2F;&#x2F;高斯牛顿迭代     Eigen::Matrix2d H &#x3D; Eigen::Matrix2d::Zero();   &#x2F;&#x2F;定义H，并进行初始化。     Eigen::Vector2d b &#x3D; Eigen::Vector2d::Zero();   &#x2F;&#x2F;定义b，并初始化.     Eigen::Vector2d J;   &#x2F;&#x2F;定义雅克比矩阵2×1     for(int iter&#x3D;0;iter&lt;iterations;iter++)     &#123;if(inverse&#x3D;&#x3D;false)&#123;  H&#x3D;Eigen::Matrix2d::Zero();  b&#x3D;Eigen::Vector2d::Zero();&#125;else&#123;  b&#x3D;Eigen::Vector2d::Zero();&#125;cost&#x3D;0;&#x2F;&#x2F;假设在这个8×8的窗口内像素具有同样的运动&#x2F;&#x2F;计算cost和Jfor(int x&#x3D;-half_patch_size;x&lt;half_patch_size;x++)  for(int y&#x3D;-half_patch_size;y&lt;half_patch_size;y++)  &#123;    &#x2F;&#x2F;GetPixelValue（）计算某点的灰度值    &#x2F;&#x2F;计算残差：I(x,y)-I(x+dx,y+dy)    double error&#x3D;GetPixelValue(img1,kp.pt.x+x,kp.pt.y+y)-GetPixelValue(img2,kp.pt.x+x+dx,kp.pt.y+y+dy);;    if(inverse&#x3D;&#x3D;false)    &#123;      &#x2F;&#x2F;雅克比矩阵为第二个图像在x+dx,y+dy处的梯度      J&#x3D;-1.0*Eigen::Vector2d(0.5*(GetPixelValue(img2,kp.pt.x+dx+x+1,kp.pt.y+dy+y)-                                 GetPixelValue(img2,kp.pt.x+dx+x-1,kp.pt.y+dy+y)),     0.5*(GetPixelValue(img2,kp.pt.x+dx+x,kp.pt.y+dy+y+1)-         GetPixelValue(img2,kp.pt.x+dx+x,kp.pt.y+dy+y-1)));    &#125;    else if(iter&#x3D;&#x3D;0)  &#x2F;&#x2F;如果是第一次迭代，梯度为第一个图像的梯度，反向光流法      &#x2F;&#x2F;在反向光流中，I(x,y)的梯度是保持不变的，可以在第一次迭代时保留计算的结果，在后续的迭代中使用。      &#x2F;&#x2F;当雅克比矩阵不变时，H矩阵不变，每次迭代只需要计算残差。    &#123;      J&#x3D;-1.0*Eigen::Vector2d(0.5*(GetPixelValue(img1,kp.pt.x+x+1,kp.pt.y+y)-                                 GetPixelValue(img1,kp.pt.x+x-1,kp.pt.y+y)),     0.5*(GetPixelValue(img1,kp.pt.x+x,kp.pt.y+y+1)-         GetPixelValue(img1,kp.pt.x+x,kp.pt.y+y-1)));    &#125;    &#x2F;&#x2F;计算H和b    b+&#x3D;-error*J;    cost+&#x3D;error*error;    if(inverse&#x3D;&#x3D;false||iter&#x3D;&#x3D;0)    &#123;      H+&#x3D;J*J.transpose();    &#125;  &#125;  &#x2F;&#x2F;计算增量update，求解线性方程Hx&#x3D;b  Eigen::Vector2d update&#x3D;H.ldlt().solve(b);  if(std::isnan(update[0]))  &#x2F;&#x2F;判断增量  &#123;    &#x2F;&#x2F;有时当我们遇到一个黑色或白色的方块，H是不可逆的，即高斯牛顿方程无解    cout&lt;&lt;&quot;update is nan&quot;&lt;&lt;endl;    succ&#x3D;false;   &#x2F;&#x2F;追踪失败    break;  &#125;  if(iter&gt;0&amp;&amp;cost&gt;lastCost)  &#123;    break;  &#125;  dx+&#x3D;update[0];  dy+&#x3D;update[1];  lastCost&#x3D;cost;  succ&#x3D;true;  if(update.norm()&lt;1e-2)  &#123;    break;  &#125;     &#125;     success[i]&#x3D;succ;     kp2[i].pt&#x3D;kp.pt+Point2f(dx,dy);   &#125; &#125;&#x2F;&#x2F;迭代完成  &#x2F;&#x2F;第二个：多层光流函数的实现 void OpticalFlowMultiLevel(   const Mat &amp;img1,   const Mat &amp;img2,   const vector&lt;KeyPoint&gt;&amp;kp1,   vector&lt;KeyPoint&gt;&amp;kp2,   vector&lt;bool&gt;&amp;success,   bool inverse) &#123;   int pyramids&#x3D;4; &#x2F;&#x2F;建立4层金字塔   double pyramid_scale&#x3D;0.5;  &#x2F;&#x2F;金字塔每层缩小0.5   double scales[]&#x3D;&#123;1.0,0.5,0.25,0.125&#125;;      &#x2F;&#x2F;建立金字塔   chrono::steady_clock::time_point t1&#x3D;chrono::steady_clock::now();  &#x2F;&#x2F;计算时间   vector&lt;Mat&gt;pyr1,pyr2;   for(int i&#x3D;0;i&lt;pyramids;i++)   &#123;     if(i&#x3D;&#x3D;0)     &#123;&#x2F;&#x2F;将两张图像存放在pyr1,pyr2中pyr1.push_back(img1);    pyr2.push_back(img2);     &#125;     else     &#123;Mat img1_pyr,img2_pyr;&#x2F;&#x2F;对图像进行缩放，参数：原图，输出图像，输出图像大小，Size（宽度，高度）cv::resize(pyr1[i-1],img1_pyr,cv::Size(pyr1[i-1].cols*pyramid_scale,pyr1[i-1].rows*pyramid_scale));cv::resize(pyr2[i-1],img2_pyr,cv::Size(pyr2[i-1].cols*pyramid_scale,pyr2[i-1].rows*pyramid_scale));pyr1.push_back(img1_pyr);pyr2.push_back(img2_pyr);     &#125;   &#125;   chrono::steady_clock::time_point t2&#x3D;chrono::steady_clock::now();   auto time_used&#x3D;chrono::duration_cast&lt;chrono::duration&lt;double&gt;&gt;(t2-t1);   cout&lt;&lt;&quot;build pyramid time:&quot;&lt;&lt;time_used.count()&lt;&lt;endl;      &#x2F;&#x2F;计算光流时，先从顶层的图像开始计算   vector&lt;KeyPoint&gt;kp1_pyr,kp2_pyr;   for(auto &amp;kp:kp1)   &#123;     auto kp_top&#x3D;kp;     kp_top.pt *&#x3D;scales[pyramids-1];   &#x2F;&#x2F;顶层     kp1_pyr.push_back(kp_top);     kp2_pyr.push_back(kp_top);   &#125;      for(int level&#x3D;pyramids-1;level&gt;&#x3D;0;level--)   &#123;     success.clear();     t1&#x3D;chrono::steady_clock::now();     OpticalFlowSingleLevel(pyr1[level],pyr2[level],kp1_pyr,kp2_pyr,success,inverse,true);     t2&#x3D;chrono::steady_clock::now();     auto time_used&#x3D;chrono::duration_cast&lt;chrono::duration&lt;double&gt;&gt;(t2-t1);     cout&lt;&lt;&quot;track pyr&quot;&lt;&lt;level&lt;&lt;&quot;cost time:&quot;&lt;&lt;time_used.count()&lt;&lt;endl;     if(level&gt;0)     &#123;for(auto &amp;kp:kp1_pyr)  kp.pt&#x2F;&#x3D;pyramid_scale;for(auto &amp;kp:kp2_pyr)  kp.pt&#x2F;&#x3D;pyramid_scale;     &#125;   &#125;   for(auto &amp;kp:kp2_pyr)     kp2.push_back(kp); &#125;<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p><a href="https://blog.csdn.net/weixin_51547017/article/details/115359316?utm_medium=distribute.pc_relevant.none-task-blog-2~default~baidujs_baidulandingword~default-0-115359316-blog-126934983.235%5Ev28%5Epc_relevant_recovery_v2&spm=1001.2101.3001.4242.1&utm_relevant_index=3">视觉SLAM第八讲视觉里程计2— LK光流—代码详细讲解</a></p><p><img src="/pic/%E9%80%89%E5%8C%BA_138.png" alt="运行结果"></p><p><img src="/pic/%E9%80%89%E5%8C%BA_137.png" alt="运行结果"></p><h3 id="ch8-x2F-direct-method-cpp"><a href="#ch8-x2F-direct-method-cpp" class="headerlink" title="ch8&#x2F;direct_method.cpp"></a>ch8&#x2F;direct_method.cpp</h3><p>在光流中，我们会首先追踪特征点的位置，再根据这些位置确定相机的运动。这样一种两步走的方案，很难保证全局最优。直接法通过在后一步中调整前一步的结果</p><p><a href="https://blog.csdn.net/pj18862486309/article/details/107829914?spm=1001.2101.3001.6650.6&utm_medium=distribute.pc_relevant.none-task-blog-2~default~BlogCommendFromBaidu~Rate-6-107829914-blog-126993782.235%5Ev28%5Epc_relevant_recovery_v2&depth_1-utm_source=distribute.pc_relevant.none-task-blog-2~default~BlogCommendFromBaidu~Rate-6-107829914-blog-126993782.235%5Ev28%5Epc_relevant_recovery_v2&utm_relevant_index=10">视觉十四讲：第八讲_直接法</a></p><p><img src="/pic/%E9%80%89%E5%8C%BA_139.png" alt="直接法的讨论"></p><p>代码注释待更新~~~~</p>]]></content>
      
      
      <categories>
          
          <category> slam </category>
          
          <category> c++ </category>
          
      </categories>
      
      
        <tags>
            
            <tag> c++ </tag>
            
            <tag> slam </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>slam14讲源代码的记录（前五讲）</title>
      <link href="/2023/04/06/slam14/"/>
      <url>/2023/04/06/slam14/</url>
      
        <content type="html"><![CDATA[<h2 id="第二讲"><a href="#第二讲" class="headerlink" title="第二讲"></a>第二讲</h2><p>本讲主要是cmake的使用以及一些库的链接方式</p><pre class="line-numbers language-cmake" data-language="cmake"><code class="language-cmake"><span class="token comment">#声明cmake的最低版本</span><span class="token keyword">cmake_minimum_required</span><span class="token punctuation">(</span><span class="token property">VERSION</span> <span class="token number">2.8</span><span class="token punctuation">)</span><span class="token comment">#声明一个cmake工程</span><span class="token keyword">project</span><span class="token punctuation">(</span>HelloSLAM<span class="token punctuation">)</span><span class="token comment">#设置编译模式</span><span class="token keyword">set</span><span class="token punctuation">(</span><span class="token variable">CMAKE_BUILD_TYPE</span><span class="token string">"Debug"</span><span class="token punctuation">)</span><span class="token comment">#添加可执行程序</span><span class="token keyword">add_executable</span><span class="token punctuation">(</span>helloSLAM helloSLAM.cpp<span class="token punctuation">)</span><span class="token comment">#下一部分</span><span class="token comment">#添加库文件</span><span class="token keyword">add_library</span><span class="token punctuation">(</span>hello <span class="token namespace">STATIC</span> libHelloSLAM.cpp<span class="token punctuation">)</span>   <span class="token comment">#静态链接库,会生成.a文件,可默认不加STATIC或者SHARED</span><span class="token comment"># add_library(hello_shared SHARED libHelloSLAM.cpp)  #动态链接库，会生成.so文件</span><span class="token keyword">add_executable</span><span class="token punctuation">(</span>useHello useHello.cpp<span class="token punctuation">)</span><span class="token keyword">target_link_libraries</span><span class="token punctuation">(</span>useHello hello<span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p><img src="/pic/11.jpg"></p><p><img src="/pic/12.jpg"></p><p><a href="https://cmake.org/cmake/help/latest/genindex.html#">CMake指令查询网址</a></p><blockquote><p>补充</p></blockquote><p>添加头文件include_directories()可使用具体路径，例如：</p><p>#添加Eigen头文件<br>include_directories(“&#x2F;usr&#x2F;include&#x2F;eigen3”)</p><p>静态库<br>• 原理:在编译时将源代码复制到程序中，运行时不用库文件依旧可以运行。<br>• 优点:运行已有代码，运行时不用再用库;无需加载库，运行更快<br>• 缺点:占用更多的空间和磁盘;静态库升级，需要重新编译程序<br>共享库(常用)<br>• 原理:编译时仅仅是记录用哪一个库里面的哪一个符号，不复制相关代码<br>• 优点:不复制代码，占用空间小;多个程序可以同时调用一个库;升级方便，无需重新编译 • 缺点:程序运行需要加载库，耗费一定时间</p><table><thead><tr><th align="center">操作系统</th><th align="center">静态库</th><th align="center">共享库</th></tr></thead><tbody><tr><td align="center">Windows</td><td align="center">lib</td><td align="center">.dll</td></tr><tr><td align="center">Linux</td><td align="center">.a</td><td align="center">.so</td></tr><tr><td align="center">Mac OS</td><td align="center">.a</td><td align="center">dylib</td></tr></tbody></table><h2 id="第三讲"><a href="#第三讲" class="headerlink" title="第三讲"></a>第三讲</h2><p>第三讲主要是三维空间刚体运动</p><p><strong>Eigen库的使用</strong></p><h3 id="ch3-x2F-useEigen-x2F-eigenMatrix-cpp中的需要记住的"><a href="#ch3-x2F-useEigen-x2F-eigenMatrix-cpp中的需要记住的" class="headerlink" title="ch3&#x2F;useEigen&#x2F;eigenMatrix.cpp中的需要记住的"></a>ch3&#x2F;useEigen&#x2F;eigenMatrix.cpp中的需要记住的</h3><pre class="line-numbers language-c++" data-language="c++"><code class="language-c++">using namespace Eigen;Vector3d v_3d;      &#x2F;&#x2F;实际上是MAtrix&lt;double,3,1&gt;Matrix3d matrix_33&#x3D;Matrix3d::Zero();&#x2F;&#x2F; Eigen::Matrix&lt;double, 3, 3&gt;Matrix&lt;float,2,3&gt; matrix_23;&#x2F;&#x2F;可以直接这样输入数据matrix_23&lt;&lt;1,2,3,4,5,6;&#x2F;&#x2F;相乘不能混用，需要显式转换Matrix&lt;double,2,1&gt; result &#x3D;matrix_23.cast&lt;double&gt;()*v_3d;matrix_33.transpose()；&#x2F;&#x2F;转置matrix_33.trace()&#x2F;&#x2F;迹matrix_33.inverse() ;&#x2F;&#x2F;求逆matrix_33.determinant();&#x2F;&#x2F;求行列式&#x2F;&#x2F;实对称矩阵一定可以对角化SelfAdjointEigenSolver&lt;Matrix3d&gt; eigen_solver(matrix_33.transpose()*matrix_33);eigen_solver.eigenvalues()；&#x2F;&#x2F;特征值eigen_solver.eigenvectors()；&#x2F;&#x2F;特征向量&#x2F;&#x2F;求解 matrix_NN * x &#x3D; v_Nd 这个方程.QR分解x&#x3D;matrix_NN.colPivHouseholderQr().solve(v_Nd);<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h3 id="ch3-x2F-useGeometry-x2F-eigenGeometry-cpp中"><a href="#ch3-x2F-useGeometry-x2F-eigenGeometry-cpp中" class="headerlink" title="ch3&#x2F;useGeometry&#x2F;eigenGeometry.cpp中"></a>ch3&#x2F;useGeometry&#x2F;eigenGeometry.cpp中</h3><pre class="line-numbers language-c++" data-language="c++"><code class="language-c++">Matrix3d rotation_matrix&#x3D;Matrix3d::Identity();&#x2F;&#x2F;单位阵,旋转矩阵AngleAxisd rotation_vector(M_PI&#x2F;4,Vector3d(0,0,1));&#x2F;&#x2F;旋转向量，使用的AngleAxisd&#x2F;&#x2F;旋转角以及旋转轴cout &lt;&lt; &quot;rotation_vector&quot; &lt;&lt; &quot;angle is: &quot; &lt;&lt; rotation_vector.angle() * (180 &#x2F; M_PI)                                 &lt;&lt; &quot; axis is: &quot; &lt;&lt; rotation_vector.axis().transpose() &lt;&lt; endl;&#x2F;&#x2F;旋转向量变旋转矩阵rotation_matrix &#x3D; rotation_vector.toRotationMatrix();&#x2F;&#x2F;旋转矩阵变旋转向量rotation_vector.fromRotationMatrix(rotation_matrix);&#x2F;&#x2F; 欧拉角: 可以将旋转矩阵直接转换成欧拉角Vector3d euler_angles &#x3D; rotation_matrix.eulerAngles(2, 1, 0); &#x2F;&#x2F; ZYX顺序，即yaw-pitch-roll顺序&#x2F;&#x2F;欧式变换使用Isometry,这里注意需要使用头文件 #include&lt;Eigen&#x2F;Geometry&gt;  Isometry3d T &#x3D; Isometry3d::Identity();                &#x2F;&#x2F; 虽然称为3d，实质上是4＊4的矩阵  T.rotate(rotation_vector);                                     &#x2F;&#x2F; 按照rotation_vector进行旋转  T.pretranslate(Vector3d(1, 3, 4));                     &#x2F;&#x2F; 把平移向量设成(1,3,4)  cout &lt;&lt; &quot;Transform matrix &#x3D; \n&quot; &lt;&lt; T.matrix() &lt;&lt; endl;&#x2F;&#x2F;四元数:拥有一个实部，3个虚部，Quaterniond是（s(q0),q1,q2,q3）Quaterniond q&#x3D;Quaterniond(rotation_vector);cout &lt;&lt; &quot;quaternion from rotation vector &#x3D; &quot; &lt;&lt; q.coeffs().transpose()&lt;&lt; endl;   &#x2F;&#x2F; 请注意coeffs的顺序是(x,y,z,w),w为实部，前三者为虚部<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><blockquote><p>补充：pretranslate 是在旋转之前的坐标轴上进行的平移操作，而 translate 是在旋转之后，pretanslate 相当于左乘，translate 相当于右乘。<br><a href="https://www.guyuehome.com/36653">Eigen 中 pretanslate 和 translate 的区别</a></p></blockquote><p>###ch3&#x2F;examples&#x2F;plotTrajectory.cpp中需要注意的</p><p>使用ifstream流来读取文件</p><p>说明：</p><p>1.ifstream类的对象创建成功的时候会返回非空值，借此判断是否创建文件对象成功</p><p>2.ifstream有个函数eof()用来判断文件是否读到尾部,没读到尾部返回false，否则返回true。</p><p>若尾部有回车，那么最后一条记录会读取两次。</p><p>若尾部没有回车，那么最后一条记录只会读取一次</p><p>3.iftream的对象假设为fin，fin在读取数据的时候会根据你的输出对象来选择输出的方式。</p><p>例程</p><pre class="line-numbers language-c++" data-language="c++"><code class="language-c++">#include&lt;iostream&gt;#include&lt;fstream&gt;using namespace std;int main()&#123;ifstream fin(&quot;abc.txt&quot;);&#x2F;&#x2F;读取文件的名字，可以相对或绝对if(!fin) &#123;cout&lt;&lt;&quot;open fail.&quot;&lt;&lt;endl;exit(1);&#125;else&#123;while(!fin.eof())&#123;char a[20],b[20],c[20];fin&gt;&gt;a&gt;&gt;b&gt;&gt;c;&#x2F;&#x2F;读取的时候遇见空格才会跳跃。cout&lt;&lt;a&lt;&lt;&quot;&quot;&lt;&lt;b&lt;&lt;&quot;  &quot;&lt;&lt;c&lt;&lt;&quot;  &quot;&lt;&lt;endl;&#125;fin.close();&#125;&#125;<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>如果STL容器中的元素是Eigen库数据结构，例如这里定义一个vector容器，元素是Matrix4d ，如下所示：</p><blockquote><p>vector<a href="Eigen::Matrix4d">Eigen::Matrix4d</a></p></blockquote><p>这个错误也是和上述一样的提示，编译不会出错，只有在运行的时候出错。解决的方法很简单，定义改成下面的方式：</p><blockquote><p>vector&lt;Eigen::Matrix4d,Eigen::aligned_allocator<a href="Eigen::Matrix4d">Eigen::Matrix4d</a>&gt;;</p></blockquote><p>其实上述的这段代码才是标准的定义容器方法，只是我们一般情况下定义容器的元素都是C++中的类型，所以可以省略，这是因为在C++11标准中，aligned_allocator管理C++中的各种数据类型的内存方法是一样的，可以不需要着重写出来。但是在Eigen管理内存和C++11中的方法是不一样的，所以需要单独强调元素的内存分配和管理。</p><p>对于轨迹文件的处理过程如下：</p><pre class="line-numbers language-c++" data-language="c++"><code class="language-c++">while (!fin.eof()) &#123;  double time, tx, ty, tz, qx, qy, qz, qw;  fin &gt;&gt; time &gt;&gt; tx &gt;&gt; ty &gt;&gt; tz &gt;&gt; qx &gt;&gt; qy &gt;&gt; qz &gt;&gt; qw;  Isometry3d Twr(Quaterniond(qw, qx, qy, qz)); &#x2F;&#x2F; 对比旋转向量的T.rotate(rotation_vector);            &#x2F;&#x2F; 按照rotation_vector进行旋转  Twr.pretranslate(Vector3d(tx, ty, tz));  poses.push_back(Twr);&#125;<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p><img src="/pic/%E9%80%89%E5%8C%BA_132.png" alt="运动轨迹"></p><p><a href="https://blog.csdn.net/weixin_41756645/article/details/122958689">旋转矩阵、变换矩阵、欧式变换</a></p><h2 id="第四讲"><a href="#第四讲" class="headerlink" title="第四讲"></a>第四讲</h2><p>主要是李群以及李代数，Sophus的使用</p><h3 id="ch4-x2F-useSophus-cpp中"><a href="#ch4-x2F-useSophus-cpp中" class="headerlink" title="ch4&#x2F;useSophus.cpp中"></a>ch4&#x2F;useSophus.cpp中</h3><pre class="line-numbers language-c++" data-language="c++"><code class="language-c++">&#x2F;&#x2F;李群的表示  &#x2F;&#x2F; 沿Z轴转90度的旋转矩阵  Matrix3d R &#x3D; AngleAxisd(M_PI &#x2F; 2, Vector3d(0, 0, 1)).toRotationMatrix();  &#x2F;&#x2F; 或者四元数  Quaterniond q(R);  Sophus::SO3d SO3_R(R);              &#x2F;&#x2F; Sophus::SO3d可以直接从旋转矩阵构造  Sophus::SO3d SO3_q(q);              &#x2F;&#x2F; 也可以通过四元数构造    &#x2F;&#x2F; 对SE(3)操作大同小异  Vector3d t(1, 0, 0);           &#x2F;&#x2F; 沿X轴平移1  Sophus::SE3d SE3_Rt(R, t);           &#x2F;&#x2F; 从R,t构造SE(3)  Sophus::SE3d SE3_qt(q, t);            &#x2F;&#x2F; 从q,t构造SE(3)  &#x2F;&#x2F;通过.matrix()查看李群的矩阵   &#x2F;&#x2F;通过对数映射转化为李代数，这里的log直接把李群转化为李代数，不是反对称矩阵  Vector3d so3 &#x3D; SO3_R.log();  cout &lt;&lt; &quot;so3 &#x3D; &quot; &lt;&lt; so3.transpose() &lt;&lt; endl;  &#x2F;&#x2F; hat 为向量到反对称矩阵  cout &lt;&lt; &quot;so3 hat&#x3D;\n&quot; &lt;&lt; Sophus::SO3d::hat(so3) &lt;&lt; endl;    &#x2F;&#x2F; 李代数se(3) 是一个六维向量，方便起见先typedef一下  typedef Eigen::Matrix&lt;double, 6, 1&gt; Vector6d;  Vector6d se3 &#x3D; SE3_Rt.log();  cout &lt;&lt; &quot;se3 &#x3D; &quot; &lt;&lt; se3.transpose() &lt;&lt; endl;  &#x2F;&#x2F; 观察输出，会发现在Sophus中，se(3)的平移在前，旋转在后.需要注意  &#x2F;&#x2F; 同样的，有hat和vee两个算符  cout &lt;&lt; &quot;se3 hat &#x3D; \n&quot; &lt;&lt; Sophus::SE3d::hat(se3) &lt;&lt; endl;  &#x2F;&#x2F; 增量扰动模型的更新，李代数求导    Vector3d update_so3(1e-4, 0, 0); &#x2F;&#x2F;假设更新量为这么多  Sophus::SO3d SO3_updated &#x3D; Sophus::SO3d::exp(update_so3) * SO3_R;  cout &lt;&lt; &quot;SO3 updated &#x3D; \n&quot; &lt;&lt; SO3_updated.matrix() &lt;&lt; endl;    Vector6d update_se3; &#x2F;&#x2F;更新量,这里不是太懂  update_se3.setZero();  update_se3(0, 0) &#x3D; 1e-4;  Sophus::SE3d SE3_updated &#x3D; Sophus::SE3d::exp(update_se3) * SE3_Rt;  cout &lt;&lt; &quot;SE3 updated &#x3D; &quot; &lt;&lt; endl &lt;&lt; SE3_updated.matrix() &lt;&lt; endl;<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p><a href="https://xsin.gitee.io/2019/01/14/SlSophus%E5%87%BD%E6%95%B0%E7%9A%84%E4%BD%BF%E7%94%A8/">Slam中Sophus函数的使用</a></p><h3 id="ch4-x2F-example-x2F-trajectoryError-cpp中比较真实轨迹与估计轨迹之间的误差"><a href="#ch4-x2F-example-x2F-trajectoryError-cpp中比较真实轨迹与估计轨迹之间的误差" class="headerlink" title="ch4&#x2F;example&#x2F;trajectoryError.cpp中比较真实轨迹与估计轨迹之间的误差"></a>ch4&#x2F;example&#x2F;trajectoryError.cpp中比较真实轨迹与估计轨迹之间的误差</h3><pre class="line-numbers language-c++" data-language="c++"><code class="language-c++">&#x2F;&#x2F;vector（动态数组）类型的TrajectoryType存储位姿typedef vector&lt;Sophus::SE3d, Eigen::aligned_allocator&lt;Sophus::SE3d&gt;&gt; TrajectoryType;&#x2F;&#x2F;通过下面这样读取至李群中  while (!fin.eof()) &#123;    double time, tx, ty, tz, qx, qy, qz, qw;    fin &gt;&gt; time &gt;&gt; tx &gt;&gt; ty &gt;&gt; tz &gt;&gt; qx &gt;&gt; qy &gt;&gt; qz &gt;&gt; qw;    Sophus::SE3d p1(Eigen::Quaterniond(qw, qx, qy, qz), Eigen::Vector3d(tx, ty, tz));    trajectory.push_back(p1);  &#125;  &#x2F;&#x2F;轨迹误差的计算方法如下    double rmse &#x3D; 0;  for (size_t i &#x3D; 0; i &lt; estimated.size(); i++) &#123;    Sophus::SE3d p1 &#x3D; estimated[i], p2 &#x3D; groundtruth[i];    double error &#x3D; (p2.inverse() * p1).log().norm();    rmse +&#x3D; error * error;  &#125;  rmse &#x3D; rmse &#x2F; double(estimated.size());  rmse &#x3D; sqrt(rmse);  cout &lt;&lt; &quot;RMSE &#x3D; &quot; &lt;&lt; rmse &lt;&lt; endl;<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p><img src="/pic/%E9%80%89%E5%8C%BA_129.png" alt="轨迹相差图"></p><p><a href="https://blog.csdn.net/qq_28087491/article/details/113540037">Pangolin可视化绘图库的使用</a></p><h2 id="第五讲"><a href="#第五讲" class="headerlink" title="第五讲"></a>第五讲</h2><p>本讲主要是相机与图像,内参外参畸变参数等，Opencv使用以及摄像头标定等</p><h3 id="ch5-x2F-imageBasics-x2F-imageBasics-cpp中操作Opencv图像中需要注意的"><a href="#ch5-x2F-imageBasics-x2F-imageBasics-cpp中操作Opencv图像中需要注意的" class="headerlink" title="ch5&#x2F;imageBasics&#x2F;imageBasics.cpp中操作Opencv图像中需要注意的"></a>ch5&#x2F;imageBasics&#x2F;imageBasics.cpp中操作Opencv图像中需要注意的</h3><pre class="line-numbers language-c++" data-language="c++"><code class="language-c++">  image &#x3D; cv::imread(argv[1]); &#x2F;&#x2F;cv::imread函数读取指定路径下的图像  &#x2F;&#x2F; 文件顺利读取, 首先输出一些基本信息  cout &lt;&lt; &quot;图像宽为&quot; &lt;&lt; image.cols &lt;&lt; &quot;,高为&quot; &lt;&lt; image.rows &lt;&lt; &quot;,通道数为&quot; &lt;&lt; image.channels() &lt;&lt; endl;  cv::imshow(&quot;image&quot;, image);      &#x2F;&#x2F; 用cv::imshow显示图像  cv::waitKey(0);                  &#x2F;&#x2F; 暂停程序,等待一个按键输入  &#x2F;&#x2F; 使用 std::chrono 来给算法计时  chrono::steady_clock::time_point t1 &#x3D; chrono::steady_clock::now();    sleep(1);    chrono::steady_clock::time_point t2 &#x3D; chrono::steady_clock::now();  chrono::duration&lt;double&gt; time_used &#x3D; chrono::duration_cast &lt; chrono::duration &lt; double &gt;&gt; (t2 - t1);  cout &lt;&lt; &quot;遍历图像用时：&quot; &lt;&lt; time_used.count() &lt;&lt; &quot; 秒。&quot; &lt;&lt; endl;&#x2F;&#x2F;time_point表示一个时间点，用来获取1970.1.1以来的秒数和当前的时间。time_point必须要clock来计时，time_point有一个函数time_since_epoch()用来获得1970年1月1日到time_point时间经过的duration&#x2F;&#x2F;时钟间隔Duration&#x2F;&#x2F; 遍历图像, 请注意以下遍历方式亦可使用于随机像素访问for (size_t y &#x3D; 0; y &lt; image.rows; y++) &#123;    &#x2F;&#x2F; 用cv::Mat::ptr获得图像的行指针    unsigned char *row_ptr &#x3D; image.ptr&lt;unsigned char&gt;(y);  &#x2F;&#x2F; row_ptr是第y行的头指针    for (size_t x &#x3D; 0; x &lt; image.cols; x++) &#123;      &#x2F;&#x2F; 访问位于 x,y 处的像素      unsigned char *data_ptr &#x3D; &amp;row_ptr[x * image.channels()]; &#x2F;&#x2F; data_ptr 指向待访问的像素数据      &#x2F;&#x2F; 输出该像素的每个通道,如果是灰度图就只有一个通道      for (int c &#x3D; 0; c !&#x3D; image.channels(); c++) &#123;        unsigned char data &#x3D; data_ptr[c]; &#x2F;&#x2F; data为I(x,y)第c个通道的值      &#125;    &#125;  &#125;    &#x2F;&#x2F; 关于 cv::Mat 的拷贝  &#x2F;&#x2F; 直接赋值并不会拷贝数据  cv::Mat image_another &#x3D; image;  &#x2F;&#x2F; 修改 image_another 会导致 image 发生变化  image_another(cv::Rect(0, 0, 100, 100)).setTo(0); &#x2F;&#x2F; 将左上角100*100的块置零  cv::imshow(&quot;image&quot;, image);  cv::waitKey(0);  &#x2F;&#x2F; 使用clone函数来拷贝数据  cv::Mat image_clone &#x3D; image.clone();  image_clone(cv::Rect(0, 0, 100, 100)).setTo(255);  cv::imshow(&quot;image&quot;, image);  cv::imshow(&quot;image_clone&quot;, image_clone);  cv::waitKey(0);<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><blockquote><p>补充</p></blockquote><p>CMakeLists.txt文件代码</p><pre class="line-numbers language-CMake" data-language="CMake"><code class="language-CMake">cmake_minimum_required( VERSION 2.8 )project(imageBasics) set( CMAKE_CXX_FLAGS &quot;-std&#x3D;c++11 -O3&quot;)find_package(OpenCV)include_directories($&#123;OpenCV&#125;)add_executable(imageBasics imageBasics.cpp)# 链接OpenCV库target_link_libraries(imageBasics $&#123;OpenCV_LIBS&#125;)add_executable(undistortImage undistortImage.cpp)# 链接OpenCV库target_link_libraries(undistortImage $&#123;OpenCV_LIBS&#125;)<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p><a href="https://blog.csdn.net/qq_43428547/article/details/88956911">二维数组与指针(详解)</a></p><p>数组名与指针的关系</p><pre class="line-numbers language-c++" data-language="c++"><code class="language-c++">a;&#x2F;&#x2F;代表数组首行地址，一般用a[0][0]的地址表示&amp;a;&#x2F;&#x2F;代表整个数组的地址，一般用a[0][0]地址表示a[i];代表了第i行起始元素的地址(网上说是代表了第i行的地址，但我觉得不是，在讲数组与指针的关系时我会验证给大家看)&amp;a[i];代表了第i行的地址，一般用a[i][0]的地址表示a[i]+j;&#x2F;&#x2F;代表了第i行第j个元素地址,a[i]就是j&#x3D;&#x3D;0的情况a[i][j];&#x2F;&#x2F;代表了第i行第j个元素&amp;a[i][j];&#x2F;&#x2F;代表了第i行第j个元素的地址*a;&#x2F;&#x2F;代表数组a首元素地址也就是a[0]或者&amp;a[0][0]*(a+i);&#x2F;&#x2F;代表了第i行首元素的地址,*a是i&#x3D;0的情况*(a+i)+j;&#x2F;&#x2F;代表了第i行j个元素的地址**a;&#x2F;&#x2F;代表a的首元素的值也就是a[0][0]*(*(a+i)+j);&#x2F;&#x2F;代表了第i行第j个元素<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p><a href="https://blog.csdn.net/u010420283/article/details/111946293?utm_medium=distribute.pc_relevant.none-task-blog-2~default~baidujs_baidulandingword~default-1-111946293-blog-121178263.235%5Ev27%5Epc_relevant_default_base1&spm=1001.2101.3001.4242.2&utm_relevant_index=4">c++中几种记时函数实例</a></p><p>几种及时函数如下：</p><pre class="line-numbers language-c++" data-language="c++"><code class="language-c++">&#x2F;&#x2F;方法1,标准库#include &lt;sys&#x2F;time.h&gt; struct timeval tv;gettimeofday(&amp;tv,NULL);auto b1&#x3D;(unsigned long long)tv.tv_sec*1000+(unsigned long long)tv.tv_usec&#x2F;1000;sleep(1);gettimeofday(&amp;tv,NULL);auto b2&#x3D;(unsigned long long)tv.tv_sec*1000+(unsigned long long)tv.tv_usec&#x2F;1000;cout&lt;&lt;&quot;method 1, cost time is:&quot;&lt;&lt;(b2-b1)&lt;&lt;endl;  &#x2F;&#x2F;方法2，chrono#include&lt;chrono&gt;using namespace std::chrono; auto t1&#x3D; duration_cast&lt;milliseconds&gt;(steady_clock::now().time_since_epoch()).count();sleep(1);auto t2&#x3D;duration_cast&lt;milliseconds&gt;(steady_clock::now().time_since_epoch()).count();cout&lt;&lt;&quot;method 2, cost time is:&quot;&lt;&lt;(t2-t1)&lt;&lt;endl; &#x2F;&#x2F;方法2副本，该代码只能求两段时间差，不能得到打印出当前时刻。auto begin&#x3D;system_clock::now();sleep(1);auto end&#x3D;system_clock::now();cout&lt;&lt;&quot;method 2-1, cost time is:&quot;&lt;&lt;duration_cast&lt;milliseconds&gt;(end - begin).count()&lt;&lt;endl;   &#x2F;&#x2F;方法3#include &lt;unistd.h&gt;clock_t c1&#x3D;clock();sleep(1);clock_t c2&#x3D;clock();cout&lt;&lt;&quot;method 3, cost time is:&quot;&lt;&lt;(c2-c1)&lt;&lt;endl; <span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h3 id="ch5-x2F-imageBasics-x2F-undistortImage-cpp中图像去畸变"><a href="#ch5-x2F-imageBasics-x2F-undistortImage-cpp中图像去畸变" class="headerlink" title="ch5&#x2F;imageBasics&#x2F;undistortImage.cpp中图像去畸变"></a>ch5&#x2F;imageBasics&#x2F;undistortImage.cpp中图像去畸变</h3><p><img src="/pic/%E9%80%89%E5%8C%BA_102.png" alt="去畸变公式"></p><pre class="line-numbers language-c++" data-language="c++"><code class="language-c++">&#x2F;&#x2F;主要知道内参以及去畸变参数  &#x2F;&#x2F; 畸变参数  double k1 &#x3D; -0.28340811, k2 &#x3D; 0.07395907, p1 &#x3D; 0.00019359, p2 &#x3D; 1.76187114e-05;  &#x2F;&#x2F; 内参  double fx &#x3D; 458.654, fy &#x3D; 457.296, cx &#x3D; 367.215, cy &#x3D; 248.375;  &#x2F;&#x2F; 计算去畸变后图像的内容  for (int v &#x3D; 0; v &lt; rows; v++) &#123;    for (int u &#x3D; 0; u &lt; cols; u++) &#123;      &#x2F;&#x2F; 按照公式，计算点(u,v)对应到畸变图像中的坐标(u_distorted, v_distorted)      double x &#x3D; (u - cx) &#x2F; fx, y &#x3D; (v - cy) &#x2F; fy;      double r &#x3D; sqrt(x * x + y * y);      double x_distorted &#x3D; x * (1 + k1 * r * r + k2 * r * r * r * r) + 2 * p1 * x * y + p2 * (r * r + 2 * x * x);      double y_distorted &#x3D; y * (1 + k1 * r * r + k2 * r * r * r * r) + p1 * (r * r + 2 * y * y) + 2 * p2 * x * y;      double u_distorted &#x3D; fx * x_distorted + cx;      double v_distorted &#x3D; fy * y_distorted + cy;      &#x2F;&#x2F; 赋值 (最近邻插值)      if (u_distorted &gt;&#x3D; 0 &amp;&amp; v_distorted &gt;&#x3D; 0 &amp;&amp; u_distorted &lt; cols &amp;&amp; v_distorted &lt; rows) &#123;        image_undistort.at&lt;uchar&gt;(v, u) &#x3D; image.at&lt;uchar&gt;((int) v_distorted, (int) u_distorted);      &#125; else &#123;        image_undistort.at&lt;uchar&gt;(v, u) &#x3D; 0;      &#125;    &#125;  &#125;<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p><strong>Opencv提供了去畸变函数cv::Undistort().</strong></p><ul><li><p>函数功能：直接对图像进行畸变矫正。</p></li><li><p>其内部调用了initUndistortRectifyMap和remap函数。</p></li></ul><pre class="line-numbers language-c++" data-language="c++"><code class="language-c++">CV_EXPORTS_W void undistort( InputArray src, &#x2F;&#x2F;原始图像                            OutputArray dst,&#x2F;&#x2F;矫正图像                             InputArray cameraMatrix,&#x2F;&#x2F;原相机的内参矩阵                             InputArray distCoeffs,&#x2F;&#x2F;相机矫正参数                             InputArray newCameraMatrix &#x3D; noArray() );&#x2F;&#x2F;新相机内参矩阵<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>直接使用Opencv中的函数主要是对参数写成矩阵形式加进去就可以了</p><pre class="line-numbers language-c++" data-language="c++"><code class="language-c++">#include &lt;opencv2&#x2F;opencv.hpp&gt;#include &lt;string&gt;using namespace std;int main(int argc, char **argv) &#123;    const cv::Mat K &#x3D; ( cv::Mat_&lt;double&gt; ( 3,3 ) &lt;&lt; 458.654, 0.0, 367.215, 0.0, 457.296, 248.375, 0.0, 0.0, 1.0 ); &#x2F;&#x2F; 相机内参矩阵    &#x2F;&#x2F;k1 &#x3D; -0.28340811, k2 &#x3D; 0.07395907, p1 &#x3D; 0.00019359, p2 &#x3D; 1.76187114e-05;    const cv::Mat D &#x3D; (cv::Mat_&lt;double&gt; ( 5,1 ) &lt;&lt;  -0.28340811, 0.07395907, 0.0, 0.00019359, 1.76187114e-05);    &#x2F;&#x2F;double k1 &#x3D; -0.28340811, k2 &#x3D; 0.07395907, k3&#x3D;0,p1 &#x3D; 0.00019359, p2 &#x3D; 1.76187114e-05;    const string str &#x3D; &quot;&#x2F;home&#x2F;chy&#x2F;slambook2&#x2F;ch5&#x2F;imageBasics&#x2F;distorted.png&quot;;  cv::Mat image &#x3D; cv::imread(str, 0);   &#x2F;&#x2F; 图像是灰度图，CV_8UC1  int rows &#x3D; image.rows, cols &#x3D; image.cols;  cv::Mat image_undistort &#x3D; cv::Mat(rows, cols, CV_8UC1);   &#x2F;&#x2F; 去畸变以后的图  cv::undistort(image,image_undistort,K,D,K);  &#x2F;&#x2F; 画图去畸变后图像  cv::imshow(&quot;distorted&quot;, image);  cv::imshow(&quot;undistorted&quot;, image_undistort);  cv::waitKey();  return 0;&#125;<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p><a href="https://blog.csdn.net/cannonfodder9527/article/details/129002265">SLAM14讲第5讲去畸变方法改进</a></p><p><strong>使用 getOptimalNewCameraMatrix + initUndistortRectifyMap + remap 矫正图像</strong></p><p><a href="https://blog.csdn.net/qq_18894441/article/details/122983176">关于OpenCV中的去畸变 c++</a></p><h3 id="ch5-x2F-stereo-x2F-stereoVision-cpp中双目视觉"><a href="#ch5-x2F-stereo-x2F-stereoVision-cpp中双目视觉" class="headerlink" title="ch5&#x2F;stereo&#x2F;stereoVision.cpp中双目视觉"></a>ch5&#x2F;stereo&#x2F;stereoVision.cpp中双目视觉</h3><p><img src="/pic/%E9%80%89%E5%8C%BA_091.png" alt="双目视觉求深度z"></p><p>使用SGBM算法计算左右图像的视差</p><pre class="line-numbers language-c++" data-language="c++"><code class="language-c++">cv::Ptr&lt;cv::StereoSGBM&gt; sgbm &#x3D; cv::StereoSGBM::create(       0, 96, 9, 8 * 9 * 9, 32 * 9 * 9, 1, 63, 10, 100, 32);    &#x2F;&#x2F; sgbm经典参数配置   cv::Mat disparity_sgbm, disparity;   sgbm-&gt;compute(left, right, disparity_sgbm);&#x2F;&#x2F;输入前面两张图，第三个参数是输出   disparity_sgbm.convertTo(disparity, CV_32F, 1.0 &#x2F; 16.0f);  &#x2F;&#x2F;将disparity_sgbm变成32F类型的disparity，这里的disparity才是视差图。  如果Mat类型数据的深度不满足上面的要求，则需要使用convertTo()函数来进行转换。convertTo()函数负责转换数据类型不同的Mat       for (int v &#x3D; 0; v &lt; left.rows; v++)       for (int u &#x3D; 0; u &lt; left.cols; u++) &#123;           if (disparity.at&lt;float&gt;(v, u) &lt;&#x3D; 0.0 || disparity.at&lt;float&gt;(v, u) &gt;&#x3D; 96.0) continue;           Vector4d point(0, 0, 0, left.at&lt;uchar&gt;(v, u) &#x2F; 255.0); &#x2F;&#x2F; 前三维为xyz,第四维为颜色           &#x2F;&#x2F; 根据双目模型计算 point 的位置           double x &#x3D; (u - cx) &#x2F; fx;           double y &#x3D; (v - cy) &#x2F; fy;           double depth &#x3D; fx * b &#x2F; (disparity.at&lt;float&gt;(v, u));           point[0] &#x3D; x * depth;           point[1] &#x3D; y * depth;           point[2] &#x3D; depth;           pointcloud.push_back(point);       &#125;<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p><strong>sgbm参数解释：</strong></p><pre class="line-numbers language-c++" data-language="c++"><code class="language-c++">cv::Ptr&lt;cv::StereoSGBM&gt; sgbm &#x3D; cv::StereoSGBM::create(    0, 96, 9, 8 * 9 * 9, 32 * 9 * 9, 1, 63, 10, 100, 32);    &#x2F;&#x2F; sgbm经典参数配置<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><p><img src="/pic/%E9%80%89%E5%8C%BA_092.png"></p><p><strong>convertTo函数</strong></p><p> 用于计算距离的视差图（CV_32F）和用于肉眼看的视差图（CV_8U）使用的格式不同，并且用于计算的视差图无需进行裁剪和归一化，这些只是为了显示的可读性和美观。所以，在对sgbm进行compute之后得到视差图disparity_sgbm，除以16得到用于计算的视差图disparity（除以16是因为每个像素值由一个16bit表示，其中低位的4位存储的是视差值得小数部分，所以真实视差值应该是该值除以16</p><p><a href="https://blog.csdn.net/weixin_70026476/article/details/127351340">参考:实践部分双目视觉代码讲解</a></p><p>这部分Pangolin的讲解也可以参考上面的链接</p><p><img src="/pic/%E9%80%89%E5%8C%BA_127.png" alt="深度图"></p><p><img src="/pic/%E9%80%89%E5%8C%BA_128.png" alt="点云图"></p><h3 id="ch5-x2F-rgbd-x2F-joinMap-cpp中"><a href="#ch5-x2F-rgbd-x2F-joinMap-cpp中" class="headerlink" title="ch5&#x2F;rgbd&#x2F;joinMap.cpp中"></a>ch5&#x2F;rgbd&#x2F;joinMap.cpp中</h3><p><a href="https://blog.csdn.net/zhiwei121/article/details/95033924">可参考</a></p><p>已知相机内外参，五张RGB图以及他们的深度信息计算任何一个像素的世界坐标系下的位置从而建立一个点云地图</p><pre class="line-numbers language-c++" data-language="c++"><code class="language-c++">&#x2F;&#x2F;起个别名，方便后面使用typedef vector&lt;Sophus::SE3d, Eigen::aligned_allocator&lt;Sophus::SE3d&gt;&gt; TrajectoryType;typedef Eigen::Matrix&lt;double, 6, 1&gt; Vector6d;&#x2F;&#x2F;通过fmt占位符读取图像，具体看下面fmt讲解链接，要先include &lt;boost&#x2F;format.hpp&gt;这个头文件。boost::format fmt(&quot;.&#x2F;%s&#x2F;%d.%s&quot;); &#x2F;&#x2F;图像文件格式colorImgs.push_back(cv::imread((fmt % &quot;color&quot; % (i + 1) % &quot;png&quot;).str()));depthImgs.push_back(cv::imread((fmt % &quot;depth&quot; % (i + 1) % &quot;pgm&quot;).str(), -1)); &#x2F;&#x2F; 使用-1读取原始图像&#x2F;&#x2F; for 在C++11的新特性,具体看下面链接讲解double data[7] &#x3D; &#123;0&#125;;for (auto &amp;d:data)     fin &gt;&gt; d; &#x2F;&#x2F; 计算点云并拼接    &#x2F;&#x2F; 相机内参     double cx &#x3D; 325.5;    double cy &#x3D; 253.5;    double fx &#x3D; 518.0;    double fy &#x3D; 519.0;    double depthScale &#x3D; 1000.0;    vector&lt;Vector6d, Eigen::aligned_allocator&lt;Vector6d&gt;&gt; pointcloud;&#x2F;&#x2F; reserve为容器预先分配内存空间，并未初始化空间元素    pointcloud.reserve(1000000);    for (int i &#x3D; 0; i &lt; 5; i++) &#123;        cout &lt;&lt; &quot;转换图像中: &quot; &lt;&lt; i + 1 &lt;&lt; endl;        cv::Mat color &#x3D; colorImgs[i];        cv::Mat depth &#x3D; depthImgs[i];        Sophus::SE3d T &#x3D; poses[i];        for (int v &#x3D; 0; v &lt; color.rows; v++)            for (int u &#x3D; 0; u &lt; color.cols; u++) &#123;                          &#x2F;*通过用Mat中的ptr模板函数 返回一个unsigned short类型的指针。v表示行 根据内部计算返回data头指针 + 偏移量来计算v行的头指针             * 图像为单通道的   depth.ptr&lt;unsigned short&gt; ( v ) 来获取行指针*&#x2F;                unsigned int d &#x3D; depth.ptr&lt;unsigned short&gt;(v)[u]; &#x2F;&#x2F; 深度值                if (d &#x3D;&#x3D; 0) continue; &#x2F;&#x2F; 为0表示没有测量到                Eigen::Vector3d point;                point[2] &#x3D; double(d) &#x2F; depthScale;&#x2F;&#x2F;实际尺度的一个缩放因子                point[0] &#x3D; (u - cx) * point[2] &#x2F; fx;                point[1] &#x3D; (v - cy) * point[2] &#x2F; fy;                Eigen::Vector3d pointWorld &#x3D; T * point;&#x2F;&#x2F;将相机坐标系转换为世界坐标系                Vector6d p;                &#x2F;&#x2F;head&lt;n&gt;()函数是对于Eigen库中的向量类型而言的，表示提取前n个元素                &#x2F;&#x2F;方法一                p.head&lt;3&gt;() &#x3D; pointWorld;                p[5] &#x3D; color.data[v * color.step + u * color.channels()];   &#x2F;&#x2F; blue                p[4] &#x3D; color.data[v * color.step + u * color.channels() + 1]; &#x2F;&#x2F; green                p[3] &#x3D; color.data[v * color.step + u * color.channels() + 2]; &#x2F;&#x2F; red                &#x2F;&#x2F;方法二：                &#x2F;&#x2F;程序上方读取了一张图片color                    p[5]&#x3D;color.at&lt;cv::Vec3b&gt;(v, u)[0];  &#x2F;&#x2F;B                     p[4]&#x3D;color.at&lt;cv::Vec3b&gt;(v, u)[1];  &#x2F;&#x2F;G                    p[3]&#x3D;color.at&lt;cv::Vec3b&gt;(v, u)[2];  &#x2F;&#x2F;R                                pointcloud.push_back(p);            &#125;    &#125;vector&lt;Vector6d, Eigen::aligned_allocator&lt;Vector6d&gt;&gt; pointcloud;pointcloud.reserve(1000000);<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p><a href="https://blog.csdn.net/hhl317/article/details/118937838">c++中的boost::format,fmt使用方法</a></p><p><a href="https://blog.csdn.net/try_again_later/article/details/81566850">boost::format 以及 for 新特性</a></p><p><a href="https://blog.csdn.net/CxC2333/article/details/107735638?spm=1001.2101.3001.6650.3&utm_medium=distribute.pc_relevant.none-task-blog-2~default~CTRLIST~Rate-3-107735638-blog-107092078.235%5Ev28%5Epc_relevant_recovery_v2&depth_1-utm_source=distribute.pc_relevant.none-task-blog-2~default~CTRLIST~Rate-3-107735638-blog-107092078.235%5Ev28%5Epc_relevant_recovery_v2&utm_relevant_index=6">Opencv关于成员函数data，step，at的使用</a></p><pre class="line-numbers language-c++" data-language="c++"><code class="language-c++">&#x2F;*                                            备注： 3通道的图像的遍历方式总结 * 对于单通道来说 每个像素占8位 3通道则是每个矩阵元素是一个Vec3b 即一个三维的向量 向量内部元素为8位数的unsigned char类型 * 1、使用at遍历图像 * for(v)row *  for(u)col *      image.at&lt;Vec3b&gt;（v,u）[0] 表示第一个通道的像素的值 *      image.at&lt;Vec3b&gt;(v,u)[1] *      image.at&lt;Vec3b&gt;(v,u)[2] * 2、使用迭代器方式 (实际上就是一个指针指向了 cv::Mat矩阵元素) * cv::MatIterator_&lt;Vec3b&gt;begin,end; * for( begin &#x3D; image.begin&lt;Vec3b&gt;(), end &#x3D; image.end&lt;Vec3b&gt;() ; begin !&#x3D; end;  ) *      (*begin)[0] &#x3D; ... *      (*begin)[1] &#x3D; ... *      (*begin)[2] &#x3D; ... * * 3、用指针的方式操作 * for(v) *  for(u) *      image.ptr&lt;Vec3b&gt;(v)[u][0] 表示第一个通道 *      image.ptr&lt;Vec3b&gt;(v)[u][0] 表示第二通道 *              . *              . *              . * *&#x2F;<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p><img src="/pic/%E9%80%89%E5%8C%BA_126.png" alt="点云图"></p>]]></content>
      
      
      <categories>
          
          <category> slam </category>
          
          <category> c++ </category>
          
      </categories>
      
      
        <tags>
            
            <tag> c++ </tag>
            
            <tag> slam </tag>
            
            <tag> CMake </tag>
            
            <tag> Sophus </tag>
            
            <tag> Eigen </tag>
            
            <tag> Pangolin </tag>
            
            <tag> 图像去畸变 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>orb_slam2改进代码的相关汇总</title>
      <link href="/2023/04/05/orb-slam2/"/>
      <url>/2023/04/05/orb-slam2/</url>
      
        <content type="html"><![CDATA[<h1 id="orb-slam2改进代码的相关汇总"><a href="#orb-slam2改进代码的相关汇总" class="headerlink" title="orb_slam2改进代码的相关汇总"></a>orb_slam2改进代码的相关汇总</h1><h2 id="点线特征相关"><a href="#点线特征相关" class="headerlink" title="点线特征相关"></a>点线特征相关</h2><p><strong>添加了线特征。从3D密集SLAM进行表面重建的增量3D线段提取</strong></p><p><a href="https://github.com/atlas-jj/ORB_Line_SLAM">Add line feature based ORB-SLAM2</a></p><p><strong>RGB-D模式下添加了点线融合</strong></p><p><a href="https://github.com/maxee1900/RGBD-PL-SLAM">RGBD-SLAM with Point and Line Features, developed based on ORB_SLAM2</a></p><p><strong>单目线特征</strong></p><p><a href="https://github.com/lanyouzibetty/ORB-SLAM2_with_line">ORB-SLAM2_with_line, Monocular ORB-SLAM with Line Features</a></p><p><strong>双目点线融合,在弱纹理环境中传统点特征方法失效的情况下拥有较高的运行鲁棒性</strong></p><p><a href="https://github.com/rubengooj/pl-slam">PL-SLAM: a Stereo SLAM System through the Combination of Points and Line Segments</a></p><h2 id="用新的特征点替代ORB"><a href="#用新的特征点替代ORB" class="headerlink" title="用新的特征点替代ORB"></a>用新的特征点替代ORB</h2><p><strong>使用了一种更好的特征选择方法</strong></p><p><a href="https://github.com/ivalab/gf_orb_slam2">GF-ORB-SLAM2, Real-Time SLAM for Monocular, Stereo and RGB-D Cameras, with Loop Detectionand Relocalization</a></p><p><strong>用SuperPoint 替代ORB来进行特征提取</strong></p><p><a href="https://github.com/KinglittleQ/SuperPoint_SLAM">SuperPoint-SLAM</a></p><p><strong>设计的GCNv2具有与ORB功能相同的描述符格式,可以替代关键点提取,精度更高,适合在嵌入式低功耗平台上运行</strong></p><p><a href="https://github.com/jiexiong2016/GCNv2_SLAM">GCNv2: Efficient Correspondence Prediction for Real-Time SLAM</a></p><h2 id="直接法代替特征点法"><a href="#直接法代替特征点法" class="headerlink" title="直接法代替特征点法"></a>直接法代替特征点法</h2><p><strong>使用SVO中直接法来跟踪代替耗时的特征点提取匹配,在保持同样精度的情况下,是原始ORB-SLAM2速度的3倍</strong></p><p><a href="https://github.com/gaoxiang12/ORB-YGZ-SLAM">ORB-YGZ-SLAM, average 3x speed up and keep almost same accuracy v.s. ORB-SLAM2, use directtracking in SVO to accelerate the feature matching</a></p><h2 id="融合其他传感器"><a href="#融合其他传感器" class="headerlink" title="融合其他传感器"></a>融合其他传感器</h2><p><strong>双目VIO版本,加入了LK光流和滑动窗口BA优化</strong></p><p><a href="https://github.com/gaoxiang12/ygz-stereo-inertial">YGZ-stereo-inertial SLAM, LK optical flow + sliding window bundle adjustment</a></p><p><strong>京胖实现的VI-ORB-SLAM2</strong></p><p><a href="https://github.com/jingpang/LearnVIORB">VIORB, An implementation of Visual Inertial ORBSLAM based on ORB-SLAM2</a></p><p><strong>支持鱼眼,不需要rectify和裁剪输入图</strong></p><p><a href="https://github.com/lsyads/fisheye-ORB-SLAM">Fisheye-ORB-SLAM, A real-time robust monocular visual SLAM system based on ORB-SLAM for fisheye cameras, without rectifying or cropping the input images</a></p><h2 id="地图相关"><a href="#地图相关" class="headerlink" title="地图相关"></a>地图相关</h2><p><strong>添加保存和导入地图功能</strong></p><p><a href="https://github.com/AlejandroSilvestri/osmap">Osmap, Save and load orb-slam2 maps</a></p><p><a href="https://github.com/Jiankai-Sun/ORB_SLAM2_Enhanced">ORB_SLAM2 with map load&#x2F;save function</a></p><p><strong>添加了地图可视化</strong></p><p><a href="https://github.com/AlejandroSilvestri/Osmap-viewer">Viewer for maps from ORB-SLAM2 Osmap</a></p><p><strong>高翔实现的添加稠密点云地图</strong></p><p><a href="https://github.com/gaoxiang12/ORBSLAM2_with_pointcloud_map">ORBSLAM2_with_pointcloud_map</a></p><p><strong>在高翔基础上添加了稠密闭环地图</strong></p><p><a href="https://github.com/tiantiandabaojian/ORB-SLAM2_RGBD_DENSE_MAP">ORB-SLAM2_RGBD_DENSE_MAP, modified from Xiang Gao’s “ORB_SLAM2_modified”. It is added a dense loopclosing map model</a></p><h2 id="动态环境"><a href="#动态环境" class="headerlink" title="动态环境"></a>动态环境</h2><p><strong>适合动态环境,增加了动态物体检测和背景修复的能力</strong></p><p><a href="https://github.com/BertaBescos/DynaSLAM">DynaSLAM, is a SLAM system robust in dynamic environments for monocular, stereo and RGB-Dsetups(Mask-Rcnn)</a></p><p><a href="https://github.com/bijustin/YOLO-DynaSLAM">YOLO版本的DynaSLAM</a></p><p><strong>使用语义分割网络和运动一致性检查的方法（光流法）相结合的方法减少视觉SLAM中动态物体造成的影响</strong></p><p><a href="https://github.com/ivipsourcecode/DS-SLAM">DS-SLAM: A Semantic Visual SLAM towards Dynamic Environments</a></p><p><strong>基于语义的实时动态vSLAM算法RDS-SLAM，跟踪线程不再需要等待语义结果</strong></p><p><a href="https://github.com/yubaoliu/RDS-SLAM">RDS-SLAM: Real-Time Dynamic SLAM Using Semantic Segmentation Methods</a></p><p><strong>一种用于动态环境下资源受限机器人的实时RGB-D惯性里程计系统-Dynamic-VINS</strong></p><p><a href="https://github.com/HITSZ-NRSL/Dynamic-VINS">RGB-D Inertial Odometry for a Resource-Restricted Robot in Dynamic Environments</a></p><p><em><strong>动态slam汇总大全</strong></em></p><p><a href="https://github.com/oceanechy/Awesome_Dynamic_SLAM">动态slam汇总大全</a></p><h2 id="语义相关"><a href="#语义相关" class="headerlink" title="语义相关"></a>语义相关</h2><p><strong>动态语义SLAM 目标检测+VSLAM+光流&#x2F;多视角几何动态物体检测+octomap地图+目标数据库</strong></p><p><a href="https://github.com/Ewenwan/ORB_SLAM2_SSD_Semantic">ORB_SLAM2_SSD_Semantic, 动态语义SLAM 目标检测+VSLAM+光流&#x2F;多视角几何动态物体检测+octomap地图+目标数据库</a></p><p><strong>用YOLO v3的语义信息来增加跟踪性能</strong></p><p><a href="https://github.com/Eralien/TE-ORB_SLAM2">TE-ORB_SLAM2, Tracking Enhanced ORB-SLAM2</a></p><p><strong>通过手持RGB-D相机进行SLAM,ORB-SLAM2作为后端,用PSPNet做语义预测并将语义融入octomap</strong></p><p><a href="https://github.com/floatlazer/semantic_slam">Semantic SLAM,Real time semantic slam in ROS with a hand held RGB-D camera orb-slam2 with semantic labelling</a></p><p><strong>用深度学习的场景理解来增强传统特征检测方法,基于贝叶斯SegNet 和ORB-SLAM2,用于长时间定位</strong></p><p><a href="https://github.com/navganti/SIVO">SIVO: Semantically Informed Visual Odometry and Mapping</a></p><blockquote><p>参考计算机视觉life内容，仅供学习使用</p></blockquote>]]></content>
      
      
      <categories>
          
          <category> slam </category>
          
      </categories>
      
      
        <tags>
            
            <tag> slam </tag>
            
            <tag> orb_slam2 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>slam14简单总结捋思路</title>
      <link href="/2023/04/05/slam14-2/"/>
      <url>/2023/04/05/slam14-2/</url>
      
        <content type="html"><![CDATA[<p><img src="/pic/%E9%80%89%E5%8C%BA_093.png"></p><p><img src="/pic/%E9%80%89%E5%8C%BA_094.png"></p><p><img src="/pic/%E9%80%89%E5%8C%BA_095.png"></p><p>实际操作 图像去畸变 双目模型生成点云 RGB-D生成点云</p><p><img src="/pic/%E9%80%89%E5%8C%BA_096.png"></p><p><img src="/pic/%E9%80%89%E5%8C%BA_097.png"></p><p><img src="/pic/%E9%80%89%E5%8C%BA_098.png"></p><p><img src="/pic/%E9%80%89%E5%8C%BA_099.png"></p><p><img src="/pic/%E9%80%89%E5%8C%BA_100.png"></p><p><img src="/pic/%E9%80%89%E5%8C%BA_101.png"></p>]]></content>
      
      
      <categories>
          
          <category> slam </category>
          
          <category> c++ </category>
          
      </categories>
      
      
        <tags>
            
            <tag> c++ </tag>
            
            <tag> slam </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Opencv系列（五）</title>
      <link href="/2023/04/04/opencv5/"/>
      <url>/2023/04/04/opencv5/</url>
      
        <content type="html"><![CDATA[<h2 id="3D相关HEF矩阵计算"><a href="#3D相关HEF矩阵计算" class="headerlink" title="3D相关HEF矩阵计算"></a>3D相关HEF矩阵计算</h2><h3 id="F矩阵含义"><a href="#F矩阵含义" class="headerlink" title="F矩阵含义"></a>F矩阵含义</h3><ul><li>相邻两帧一组对应像素点的约束关系</li><li>像素点在下一帧极线上的位置</li></ul><p><img src="/pic/%E9%80%89%E5%8C%BA_056.png" alt="运动示意图"></p><p><img src="/pic/%E9%80%89%E5%8C%BA_057.png" alt="约束关系"></p><p><img src="/pic/%E9%80%89%E5%8C%BA_058.png" alt="极线位置"></p><p><img src="/pic/%E9%80%89%E5%8C%BA_059.png" alt="八点法前的归一化"></p><p><img src="/pic/%E9%80%89%E5%8C%BA_060.png" alt="八点法的讨论"></p><p><img src="/pic/%E9%80%89%E5%8C%BA_061.png" alt="最小二乘求解(≥8点)"></p><p><img src="/pic/%E9%80%89%E5%8C%BA_062.png" alt="最小二乘求解的讨论"></p><p><img src="/pic/%E9%80%89%E5%8C%BA_063.png" alt="无敌的RANSAC(≥8点)"></p><p><img src="/pic/%E9%80%89%E5%8C%BA_064.png" alt="无敌的RANSAC的讨论"></p><p><img src="/pic/%E9%80%89%E5%8C%BA_065.png" alt="无敌的RANSAC的讨论"></p><p><img src="/pic/%E9%80%89%E5%8C%BA_066.png" alt="无敌的RANSAC的讨论"></p><p><img src="/pic/%E9%80%89%E5%8C%BA_067.png" alt="Opencv中的函数"></p><h3 id="E矩阵含义"><a href="#E矩阵含义" class="headerlink" title="E矩阵含义"></a>E矩阵含义</h3><p>本质矩阵是归一化平面下的基本矩阵的特殊形式</p><p><img src="/pic/%E9%80%89%E5%8C%BA_068.png" alt="约束关系"></p><p><img src="/pic/%E9%80%89%E5%8C%BA_069.png" alt="具体求解方法(不展开)"></p><p><img src="/pic/%E9%80%89%E5%8C%BA_070.png" alt="关于E矩阵的讨论"></p><p><img src="/pic/%E9%80%89%E5%8C%BA_071.png" alt="Opencv中的函数"></p><p><img src="/pic/%E9%80%89%E5%8C%BA_072.png" alt="根据Essential Matrix恢复位姿信息R与t)"></p><p><img src="/pic/%E9%80%89%E5%8C%BA_073.png" alt="根据Essential Matrix恢复位姿信息R与t)"></p><p><img src="/pic/%E9%80%89%E5%8C%BA_074.png" alt="Opencv中的函数"></p><p><img src="/pic/%E9%80%89%E5%8C%BA_075.png" alt="根据E估计深度(三角化)"></p><h3 id="H矩阵意义"><a href="#H矩阵意义" class="headerlink" title="H矩阵意义"></a>H矩阵意义</h3><p><img src="/pic/%E9%80%89%E5%8C%BA_076.png"></p><p><img src="/pic/%E9%80%89%E5%8C%BA_077.png" alt="Opencv中的函数"></p><blockquote><p>注：以上截图内容参考计算机视觉life</p></blockquote><h2 id="位姿求取"><a href="#位姿求取" class="headerlink" title="位姿求取"></a>位姿求取</h2><h3 id="求取位姿的方法"><a href="#求取位姿的方法" class="headerlink" title="求取位姿的方法"></a>求取位姿的方法</h3><ol><li>对极约束：2D-2D，通过二维图像点的对应关系，恢复出在两帧之间摄像机的运动。</li><li>PnP：3D-2D,求解3D到2D点对运动的方法。描述了当知道n个3D空间点及其投影位置时，如何估计相机的位姿。</li><li>ICP：3D-3D，配对好的3D点,已知世界参考系下的3D点和相机参考系下的3D点</li></ol><h3 id="PnP概念"><a href="#PnP概念" class="headerlink" title="PnP概念"></a>PnP概念</h3><p>如果场景的三维结构已知，利用多个控制点在三维场景中的坐标及其在图像中的透视投影坐标即可<br>求解出相机坐标系与世界坐标系之间的绝对位姿关系，包括绝对平移向量t以及旋转矩阵R，该类求<br>解方法统称为N点透视位姿求解（Perspective-N-Point，PNP问题）。这里的控制点是指准确知道三<br>维空间坐标位置，同时也知道对应图像平面坐标的点。<br><strong>已知条件：</strong></p><ul><li>n3D参考点(3D reference points)坐标; </li><li>与这n个3D点对应的、投影在图像上的2D参考点(2D reference points)坐标; </li><li>相机的的内参K;</li></ul><p><strong>求解：</strong></p><ul><li>相机的位姿。</li></ul><h3 id="PnP应用场景"><a href="#PnP应用场景" class="headerlink" title="PnP应用场景"></a>PnP应用场景</h3><p><strong>场景主要有两个</strong></p><ol><li>求解相机的位姿，一般应用于AR，人脸跟踪等；<br>通常输入的是物体在世界坐标系下的3D点以及这些3D点在图像上投影的2D点，因此求得的是相机（相机坐标系）相对于真实物体（世界坐标系）的位姿</li></ol><p><img src="/pic/%E9%80%89%E5%8C%BA_078.png"></p><ol start="2"><li>求取前一帧到当前帧的相机位姿变化，一般用于slam中；</li></ol><p>通常输入的是上一帧中的3D点（在上一帧的相机坐标系下表示的点）和这些3D点在当前帧中的投影得到的2D点，所以它求得的是当前帧相对于上一帧的位姿变换</p><p><img src="/pic/%E9%80%89%E5%8C%BA_079.png"></p><h3 id="Opencv中的函数"><a href="#Opencv中的函数" class="headerlink" title="Opencv中的函数"></a>Opencv中的函数</h3><p><strong>solvePnP</strong></p><p><img src="/pic/%E9%80%89%E5%8C%BA_080.png"></p><p>flags有如下的求解方法：</p><pre class="line-numbers language-c++" data-language="c++"><code class="language-c++">enum &#123; SOLVEPNP_ITERATIVE &#x3D; 0,       SOLVEPNP_EPNP      &#x3D; 1, &#x2F;&#x2F;!&lt; EPnP: Efficient Perspective-n-Point Camera Pose Estimation @cite lepetit2009epnp       SOLVEPNP_P3P       &#x3D; 2, &#x2F;&#x2F;!&lt; Complete Solution Classification for the Perspective-Three-Point Problem @cite gao2003complete       SOLVEPNP_DLS       &#x3D; 3, &#x2F;&#x2F;!&lt; A Direct Least-Squares (DLS) Method for PnP  @cite hesch2011direct       SOLVEPNP_UPNP      &#x3D; 4, &#x2F;&#x2F;!&lt; Exhaustive Linearization for Robust Camera Pose and Focal Length Estimation @cite penate2013exhaustive       SOLVEPNP_AP3P      &#x3D; 5, &#x2F;&#x2F;!&lt; An Efficient Algebraic Solution to the Perspective-Three-Point Problem @cite Ke17       SOLVEPNP_MAX_COUNT      &#x2F;&#x2F;!&lt; Used for count&#125;;<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><blockquote><p>提示</p></blockquote><ol><li>SOLVEPNP_ITERATIVE，默认值，它通过迭代求出重投影误差最小的解作为问题的最优解。该方法<br>实质是迭代求出重投影误差最小的解，这个解显然不一定是正解。</li></ol><p><strong>使用范围：</strong>4对共面匹配点，小于4组点不准确</p><ol start="2"><li>SOLVEPNP_P3P，是使用非常经典的Gao的P3P问题求解算法。P3P算法要求输入的控制点个数只能是4对。3对点求出4组可能的解，通过对第4对点进行重投影，返回重投影误差最小的，确定最优解。可以使用任意4对匹配点求解，不要共面，特征点数量不为4时报错.</li></ol><p><strong>使用范围：</strong>4对不共面匹配点</p><ol start="3"><li>SOLVEPNP_EPNP，该方法使用EfficientPNP方法，求解问题EPnP使用大于等于3组点，是目前最有效的PnP求解方法。</li></ol><p><strong>使用范围：</strong>最少需要4对不共面的匹配点（对于共面的情况只需要3对）,点太少能运行，但不准确</p><p><strong>注：</strong>方法 SOLVEPNP_DLS 和 SOLVEPNP_UPNP 不能使用，因为当前的实现是不稳定的，有时会给出完全错误的结果。如果传递这两个标志之一，则将使用 SOLVEPNP_EPNP 方法。<br>建议：小于4组匹配点时，用SOLVEPNP_ITERATIVE；在4组匹配点时，用SOLVEPNP_P3P；大于4组匹配点时，用SOLVEPNP_EPNP。</p><p><strong>solvePnPRansac</strong></p><p>solvePnP的一个缺点是对异常值不够鲁棒，当我们用相机定位真实世界的点，可能存在错配，对误<br>匹配进行Ransac过滤，RANSAC是“Random Sample Consensus（随机抽样一致）”的缩写</p><p><img src="/pic/%E9%80%89%E5%8C%BA_081.png"></p><p><a href="https://docs.opencv.org/4.0.1/d9/d0c/group__calib3d.html#ga549c2075fac14829ff4a58bc931c033d">可查看官方文档</a></p><h2 id="扩展-PnP问题求解原理"><a href="#扩展-PnP问题求解原理" class="headerlink" title="扩展-PnP问题求解原理"></a>扩展-PnP问题求解原理</h2><p>目前主要有直接线性变换（DLT），P3P，EPnP，UPnP以及非线性优化方法。</p><p>关于PnP的求解问题</p><p><img src="/pic/%E9%80%89%E5%8C%BA_082.png"></p><p>把（3）式带入（1）式和（2）式，整理得：</p><p><img src="/pic/%E9%80%89%E5%8C%BA_083.png"></p><p>我们可以看出，fx fy u0 v0是相机内参，上一节中已经求出，Xw Yw x y是一组3D&#x2F;2D点的坐标，所以未知数有R11 R12 R13 R21 R22 R23 R31 R32 R33 T1 T2 T3一共12个，由于旋转矩阵是正交矩阵，每行每列都是单位向量且两两正交，所以R的自由度为3，秩也是3，比如知道R11 R12 R21就能求出剩下的Rxx。加上平移向量的3个未知数，一共6个未知数，而每一组2D&#x2F;3D点提供的x y Xw Yw Zw可以确立两个方程，所以3组2D&#x2F;3D点的坐标能确立6个方程从而解出6个未知数。故PnP需要知道至少3组2D&#x2F;3D点。</p><h3 id="DLT"><a href="#DLT" class="headerlink" title="DLT"></a>DLT</h3><p>通过2D-3D的关系直接构建方程，直接求解：<br>DLT主要是通过构建一个12维的增广矩阵（R|t），然后通过投影矩阵构建一个方程：</p><p>通过最后一行，消去s，最后构建一个12维的线性方程组，通过6对匹配点（一对点两个方程）来求解中间的矩阵。 最后将[R|t]左侧3<em>3矩阵块进行QR分解，用一个旋转矩阵去近似（将3</em>3矩阵空间投影到SE(3)流形上）。</p><p><img src="/pic/%E9%80%89%E5%8C%BA_085.png"></p><p>优点：求解简单<br>缺点：直接将 T 矩阵看成了 12 个未知数，忽略了它们之间的联系。因为旋转矩阵 R ∈ SO(3)，用 DLT 求出的解不一定满足该约束，它是一个一般矩阵。寻找一个最好的旋转矩阵对它进行近似</p><h3 id="P3P（ICP-3D-3D）"><a href="#P3P（ICP-3D-3D）" class="headerlink" title="P3P（ICP:3D-3D）"></a>P3P（ICP:3D-3D）</h3><p>采用一种变换的形式，把2D-3D匹配关系，变换成3D-3D点的匹配关系：<br>将世界坐标系下的ABC三点和图像坐标系下的abc三点匹配，其中AB，BC，AC的⻓度已知，<br>&lt;a,b&gt;，&lt;b,c&gt;，&lt;a,c&gt;也是已知，通过余弦定理得出方程组</p><p><img src="/pic/%E9%80%89%E5%8C%BA_086.png"></p><p>类似于分解 E 的情况，该方程最多可能得到四个解，但我们可以用验证点来计算最可能的解，得到<br>A, B, C 在相机坐标系下的 3D 坐标。然后，根据 3D-3D 的点对，使用类似ICP的坐标系对，计算相<br>机的运动 R, t。</p><p>优点：需要的匹配点少</p><p>缺点：P3P 只利用三个点的信息。当给定的配对点多于 3 组时，难以利用更多的信息；如果 3D 点<br>或 2D 点受噪声影响，或者存在误匹配，则算法失效。</p><p>可参考论文《<a href="https://cmp.felk.cvut.cz/~zimmerk/track3D/papers/p3p.pdf">CompleteSolution Classification for the Perspective-Three-Point Problem</a>》</p><h3 id="EPnP"><a href="#EPnP" class="headerlink" title="EPnP"></a>EPnP</h3><p>将世界坐标系中的3D坐标表示为一组虚拟的控制点的加权和。</p><p><img src="/pic/%E9%80%89%E5%8C%BA_087.png"></p><p>可参考论文《<a href="https://github.com/cvlab-epfl/EPnP">EPnP: Efficient Perspective-n-Point Camera Pose Estimation</a>》</p>]]></content>
      
      
      <categories>
          
          <category> Opencv </category>
          
          <category> PnP </category>
          
          <category> HEF </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Opencv </tag>
            
            <tag> HEF矩阵 </tag>
            
            <tag> PnP </tag>
            
            <tag> EPnP </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Opencv系列（四）</title>
      <link href="/2023/04/03/opencv4/"/>
      <url>/2023/04/03/opencv4/</url>
      
        <content type="html"><![CDATA[<h1 id="Opencv相机标定"><a href="#Opencv相机标定" class="headerlink" title="Opencv相机标定"></a>Opencv相机标定</h1><p>基本理论知识请查看如下链接：</p><p><a href="https://zhuanlan.zhihu.com/p/520357612">相机模型</a></p><h2 id="张正友标定法"><a href="#张正友标定法" class="headerlink" title="张正友标定法"></a>张正友标定法</h2><p><img src="/pic/%E9%80%89%E5%8C%BA_055.png"></p><p><strong>张正友标定法假设</strong></p><ul><li>标定板的角点在一个平面上</li><li>世界坐标系的xy平面在标定板平面上，Z&#x3D;0</li><li>相机模型不考虑畸变</li></ul><h2 id="Opencv现有函数"><a href="#Opencv现有函数" class="headerlink" title="Opencv现有函数"></a>Opencv现有函数</h2> <pre class="line-numbers language-c++" data-language="c++"><code class="language-c++">cv::calibrateCamera(object_points, &#x2F;&#x2F;三维点坐标                        image_points_seq, &#x2F;&#x2F;像素坐标                        image_size, &#x2F;&#x2F;图像尺寸                        cameraMatrix, &#x2F;&#x2F;输出相机矩阵                        distCoeffs, &#x2F;&#x2F;输出畸变矩阵                        rvecsMat, tvecsMat, &#x2F;&#x2F;r,t                        0);cv::findChessboardCorners(imageInput,&#x2F;&#x2F;输入图像                          board_size,&#x2F;&#x2F;标定板上角点的行列                          image_points_buf&#x2F;&#x2F;输出角点的像素坐标                         )cv::cornerSubPix(view_gray,&#x2F;&#x2F;输入图像，最好是灰度图                 image_points_buf,&#x2F;&#x2F;输入输出角点的像素坐标                 cv::Size(5,5),&#x2F;&#x2F;搜索窗口的半径                 cv::Size(-1,-1),&#x2F;&#x2F;-1表示忽略                 cv::TermCriteria(cv::TermCriteria::MAX_ITER + cv::TermCriteria::EPS,30,&#x2F;&#x2F;最大迭代次数                    0.1)&#x2F;&#x2F;最小精度                );cv::drawChessboardCorners(view_gray, board_size, image_points_buf,                           true);&#x2F;&#x2F;如果角点全部找到，返回true<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p><a href="https://blog.csdn.net/dcrmg/article/details/52929669?ops_request_misc=&request_id=&biz_id=102&utm_term=boardsize%20%20opencv&utm_medium=distribute.pc_search_result.none-task-blog-2~all~sobaiduweb~default-4-.first_rank_v2_pc_rank_v29&spm=1018.2226.3001.4187">Opencv 张正友相机标定傻瓜教程</a></p>]]></content>
      
      
      <categories>
          
          <category> Opencv </category>
          
          <category> Camera </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Opencv </tag>
            
            <tag> 相机模型 </tag>
            
            <tag> 相机标定 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Opencv系列（三）</title>
      <link href="/2023/04/03/opencv3/"/>
      <url>/2023/04/03/opencv3/</url>
      
        <content type="html"><![CDATA[<h1 id="Opencv显示与绘制"><a href="#Opencv显示与绘制" class="headerlink" title="Opencv显示与绘制"></a>Opencv显示与绘制</h1><h2 id="OpenCV-的基础绘制功能"><a href="#OpenCV-的基础绘制功能" class="headerlink" title="OpenCV 的基础绘制功能"></a>OpenCV 的基础绘制功能</h2><p>OpenCV 中提供直线、椭圆、矩形、圆以及多边形的绘制功能。 在OpenCV 的图形绘制中我们会经常使用以下两种结构：</p><ul><li>cv::Point 和cv::Scalar</li></ul><blockquote><p>cv::Point 表示2D 平面上的点，通过指定其在图像上的坐标位置x 和y 来实现。<br>语法结构如下：<br>Point pt; pt.x &#x3D; 10； pt.y &#x3D; 8; 或者 Point pt &#x3D; Point(10, 8);</p></blockquote><blockquote><p>cv::Scalar 表示一个含有4 个元素的向量，OpenCV 中用来传递像素值。<br>语法结构如下：<br>Scalar( a, b, c )<br>其中Blue &#x3D; a, Green &#x3D; b, Red &#x3D; c。这里由于我们只有BGR 三个颜色值，所以我们只需要定义三个变量，最后一个元素可以省略。 </p></blockquote><p>下面开始介绍直线、椭圆、矩形、圆以及多边形分别的语法结构:</p><p><strong>直线</strong></p><pre class="line-numbers language-c++" data-language="c++"><code class="language-c++">CV_EXPORTS_W void line(InputOutputArray img, Point pt1, Point pt2, const Scalar&amp; color,                     int thickness &#x3D; 1, int lineType &#x3D; LINE_8, int shift &#x3D; 0);<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><p>参数：</p><ul><li>img:  图像 </li><li>pt:  线段的起点 </li><li>pt2:  线段的终点 </li><li>color:  直线的颜色 </li><li>thickness:  直线的粗细 </li><li>lineType:  直线类型，具体可以参考下表 </li><li>shift:  点坐标中的小数点位数</li></ul><p><img src="/pic/%E9%80%89%E5%8C%BA_047.png"></p><p><img src="/pic/%E9%80%89%E5%8C%BA_048.png"></p><p>8 联通线是指下一个点连接上一个点的边或者角，4 联通线是指下一个点与上一个点边相连。4 联通线消除了8 联通线的断裂瑕疵</p><p><strong>椭圆</strong></p><pre class="line-numbers language-c++" data-language="c++"><code class="language-c++">CV_EXPORTS_W void ellipse(InputOutputArray img, Point center, Size axes,                        double angle, double startAngle, double endAngle,                        const Scalar&amp; color, int thickness &#x3D; 1,                        int lineType &#x3D; LINE_8, int shift &#x3D; 0);<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span></span></code></pre><p>参数： </p><ul><li>img:  图像 </li><li>center:  椭圆中心 </li><li>axes:  椭圆主轴尺寸的一半 </li><li>angle:  椭圆旋转角度 </li><li>startAngle:  椭圆弧的起始角度 </li><li>engAngle:  椭圆弧的终止角度 </li><li>color:  椭圆的颜色 </li><li>thickness:  椭圆轮廓的粗细，如果不设置默认填充</li><li>lineType:  椭圆边界线类型 shift:  中心坐标和轴值的小数点位数</li></ul><p>cv::ellipse 可以用来绘制椭圆线、实心椭圆、椭圆弧、椭圆扇面。如果想要绘制一个完整的椭圆，startAngle&#x3D;0、endAngle&#x3D;360。如果起始角度大于终止角度，他们会进行交换。下图 在绘制蓝色弧时各个参数的含义。</p><p><img src="/pic/%E9%80%89%E5%8C%BA_049.png" alt="椭圆弧绘制中各参数的意义"></p><p><strong>矩形</strong></p><pre class="line-numbers language-c++" data-language="c++"><code class="language-c++">CV_EXPORTS_W void rectangle(InputOutputArray img, Point pt1, Point pt2,                          const Scalar&amp; color, int thickness &#x3D; 1,                          int lineType &#x3D; LINE_8, int shift &#x3D; 0);<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre><p>参数： </p><ul><li>img:  图像</li><li>pt1:  矩形的顶点 </li><li>pt2:与pt1 相对的矩形顶点 </li><li>color:  矩形颜色或者亮度 </li><li>thickness:  矩形轮廓的粗细 </li><li>lineType:  线条类型 </li><li>shift:  点坐标中的小数点位数</li></ul><p><strong>多线段</strong></p><pre class="line-numbers language-c++" data-language="c++"><code class="language-c++">CV_EXPORTS void polylines(Mat&amp; img, const Point* const* pts, const int* npts,                          int ncontours, bool isClosed, const Scalar&amp; color,                          int thickness &#x3D; 1, int lineType &#x3D; LINE_8, int shift &#x3D; 0 );<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre><p>参数： </p><ul><li>img:  图像 </li><li>pts:  多边形曲线阵列 </li><li>isClosed:  所绘制多段线是否闭合 </li><li>color:  多段线颜色 </li><li>thickness:  多段线粗细 </li><li>lineType:  多段线种类 </li><li>shift:  点坐标中的小数点位数</li></ul><p><strong>多边形填充</strong></p><pre class="line-numbers language-c++" data-language="c++"><code class="language-c++">CV_EXPORTS void fillPoly(Mat&amp; img, const Point** pts,                         const int* npts, int ncontours,                         const Scalar&amp; color, int lineType &#x3D; LINE_8, int shift &#x3D; 0,                         Point offset &#x3D; Point() );<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span></span></code></pre><p>cv::fillPoly 可以填充由多边形包围的区域，可用于填充复杂区域。 </p><p>参数： </p><ul><li>img:  图像 </li><li>pts:  多边形数组，每个多边形都是一组点 </li><li>npts:  顶点个数 </li><li>ncontours:  多边形轮廓个数 </li><li>color:  多边形颜色 </li><li>lineType:  多边形边界粗细 </li><li>shift:  点坐标的小数点位数 </li><li>offset:  可选择轮廓点偏移量</li></ul><p><strong>实例</strong></p><pre class="line-numbers language-c++" data-language="c++"><code class="language-c++">#include &lt;opencv2&#x2F;core.hpp&gt; #include &lt;opencv2&#x2F;imgproc.hpp&gt; #include &lt;opencv2&#x2F;highgui.hpp&gt;#define w 400 using namespace cv; void MyEllipse( Mat img, double angle ); void MyFilledCircle( Mat img, Point center ); void MyPolygon( Mat img ); void MyLine( Mat img, Point start, Point end ); int main( void )&#123;&#x2F;&#x2F;创建两个窗口和两个图像来进行图形绘制    char atom_window[] &#x3D; &quot;Drawing 1: Atom&quot;;     char rook_window[] &#x3D; &quot;Drawing 2: Rook&quot;;     Mat atom_image &#x3D; Mat::zeros( w, w, CV_8UC3 );     Mat rook_image &#x3D; Mat::zeros( w, w, CV_8UC3 ); &#x2F;&#x2F;原子图像绘制，创建了MyEllipse 和MyFilledCircle 两个函数     MyEllipse( atom_image, 90 );     MyEllipse( atom_image, 0 );     MyEllipse( atom_image, 45 );     MyEllipse( atom_image, -45 );     MyFilledCircle( atom_image, Point( w&#x2F;2, w&#x2F;2) );    &#x2F;&#x2F;绘制国际象棋的车创建了MyPolygon 和MyLine 函数，且应用了矩形的绘制函数     MyPolygon( rook_image );     rectangle( rook_image,                Point( 0, 7*w&#x2F;8 ),                Point( w, w),                Scalar( 0, 255, 255 ),                FILLED,                LINE_8 );     MyLine( rook_image, Point( 0, 15*w&#x2F;16 ), Point( w, 15*w&#x2F;16 ) );     MyLine( rook_image, Point( w&#x2F;4, 7*w&#x2F;8 ), Point( w&#x2F;4, w ) );     MyLine( rook_image, Point( w&#x2F;2, 7*w&#x2F;8 ), Point( w&#x2F;2, w ) );     MyLine( rook_image, Point( 3*w&#x2F;4, 7*w&#x2F;8 ), Point( 3*w&#x2F;4, w ) );     imshow( atom_window, atom_image );     moveWindow( atom_window, 0, 200 );     imshow( rook_window, rook_image );     moveWindow( rook_window, w, 200 );     waitKey( 0 );     return(0); &#125; void MyEllipse( Mat img, double angle ) &#123;     int thickness &#x3D; 2;     int lineType &#x3D; 8;     ellipse( img,              Point( w&#x2F;2, w&#x2F;2 ),              Size( w&#x2F;4, w&#x2F;16 ),              angle,              0,              360,              Scalar( 255, 0, 0 ),              thickness,              lineType ); &#125; void MyFilledCircle( Mat img, Point center ) &#123;     circle( img,             center,             w&#x2F;32,             Scalar( 0, 0, 255 ),             FILLED,             LINE_8 ); &#125; void MyPolygon( Mat img ) &#123;     int lineType &#x3D; LINE_8;     Point rook_points[1][20];     rook_points[0][0]  &#x3D; Point(    w&#x2F;4,   7*w&#x2F;8 );     rook_points[0][1]  &#x3D; Point(  3*w&#x2F;4,   7*w&#x2F;8 );     rook_points[0][2]  &#x3D; Point(  3*w&#x2F;4,  13*w&#x2F;16 );     rook_points[0][3]  &#x3D; Point( 11*w&#x2F;16, 13*w&#x2F;16 );     rook_points[0][4]  &#x3D; Point( 19*w&#x2F;32,  3*w&#x2F;8 );     rook_points[0][5]  &#x3D; Point(  3*w&#x2F;4,   3*w&#x2F;8 );     rook_points[0][6]  &#x3D; Point(  3*w&#x2F;4,     w&#x2F;8 );     rook_points[0][7]  &#x3D; Point( 26*w&#x2F;40,    w&#x2F;8 );     rook_points[0][8]  &#x3D; Point( 26*w&#x2F;40,    w&#x2F;4 );     rook_points[0][9]  &#x3D; Point( 22*w&#x2F;40,    w&#x2F;4 );     rook_points[0][10] &#x3D; Point( 22*w&#x2F;40,    w&#x2F;8 );     rook_points[0][11] &#x3D; Point( 18*w&#x2F;40,    w&#x2F;8 );     rook_points[0][12] &#x3D; Point( 18*w&#x2F;40,    w&#x2F;4 );     rook_points[0][13] &#x3D; Point( 14*w&#x2F;40,    w&#x2F;4 );     rook_points[0][14] &#x3D; Point( 14*w&#x2F;40,    w&#x2F;8 );     rook_points[0][15] &#x3D; Point(    w&#x2F;4,     w&#x2F;8 );     rook_points[0][16] &#x3D; Point(    w&#x2F;4,   3*w&#x2F;8 );     rook_points[0][17] &#x3D; Point( 13*w&#x2F;32,  3*w&#x2F;8 );     rook_points[0][18] &#x3D; Point(  5*w&#x2F;16, 13*w&#x2F;16 );     rook_points[0][19] &#x3D; Point(    w&#x2F;4,  13*w&#x2F;16 );     const Point* ppt[1] &#x3D; &#123; rook_points[0] &#125;;     int npt[] &#x3D; &#123; 20 &#125;;     fillPoly( img,               ppt,               npt,               1,               Scalar( 255, 255, 255 ),               lineType ); &#125; void MyLine( Mat img, Point start, Point end ) &#123;     int thickness &#x3D; 2;     int lineType &#x3D; LINE_8;     line( img,           start,           end,           Scalar( 0, 0, 0 ),           thickness,           lineType ); &#125;<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p><img src="/pic/%E9%80%89%E5%8C%BA_050.png" alt="绘制结果"></p><h2 id="轮廓"><a href="#轮廓" class="headerlink" title="轮廓"></a>轮廓</h2><h3 id="如何在图像中寻找物体轮廓"><a href="#如何在图像中寻找物体轮廓" class="headerlink" title="如何在图像中寻找物体轮廓"></a>如何在图像中寻找物体轮廓</h3><p><strong>在二值图像中寻找轮廓</strong></p><pre class="line-numbers language-c++" data-language="c++"><code class="language-c++">CV_EXPORTS_W void findContours( InputOutputArray image, OutputArrayOfArrays contours,                              OutputArray hierarchy, int mode,                              int method, Point offset &#x3D; Point());<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre><p>参数： </p><ul><li>image:  8 位单通道图像，非零像素视为 1，零值像素视为 0，因此所有的输入图像均被视为二值图像。因此在应用时我们一般会利用 Canny,  compare,  InRange,  threshold, adaptiveThreshold 等函数处理得到二值图像。如果模式为RETR_CCOMP 或者RETR_FLOODFILL，输入也可以是32 位的灰度图像。 </li><li>contours: 检测到的轮廓，每个轮廓都储存为一组点向量，定义为std::vector&lt;std::vector<a href="cv::Point">cv::Point</a>&gt;。 </li><li>hierarchy:  可选输出向量，定义为 std::vector<a href="cv::Vec4i">cv::Vec4i</a>，包含有图像拓扑信息。其内部元素的个数等同于所检测轮廓的个数。Vec 4i 是Vec &lt;int,  4&gt;的别名，定义为向量内每一个元素包含4 个int 型变量。hierarchy 向量内第i 个轮廓的4 个int型变量（hierarchy[i][0], hierarchy[i][1], hierarchy[i][2], hierarchy[i][3]）分别表示其同一层级下的后一个轮廓，前一个轮廓，子轮廓以及父轮廓的索引编号。如果轮廓 i 没有对应的上述轮廓，则 hierarchy[i][0],  hierarchy[i][1],  hierarchy[i][2],  hierarchy[i][3]被置为-1。 </li><li>mode:  轮廓的检索模式。具体见下表二。 </li><li>method:  轮廓的近似方法。具体见表三。</li><li>offset: Point 偏移量</li></ul><p><img src="/pic/%E9%80%89%E5%8C%BA_051.png"></p><p><strong>轮廓的绘制</strong></p><pre class="line-numbers language-c++" data-language="c++"><code class="language-c++">CV_EXPORTS_W void drawContours( InputOutputArray image, InputArrayOfArrays contours,                              int contourIdx, const Scalar&amp; color,                              int thickness &#x3D; 1, int lineType &#x3D; LINE_8,                              InputArray hierarchy &#x3D; noArray(),                              int maxLevel &#x3D; INT_MAX, Point offset &#x3D; Point() );<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>参数： </p><ul><li>image:  目的图像。 </li><li>contours:  所有输入轮廓，每个轮廓储存为一组点向量。 </li><li>contourIdx:  指示要绘制的轮廓线参数，如果为负，则绘制所有轮廓线。 </li><li>color:  轮廓线颜色。 </li><li>thickness:  绘制轮廓线的粗细，如果为负（thickness&#x3D;FILLED）,则轮廓内部也会被绘制。当thickness&#x3D;FILLED 时，即使没有提供层级数据，也能成功绘制有孔的轮廓。 </li><li>lineType:  直线绘制算法。 </li><li>hierarchy:  可选层级信息，只有当想绘制多条轮廓时才需要用到。 </li><li>mexLevel:  所绘制轮廓的最大层级。如果是0，则只绘制指定的轮廓；如果是1，则绘制指定轮廓以及嵌套轮廓；如果是2，则绘制指定轮廓、嵌套轮廓以及嵌套轮廓的嵌套轮廓以此类推。该参数只在层级参数可用时可以使用。 </li><li>offset: Point 的偏移</li></ul><p> <strong>实例</strong></p> <pre class="line-numbers language-c++" data-language="c++"><code class="language-c++">#include &quot;opencv2&#x2F;imgcodecs.hpp&quot; #include &quot;opencv2&#x2F;highgui.hpp&quot; #include &quot;opencv2&#x2F;imgproc.hpp&quot; #include &lt;iostream&gt; using namespace cv; using namespace std; Mat src_gray; int thresh &#x3D; 100; RNG rng(12345); void thresh_callback(int, void* ); int main( int argc, char** argv ) &#123;    &#x2F;&#x2F;读取图像     Mat src &#x3D; imread( &quot;1.jpg&quot;, IMREAD_COLOR );     if( src.empty() )     &#123;         cout &lt;&lt; &quot;Could not open or find the image!\n&quot; &lt;&lt; endl;         cout &lt;&lt; &quot;Usage: &quot; &lt;&lt; argv[0] &lt;&lt; &quot; &lt;Input image&gt;&quot; &lt;&lt; endl;         return -1;&#125;     &#x2F;&#x2F;转化为灰度图像，模糊处理以去除噪声     cvtColor( src, src_gray, COLOR_BGR2GRAY );     blur( src_gray, src_gray, Size(3,3) );     &#x2F;&#x2F;创建名为Source 的窗口并在其中显示输入图像     const char* source_window &#x3D; &quot;Source&quot;;     namedWindow( source_window );     imshow( source_window, src );     const int max_thresh &#x3D; 255;     createTrackbar( &quot;Canny thresh:&quot;, source_window, &amp;thresh, max_thresh, thresh_callback );     thresh_callback( 0, 0 );     waitKey();     return 0; &#125; void thresh_callback(int, void* ) &#123;     Mat canny_output;     &#x2F;&#x2F;Canny 边缘检测     Canny( src_gray, canny_output, thresh, thresh*2 );     vector&lt;vector&lt;Point&gt; &gt; contours;     vector&lt;Vec4i&gt; hierarchy;     &#x2F;&#x2F;寻找轮廓的函数     findContours( canny_output, contours, hierarchy, RETR_TREE, CHAIN_APPROX_SIMPLE );     Mat drawing &#x3D; Mat::zeros( canny_output.size(), CV_8UC3 );     for( size_t i &#x3D; 0; i&lt; contours.size(); i++ )     &#123;         Scalar color &#x3D; Scalar( rng.uniform(0, 256), rng.uniform(0,256), rng.uniform(0,256) );     &#x2F;&#x2F;轮廓绘制         drawContours( drawing, contours, (int)i, color, 2, LINE_8, hierarchy, 0 );     &#125;     imshow( &quot;Contours&quot;, drawing ); &#125;<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p><img src="/pic/403.gif"></p><h3 id="凸包函数"><a href="#凸包函数" class="headerlink" title="凸包函数"></a>凸包函数</h3><pre class="line-numbers language-c++" data-language="c++"><code class="language-c++">CV_EXPORTS_W void convexHull( InputArray points, OutputArray hull,                              bool clockwise &#x3D; false, bool returnPoints &#x3D; true );<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><p>参数： </p><ul><li>points:  输入的二维点集，存储在std::vector 或者Mat 中 </li><li>hull:  输出找到的凸包。 </li><li>clockwise:  操作方向，当 clockwise&#x3D;true 时，输出凸包为顺时针方向。否则输出凸包方向为逆时针方向。 </li><li>returnPoints:  操作标识符，默认值为 true，此时返回各凸包的各点，否则返回凸包各点的索引。当输出数组为std::vector 时，此标识被忽略</li></ul> <pre class="line-numbers language-c++" data-language="c++"><code class="language-c++">#include &quot;opencv2&#x2F;imgcodecs.hpp&quot; #include &quot;opencv2&#x2F;highgui.hpp&quot; #include &quot;opencv2&#x2F;imgproc.hpp&quot; #include &lt;iostream&gt; using namespace cv; using namespace std; Mat src_gray; int thresh &#x3D; 100; RNG rng(12345); void thresh_callback(int, void* ); int main( int argc, char** argv ) &#123;     Mat src &#x3D; imread( &quot;11.png&quot; , IMREAD_COLOR);     if( src.empty() )     &#123;         cout &lt;&lt; &quot;Could not open or find the image!\n&quot; &lt;&lt; endl;         cout &lt;&lt; &quot;Usage: &quot; &lt;&lt; argv[0] &lt;&lt; &quot; &lt;Input image&gt;&quot; &lt;&lt; endl;         return -1;     &#125;     cvtColor( src, src_gray, COLOR_BGR2GRAY );     blur( src_gray, src_gray, Size(3,3) );     const char* source_window &#x3D; &quot;Source&quot;;     namedWindow( source_window );     imshow( source_window, src );     const int max_thresh &#x3D; 255;     createTrackbar( &quot;Canny thresh:&quot;, source_window, &amp;thresh, max_thresh, thresh_callback );     thresh_callback( 0, 0 );     waitKey();     return 0; &#125; void thresh_callback(int, void* ) &#123;     Mat canny_output;     Canny( src_gray, canny_output, thresh, thresh*2 );     vector&lt;vector&lt;Point&gt; &gt; contours;     findContours( canny_output, contours, RETR_TREE, CHAIN_APPROX_SIMPLE );     &#x2F;&#x2F;利用凸包函数找到图形中的凸包     vector&lt;vector&lt;Point&gt; &gt;hull( contours.size() );     for( size_t i &#x3D; 0; i &lt; contours.size(); i++ )     &#123;         convexHull( contours[i], hull[i] );     &#125;     &#x2F;&#x2F;绘制轮廓与凸包     Mat drawing &#x3D; Mat::zeros( canny_output.size(), CV_8UC3 );     for( size_t i &#x3D; 0; i&lt; contours.size(); i++ )     &#123;         Scalar color &#x3D; Scalar( rng.uniform(0, 256), rng.uniform(0,256), rng.uniform(0,256) );         drawContours( drawing, contours, (int)i, color );         drawContours( drawing, hull, (int)i, color );     &#125;     imshow( &quot;Hull demo&quot;, drawing ); &#125;<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>]]></content>
      
      
      <categories>
          
          <category> Opencv </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Opencv </tag>
            
            <tag> 显示与绘制 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Opencv系列（二）</title>
      <link href="/2023/04/02/opencv2/"/>
      <url>/2023/04/02/opencv2/</url>
      
        <content type="html"><![CDATA[<h1 id="Opencv图像滤波"><a href="#Opencv图像滤波" class="headerlink" title="Opencv图像滤波"></a>Opencv图像滤波</h1><h2 id="连通域-amp-直方图"><a href="#连通域-amp-直方图" class="headerlink" title="连通域&amp;直方图"></a>连通域&amp;直方图</h2><p><img src="/pic/%E9%80%89%E5%8C%BA_042.png"></p><p><strong>计算图像直方图</strong></p><pre class="line-numbers language-c++" data-language="c++"><code class="language-c++">void cv::calcHist(       const Mat *     images,           &#x2F;&#x2F;输入图像    int     nimages,                  &#x2F;&#x2F;源图像的个数（通常为1）    const int *     channels,         &#x2F;&#x2F;列出通道    InputArray  mask,                 &#x2F;&#x2F;输入掩码（需处理的像素）    OutputArray     hist,             &#x2F;&#x2F;输出直方图    int     dims,                     &#x2F;&#x2F;直方图的维度（通道数量）    const int *     histSize,         &#x2F;&#x2F;每个维度位数    const float **  ranges,           &#x2F;&#x2F;每个维度的范围    bool    uniform &#x3D; true,           &#x2F;&#x2F;true表示箱子间距相同    bool    accumulate &#x3D; false        &#x2F;&#x2F;是否在多次调用时进行累积    )<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p><strong>二值函数</strong></p><p>cv::threshold()</p><blockquote><p>〖原理〗：通过将所有像素与某个阈值（第三个参数）进行比较赋值，将图像表示为只有两种像素值的图像（例子：学生排队）。</p></blockquote><p>阈值是图像分割的标尺。</p><pre class="line-numbers language-c++" data-language="c++"><code class="language-c++">#include&lt;opencv2&#x2F;imgproc.hpp&gt;&#x2F;&#x2F;创建二值图像double cv::threshold(       InputArray  src,          &#x2F;&#x2F;输入图像（多通道、8位或32位浮点类型）       OutputArray     dst,      &#x2F;&#x2F;输出图像    double  thresh,           &#x2F;&#x2F;指定阈值    double  maxval,           &#x2F;&#x2F;设定最大值（常取255）    int     type              &#x2F;&#x2F;阈值类型    )&#x2F;*type，详见参数介绍1、THRESH_BINARY：将所有大于thresh的像素赋值为maxval，将其他像素赋值为0；2、THRESH_BINARY_INY：将所有大于thresh的像素赋值为0，将其他像素赋值为maxval；3、THRESH_TRUNC：截断，将所有大于thresh的像素赋值为thresh，其他像素值不变；4、THRESH_TOZERO：所有大于thresh的像素值保持不变，将其他像素赋值为0；5、THRESH_TOZERO_INV：所有大于thresh的像素值赋值为0，其他像素值保持不变.6、THRESH_OTSU:使用Otsu算法去寻找到最优的阈值7、THRESH_TRIANGLE：使用三角化方法寻找到最有的阈值*&#x2F;<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><blockquote><p>【应用】：可用于区分图像的前景和背景（通常前景的像素值大于背景的像素值）；</p></blockquote><h2 id="形态学运算变换图像"><a href="#形态学运算变换图像" class="headerlink" title="形态学运算变换图像"></a>形态学运算变换图像</h2><p><strong>概念</strong></p><ul><li>形态学是一种滤波器，用结构元素探测图像中每个像素的操作过程称为形态学滤波器的应用过程；</li><li>结构元素是一堆像素的组合，原则上可以是任何形状，通常是正方形、圆形或菱形，中心点为原点（锚点）。<br> <strong>作用</strong></li><li>可用于强化或消除特殊形状</li></ul><p><img src="/pic/%E9%80%89%E5%8C%BA_043.png"></p><p><strong>腐蚀与膨胀</strong></p><pre class="line-numbers language-c++" data-language="c++"><code class="language-c++">&#x2F;&#x2F;腐蚀图像&#x2F;&#x2F;原理：在某个像素上应用结构元素时，结构元素的锚点与该像素对齐，腐蚀就是把当前像素替换成所定义像素集合中的最小像素值。void cv::erode  (       InputArray  src,           &#x2F;&#x2F;输入图像：灰度图像&amp;彩色图像    OutputArray     dst,       &#x2F;&#x2F;输出图像    InputArray  kernel,           &#x2F;&#x2F;结构元素，默认cv::Mat()，3x3的正方形    Point   anchor &#x3D; Point(-1,-1),  &#x2F;&#x2F;结构元素的锚点位置，默认为中心    int     iterations &#x3D; 1,         &#x2F;&#x2F;腐蚀次数    int     borderType &#x3D; BORDER_CONSTANT,  &#x2F;&#x2F;边界类型（像素外推的方法）    const Scalar &amp;  borderValue &#x3D; morphologyDefaultBorderValue()   &#x2F;&#x2F;连续边界的边界值)&#x2F;&#x2F;膨胀图像&#x2F;&#x2F;原理：把当前像素替换成所定义像素集合中的最大像素值。void cv::dilate (       InputArray  src,    OutputArray     dst,    InputArray  kernel,    Point   anchor &#x3D; Point(-1,-1),    int     iterations &#x3D; 1,    int     borderType &#x3D; BORDER_CONSTANT,    const Scalar &amp;  borderValue &#x3D; morphologyDefaultBorderValue() )       &#x2F;&#x2F;增加腐蚀&#x2F;膨胀次数或者使用更大的结构元素，都会增加腐蚀&#x2F;膨胀的效果。&#x2F;&#x2F;以腐蚀和膨胀操作作为基础，执行高级的形态变换void cv::morphologyEx   (       InputArray  src,    OutputArray     dst,    int     op,             &#x2F;&#x2F;形态学操作类型    InputArray  kernel,    Point   anchor &#x3D; Point(-1,-1),    int     iterations &#x3D; 1,    int     borderType &#x3D; BORDER_CONSTANT,         const Scalar &amp;  borderValue &#x3D; morphologyDefaultBorderValue() )   <span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p><strong>形态学梯度运算提取图像边缘</strong></p><pre class="line-numbers language-c++" data-language="c++"><code class="language-c++">&#x2F;&#x2F;以腐蚀和膨胀操作作为基础，执行高级的形态变换void cv::morphologyEx   (       InputArray  src,    OutputArray     dst,    int     op,             &#x2F;&#x2F;形态学操作类型    InputArray  kernel,    Point   anchor &#x3D; Point(-1,-1),    int     iterations &#x3D; 1,    int     borderType &#x3D; BORDER_CONSTANT,         const Scalar &amp;  borderValue &#x3D; morphologyDefaultBorderValue() )   <span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p><strong>图形分割</strong></p><pre class="line-numbers language-c++" data-language="c++"><code class="language-c++">CV_EXPORTS_W void watershed( InputArray image, InputOutputArray markers );&#x2F;&#x2F; cv::InputArray image：待分割的源图像；&#x2F;&#x2F; cv::InputOutputArray markers：标记图像；即这个参数用于存放函数调后的输出结果，需和源图片有一样的尺寸和类型。<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre><p><a href="https://blog.csdn.net/weixin_43863869/article/details/128534217">利用分水岭算法实现图像分割</a></p><h2 id="图像滤波"><a href="#图像滤波" class="headerlink" title="图像滤波"></a>图像滤波</h2><ul><li>概念：即选择性地提取图像中某些方面的内容，这些内容通常在特定的应用环境下传达了重要信息。</li><li>作用：滤波器是一种放大（也可以不改变）图像中某些频段，同时滤掉（或减弱）其他频段的算子，分为低通滤波器&amp;高通滤波器。</li><li>示例：去噪（噪声点）、重采样</li></ul><h3 id="频域分析"><a href="#频域分析" class="headerlink" title="频域分析"></a>频域分析</h3><p>描述图像的两种形式：</p><ul><li>频域：观察图像内容强度值（灰度值）变化的频率（蓝天 VS 杂货间），图像中精致的细节对应着高频；</li></ul><p><img src="/pic/%E9%80%89%E5%8C%BA_044.png"></p><ul><li>空域：观察图像内容灰度分布来描述图像特征（直方图）</li></ul><p><strong>频域分析</strong>：把图像分解成从低频到高频的频率成分。图像强度值变化慢的区域只包含低频率，强度值变化快的区域产生高频率。</p><p>二维图像的频率分为垂直频率和水平频率。</p><h3 id="低通滤波器"><a href="#低通滤波器" class="headerlink" title="低通滤波器"></a>低通滤波器</h3><p>目的：消除图像中的高频部分，减少图像变化的幅度（把前景变得光滑；把前景和背景之间的差异变小）。</p><p>常用方法：把每个像素的值替换成它周围像素的平均值，线性滤波。</p><p><strong>块滤波器（box filter）——&gt;卷积核（掩膜）</strong></p><pre class="line-numbers language-c++" data-language="c++"><code class="language-c++">&#x2F;&#x2F;典型示例一：均值滤波器void cv::blur   (       InputArray  src,                  &#x2F;&#x2F;输入图像    OutputArray     dst,              &#x2F;&#x2F;输出图像    Size    ksize,                    &#x2F;&#x2F;卷积核大小（值为系统默认指定？）    Point   anchor &#x3D; Point(-1,-1),     &#x2F;&#x2F;锚点位置    int     borderType &#x3D; BORDER_DEFAULT &#x2F;&#x2F;边界类型)   void cv::boxFilter  (       InputArray  src,    OutputArray     dst,    int     ddepth,    Size    ksize,    Point   anchor &#x3D; Point(-1,-1),    bool    normalize &#x3D; true,    int     borderType &#x3D; BORDER_DEFAULT )   void cv::filter2D   (       InputArray  src,    OutputArray     dst,    int     ddepth,    InputArray  kernel,                        &#x2F;&#x2F;卷积核值    Point   anchor &#x3D; Point(-1,-1),    double  delta &#x3D; 0,    int     borderType &#x3D; BORDER_DEFAULT )   &#x2F;&#x2F;典型示例二：高斯滤波器void cv::GaussianBlur   (       InputArray  src,    OutputArray     dst,    Size    ksize,                   &#x2F;&#x2F;滤波器尺寸,必须为奇数，否则会引发错误    double  sigmaX,                  &#x2F;&#x2F;控制高斯曲线水平方向形状的参数    double  sigmaY &#x3D; 0,              &#x2F;&#x2F;控制高斯曲线垂直方向形状的参数    int     borderType &#x3D; BORDER_DEFAULT     )  <span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p><strong>图像降采样</strong></p><ul><li>降低图像精度的过程称为缩减像素采样（downsampling）；</li><li>提升图像精度的过程称为提升像素采样（upsampling）</li></ul><p>难点：重采样的过程需要尽可能地保持图像质量</p><p><strong>中值滤波器</strong></p><pre class="line-numbers language-c++" data-language="c++"><code class="language-c++">&#x2F;&#x2F;原理：非线性滤波，把当前像素和它的邻域组成一个集合，然后计算出这个集合的中间值，以此作为当前像素的值（用邻域内集合的中位数代替当前像素值）void cv::medianBlur (       InputArray  src,    OutputArray     dst,    int     ksize                    &#x2F;&#x2F;注意这里的ksize类型是 int类型)<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h3 id="高通滤波器"><a href="#高通滤波器" class="headerlink" title="高通滤波器"></a>高通滤波器</h3><p><strong>定向滤波器（边缘检测）</strong></p><ul><li>二维图像分为水平方向和垂直方向；</li><li>比较经典的卷积核称为算子</li></ul><p><img src="/pic/%E9%80%89%E5%8C%BA_045.png"></p><p><img src="/pic/%E9%80%89%E5%8C%BA_046.png"></p><blockquote><p>Q:如何辨别x方向和y方向的滤波？（分别沿x&#x2F;y方向去找灰度值发生急剧变化的边缘处）</p></blockquote><pre class="line-numbers language-c++" data-language="c++"><code class="language-c++">&#x2F;&#x2F;Sobel滤波器：只对垂直或水平方向的图像频率起作用void cv::Sobel  (       InputArray  src,    OutputArray     dst,    int     ddepth,             &#x2F;&#x2F;位深，-1代表输出图像和源图像的位深相同    int     dx,                 &#x2F;&#x2F;x方向的微分，几阶导数    int     dy,                 &#x2F;&#x2F;y方向的微分，几阶导数    int     ksize &#x3D; 3,          &#x2F;&#x2F;sobel内核尺寸，只能为 1,3,5或7    double  scale &#x3D; 1,    double  delta &#x3D; 0,    int     borderType &#x3D; BORDER_DEFAULT )   &#x2F;&#x2F;Schar滤波器void cv::Scharr (       InputArray  src,    OutputArray     dst,    int     ddepth,           &#x2F;&#x2F;注意，此处位深最好选用CV_16S,否则会丢失很多信息    int     dx,    int     dy,    double  scale &#x3D; 1,    double  delta &#x3D; 0,    int     borderType &#x3D; BORDER_DEFAULT )      &#x2F;&#x2F;Laplacian算子    void cv::Laplacian  (       InputArray  src,    OutputArray     dst,    int     ddepth,    int     ksize &#x3D; 1,    double  scale &#x3D; 1,    double  delta &#x3D; 0,    int     borderType &#x3D; BORDER_DEFAULT )       <span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h3 id="Canny边缘检测算子"><a href="#Canny边缘检测算子" class="headerlink" title="Canny边缘检测算子"></a>Canny边缘检测算子</h3><pre class="line-numbers language-c++" data-language="c++"><code class="language-c++">&#x2F;&#x2F;Canny边缘检测算法void cv::Canny  (       InputArray  image,           &#x2F;&#x2F;8位的输入图像    OutputArray     edges,       &#x2F;&#x2F;输出图像，一般是二值图像    double  threshold1,          &#x2F;&#x2F;低阈值，常取高阈值的1&#x2F;2或1&#x2F;3    double  threshold2,          &#x2F;&#x2F;高阈值    int     apertureSize &#x3D; 3,    &#x2F;&#x2F;sobel算子的size，通常取值3    bool    L2gradient &#x3D; false   &#x2F;&#x2F;选择true表示用L2归一化，选择false表示用L1来归一化)<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>]]></content>
      
      
      <categories>
          
          <category> Opencv </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Opencv </tag>
            
            <tag> 直方图 </tag>
            
            <tag> 腐蚀膨胀 </tag>
            
            <tag> 图像滤波 </tag>
            
            <tag> Canny </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Opencv系列（一）</title>
      <link href="/2023/04/01/opencv/"/>
      <url>/2023/04/01/opencv/</url>
      
        <content type="html"><![CDATA[<h1 id="Opencv"><a href="#Opencv" class="headerlink" title="Opencv"></a>Opencv</h1><p><strong>main modules</strong></p><ul><li><p>core</p><p> 定义基本数据结构的紧凑模块，包括稠密的多维数组 Mat 和所有其他模块使用的基本功能。</p></li><li><p>imgproc</p><p> 一个图像处理模块，包括线性和非线性图像滤波、几何图像变换（调整大小、仿射和透视变换、通用的基于表的重映射）、色彩空间转换、直方图等。</p></li><li><p>imgcodecs</p><p> 图像文件的读取和写入</p></li><li><p>videoio</p><p> 视频输入和输出</p></li><li><p>highgui</p><p> GUI界面</p></li><li><p>calib3d</p><p> 相机标定和三维重建</p></li><li><p>features2d</p><p> 2d特征的检测，描述，匹配以及在图像上绘制2d特征点和匹配对</p></li><li><p>objdetect</p><p> 用于目标检测的基于Haar 特征的级联分类器  </p></li><li><p>dnn</p><p> 用于构建深度神经网络，主要是测试网络的输出，不支持网络训练</p></li><li><p>ml</p><p> 一组用于统计分类、回归和数据聚类的类和函数。</p><ul><li>flann<br>  FLANN库的opencv接口（功能不完整）</li></ul></li><li><p>photo</p><p> 照片处理算法，包括修补，去噪，HDR成像等</p></li><li><p>stitching</p><p> 图像拼接</p></li><li><p>gapi</p><p> OpenCV Graph API（或 G-API）是一个新的 OpenCV 模块，旨在使常规图像处理快速且便携。这两个目标是通过引入新的基于图的执行模型来实现的</p></li></ul><p><strong>contrib modules</strong></p><ul><li><p>alphamat</p><p> 从背景图像中提取具有软边界的前景</p></li><li><p>aruco</p><p> ArUco 标记是二进制方形基准标记，可用于相机姿态估计。他们的主要好处是他们的检测是鲁棒、快速和简单的。</p><p> aruco 模块包括这些类型的标记的检测以及使用它们进行姿势估计和相机校准的工具。 </p></li><li><p>bgsegm</p><p> 背景分割</p></li><li><p>bioinspired</p><p> 视网膜模型及其在图像处理中的应用</p></li><li><p>ccalib</p><p> 多相机和广角相机标定</p></li><li><p>cnn_3dobj</p><p> 用于3D物体分类和位姿估计的卷积神经网络 </p></li><li><p>cvv</p><p> 应用于计算机视觉类应用的交互式Debug </p></li><li><p>dnn_objdetect</p><p> 使用卷积神经网络进行目标检测</p></li><li><p>dnn_superres</p><p> 使用卷积神经网络进行图像放大（提高分辨率）</p></li><li><p>face</p><p>  人脸识别的相关算法</p></li><li><p>fuzzy</p><p> 模糊数学理论在图像处理中的应用，主要是F变换 </p></li><li><p>hdf</p><p> hdf5文件的输入和输出</p></li><li><p>Julia</p><p> OpenCV的Julia语言封装</p></li><li><p>line_descriptor</p><p> 从图像中检测直线</p></li><li><p>mcc</p><p> 图像色彩校正</p></li><li><p>phase_unwrapping</p><p> 二维相位展开</p></li><li><p>sfm</p><p> 运动结构恢复</p></li><li><p>stereo</p><p> 稠密立体匹配</p></li><li><p>structured_light</p><p> 结构光反射图案的解析</p></li><li><p>Text</p><p> Tesseract文字识别框架</p></li><li><p>tracking</p><p> 图像中的物体追踪</p></li><li><p>viz</p><p> 可视化窗口（类似Qt）</p></li><li><p>ximgproc</p><p> 拓展图像处理模块。包含结构森林，变化域滤波器，导向滤波，自适应流行滤波器，联合双边滤波器和超像素。</p></li><li><p>xphoto</p><p> 白平衡调整</p></li></ul><h1 id="Opencv如何对像素进行操作"><a href="#Opencv如何对像素进行操作" class="headerlink" title="Opencv如何对像素进行操作"></a>Opencv如何对像素进行操作</h1><h2 id="取"><a href="#取" class="headerlink" title="取"></a>取</h2><pre class="line-numbers language-c++" data-language="c++"><code class="language-c++">Mat cv::imread  (   const String &amp;  filename,                    int     flags &#x3D; IMREAD_COLOR                 )   <span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre><p>filename （必须的）：需要读取的图像名，你也可以写成读取的路径：绝对路劲和相对路径都可</p><p>flag （可选）：flag时读取图像的格式。<br>如果你没有flag选项就按照原始的图像格式，如果有flag选项就按照flag格式读取<br>flag可以是数字，也可以是具体的类型（枚举)</p><pre class="line-numbers language-c++" data-language="c++"><code class="language-c++">enum ImreadModes &#123;       IMREAD_UNCHANGED            &#x3D; -1, &#x2F;&#x2F;!&lt; If set, return the loaded image as is (with alpha channel, otherwise it gets cropped).       IMREAD_GRAYSCALE            &#x3D; 0,  &#x2F;&#x2F;!&lt; If set, always convert image to the single channel grayscale image.       IMREAD_COLOR                &#x3D; 1,  &#x2F;&#x2F;!&lt; If set, always convert image to the 3 channel BGR color image.       IMREAD_ANYDEPTH             &#x3D; 2,  &#x2F;&#x2F;!&lt; If set, return 16-bit&#x2F;32-bit image when the input has the corresponding depth, otherwise convert it to 8-bit.       IMREAD_ANYCOLOR             &#x3D; 4,  &#x2F;&#x2F;!&lt; If set, the image is read in any possible color format.       IMREAD_LOAD_GDAL            &#x3D; 8,  &#x2F;&#x2F;!&lt; If set, use the gdal driver for loading the image.       IMREAD_REDUCED_GRAYSCALE_2  &#x3D; 16, &#x2F;&#x2F;!&lt; If set, always convert image to the single channel grayscale image and the image size reduced 1&#x2F;2.       IMREAD_REDUCED_COLOR_2      &#x3D; 17, &#x2F;&#x2F;!&lt; If set, always convert image to the 3 channel BGR color image and the image size reduced 1&#x2F;2.       IMREAD_REDUCED_GRAYSCALE_4  &#x3D; 32, &#x2F;&#x2F;!&lt; If set, always convert image to the single channel grayscale image and the image size reduced 1&#x2F;4.       IMREAD_REDUCED_COLOR_4      &#x3D; 33, &#x2F;&#x2F;!&lt; If set, always convert image to the 3 channel BGR color image and the image size reduced 1&#x2F;4.       IMREAD_REDUCED_GRAYSCALE_8  &#x3D; 64, &#x2F;&#x2F;!&lt; If set, always convert image to the single channel grayscale image and the image size reduced 1&#x2F;8.       IMREAD_REDUCED_COLOR_8      &#x3D; 65, &#x2F;&#x2F;!&lt; If set, always convert image to the 3 channel BGR color image and the image size reduced 1&#x2F;8.       IMREAD_IGNORE_ORIENTATION   &#x3D; 128 &#x2F;&#x2F;!&lt; If set, do not rotate the image according to EXIF&#39;s orientation flag.     &#125;;     <span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h2 id="存"><a href="#存" class="headerlink" title="存"></a>存</h2><pre class="line-numbers language-c++" data-language="c++"><code class="language-c++">bool cv::imwrite    (   const String &amp;  filename,                        InputArray  img,                        const std::vector&lt; int &gt; &amp;params&#x3D;std::vector&lt; int &gt;()                     )   <span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span></span></code></pre><p>filename （必须）同上<br>img  （必须）表示需要保存的Mat类型的图像数据<br>通常，使用此功能只能保存 8 位单通道或 3 通道（具有“BGR”通道顺序）图像，除去一些特殊情况。</p><pre class="line-numbers language-c++" data-language="c++"><code class="language-c++">enum ImwriteFlags &#123;       IMWRITE_JPEG_QUALITY        &#x3D; 1,  &#x2F;&#x2F;!&lt; For JPEG, it can be a quality from 0 to 100 (the higher is the better). Default value is 95.       IMWRITE_JPEG_PROGRESSIVE    &#x3D; 2,  &#x2F;&#x2F;!&lt; Enable JPEG features, 0 or 1, default is False.       IMWRITE_JPEG_OPTIMIZE       &#x3D; 3,  &#x2F;&#x2F;!&lt; Enable JPEG features, 0 or 1, default is False.       IMWRITE_JPEG_RST_INTERVAL   &#x3D; 4,  &#x2F;&#x2F;!&lt; JPEG restart interval, 0 - 65535, default is 0 - no restart.       IMWRITE_JPEG_LUMA_QUALITY   &#x3D; 5,  &#x2F;&#x2F;!&lt; Separate luma quality level, 0 - 100, default is 0 - don&#39;t use.       IMWRITE_JPEG_CHROMA_QUALITY &#x3D; 6,  &#x2F;&#x2F;!&lt; Separate chroma quality level, 0 - 100, default is 0 - don&#39;t use.       IMWRITE_PNG_COMPRESSION     &#x3D; 16, &#x2F;&#x2F;!&lt; For PNG, it can be the compression level from 0 to 9. A higher value means a smaller size and longer compression time. If specified, strategy is changed to IMWRITE_PNG_STRATEGY_DEFAULT (Z_DEFAULT_STRATEGY). Default value is 1 (best speed setting).       IMWRITE_PNG_STRATEGY        &#x3D; 17, &#x2F;&#x2F;!&lt; One of cv::ImwritePNGFlags, default is IMWRITE_PNG_STRATEGY_RLE.       IMWRITE_PNG_BILEVEL         &#x3D; 18, &#x2F;&#x2F;!&lt; Binary level PNG, 0 or 1, default is 0.       IMWRITE_PXM_BINARY          &#x3D; 32, &#x2F;&#x2F;!&lt; For PPM, PGM, or PBM, it can be a binary format flag, 0 or 1. Default value is 1.       IMWRITE_EXR_TYPE            &#x3D; (3 &lt;&lt; 4) + 0, &#x2F;* 48 *&#x2F; &#x2F;&#x2F;!&lt; override EXR storage type (FLOAT (FP32) is default)       IMWRITE_WEBP_QUALITY        &#x3D; 64, &#x2F;&#x2F;!&lt; For WEBP, it can be a quality from 1 to 100 (the higher is the better). By default (without any parameter) and for quality above 100 the lossless compression is used.       IMWRITE_PAM_TUPLETYPE       &#x3D; 128,&#x2F;&#x2F;!&lt; For PAM, sets the TUPLETYPE field to the corresponding string value that is defined for the format       IMWRITE_TIFF_RESUNIT &#x3D; 256,&#x2F;&#x2F;!&lt; For TIFF, use to specify which DPI resolution unit to set; see libtiff documentation for valid values       IMWRITE_TIFF_XDPI &#x3D; 257,&#x2F;&#x2F;!&lt; For TIFF, use to specify the X direction DPI       IMWRITE_TIFF_YDPI &#x3D; 258 &#x2F;&#x2F;!&lt; For TIFF, use to specify the Y direction DPI     &#125;;<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h2 id="ROI区域"><a href="#ROI区域" class="headerlink" title="ROI区域"></a>ROI区域</h2><blockquote><p>定义<br>有事需要让一个处理函数只在图像的某个部分起作用，所以需要定义图像的子区域，也就是ROI区域（region of interest）感兴趣区域</p></blockquote><pre class="line-numbers language-c++" data-language="c++"><code class="language-c++">int  main()&#123;cv::Mat image&#x3D; cv::imread(&quot;3.png&quot;);cv::Mat logo&#x3D; cv::imread(&quot;2.jpg&quot;);cv::Mat imageROI(image,                                    cv::Rect(0,                                    0,                                    logo.cols,                                    logo.rows));imshow(&quot;1&quot;,imageROI);&#x2F;&#x2F;将logo替换image中的感兴区域imageROIlogo.copyTo(imageROI);imshow(&quot;2&quot;,logo);imshow(&quot;3&quot;,image);cv::waitKey(0);return 0;&#125;<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><blockquote><p>扩展：除了使用起点和终点位置，还可以通过列数和行数实现ROI区域定义：</p></blockquote><pre class="line-numbers language-c++" data-language="c++"><code class="language-c++">cv:: Mat imageROI &#x3D; image(cv::Range(0,logo.rows),                                  cv::Range(0,logo.cols));<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><h2 id="指针遍历"><a href="#指针遍历" class="headerlink" title="指针遍历"></a>指针遍历</h2><p>像素遍历就是将图像的所有像素都访问一次。由于图像像素数量非常庞大，高效遍历就十分必要</p><pre class="line-numbers language-c++" data-language="c++"><code class="language-c++">int nl&#x3D; image.rows; &#x2F;&#x2F; 行数 &#x2F;&#x2F; 每行的元素数量 int nc&#x3D; image.cols * image.channels();  for (int j&#x3D;0; j&lt;nl; j++) &#123;  &#x2F;&#x2F; 取得行 j 的地址,这里以uchar图像类型作为例子 uchar* data&#x3D; image.ptr&lt;uchar&gt;(j);  for (int i&#x3D;0; i&lt;nc; i++) &#123; &#x2F;&#x2F; 处理每个像素 data[i] &#125; &#x2F;&#x2F; 一行结束 &#125;<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>例子：减色算法，对每个像素做减色，对于8为无符号字符类型的彩色图有256x256x256中颜色，减色就是减色颜色的种类</p><pre class="line-numbers language-c++" data-language="c++"><code class="language-c++">&#x2F;*减色算法：假设 N 是减色因子，将图像中每个像素的值除以 N（这里假定使用整数除法，不保留余数）。然后将结果乘以 N，得到 N 的倍数，并且刚好不超过原始像素 值。加上 N &#x2F; 2，就得到相邻的 N 倍数之间的中间值。对所有 8 位通道值重复这个过程，就会得到  (256 &#x2F; N) × (256 &#x2F; N) × (256 &#x2F; N)种可能的颜色值*&#x2F;void colorReduce(cv::Mat image, int div &#x3D;64)&#123;    int nl &#x3D;image.rows;    int nc &#x3D;image.cols;    for(int j&#x3D;0;j&lt;nl;j++)&#123;        uchar* data&#x3D;image.ptr&lt;uchar&gt;(j);        for(int i&#x3D;0;i&lt;nc;i++)&#123;            data[i]&#x3D; data[i]&#x2F;div*div+div&#x2F;2;                    &#125;    &#125;&#125;int  main()&#123;    cv::Mat image&#x3D;cv::imread(&quot;1.jpg&quot;);    colorReduce(image);    cv::imshow(&quot;1&quot;,image);    cv::waitKey();    return 0;&#125;<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h2 id="迭代器遍历"><a href="#迭代器遍历" class="headerlink" title="迭代器遍历"></a>迭代器遍历</h2><pre class="line-numbers language-c++" data-language="c++"><code class="language-c++">&#x2F;&#x2F;创建迭代器bagin和end，注意要给出图像的数据类型，此处以cv::Vec3b为例&#x2F;&#x2F;在opencv中cv::Vec3b向量包含三个无符号字符类型的数据，可以用于描述彩色图的三通道。cv::Mat_&lt;cv::Vec3b&gt;::iterator it&#x3D; image.begin&lt;cv::Vec3b&gt;(); cv::Mat_&lt;cv::Vec3b&gt;::iterator itend&#x3D; image.end&lt;cv::Vec3b&gt;();  &#x2F;&#x2F; 扫描全部像素 for ( ; it!&#x3D; itend; ++it) &#123; &#x2F;&#x2F;处理每个像素 (*it)[0] (*it)[1] (*it)[2] &#125; &#x2F;&#x2F;或者while (it!&#x3D; itend) &#123;  &#x2F;&#x2F; 处理每个像素 ---------------------  ... &#x2F;&#x2F; 像素处理结束 ---------------------  ++it; &#125; <span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h1 id="Opencv点检测"><a href="#Opencv点检测" class="headerlink" title="Opencv点检测"></a>Opencv点检测</h1><h2 id="Harris角点检测"><a href="#Harris角点检测" class="headerlink" title="Harris角点检测"></a>Harris角点检测</h2><h3 id="角点定义"><a href="#角点定义" class="headerlink" title="角点定义"></a>角点定义</h3><p>角点是图像中某些属性较为突出的像素点，例如像素值最大或者最小的点、线段的顶点、孤立的边缘点等，图中圆圈包围的线段的拐点就是一些常见的角点。常用的角点有以下几种。  </p><ul><li>灰度梯度的最大值对应的像素点；  </li><li>两条直线或者曲线的交点；  </li><li>一阶梯度的导数最大值和梯度方向变化率最大的像素点；  </li><li>一阶导数值最大，但是二阶导数值为0的像素点；</li></ul><h3 id="Harris算法原理"><a href="#Harris算法原理" class="headerlink" title="Harris算法原理"></a>Harris算法原理</h3><p>Harris角点是最经典的角点之一，其从像素值变化度对角点进行定义，像素值的局部最大峰值即为Harris角点。Harris角点的检测过程如图9-3所示，首先以某个像素为中心构建一个矩形滑动窗口，滑动窗口覆盖图像像素值通过线性叠加得到得到滑动窗口所有像素值的衡量系数，该系数与滑动窗口范围内的像素值成正比，当滑动窗口范围内像素值整体变大时，该衡量系数也变大。在图像中以每个像素为中心向各个方向移动滑动窗口，当滑动窗口无论向哪个方向移动像素值衡量系数都缩小时，滑动窗口中心点对应的像素点即为Haris角点</p><p><img src="/pic/%E9%80%89%E5%8C%BA_032.png"></p><p>角点检测最原始的想法就是取某个像素的一个邻域窗口，当这个窗口在各个方向上进行小范围移动时，观察窗口内平均的像素灰度值的变化（即E(u,v)，Window-averaged change of intensity）。从上图可知，我们可以将一幅图像大致分为三个区域（‘flat’，‘edge’，‘corner’），这三个区域变化是不一样的。</p><p><img src="/pic/%E9%80%89%E5%8C%BA_035.png"></p><p>其中：  </p><ul><li>u、v是窗口在水平，竖直方向的偏移；  </li><li>w(x,y)表示滑动窗口权重函数，可以是常数，也可以是高斯函数；</li></ul><p><img src="/pic/%E9%80%89%E5%8C%BA_034.png"></p><p>图中蓝线圈出的地方我们称之为Harris角点的梯度协方差矩阵，记为M。其中，Ix和Iy分别为X方向和Y方向的梯度。  由于E(x,y)取值与M相关，进一步对其进行简化，定义Harris角点评价系数R为：</p><blockquote><p>R&#x3D;det(M)-k(tr(M))^2</p></blockquote><p>其中k为常值权重系数，det(M)&#x3D;λ1λ2,tr(M)&#x3D;λ1+λ2,λ1和λ2是梯度协方差矩阵M的特征向量，将特征向量代入得:</p><blockquote><p>R&#x3D;λ1λ2-k(λ1+λ2)^2</p></blockquote><p>当R较大时，说明两个特征向量较相似或者接近，则该点为角点；当R&lt;0时，说明两个特征向量相差较大，则该点位于直线上；当|R|较小，说明两个特征值较小，则该点位于平面。</p><h3 id="Opencv实现"><a href="#Opencv实现" class="headerlink" title="Opencv实现"></a>Opencv实现</h3><p><strong>cornerHarris()——计算角点Harris评价系数R</strong></p><pre class="line-numbers language-c++" data-language="c++"><code class="language-c++">void cv::cornerHarris( InputArray src, OutputArray dst, int blockSize, int ksize, double k, int borderType &#x3D; BORDER_DEFAULT)<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><ul><li>src:待检测Harris角点的输入图像，图像必须是CV_8U或者CV_32F的单通道灰度图像；</li><li>dst:存放Harris评价系数R的矩阵，数据类型为CV_32F的单通道图像，与输入图像具有相同的尺寸</li><li>blockSize:邻域大小（窗口大小），通常取2；</li><li>ksize：Sobel算子的半径，用于得到图像梯度信息，该参数需要是奇数，多使用3或者5；</li><li>k:计算Harris评价系数R的权重系数，一般取值为0.02~0.04;</li><li>borderType：像素外推算法标志，这里使用默认。</li></ul><p><strong>drawKeypoints()——一次性绘制所有的角点（关键词）</strong></p><blockquote><p>绘制关键点</p></blockquote><pre class="line-numbers language-c++" data-language="c++"><code class="language-c++">void drawKeypoints(  InputArray image,  const std::vector&lt;KeyPoint&gt;&amp; keypoints, InputOutputArray outImage, const Scalar&amp; color&#x3D;Scalar::all(-1), int flags&#x3D;DrawMatchesFlags::DEFAULT )   &#x2F;&#x2F;KeyPoint类数据 class KeyPoint&#123; float angle      &#x2F;&#x2F;关键点的角度 int class_id     &#x2F;&#x2F;关键点的分类号 int octave       &#x2F;&#x2F;特征点来源（“金字塔”） Point2f pt       &#x2F;&#x2F;关键点坐标 float response   &#x2F;&#x2F;最强关键点的响应 float size       &#x2F;&#x2F;关键点邻域的直径 <span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><ul><li>image:绘制关键点的原图像，图像可以是单通道的灰度图像和三通道的彩色图像；</li><li>keypoints:来自原图像中的关键点向量，vector向量中存放着表示关键点的KeyPoint类型的数据；</li><li>outImage：绘制关键点后的输出图像；</li><li>color：关键点空心圆的颜色，默认使用随机颜色绘制空心圆；</li><li>flag:绘制功能选择标志，其实就是设置特征点的那些信息需要绘制，那些不需要绘制，有以下几种模式可选：</li></ul><table><thead><tr><th align="center">标志参数</th><th align="center">简记</th><th align="center">含义</th></tr></thead><tbody><tr><td align="center">DEFAULT</td><td align="center">0</td><td align="center">只绘制特征点的坐标点,显示在图像上就是一个个小圆点,每个小圆点的圆心坐标都是特征点的坐标。</td></tr><tr><td align="center">DRAW_OVER_OUTIMG</td><td align="center">1</td><td align="center">函数不创建输出的图像,而是直接在输出图像变量空间绘制,要求本身输出图像变量就是一个初始化好了的,size与type都是已经初始化好的变量</td></tr><tr><td align="center">NOT_DRAW_SINGLE_POINTS</td><td align="center">2</td><td align="center">单点的特征点不被绘制</td></tr><tr><td align="center">DRAW_RICH_KEYPOINTS</td><td align="center">4</td><td align="center">绘制特征点的时候绘制的是一个个带有方向的圆,这种方法同时显示图像的坐标,size，和方向,是最能显示特征的一种绘制方式</td></tr></tbody></table><blockquote><p>Harris 算法实现步骤：<br>（1）计算图像在两个方向上的梯度<br>（2）计算两个方向梯度乘积<br>（3）使用高斯函数进行加权平均，生成矩阵元素和<br>（4）计算每个像素Harris响应值，并对小于某一个阈值的像素置0<br>（5）在阈值的邻域内进行非最大值抑制，局部最大值即为Harris角点Harris算法优劣：  </p></blockquote><blockquote><p>（1）优点：计算简单，提取的特征点均匀且合理稳定（对图像旋转、亮度变化、噪声影响和视点变换不敏感）；<br>（2）缺点：a.对尺度很敏感，不具有尺度不变性；b.提取的角点精度是像素级的；c.需要设计对应的描述子和匹配算法；</p></blockquote><pre class="line-numbers language-c++" data-language="c++"><code class="language-c++">int  main()&#123;    cv::Mat image&#x3D;cv::imread(&quot;1.png&quot;);    cv::Mat grayImage;    cv::cvtColor(image,grayImage,CV_BGR2GRAY);    cv::Mat dstImage;    cv::cornerHarris(grayImage,dstImage,2,3,0.01);    cv::imshow(&quot;直接显示&quot;,dstImage);    cv::Mat thredImage;threshold(dstImage, thredImage, 0.0001, 255, CV_THRESH_BINARY);imshow(&quot;【阀值后显示】&quot;, thredImage);    cv::waitKey();    return 0;&#125;<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><blockquote><p>绘制匹配点</p></blockquote><pre class="line-numbers language-c++" data-language="c++"><code class="language-c++">void cv::drawMatches (InputArray img1,const std::vector&lt; KeyPoint &gt; &amp; keypoints1,InputArray img2,const std::vector&lt; KeyPoint &gt; &amp; keypoints2,const std::vector&lt; DMatch &gt; &amp; matches1to2,InputOutputArray outImg,const Scalar &amp; matchColor &#x3D; Scalar::all(-1),const Scalar &amp; singlePointColor &#x3D; Scalar::all(-1),const std::vector&lt; char &gt; &amp; matchesMask &#x3D; std::vector&lt; char &gt;(),DrawMatchesFlags flags &#x3D; DrawMatchesFlags::DEFAULT)<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><ul><li>img1:第一个源图像，</li><li>keypoints1:第一个源图像的关键点，</li><li>img2:第二个源图像，</li><li>keypoints2:第二个源图像的关键点，</li><li>matches1to2:从第一张图像匹配到第二张图像，</li><li>outimg: 输出图像。它的内容取决于定义在输出图像中绘制的内容的标志值，</li><li>matchColor:匹配的颜色（线和连接的关键点），</li><li>singlePointColor:单个关键点（圆圈）的颜色，表示关键点不匹配，</li><li>matchesMask:确定绘制哪些匹配项的掩码。如果掩码为空，则绘制所有匹配项。</li><li>flags:标志设置绘图功能</li></ul><h2 id="SIFT特征点检测"><a href="#SIFT特征点检测" class="headerlink" title="SIFT特征点检测"></a>SIFT特征点检测</h2><h3 id="SIFT综述"><a href="#SIFT综述" class="headerlink" title="SIFT综述"></a>SIFT综述</h3><p>尺度不变特征转换(SIFT)是一种电脑视觉的算法用来侦测与描述影像中的局部性特征，它在空间尺度中寻找极值点，并提取出其位置、尺度、旋转不变量，此算法由 David Lowe在1999年所发表，2004年完善总结。</p><p>Lowe将SIFT算法分解为如下四步：  </p><ul><li>尺度空间极值检测：搜索所有尺度上的图像位置。通过高斯微分函数来识别潜在的对于尺度和旋转不变的兴趣点。  </li><li>关键点定位：在每个候选的位置上，通过一个拟合精细的模型来确定位置和尺度。关键点的选择依据于它们的稳定程度。  </li><li>方向确定：基于图像局部的梯度方向，分配给每个关键点位置一个或多个方向。所有后面的对图像数据的操作都相对于关键点的方向、尺度和位置进行变换，从而提供对于这些变换的不变性。  </li><li>关键点描述：在每个关键点周围的邻域内，在选定的尺度上测量图像局部的梯度。这些梯度被变换成一种表示，这种表示允许比较大的局部形状的变形和光照变化。</li></ul><h3 id="SIFT算法的OpenCV实现"><a href="#SIFT算法的OpenCV实现" class="headerlink" title="SIFT算法的OpenCV实现"></a>SIFT算法的OpenCV实现</h3><p>OpenCV中的SIFT函数主要有两个接口。</p><p>构造函数：</p><pre class="line-numbers language-c++" data-language="c++"><code class="language-c++">SIFT::SIFT(int nfeatures&#x3D;0, int nOctaveLayers&#x3D;3, double contrastThreshold&#x3D;0.04, double edgeThreshold&#x3D;10, double sigma&#x3D;1.6) <span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><ul><li>nfeatures：特征点数目（算法对检测出的特征点排名，返回最好的nfeatures个特征点）。</li><li>nOctaveLayers：金字塔中每组的层数（算法中会自己计算这个值，后面会介绍）。</li><li>contrastThreshold：过滤掉较差的特征点的对阈值。contrastThreshold越大，返回的特征点越少。</li><li>edgeThreshold：过滤掉边缘效应的阈值。edgeThreshold越大，特征点越多（被多滤掉的越少）。</li><li>sigma：金字塔第0层图像高斯滤波系数，也就是σ。</li></ul><p>重载操作符：</p><pre class="line-numbers language-c++" data-language="c++"><code class="language-c++">void SIFT::operator()(InputArray img, InputArray mask, vector&lt;KeyPoint&gt;&amp; keypoints, OutputArray descriptors, bool useProvidedKeypoints&#x3D;false) <span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre><ul><li>img：8bit灰度图像 mask：图像检测区域（可选）</li><li>keypoints：特征向量矩阵</li><li>descipotors：特征点描述的输出向量（如果不需要输出，需要传cv::noArray()）。</li><li>useProvidedKeypoints：是否进行特征点检测。ture，则检测特征点；false，只计算图像特征描述。</li></ul><h3 id="SURF特征"><a href="#SURF特征" class="headerlink" title="SURF特征"></a>SURF特征</h3><p>SURF（Speeded Up Robust Features）是对SIFT的一种改进，主要特点是快速。SURF与SIFT主要有以下几点不同处理：</p><blockquote><p>1、SIFT在构造DOG金字塔以及求DOG局部空间极值比较耗时，SURF的改进是使用Hessian矩阵变换图像，极值的检测只需计算Hessian矩阵行列式，作为进一步优化，使用一个简单的方程可以求出Hessian行列式近似值，使用盒状模糊滤波（box  blur）求高斯模糊近似值。<br>2、 SURF不使用降采样，通过保持图像大小不变，但改变盒状滤波器的大小来构建尺度金字塔。<br>3、在计算关键点主方向以及关键点周边像素方向的方法上，SURF不使用直方图统计，而是使用哈尔(haar)小波转换。SIFT的KPD达到128维，导致KPD的比较耗时，SURF使用哈尔(haar)小波转换得到的方向，让SURF的KPD降到64维，减少了一半，提高了匹配速度</p></blockquote><h2 id="ORB特征"><a href="#ORB特征" class="headerlink" title="ORB特征"></a>ORB特征</h2><p>ORB特征由关键点和描述子两部分组成，关键点称为“Oriented FAST”，是一种改进的FAST 角点。它的描述子称为BRIEF(Binary Robust Independent Elementary Feature)。</p><p>提取 ORB 特征分为如下两个步骤：  </p><ul><li>FAST 角点提取：找出图像中的“角点”。相较于原始的 FAST，ORB 中计算了特征点的主方向，为BRIEF 描述子增加了旋转不变特性。  </li><li>BRIEF 描述子的计算：对前一步提取出特征点的周围图像区域进行描述。ORB 对 BRIEF 进行了改进，主要是在BRIEF 中使用了先前计算的方向信息。</li></ul><h3 id="FAST关键点"><a href="#FAST关键点" class="headerlink" title="FAST关键点"></a>FAST关键点</h3><p>FAST 是一种角点，主要检测局部像素灰度变化明显的地方，以速度快著称。它的思想是：如果一个像素与邻域的像素差别较大（过亮或过暗），那么它可能是角点。检测步骤如下：</p><ul><li>在图像中选取像素 p，假设它的亮度为Ip。</li><li>设置一个阈值 T（比如，Ip的20%）。</li><li>以像素 p 为中心，选取半径为3的圆上的16个像素点。</li><li>假如选取的圆上有连续的 N 个点的亮度大于 Ip+T 或小于 Ip−T，那么像素p 可以被认为是特征点（N通常取12，即为 FAST-12。其他常用的N取值为9和11，它们分别被称为FAST-9和FAST-11）。</li><li>循环以上四步，对每一个像素执行相同的操作。</li></ul><p>在FAST-12算法中，可以进行预测试操作，以快速地排除绝大多数不是角点的像素。</p><blockquote><p>具体操作为，对于每个像素，直接检测邻域圆上的第 1, 5, 9, 13 个像素的亮度。只有当这 4个像素中有 3 个同时大于 Ip+T或小于 Ip−T 时，当前像素才有可能是一个角点，否则应该直接排除。这大大加速了角点检测。</p></blockquote><p>还需要用非极大值抑制(Non-maximal suppression)，在一定区域内仅保留响应极大值的角点，避免角点集中的问题。  </p><p><img src="/pic/%E9%80%89%E5%8C%BA_037.png" alt="FAST特征点"></p><p>FAST特征点的计算仅仅是比较像素间亮度的差异，所以速度非常快。它的缺点是重复性不强，分布不均匀，不具有方向信息。同时，由于它固定取半径为3的圆，存在尺度问题：远处看着像是角点的地方，接近后看可能就不是角点了。</p><blockquote><p>针对 FAST 角点不具有方向性和尺度的弱点，ORB添加了尺度和旋转的描述。尺度不变性由构建图像金字塔解决，在金字塔的每一层上检测角点。特征的旋转是由灰度质心法(Intensity Centroid)实现。</p></blockquote><p><img src="/pic/%E9%80%89%E5%8C%BA_038.png" alt="使用金字塔可以匹配不同缩放倍率下的图像"></p><p>图像金字塔如上图，金字塔底层是原始图像，每往上一层，就对图像进行一个固定倍率的缩放，这样就有了不同分辨率的图像。较小的图像可以看成是远处看过来的场景。在特征匹配算法中，我们可以匹配不同层上的图像，从而实现尺度不变性。例如，如果相机在后退，那么我们应该能够在上一个图像金字塔的上层和下一个图像的下层中找到匹配。</p><p><img src="/pic/%E9%80%89%E5%8C%BA_039.png"></p><p>通过以上方法，FAST 角点便具有了尺度与旋转的描述，从而大大提升了其在不同图像之间表述的鲁棒性。所以在 ORB中，把这种改进后的 FAST 称为 Oriented FAST。</p><h3 id="BRIEF描述子"><a href="#BRIEF描述子" class="headerlink" title="BRIEF描述子"></a>BRIEF描述子</h3><p>在提取 Oriented FAST 关键点后，对每个点计算其描述子，ORB 使用改进的BRIEF特征描述。BRIEF 是一种二进制描述子，其描述向量由许多个 0 和 1 组成，这里的 0 和 1 编码了关键点附近两个随机像素（比如p和q）的大小关系：如果p 比 q 大，则取 1，反之就取 0。如果我们取了 128个这样的 p, q，最后就得到 128 维由 0、1 组成的向量。关于一对随机点的选择方法，ORB论文原作者测试了以下5种方法，发现方法（2）比较好：</p><p><img src="/pic/%E9%80%89%E5%8C%BA_040.png"></p><p>原始的 BRIEF 描述子不具有旋转不变性，因此在图像发生旋转时容易丢失。而 ORB 在 FAST 特征点提取阶段计算了关键点的方向，所以可以利用方向信息，计算了旋转之后的“Steer BRIEF”特征使 ORB 的描述子具有较好的旋转不变性。</p><p><strong>ORB类定义</strong></p><pre class="line-numbers language-c++" data-language="c++"><code class="language-c++">CV_WRAP static Ptr&lt;ORB&gt; create(int nfeatures&#x3D;500, float scaleFactor&#x3D;1.2f, int nlevels&#x3D;8, int edgeThreshold&#x3D;31,    int firstLevel&#x3D;0, int WTA_K&#x3D;2, int scoreType&#x3D;ORB::HARRIS_SCORE, int patchSize&#x3D;31, int fastThreshold&#x3D;20);<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><p>其中：</p><ul><li>nfeatures:需要的特征点总数；</li><li>scaleFactor:尺度因子；</li><li>nlevels:金字塔层数；</li><li>edgeThreshold:边界阈值；</li><li>firstLevel:起始层； </li><li>WTA_K：描述子形成方法,WTA_K&#x3D;2表示，采用两两比较；</li><li>scoreType:角点响应函数，可以选择Harris或者Fast的方法；</li><li>patchSize:特征点邻域大小</li></ul><pre class="line-numbers language-c++" data-language="c++"><code class="language-c++">int  main()&#123;    cv::Mat image&#x3D;cv::imread(&quot;1.jpg&quot;);    cv::Mat grayImage;    cv::cvtColor(image,grayImage,CV_BGR2GRAY);    vector&lt;KeyPoint&gt; keypoints_1;    Mat descriptors_1;    Ptr&lt;FeatureDetector&gt; detector &#x3D; ORB::create();    Ptr&lt;DescriptorExtractor&gt; descriptor &#x3D; ORB::create();     &#x2F;&#x2F;-- 第一步:检测 Oriented FAST 角点位置    detector-&gt;detect ( grayImage,keypoints_1 );    &#x2F;&#x2F;-- 第二步:根据角点位置计算 BRIEF 描述子    descriptor-&gt;compute ( grayImage, keypoints_1, descriptors_1 );    Mat outimg1;    Mat outimg2;    &#x2F;&#x2F;-- 第三步:显示特征点    drawKeypoints( grayImage, keypoints_1, outimg1, Scalar::all(-1), DrawMatchesFlags::DEFAULT );    drawKeypoints( grayImage, keypoints_1, outimg2, Scalar::all(-1), DrawMatchesFlags::DRAW_RICH_KEYPOINTS );    imshow(&quot;ORB特征点&quot;,outimg1);    imshow(&quot;ORB特征方向圆&quot;,outimg2);    cv::waitKey();    return 0;&#125;<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p><img src="/pic/%E9%80%89%E5%8C%BA_041.png"></p><p><strong>特征提取及形成描述子</strong></p><pre class="line-numbers language-c++" data-language="c++"><code class="language-c++">void ORB::operator()( InputArray _image, InputArray _mask, vector&lt;KeyPoint&gt;&amp; _keypoints,                        OutputArray _descriptors, bool useProvidedKeypoints)<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><ul><li>_image:输入图像；</li><li>_mask:掩码图像;</li><li>_keypoints:输入角点；</li><li>_descriptors:如果为空，只寻找特征点，不计算特征描述子；</li><li>_useProvidedKeypoints:如果为true,函数只计算特征描述子</li></ul><pre class="line-numbers language-c++" data-language="c++"><code class="language-c++">#include &lt;iostream&gt;#include &lt;chrono&gt;using namespace std;#include &lt;opencv2&#x2F;core&#x2F;core.hpp&gt;#include &lt;opencv2&#x2F;highgui&#x2F;highgui.hpp&gt;int main ( int argc, char** argv )&#123;    &#x2F;&#x2F; 读取argv[1]指定的图像    cv::Mat image;    image &#x3D; cv::imread ( argv[1] );     &#x2F;&#x2F; 判断图像文件是否正确读取    if ( image.data &#x3D;&#x3D; nullptr ) &#x2F;&#x2F;数据不存在,可能是文件不存在    &#123;        cerr&lt;&lt;&quot;文件&quot;&lt;&lt;argv[1]&lt;&lt;&quot;不存在.&quot;&lt;&lt;endl;        return 0;    &#125;    &#x2F;&#x2F; 文件顺利读取, 首先输出一些基本信息 H*W  rows*cols    cout&lt;&lt;&quot;图像宽为&quot;&lt;&lt;image.cols&lt;&lt;&quot;,高为&quot;&lt;&lt;image.rows&lt;&lt;&quot;,通道数为&quot;&lt;&lt;image.channels()&lt;&lt;endl;    cv::imshow ( &quot;image&quot;, image );      &#x2F;&#x2F; 用cv::imshow显示图像    cv::waitKey ( 0 );                  &#x2F;&#x2F; 暂停程序,等待一个按键输入    &#x2F;&#x2F; 判断image的类型    if ( image.type() !&#x3D; CV_8UC1 &amp;&amp; image.type() !&#x3D; CV_8UC3 )    &#123;        &#x2F;&#x2F; 图像类型不符合要求        cout&lt;&lt;&quot;请输入一张彩色图或灰度图.&quot;&lt;&lt;endl;        return 0;    &#125;    &#x2F;&#x2F; 遍历图像, 请注意以下遍历方式亦可使用于随机像素访问    &#x2F;&#x2F; 使用 std::chrono 来给算法计时    chrono::steady_clock::time_point t1 &#x3D; chrono::steady_clock::now();    for ( size_t y&#x3D;0; y&lt;image.rows; y++ )    &#123;        &#x2F;&#x2F; 用cv::Mat::ptr获得图像的行指针        unsigned char* row_ptr &#x3D; image.ptr&lt;unsigned char&gt; ( y );  &#x2F;&#x2F; row_ptr是第y行的头指针        for ( size_t x&#x3D;0; x&lt;image.cols; x++ )        &#123;            &#x2F;&#x2F; 访问位于 x,y 处的像素            unsigned char* data_ptr &#x3D; &amp;row_ptr[ x*image.channels() ]; &#x2F;&#x2F; data_ptr 指向待访问的像素数据            &#x2F;&#x2F; 输出该像素的每个通道,如果是灰度图就只有一个通道            for ( int c &#x3D; 0; c !&#x3D; image.channels(); c++ )            &#123;                unsigned char data &#x3D; data_ptr[c]; &#x2F;&#x2F; data为I(x,y)第c个通道的值            &#125;        &#125;    &#125;    chrono::steady_clock::time_point t2 &#x3D; chrono::steady_clock::now();    chrono::duration&lt;double&gt; time_used &#x3D; chrono::duration_cast&lt;chrono::duration&lt;double&gt;&gt;( t2-t1 );    cout&lt;&lt;&quot;遍历图像用时：&quot;&lt;&lt;time_used.count()&lt;&lt;&quot; 秒。&quot;&lt;&lt;endl;    &#x2F;&#x2F; 关于 cv::Mat 的拷贝    &#x2F;&#x2F; 直接赋值并不会拷贝数据    cv::Mat image_another &#x3D; image;    &#x2F;&#x2F; 修改 image_another 会导致 image 发生变化    image_another ( cv::Rect ( 0,0,100,100 ) ).setTo ( 0 ); &#x2F;&#x2F; 将左上角100*100的块置零    cv::imshow ( &quot;image&quot;, image );    cv::waitKey ( 0 );    &#x2F;&#x2F; 使用clone函数来拷贝数据    cv::Mat image_clone &#x3D; image.clone();    image_clone ( cv::Rect ( 0,0,100,100 ) ).setTo ( 255 );    cv::imshow ( &quot;image&quot;, image );    cv::imshow ( &quot;image_clone&quot;, image_clone );    cv::waitKey ( 0 );    &#x2F;&#x2F; 对于图像还有很多基本的操作,如剪切,旋转,缩放等,限于篇幅就不一一介绍了,请参看OpenCV官方文档查询每个函数的调用方法.    cv::destroyAllWindows();    return 0;&#125;cmake_minimum_required( VERSION 2.8 )project( imageBasics )# 添加c++ 11标准支持set( CMAKE_CXX_FLAGS &quot;-std&#x3D;c++11&quot; )# 寻找OpenCV库set(OpenCV_DIR  ~&#x2F;ssd&#x2F;software&#x2F;opencv3.3.1&#x2F;build)   #添加OpenCVConfig.cmake的搜索路径find_package( OpenCV 3 REQUIRED )# 添加头文件include_directories( $&#123;OpenCV_INCLUDE_DIRS&#125; )add_executable( imageBasics imageBasics.cpp )# 链接OpenCV库target_link_libraries( imageBasics $&#123;OpenCV_LIBS&#125; )         <span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>]]></content>
      
      
      <categories>
          
          <category> Opencv </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Opencv </tag>
            
            <tag> Harris </tag>
            
            <tag> SIFT </tag>
            
            <tag> SURF </tag>
            
            <tag> ORB </tag>
            
            <tag> FAST </tag>
            
            <tag> BRIEF描述子 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>服务器开发与工具使用（vscode,anaconda,DL等）</title>
      <link href="/2023/03/31/net/"/>
      <url>/2023/03/31/net/</url>
      
        <content type="html"><![CDATA[<h1 id="服务器"><a href="#服务器" class="headerlink" title="服务器"></a>服务器</h1><p>首先拥有一个属于自己的服务器账号，包括服务器ip，端口，用户名以及密码等。</p><p>接着，需要安装ssh,一般linux系统都自带ssh，windows一般自带ssh客户端。如未安装，可查看以下教程安装</p><p><a href="https://blog.csdn.net/weixin_50964512/article/details/123588745">ssh安装与配置，详解版</a></p><p><a href="https://blog.csdn.net/qq_33594636/article/details/128849482">Windows安装和启动SSH服务</a></p><h2 id="软件安装"><a href="#软件安装" class="headerlink" title="软件安装"></a>软件安装</h2><p><strong>xftp7以及xshell7的安装</strong></p><ul><li><p>Xftp7是一个功能强大但轻量级的SFTP&#x2F;FTP客户机，用于需要在网络上安全地传输文件的用户。通过使用 拖放、直接编辑、增强的同步、传输调度和更直观的选项卡界面等特性，文件传输得到了简化。</p></li><li><p>Xshell 是一个强大的安全终端模拟软件，它支持SSH1、SHH2、以及 Microsoft Windows 平台的TELNET协议。Xshell 通过互联网到远程主机的安全连接以及它创新性的设计和特色帮助用户在复杂的网络环境中享受他们的工作。</p></li><li><p>Xshell 可以在 Windows 界面下用来访问远端不同系统下的服务器，从而比较好的达到远程控制终端的目的。除此之外，其还有丰富的外观配色方案以及样式选择。</p></li></ul><p>下载需要填个邮箱，之后它会给你的邮箱发个下载链接，点击下载链接安装即可。</p><p><a href="https://www.xshell.com/zh/free-for-home-school/">学生版下载网址</a></p><p><a href="https://www.xshell.com/">官网网址</a></p><p><strong>vscode软件安装</strong></p><p>Visual Studio Code 简称 VSCode ，2015 年由微软公司发布。</p><p>可用于 Windows，macOS 和 Linux。它具有对 JavaScript，TypeScript 和 Node.js 的内置支持，并具有丰富的其他语言（例如 C++，C＃，Java，Python，PHP，Go</p><p><a href="https://code.visualstudio.com/">vscode下载官网</a></p><p><a href="https://blog.csdn.net/weixin_44950987/article/details/128129613">vscode安装教程</a></p><p><strong>vscode链接远程服务器</strong></p><p>在vscode软件里安装插件<em><strong>Remote-SSH</strong></em>，安装完成后需要添加服务器连接配置，具体操作如下链接：</p><p><a href="https://blog.csdn.net/zhaxun/article/details/120568402">vscode连接远程服务器</a></p><blockquote><p>总结：xftp7用来传数据，xshell7与vscode都可以通过命令行形式对远程服务器进行操控，其中vscode依靠它强大的性能可以直接对服务器中个人程序进行图形化编写，运行，调试等</p></blockquote><h1 id="服务器中开发环境配置"><a href="#服务器中开发环境配置" class="headerlink" title="服务器中开发环境配置"></a>服务器中开发环境配置</h1><h2 id="Anaconda-x2F-miniconda安装"><a href="#Anaconda-x2F-miniconda安装" class="headerlink" title="Anaconda&#x2F;miniconda安装"></a>Anaconda&#x2F;miniconda安装</h2><p>对于需要进行深度学习的同学，需要安装Anaconda以管理自己的服务器环境，对于本实验室的服务器，可直接访问&#x2F;share&#x2F;software来获得此前下载过的Anaconda各版本</p><p>如果想自己安装不同版本的anaconda，请查看如下教程：</p><p><a href="https://zhuanlan.zhihu.com/p/32925500">Anaconda介绍，安装及使用教程</a></p><p><a href="https://blog.csdn.net/qq_42257666/article/details/121383450">anaconda安装配置教程</a></p><p><strong>conda是Anaconda提供的一个管理版本和Python环境的工具，我们一般都使用它创建虚拟环境，能够隔离不同的Python软件包环境，也不会影响到其他用户。强烈建议每个用户单独将它安装在用户目录用于Python环境管理。不建议使用系统自带的Python环境。</strong></p><p><a href="https://zhuanlan.zhihu.com/p/537439636">conda的使用,创建虚拟环境</a></p><p>安装完成之后，可以替换镜像源为清华源，安装软件包时速度一般能更快，换源教程如下：</p><p><a href="https://blog.csdn.net/qq_43115981/article/details/129064657">anaconda换源</a></p><h2 id="在自己创建的虚拟环境中安装PyTorch"><a href="#在自己创建的虚拟环境中安装PyTorch" class="headerlink" title="在自己创建的虚拟环境中安装PyTorch"></a>在自己创建的虚拟环境中安装PyTorch</h2><p>参照官网，选择好对应的cuda版本和python语言以及linux系统即可。</p><p>注意使用conda安装时，会自动安装cudatoolkit与cudnn，</p><p><a href="https://pytorch.org/">pytorch官网</a></p><p><a href="https://pytorch.org/get-started/previous-versions/">pytorch以往版本</a></p><p>安装好pytorch后，就可以开始自己的深度学习编程之旅了~~~</p><h1 id="深度学习教程"><a href="#深度学习教程" class="headerlink" title="深度学习教程"></a>深度学习教程</h1><p>这里推荐一些深度学习教程</p><p><a href="https://zh-v2.d2l.ai/">动手学深度学习</a></p><p><a href="hhttps://www.bilibili.com/video/BV1FT4y1E74V/?p=3&vd_source=c3ce2553ecbf3f37d2f4be6f466233fa">吴恩达深度学习</a></p><p><a href="https://www.bilibili.com/video/BV1nJ411z7fe/?spm_id_from=333.337.search-card.all.click&vd_source=c3ce2553ecbf3f37d2f4be6f466233fa">斯坦福李飞飞cs231n计算机视觉课程【附中文字幕】</a></p><p>强烈推荐B站UP主：<a href="https://space.bilibili.com/18161609/channel/series">霹雳吧啦Wz</a></p><p>他对深度学习基础的网络进行了详细的讲解并附上源码，包括图像分类、语义分割、关键点检测、目标检测，实例分割等，强烈推荐学习</p><p><a href="https://github.com/WZMIAOMIAO/deep-learning-for-image-processing">源码地址</a></p><p>另外推荐一些开源的深度学习开源工具箱，零基础上手就可以跑起来</p><p><a href="https://github.com/open-mmlab">OpenMMLab 平台</a></p><p><a href="https://github.com/open-mmlab/mmdetection">MMDetection基于 PyTorch 的目标检测开源工具箱</a></p><p><a href="https://github.com/open-mmlab/mmyolo">mmyolo基于 PyTorch 和 MMDetection 的 YOLO 系列算法开源工具箱</a></p>]]></content>
      
      
      <categories>
          
          <category> net </category>
          
          <category> DL </category>
          
      </categories>
      
      
        <tags>
            
            <tag> net </tag>
            
            <tag> ssh </tag>
            
            <tag> DL </tag>
            
            <tag> vscode </tag>
            
            <tag> anaconda </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>ROS介绍及快速体验</title>
      <link href="/2023/03/30/ros/"/>
      <url>/2023/03/30/ros/</url>
      
        <content type="html"><![CDATA[<h1 id="ROS概念"><a href="#ROS概念" class="headerlink" title="ROS概念"></a>ROS概念</h1><p><strong>ROS全称Robot Operating System(机器人操作系统)</strong></p><ul><li><p>ROS是适用于机器人的开源元操作系统</p></li><li><p>ROS集成了大量的工具，库，协议，提供类似OS所提供的功能，简化对机器人的控制</p></li><li><p>还提供了用于在多台计算机上获取，构建，编写和运行代码的工具和库，ROS在某些方面类似于“机器人框架”</p></li><li><p>ROS设计者将ROS表述为“ROS &#x3D; Plumbing + Tools + Capabilities + Ecosystem”，即ROS是通讯机制、工具软件包、机器人高层技能以及机器人生态系统的集合体</p></li></ul><p><img src="/pic/%E9%80%89%E5%8C%BA_028.png"></p><h1 id="ROS框架"><a href="#ROS框架" class="headerlink" title="ROS框架"></a>ROS框架</h1><p>ROS 框架主要分成三个层级，分别是 ROS 文件系统、ROS 计算图和 ROS 社区。</p><h2 id="ROS文件系统"><a href="#ROS文件系统" class="headerlink" title="ROS文件系统"></a>ROS文件系统</h2><p>ROS 的文件系统主要介绍了硬盘上 ROS 文件的组织形式。其中，我们必须了解的主要有以下几个方面：</p><ul><li>软件包（Package）：ROS 软件包是 ROS 软件框架的独立单元。ROS 软件包可能包含源代码、第三方软件库、配置文件等。ROS 软件包可以复用和共享。</li><li>软件包清单（Package Manifest）：清单文件（package.xml）列出了软件包的所有详细信息，包括名称、描述、许可信息以及最重要的依赖关系。</li><li>消息（msg）类型：消息的描述存储在软件包的 msg 文件夹下。ROS 消息是一组通过 ROS 的消息传递系统进行数据发送的数据结构。消息的定义存储在扩展名为 .msg 的文件里。</li><li>服务（srv）类型：服务的描述使用扩展名 .srv 存储在 srv 文件夹下。该文件定义了 ROS 内服务请求和响应的数据结构。</li></ul><p><img src="/pic/%E9%80%89%E5%8C%BA_029.png"></p><pre class="line-numbers language-none"><code class="language-none">WorkSpace --- 自定义的工作空间    |--- build:编译空间，用于存放CMake和catkin的缓存信息、配置信息和其他中间文件。    |--- devel:开发空间，用于存放编译后生成的目标文件，包括头文件、动态&amp;静态链接库、可执行文件等。    |--- src: 源码        |-- package：功能包(ROS基本单元)包含多个节点、库与配置文件，包名所有字母小写，只能由字母、数字与下划线组成            |-- CMakeLists.txt 配置编译规则，比如源文件、依赖项、目标文件            |-- package.xml 包信息，比如:包名、版本、作者、依赖项...(以前版本是 manifest.xml)            |-- scripts 存储python文件            |-- src 存储C++源文件            |-- include 头文件            |-- msg 消息通信格式文件            |-- srv 服务通信格式文件            |-- action 动作格式文件            |-- launch 可一次性运行多个节点             |-- config 配置信息        |-- CMakeLists.txt: 编译的基本配置<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h2 id="ROS计算图"><a href="#ROS计算图" class="headerlink" title="ROS计算图"></a>ROS计算图</h2><p>ros 程序运行之后，不同的节点之间是错综复杂的，ROS 中提供了一个实用的工具:rqt_graph。</p><p>rqt_graph能够创建一个显示当前系统运行情况的动态图形。ROS 分布式系统中不同进程需要进行数据交互，计算图可以以点对点的网络形式表现数据交互过程。rqt_graph是rqt程序包中的一部分。</p><p>ROS 计算图中的基本功能包括节点、ROS 控制器、参数服务器、消息和服务：</p><ul><li><strong>节点（Node）</strong>：ROS 节点是使用 ROS 功能处理数据的进程。节点的基本功能是计算。例如，节点可以对激光扫描仪数据进行处理，以检查是否存在碰撞。ROS 节点的编写需要 ROS 客户端库文件（如roscpp和rospy）的支持。</li><li><strong>ROS 控制器（Master）</strong>：ROS 节点可以通过名为 ROS 控制器的程序相互连接。此程序提供计算图其他节点的名称、注册和查找信息。如果不运行这个控制器，节点之间将无法相互连接和发送消息。</li><li><strong>参数服务器（Parameter server）</strong>：ROS 参数是静态值，存储在叫作参数服务器的全局位置。所有节点都可以从参数服务器访问这些值。我们甚至可以将参数服务器的范围设置为 private 以访问单个节点，或者设置为 public 以访问所有节点。</li><li><strong>ROS主题（Topic）</strong>：ROS 节点使用命名总线（叫作 ROS 主题）彼此通信。数据以消息的形式流经主题。通过主题发送消息称为发布，通过主题接收数据称为订阅。</li><li><strong>消息（Message）</strong>：ROS 消息是一种数据类型，可以由基本数据类型（如整型、浮点型、布尔类型等）组成。ROS 消息流经 ROS 主题。一个主题一次只能发送&#x2F;接收一种类型的消息。我们可以创建自己的消息定义并通过主题发送它。</li><li><strong>服务（Service）</strong>：我们看到使用 ROS 主题的发布&#x2F;订阅模型是一种非常灵活的通信模式，这是一种一对多的通信模式，意味着一个主题可以被任意数量的节点订阅。在某些情况下，可能还需要一种<strong>请求&#x2F;应答</strong>类型的交互方式，它可以用于分布式系统。这种交互方式可以使用 ROS 服务实现。ROS 服务的工作方式与 ROS 主题类似，因为它们都有消息类型定义。使用该消息定义可以将服务请求发送到另一个提供该服务的节点。服务的结果将作为应答发送。该节点必须等待，直到从另一个节点接收到结果。</li><li><strong>ROS 消息记录包（Bag）</strong>：这是一种用于保存和回放 ROS 主题的文件格式。ROS 消息记录包是记录传感器数据和处理数据的重要工具。这些包之后可以用于离线测试算法</li></ul><blockquote><p>演示</p></blockquote><p>首先，按照前面所示，运行案例</p><p>然后，启动新终端，键入: rqt_graph 或 rosrun rqt_graph rqt_graph，可以看到类似下图的网络拓扑图，该图可以显示不同节点之间的关系。</p><p><img src="/pic/%E9%80%89%E5%8C%BA_030.png"></p><h2 id="ROS快速体验"><a href="#ROS快速体验" class="headerlink" title="ROS快速体验"></a>ROS快速体验</h2><h3 id="HelloWorld实现简介"><a href="#HelloWorld实现简介" class="headerlink" title="HelloWorld实现简介"></a>HelloWorld实现简介</h3><p>ROS中涉及的编程语言以C++和Python为主，ROS中的大多数程序两者都可以实现,ROS中的程序即便使用不同的编程语言，实现流程也大致类似，以当前HelloWorld程序为例，实现流程大致如下：</p><ul><li>先创建一个工作空间；</li><li>再创建一个功能包；</li><li>编辑源文件；</li><li>编辑配置文件；</li><li>编译并执行。</li></ul><p><strong>1.创建工作空间并初始化</strong></p><pre class="line-numbers language-none"><code class="language-none">mkdir -p 自定义空间名称&#x2F;srccd 自定义空间名称catkin_make<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre><p><strong>2.进入 src 创建 ros 包并添加依赖</strong></p><pre class="line-numbers language-none"><code class="language-none">cd srccatkin_create_pkg 自定义ROS包名 roscpp rospy std_msgs<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><p>上述命令，会在工作空间下生成一个功能包，该功能包依赖于** <em>roscpp、rospy 与 std_msgs</em>**，其中roscpp是使用C++实现的库，而rospy则是使用python实现的库，std_msgs是标准消息库，创建ROS功能包时，一般都会依赖这三个库实现。</p><p><em><strong>补充</strong></em> ：在ROS中，虽然实现同一功能时，C++和Python可以互换，但是具体选择哪种语言，需要视需求而定，因为两种语言相较而言:C++运行效率高但是编码效率低，而Python则反之，基于二者互补的特点，ROS设计者分别设计了roscpp与rospy库，前者旨在成为ROS的高性能库，而后者则一般用于对性能无要求的场景，旨在提高开发效率。</p><h3 id="HelloWorld-C-版"><a href="#HelloWorld-C-版" class="headerlink" title="HelloWorld(C++版)"></a>HelloWorld(C++版)</h3><p><strong>1.进入 ros 包的 src 目录编辑源文件</strong></p><pre class="line-numbers language-none"><code class="language-none">cd 自定义的包<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>C++源码实现(文件名自定义)</p><pre class="line-numbers language-c++" data-language="c++"><code class="language-c++">#include &quot;ros&#x2F;ros.h&quot;int main(int argc, char *argv[])&#123;    &#x2F;&#x2F;执行 ros 节点初始化    ros::init(argc,argv,&quot;hello&quot;);    &#x2F;&#x2F;创建 ros 节点句柄(非必须)    ros::NodeHandle n;    &#x2F;&#x2F;控制台输出 hello world    ROS_INFO(&quot;hello world!&quot;);    return 0;&#125;<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p><strong>2.编辑 ros 包下的 Cmakelist.txt文件</strong></p><pre class="line-numbers language-c++" data-language="c++"><code class="language-c++">add_executable(步骤3的源文件名  src&#x2F;步骤3的源文件名.cpp)target_link_libraries(步骤3的源文件名  $&#123;catkin_LIBRARIES&#125;)<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p><strong>3.进入工作空间目录并编译</strong></p><pre class="line-numbers language-none"><code class="language-none">cd 自定义空间名称catkin_make<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><p>生成 build devel ….</p><p><strong>4.执行</strong></p><p><em>先启动命令行1：</em></p><pre class="line-numbers language-none"><code class="language-none">roscore<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p><em>再启动命令行2：</em></p><pre class="line-numbers language-none"><code class="language-none">cd 工作空间source .&#x2F;devel&#x2F;setup.bashrosrun 包名 C++节点<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre><p>命令行输出: HelloWorld!</p><p><em><strong>补充</strong></em>：source ~&#x2F;工作空间&#x2F;devel&#x2F;setup.bash可以添加进.bashrc文件，使用上更方便</p><p><strong>添加方式1: 直接使用 gedit 或 vi 编辑 .bashrc 文件，最后添加该内容</strong></p><p><strong>添加方式2:echo “source ~&#x2F;工作空间&#x2F;devel&#x2F;setup.bash” &gt;&gt; ~&#x2F;.bashrc</strong></p><blockquote><p>参考文献</p></blockquote><p><a href="http://c.biancheng.net/view/9853.html">ROS机器人操作系统简介</a></p><p><a href="http://www.autolabor.com.cn/book/ROSTutorials/chapter1/15-ben-zhang-xiao-jie/153-rosji-suan-tu.html">机器人入门教程</a></p>]]></content>
      
      
      <categories>
          
          <category> ROS </category>
          
          <category> c++ </category>
          
      </categories>
      
      
        <tags>
            
            <tag> c++ </tag>
            
            <tag> ros </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>RANSAC、LMEDS以及Hough变换原理</title>
      <link href="/2023/03/28/ransac/"/>
      <url>/2023/03/28/ransac/</url>
      
        <content type="html"><![CDATA[<h1 id="RANSAC算法简介"><a href="#RANSAC算法简介" class="headerlink" title="RANSAC算法简介"></a>RANSAC算法简介</h1><p>RANSAC是”RANdom SAmple Consensus”（随机采样一致）的缩写。它可以从一组包含“局外点”的观测数据集中，通过迭代方式估计数学模型的参数，它是一种不确定的算法—-它有一定的概率得出一个合理的结果；为了提高概率必须提高迭代次数。</p><blockquote><p>RANSAC基本假设</p></blockquote><ul><li>数据由<strong>局内点</strong>组成， 例如，数据的分布可以用一些模型（比如直线方程）参数来解释；</li><li><strong>局外点</strong>是不能适应该模型的参数；</li><li>除此之外的数据属于噪声；</li></ul><p>局外点产生的原因有：噪声的极值；错误的测量方法；对数据的错误假设等；</p><blockquote><p>RANSAC概述</p></blockquote><p>RANSAC算法的输入时一组观测数据，一个可以解释或者适应于观测数据的参数化模型，一些可行的参数。</p><p>RANSAC通过反复选择数据中的一组随机子集来达成目标。被选取的子集被假设为局内点，并用下述方法进行验证：</p><ul><li>有一个模型适应于假定的局内点，即所有的未知参数都能从假设的局内点计算得出；</li><li>用1中得到的模型去测试所有的其它数据，如果某个点适用于估计的模型，认为他也是局内点；</li><li>如果有足够多的点呗归类为假设的局内点，那么估计的模型就足够合理；</li><li>然后，用所有假设的局内点去重新估计模型，因为它仅仅被初始的假设局内点估计过；</li><li>最后，通过估计局内点与模型的错误率来评估模型；</li></ul><p>这个过程被重复执行固定的次数，每次产生的模型要么因为局内点太少而被抛弃，要么因为比现有的模型更好而被选用。</p><p>关于模型好坏算法实现上有两种方式：</p><ul><li>规定一个点数，达到这个点数后，算这些点与模型间的误差，找误差最小的模型。 对应下面算法一</li><li>规定一个误差，找匹配模型并小于这个误差的所有点，匹配的点最多的模型，就是最好模型。 对应下面算法二</li></ul><p>算法伪代码一：</p><pre class="line-numbers language-伪代码" data-language="伪代码"><code class="language-伪代码">输入：data ---- 一组观测数据model ---- 适应于数据的模型n ---- 适用于模型的最少数据个数k ---- 算法的迭代次数t ---- 用于决定数据是否适应于模型的阈值d ---- 判定模型是否适用于数据集的数据数目输出：best_model —— 跟数据最匹配的模型参数（如果没有找到，返回null）best_consensus_set —— 估计出模型的数据点best_error —— 跟数据相关的估计出的模型的错误iterations &#x3D; 0best_model &#x3D; nullbest_consensus_set &#x3D; nullbest_error &#x3D; 无穷大while( iterations &lt; k )    maybe_inliers &#x3D;  从数据集中随机选择n个点    maybe_model &#x3D; 适合于maybe_inliers的模型参数    consensus_set &#x3D; maybe_inliers    for (每个数据集中不属于maybe_inliers的点)        if （如果点适合于maybe_model，并且错误小于t）           将该点添加到consensus_set    if (consensus_set中的点数大于d)        已经找到了好的模型， 现在测试该模型到底有多好       better_model &#x3D; 适用于consensus_set中所有点的模型参数       this_error &#x3D;  better_model 究竟如何适合这些点的度量        if （this_error &lt; best_error）        发现比以前好的模型，保存该模型直到更好的模型出现        best_model &#x3D; better_model        best_consensus_set &#x3D; consensus_set        best_error &#x3D; this_error    iterations ++函数返回best_model, best_consensus_set, best_error<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>RANSAC算法的可能变化包括以下几种：</p><ul><li>如果发现一种足够好的模型（该模型有足够下的错误率）， 则跳出主循环，这样节约不必要的计算；设置一个错误率的阈值，小于这个值就跳出循环；</li><li>可以直接从maybe_model计算this_error，而不从consensus_set重新估计模型，这样可能会节约时间，但是可能会对噪音敏感。</li></ul><p>算法伪代码二：</p><pre class="line-numbers language-伪代码" data-language="伪代码"><code class="language-伪代码">输入：data ---- 一组观测数据numForEstimate ----- 初始模型需要的点数delta ------ 判定点符合模型的误差probability ----- 表示迭代过程中从数据集内随机选取出的点均为局内点的概率输出：best_model —— 跟数据最匹配的模型参数（如果没有找到，返回null）best_consensus_set —— 估计出模型的数据点k &#x3D; 1000&#x2F;&#x2F;设置初始值iterations &#x3D; 0best_model &#x3D; nullbest_consensus_set &#x3D; nullwhile( iterations &lt; k )    maybe_inliers &#x3D;  从数据集中随机选择numForEstimate个点    maybe_model &#x3D; 适合于maybe_inliers的模型参数，比如直线，取两个点，得直线方程    for (每个数据集中不属于maybe_inliers的点)        if （如果点适合于maybe_model，并且错误小于delta）            将该点添加到maybe_inliers    if(maybe_inliers的点数 &gt; best_consensus_set 的点数）&#x2F;&#x2F;找到更好的模型        best_model &#x3D; maybe_model        best_consensus_set  &#x3D; maybe_inliers        根据公式k&#x3D;log(1-p)&#x2F;log(1-pow(w,n))重新计算k    iterations ++函数返回best_model, best_consensus_set,<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h2 id="RANSAC参数"><a href="#RANSAC参数" class="headerlink" title="RANSAC参数"></a>RANSAC参数</h2><p>我们不得不根据特定的问题和数据集通过实验来确定参数t和d。然而参数k（迭代次数）可以从理论结果推断。当我们从估计模型参数时，用p表示一些迭代过程中从数据集内随机选取出的点均为局内点的概率；此时，结果模型很可能有用，因此p也表征了算法产生有用结果的概率。用w表示每次从数据集中选取一个局内点的概率，如下式所示： w &#x3D; 局内点的数目 &#x2F; 数据集的数目 通常情况下，我们事先并不知道w的值，但是可以给出一些鲁棒的值。假设估计模型需要选定n个点，wn是所有n个点均为局内点的概率；1 − wn是n个点中至少有一个点为局外点的概率，此时表明我们从数据集中估计出了一个不好的模型。 (1 − wn)k表示算法永远都不会选择到n个点均为局内点的概率，它和1-p相同。因此，<br>$$<br>1-p&#x3D;\left(1-w^n\right)^k<br>$$</p><p>其中</p><ul><li><p>p 表示置信度confidence</p></li><li><p>w 表示数据集中inlier占的比例</p></li><li><p>n 表示采样点数</p></li><li><p>k 表示需要迭代采样的最少次数</p></li><li><p>1 − wn 表示采样一次，n个点中至少有一个outlier的概率</p></li><li><p>(1 − wn)k 表示采样k次，n个点中至少有一个outlier的概率</p></li><li><p>因为p为采样k次，能有至少一次n个点都是inlier的概率</p></li><li><p>所以(1 − p)和(1 − wn)k相等时，k 为需要迭代采样的最少次数</p></li></ul><p>下面是k的解析解：</p><p>$$<br>k&#x3D;\frac{\log (1-p)}{\log \left(1-w^n\right)}<br>$$</p><p>值得注意的是，这个结果假设n个点都是独立选择的；也就是说，某个点被选定之后，它可能会被后续的迭代过程重复选定到。这种方法通常都不合理，由此推导出的k值被看作是选取不重复点的上限。例如，要从上图中的数据集寻找适合的直线，RANSAC算法通常在每次迭代时选取2个点，计算通过这两点的直线maybe_model，要求这两点必须唯一。</p><p>为了得到更可信的参数，标准偏差或它的乘积可以被加到k上。k的标准偏差定义为：</p><p>$$<br>S D(k)&#x3D;\frac{\sqrt{1-w^n}}{w^n}<br>$$</p><blockquote><p>RANSAC的函数接口 参照opencv来说主要需要3-4个参数（第四个不是必须的）</p></blockquote><ul><li>误差阈值ransacThreshold：区分inlier和outliner的依据</li><li>置信度confidence：设置之后代表RANSAC采样n次过程中会出现（至少一次）采样点数据集中的点都为内点的概率。这个值设置的太大，会增加采样次数。太小，会使结果不太理想。一般取0.95-0.99</li><li>最大采样迭代次数maxIters：为了防止一直在采样计算</li><li>最大精细迭代次数refineIters：在采样之后，选取最优解。可以增加精确优化，比如使用LM算法获得更优解。</li></ul><p>以上4个参数，有三个是经验值。其中最大采样迭代次数maxIters是可以有数值解的。</p><h2 id="RANSAC优点与缺点"><a href="#RANSAC优点与缺点" class="headerlink" title="RANSAC优点与缺点"></a>RANSAC优点与缺点</h2><p>RANSAC的优点是它能鲁棒的估计模型参数。例如，它能从包含大量局外点的数据集中估计出高精度的参数。</p><p>RANSAC的缺点是它计算参数的迭代次数没有上限；如果设置迭代次数的上限，得到的结果可能不是最优的结果，甚至可能得到错误的结果。RANSAC只有一定的概率得到可信的模型，概率与迭代次数成正比。RANSAC的另一个缺点是它要求设置跟问题相关的阀值。</p><p>RANSAC只能从特定的数据集中估计出一个模型，如果存在两个（或多个）模型，RANSAC不能找到别的模型。如果有多个模型，可以先估算出一个，然后用剩余的数据重新运算，重复这个过程，直到没有模型。</p><h2 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h2><p><a href="https://www.cnblogs.com/xrwang/archive/2011/03/09/ransac-1.html">王先荣RANSAC介绍</a></p><p><a href="https://zhuanlan.zhihu.com/p/402727549">RANSAC详解，保姆级教程</a></p><p><a href="https://zhuanlan.zhihu.com/p/45532306">计算机视觉基本原理</a></p><h2 id="RANSAC应用"><a href="#RANSAC应用" class="headerlink" title="RANSAC应用"></a>RANSAC应用</h2><p>OpenCV中使用RANSAC算法实现多张图像拼接思路：</p><ul><li><p>获取图像的特征点，将每张图片的特征点保存到一个vector中；</p></li><li><p>通过特征点匹配的方法，找到每张图片的共有特征点，并将其保存到一个vector中；</p></li><li><p>通过RANSAC算法求解出拼接的变换矩阵；</p></li><li><p>根据变换矩阵对每张图片进行仿射变换；</p></li><li><p>将拼接后的图片进行裁剪；</p></li><li><p>将裁剪后的图片拼接起来，最终得到拼接后的图片。</p></li></ul><p><a href="https://blog.csdn.net/qq_39312146/article/details/129053592">OpenCV中使用RANSAC算法实现多张图像拼接</a></p><p>OpenCV中的solvePnPRansac函数和findHomography函数都具有RANSAC特性，该特性使算法对少量的错误数据鲁棒。<br>这两个函数利用RANSACPointSetRegistrator类实现RANSAC算法，但这个类并没有对外开放，因此只能通过阅读OpenCV源代码学习RANSAC算法的实现和使用。</p><p>类的实现在ptsetreg.cpp中，可通过调用precomp.hpp文件中的createRANSACPointSetRegistrator函数使用。此外，该文件还提供了createLMeDSPointSetRegistrator函数调用最小中值算法。</p><p><a href="https://blog.csdn.net/HopefulLight/article/details/78775974">在OpenCV中使用RANSAC</a></p><h1 id="LMEDS算法概述（最小中值法：Least-Median-of-Squares）"><a href="#LMEDS算法概述（最小中值法：Least-Median-of-Squares）" class="headerlink" title="LMEDS算法概述（最小中值法：Least Median of Squares）"></a>LMEDS算法概述（最小中值法：Least Median of Squares）</h1><blockquote><p>经典步骤</p></blockquote><ul><li><p>随机采样</p></li><li><p>计算模型参数</p></li><li><p>计算相对模型的点集偏差err，并求出偏差中值Med(err)</p></li><li><p>迭代2. 3.步直至获得符合阈值的最优解：Med(err)最小</p></li><li><p>精确优化模型参数（LM算法迭代优化）</p></li></ul><blockquote><p>LMedS的函数接口 参照opencv来说主要需要2-3个参数（第三个不是必须的）</p></blockquote><ul><li><p>置信度confidence：设置之后代表RANSAC采样n次过程中会出现（至少一次）采样点数据集中的点都为内点的概率，这个值设置的太大，会增加采样次数。太小，会使结果不太理想。一般取0.95-0.99</p></li><li><p>最大采样迭代次数maxIters：为了防止一直在采样计算</p></li><li><p>最大精细迭代次数refineIters：在采样之后，选取最优解。可以增加精确优化，比如使用LM算法获得更优解。</p></li></ul><p>注意： 相对于RANSAC，LMedS有一个优点：不需要指定 - 误差阈值ransacThreshold：区分inlier和outliner的依据</p><blockquote><p>RANSAC与LMEDS两者的区别</p></blockquote><p><em><strong>RANSAC的阈值在具有物理意义或者几何意义的时候比较容易确定，但是当阈值不具有这些特征的时候，就成了一个不太好调整的参数了。这时LMedS可以自适应迭代获得最优解。</strong></em></p><blockquote><p>此外，LMedS也能自适应获得inlier和outliner,公式如下：</p></blockquote><p>$$<br>\hat{\sigma}&#x3D;1.4826\left(1+\frac{5}{n-p}\right) \operatorname{med}_i \sqrt{r_i^2}<br>$$</p><p>其中</p><ul><li><p>n 表示点集的个数</p></li><li><p>p 表示计算模型一次采样的点个数</p></li><li><p>ri2  表示误差</p></li><li><p>med(ri2 ) 表示误差中值</p></li></ul><p> 筛选条件为：</p> <!-- $$w_i=\left\{\begin{array}{cc}1 & \frac{\left|r_i\right|}{\hat{\sigma}} \leq 2.5 \\ 0 & \frac{\left|r_i\right|}{\hat{\sigma}}>2.5\end{array}\right.$$ --><p><img src="/pic/%E9%80%89%E5%8C%BA_027.png"></p><p><strong>由于LMedS会需要对整个点集的err求中值，当点集很大的时候，求中值的过程会很消耗时间</strong></p><p><a href="https://blog.csdn.net/billbliss/article/details/78592216">RANSAC LMedS 详细分析</a></p><h1 id="霍夫变换-Hough"><a href="#霍夫变换-Hough" class="headerlink" title="霍夫变换(Hough)"></a>霍夫变换(Hough)</h1><p><strong>霍夫变换</strong>是一种特征提取(feature extraction)，被广泛应用在图像分析（image analysis）、计算机视觉(computer vision)以及数位影像处理(digital image processing)。霍夫变换是用来辨别找出物件中的特征，例如：线条。他的算法流程大致如下，给定一个物件、要辨别的形状的种类，算法会在参数空间(parameter space)中执行投票来决定物体的形状，而这是由累加空间(accumulator space)里的局部最大值(local maximum)来决定。</p><p>经典的霍夫变换是侦测图片中的直线，之后，霍夫变换不仅能识别直线，也能够识别任何形状，常见的有圆形、椭圆形。1981年，因为DanaH.Ballard的一篇期刊论文”Generalizing the Hough transform to detect arbitrary shapes”，让霍夫变换开始流行于计算机视觉界。</p><p><a href="https://zhuanlan.zhihu.com/p/47649796">霍夫变换-神奇的特征提取方法</a></p><p><a href="https://zhuanlan.zhihu.com/p/203292567">通俗易懂-霍夫变换原理</a></p><p><a href="https://zhuanlan.zhihu.com/p/386048978">霍夫变换及代码实现</a></p>]]></content>
      
      
      <categories>
          
          <category> Opencv </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Opencv </tag>
            
            <tag> RANSAC </tag>
            
            <tag> LMEDS </tag>
            
            <tag> Hough </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>视觉及其融合惯性SLAM技术发展综述</title>
      <link href="/2023/03/27/slam/"/>
      <url>/2023/03/27/slam/</url>
      
        <content type="html"><![CDATA[<h1 id="SLAM技术"><a href="#SLAM技术" class="headerlink" title="SLAM技术"></a>SLAM技术</h1><p>同步定位与建图（Simultaneous Localization and Mapping，简称SLAM）问题可以描述为：机器人在未知环境中从一个未知位置开始移动,在移动过程中根据位置和地图进行自身定位，同时在自身定位的基础上建造增量式地图，实现机器人的自主定位和导航。</p><blockquote><p>SLAM 技术主要呈现以下 3 点发展趋势。</p></blockquote><ul><li>理论优化改进：由于应用场景需求的多样化，结合惯性、异源图像等多传感器的信息融合模式成为 SLAM 主流，促进了以紧耦合为主的信息融合理论发展，而随着大场景 SLAM 应用需求及图优化理论的推进，逐步形成了<strong>基于扩展卡尔曼滤波</strong>框架的改进滤波器优化架构，和以<strong>光束法平差（BA）</strong>为主的非线性优化架构两种研究趋势。</li><li>新型技术引入：随着<strong>深度学习</strong>技术在计算机视觉中的广泛应用，视觉 SLAM 呈现出由传统几何变换方式逐步转向结合深度学习的智能融合趋势。一方面<strong>视觉图像与语义信息</strong>的紧密联系，使得集成语义信息的视觉 SLAM 得到更多探索；另一方面为减少对传统方式依赖，利用<strong>神经网络架构</strong>替代 SLAM 的部分模块或端到端<strong>强化学习</strong>的模式得以广泛研究。</li><li>应用领域推广：视觉 SLAM 目前在<em>智能家居、自动驾驶、无人机</em>等领域得到了不同层次的应用，随着硬件性能的提升，视觉 SLAM</li></ul><p><img src="/pic/%E5%9B%BE%E7%89%871.png" alt="视觉、惯性SLAM系统框架结构"></p><p><img src="/pic/%E5%9B%BE%E7%89%872.png" alt="视觉SLAM构建地图类型"></p><p><img src="/pic/%E5%9B%BE%E7%89%873.png"></p><p><img src="/pic/%E5%9B%BE%E7%89%874.png"></p><p><img src="/pic/%E5%9B%BE%E7%89%875.png"></p><p><img src="/pic/%E5%9B%BE%E7%89%876.png"></p><p><img src="/pic/%E5%9B%BE%E7%89%877.png" alt="标准视觉SLAM Pipeline"></p><p><img src="/pic/%E5%9B%BE%E7%89%878.png" alt="极具影响力的视觉SLAM方法"></p><blockquote><p>VSLAM 常用数据集：表内的GT 是指真值的可用性</p></blockquote><p><img src="/pic/%E5%9B%BE%E7%89%879.png"></p><p><img src="/pic/%E5%9B%BE%E7%89%8710.png" alt="各种论文中用于评估的一些主流视觉SLAM数据集的实例"></p>]]></content>
      
      
      
        <tags>
            
            <tag> slam </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>图像变换(image_transformer)</title>
      <link href="/2023/03/27/image-transformer/"/>
      <url>/2023/03/27/image-transformer/</url>
      
        <content type="html"><![CDATA[<h1 id="图像变换"><a href="#图像变换" class="headerlink" title="图像变换"></a>图像变换</h1><p>基本的图像变换有<strong>刚性变换(等距变换、欧式变换)、相似变换、仿射变换、射影变换(透视变换、投影变换)</strong></p><blockquote><p>刚性变化：只对图像进行平移与旋转，形状保持不变</p></blockquote><p>欧式变换（等距变换）保持了向量的<strong>长度和夹角</strong>，相当于我们把一个刚体原封不动地进行移动或旋转，不改变它自身的样子</p><p><img src="/pic/%E9%80%89%E5%8C%BA_020.png" alt="刚体变换矩阵"></p><blockquote><p>相似变换： 等距变换与一个均匀缩放的复合；等距变换+ 均匀缩放，类似相似三角形，比例不变</p></blockquote><p>相似变换比欧氏变换多了一个自由度，它允许物体进行均匀的放缩，其矩阵表示形式为：</p><p><img src="/pic/%E9%80%89%E5%8C%BA_021.png" alt="相似变换矩阵"></p><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">import</span> cv2<span class="token keyword">import</span> matplotlib<span class="token punctuation">.</span>pyplot <span class="token keyword">as</span> plt<span class="token keyword">import</span> numpy <span class="token keyword">as</span> npimg <span class="token operator">=</span> cv2<span class="token punctuation">.</span>imread<span class="token punctuation">(</span><span class="token string">'/hone/chy/pic/git.png'</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span>img <span class="token operator">=</span> cv2<span class="token punctuation">.</span>cvtColor<span class="token punctuation">(</span>img<span class="token punctuation">,</span> cv2<span class="token punctuation">.</span>COLOR_BGR2RGB<span class="token punctuation">)</span><span class="token comment"># print(img.shape)</span><span class="token comment"># 得到相似变换的矩阵  # center：旋转中心 angle：旋转角度   scale：缩放比例</span>M <span class="token operator">=</span> cv2<span class="token punctuation">.</span>getRotationMatrix2D<span class="token punctuation">(</span>center <span class="token operator">=</span> <span class="token punctuation">(</span>img<span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token operator">/</span><span class="token number">2</span><span class="token punctuation">,</span>img<span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token operator">/</span><span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">,</span>                              angle <span class="token operator">=</span> <span class="token number">30</span><span class="token punctuation">,</span>                              scale <span class="token operator">=</span> <span class="token number">0.5</span><span class="token punctuation">)</span> <span class="token comment"># 原图像按照相似矩阵进行相似变换  三个参数：原图像，相似矩阵，画布面积</span>img_rotate <span class="token operator">=</span> cv2<span class="token punctuation">.</span>warpAffine<span class="token punctuation">(</span>img<span class="token punctuation">,</span> M<span class="token punctuation">,</span> <span class="token punctuation">(</span>img<span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">,</span> img<span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">)</span>plt<span class="token punctuation">.</span>figure<span class="token punctuation">(</span>figsize<span class="token operator">=</span><span class="token punctuation">(</span><span class="token number">8</span><span class="token punctuation">,</span><span class="token number">8</span><span class="token punctuation">)</span><span class="token punctuation">)</span>plt<span class="token punctuation">.</span>subplot<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span><span class="token number">2</span><span class="token punctuation">,</span><span class="token number">1</span><span class="token punctuation">)</span>plt<span class="token punctuation">.</span>imshow<span class="token punctuation">(</span>img<span class="token punctuation">)</span>plt<span class="token punctuation">.</span>subplot<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span><span class="token number">2</span><span class="token punctuation">,</span><span class="token number">2</span><span class="token punctuation">)</span>plt<span class="token punctuation">.</span>imshow<span class="token punctuation">(</span>img_rotate<span class="token punctuation">)</span>plt<span class="token punctuation">.</span>show<span class="token punctuation">(</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p><img src="/pic/%E9%80%89%E5%8C%BA_019.png" alt="相似变换结果"></p><blockquote><p>仿射变换：非齐次坐标下的一个非奇异线性变换与一个平移变换的复合，（即第三行是0,0,1）; 旋转+平移+缩放+切变，保持平行性</p></blockquote><p>仿射变换只要求 A 是一个可逆矩阵，而不必是正交矩阵。仿射变换也叫正交投影。经过仿射变换之后，立方体就不再是方的了，但是各个面仍是平行四边形 </p><ul><li>性质：Parallel lines are still parallel lines（不再具有保角性，具有保平行性）</li><li>三个非共线的点对（6 parameters）确定一个仿射变换。</li></ul><p><img src="/pic/%E9%80%89%E5%8C%BA_022.png" alt="仿射变换矩阵"></p><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">import</span> cv2<span class="token keyword">import</span> matplotlib<span class="token punctuation">.</span>pyplot <span class="token keyword">as</span> plt<span class="token keyword">import</span> numpy <span class="token keyword">as</span> np<span class="token comment"># 3 Src(原始) Points + 3 Dst(目标) Points</span><span class="token comment"># cols：列/长  rows：行/宽</span>img <span class="token operator">=</span> cv2<span class="token punctuation">.</span>imread<span class="token punctuation">(</span><span class="token string">'lenna.jpg'</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span>img <span class="token operator">=</span> cv2<span class="token punctuation">.</span>cvtColor<span class="token punctuation">(</span>img<span class="token punctuation">,</span> cv2<span class="token punctuation">.</span>COLOR_BGR2RGB<span class="token punctuation">)</span><span class="token comment"># print(img.shape)</span>cols <span class="token operator">=</span> img<span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span>rows <span class="token operator">=</span> img<span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span>pt1 <span class="token operator">=</span> np<span class="token punctuation">.</span>float32<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">,</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token punctuation">[</span>cols<span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">,</span> rows<span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">)</span>pt2 <span class="token operator">=</span> np<span class="token punctuation">.</span>float32<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token punctuation">[</span>cols<span class="token operator">*</span><span class="token number">0.3</span><span class="token punctuation">,</span> rows<span class="token operator">*</span><span class="token number">0.3</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token punctuation">[</span>cols<span class="token operator">*</span><span class="token number">0.8</span><span class="token punctuation">,</span> rows<span class="token operator">*</span><span class="token number">0.2</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token punctuation">[</span>cols<span class="token operator">*</span><span class="token number">0.1</span><span class="token punctuation">,</span> rows<span class="token operator">*</span><span class="token number">0.9</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token comment"># [[0,0], [cols, 0], [0, rows]] --> [[cols*0.3, rows*0.3], [cols*0.8, rows*0.2], [cols*0.1, rows*0.9]]</span>M <span class="token operator">=</span> cv2<span class="token punctuation">.</span>getAffineTransform<span class="token punctuation">(</span>pt1<span class="token punctuation">,</span> pt2<span class="token punctuation">)</span>       <span class="token comment"># 仿射变换矩阵</span>dst <span class="token operator">=</span> cv2<span class="token punctuation">.</span>warpAffine<span class="token punctuation">(</span>img<span class="token punctuation">,</span> M<span class="token punctuation">,</span> <span class="token punctuation">(</span>cols<span class="token punctuation">,</span> rows<span class="token punctuation">)</span><span class="token punctuation">)</span>plt<span class="token punctuation">.</span>figure<span class="token punctuation">(</span>figsize<span class="token operator">=</span><span class="token punctuation">(</span><span class="token number">8</span><span class="token punctuation">,</span><span class="token number">8</span><span class="token punctuation">)</span><span class="token punctuation">)</span>plt<span class="token punctuation">.</span>subplot<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span><span class="token number">2</span><span class="token punctuation">,</span><span class="token number">1</span><span class="token punctuation">)</span>plt<span class="token punctuation">.</span>imshow<span class="token punctuation">(</span>img<span class="token punctuation">)</span>plt<span class="token punctuation">.</span>subplot<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span><span class="token number">2</span><span class="token punctuation">,</span><span class="token number">2</span><span class="token punctuation">)</span>plt<span class="token punctuation">.</span>imshow<span class="token punctuation">(</span>dst<span class="token punctuation">)</span>plt<span class="token punctuation">.</span>show<span class="token punctuation">(</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p><img src="/pic/%E9%80%89%E5%8C%BA_024.png" alt="仿射变换结果"></p><blockquote><p>射影变换:次坐标的一般非奇异线性变换 。射影变换可以分解为相似变换，仿射变换，射影变换的复合，不保留平行性，保留重合关系、长度的交比</p></blockquote><p><img src="/pic/%E9%80%89%E5%8C%BA_026.png" alt="射影变换矩阵"></p><p>它左上角为可逆矩阵 A，右上为平移 t，左下缩放 a^{T} 。由于采用齐坐标，当 v \neq 0 时吗我们可以对整个矩阵除于 v得到一个右下角为 1 的矩阵；否则，则得到右下角为 0 的矩阵。因此，2D 的射影变换一共有8个自由度，3D则共有15个自由度。</p><ul><li>性质：Lines are still lines（不保角，不保平行，保直线性）</li><li>四个非共线的点对（8 parameters）确定一个透视变换</li></ul><p><img src="/pic/%E9%80%89%E5%8C%BA_023.png" alt="射影变换矩阵"></p><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">import</span> cv2<span class="token keyword">import</span> matplotlib<span class="token punctuation">.</span>pyplot <span class="token keyword">as</span> plt<span class="token keyword">import</span> numpy <span class="token keyword">as</span> npimg <span class="token operator">=</span> cv2<span class="token punctuation">.</span>imread<span class="token punctuation">(</span><span class="token string">'lenna.jpg'</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span>img <span class="token operator">=</span> cv2<span class="token punctuation">.</span>cvtColor<span class="token punctuation">(</span>img<span class="token punctuation">,</span> cv2<span class="token punctuation">.</span>COLOR_BGR2RGB<span class="token punctuation">)</span>width <span class="token operator">=</span> img<span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span>height <span class="token operator">=</span> img<span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span>pts1 <span class="token operator">=</span> np<span class="token punctuation">.</span>float32<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">,</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token punctuation">[</span>width<span class="token punctuation">,</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">,</span>height<span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token punctuation">[</span>width<span class="token punctuation">,</span>height<span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">)</span>pts2 <span class="token operator">=</span> np<span class="token punctuation">.</span>float32<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token punctuation">[</span>width<span class="token operator">*</span><span class="token number">0.1</span><span class="token punctuation">,</span>height<span class="token operator">*</span><span class="token number">0.1</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token punctuation">[</span>width<span class="token operator">*</span><span class="token number">0.9</span><span class="token punctuation">,</span> width<span class="token operator">*</span><span class="token number">0.1</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token punctuation">[</span>height<span class="token operator">*</span><span class="token number">0.2</span><span class="token punctuation">,</span>height<span class="token operator">*</span><span class="token number">0.8</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token punctuation">[</span>width<span class="token operator">*</span><span class="token number">0.7</span><span class="token punctuation">,</span>height<span class="token operator">*</span><span class="token number">0.7</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">)</span>M_warp <span class="token operator">=</span> cv2<span class="token punctuation">.</span>getPerspectiveTransform<span class="token punctuation">(</span>pts1<span class="token punctuation">,</span> pts2<span class="token punctuation">)</span>     <span class="token comment"># 单应性矩阵</span>img_warp <span class="token operator">=</span> cv2<span class="token punctuation">.</span>warpPerspective<span class="token punctuation">(</span>img<span class="token punctuation">,</span> M_warp<span class="token punctuation">,</span> <span class="token punctuation">(</span>width<span class="token punctuation">,</span> height<span class="token punctuation">)</span><span class="token punctuation">)</span>plt<span class="token punctuation">.</span>figure<span class="token punctuation">(</span>figsize<span class="token operator">=</span><span class="token punctuation">(</span><span class="token number">8</span><span class="token punctuation">,</span><span class="token number">8</span><span class="token punctuation">)</span><span class="token punctuation">)</span>plt<span class="token punctuation">.</span>subplot<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span><span class="token number">2</span><span class="token punctuation">,</span><span class="token number">1</span><span class="token punctuation">)</span>plt<span class="token punctuation">.</span>imshow<span class="token punctuation">(</span>img<span class="token punctuation">)</span>plt<span class="token punctuation">.</span>subplot<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span><span class="token number">2</span><span class="token punctuation">,</span><span class="token number">2</span><span class="token punctuation">)</span>plt<span class="token punctuation">.</span>imshow<span class="token punctuation">(</span>img_warp<span class="token punctuation">)</span>plt<span class="token punctuation">.</span>show<span class="token punctuation">(</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p><img src="/pic/%E9%80%89%E5%8C%BA_025.png" alt="射影变换结果"></p>]]></content>
      
      
      <categories>
          
          <category> cs231A </category>
          
      </categories>
      
      
        <tags>
            
            <tag> transformer </tag>
            
            <tag> opencv </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>对极几何(Epipolar_Geometry)</title>
      <link href="/2023/03/27/epipolar-geometry/"/>
      <url>/2023/03/27/epipolar-geometry/</url>
      
        <content type="html"><![CDATA[<h1 id="对极几何基本用处"><a href="#对极几何基本用处" class="headerlink" title="对极几何基本用处"></a>对极几何基本用处</h1><h2 id="立体匹配"><a href="#立体匹配" class="headerlink" title="立体匹配"></a>立体匹配</h2><p>对于已知两视角空间位置关系的情况下，由于对极几何这个几何模型限定的约束条件，使得在立体图像对上搜索空间上的分别在两个图像中的位置只需要相应的对极线上找，把原来的二维搜搜问题，直接简化为一维搜索，双目测距就是这方面得应用之一。</p><h2 id="确定两个摄像点的相对位置与姿态问题"><a href="#确定两个摄像点的相对位置与姿态问题" class="headerlink" title="确定两个摄像点的相对位置与姿态问题"></a>确定两个摄像点的相对位置与姿态问题</h2><p>在未知视角位置的情况下，通过搜索图像对中的匹配点，可以求得两个位置和姿态得相对关系，这一点常用在机器人导航、地图得生成、三维重建等方面。</p><blockquote><p>基本概念</p></blockquote><ul><li>极点（Epipoles）：两个相机得基线与两个成像平面得交点</li><li>极线（Epipolar Lines）：空间中点在成像平面上的投影点与极点的连线</li><li>极平面（Epipolar Plane）：空间中的点与两个相机的光轴中心点所组成的平面</li></ul><blockquote><p>本质矩阵</p></blockquote><p><img src="/pic/duiji2.png"></p><blockquote><p>对极约束(E)</p></blockquote><p><img src="/pic/duiji.png"></p><blockquote><p>基础矩阵</p></blockquote><p><img src="/pic/duiji3.png"></p><blockquote><p>对极约束(F)</p></blockquote><p><img src="/pic/duiji4.png"></p>]]></content>
      
      
      <categories>
          
          <category> cs231A </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 对极几何 </tag>
            
            <tag> 对极约束 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Git 基础用法</title>
      <link href="/2023/03/26/git/"/>
      <url>/2023/03/26/git/</url>
      
        <content type="html"><![CDATA[<h1 id="Git介绍"><a href="#Git介绍" class="headerlink" title="Git介绍"></a>Git介绍</h1><p>Git是一个开源的<strong>分布式版本控制系统</strong>，可以有效、高速地处理从很小到非常大的项目版本管理。也是Linus Torvalds为了帮助管理Linux内核开发而开发的一个开放源码的版本控制软件。与常用的版本控制工具<strong>CVS, Subversion</strong>等不同，它采用了<strong>分布式版本库</strong>的方式，不必服务器端软件支持（wingeddevil注：这得分是用什么样的服务端，使用http协议或者git协议等不太一样。并且在push和pull的时候和服务器端还是有交互的。），使源代码的发布和交流极其方便。 Git 的速度很快，这对于诸如 Linux kernel 这样的大项目来说自然很重要。 Git 最为出色的是它的合并跟踪（merge tracing）能力。</p><p>Torvalds 开始着手开发 Git 是为了作为一种过渡方案来替代 BitKeeper</p><p><em>分布式相比于集中式的最大区别在于开发者可以提交到本地，每个开发者通过克隆（git clone），在本地机器上拷贝一个完整的Git仓库</em></p><h2 id="Git基本工作过程"><a href="#Git基本工作过程" class="headerlink" title="Git基本工作过程"></a>Git基本工作过程</h2><blockquote><p>9个常见操作，具体如下</p></blockquote><ul><li><p>1.新建项目文件夹（只做一次）</p></li><li><p>2.进入文件夹 (重要)</p></li><li><p>3.初始化仓库：git init（只做一次）</p></li><li><p>4.编码</p></li><li><p>5.添加文件信息： git add .</p></li><li><p>6.确认添加信息：git commit -m”描述信息”</p></li><li><p>7.查看详细日志信息：git log</p></li><li><p>8.查看简略日志信息：git log –oneline</p></li><li><p>9.版本回滚:git reset –hard 版本号</p></li></ul><img src="/pic/git.png"><h2 id="其他的常用操作"><a href="#其他的常用操作" class="headerlink" title="其他的常用操作"></a>其他的常用操作</h2><h3 id="git-init"><a href="#git-init" class="headerlink" title="git init"></a>git init</h3><blockquote><p>用法：git clone [url]</p></blockquote><p>该命令可用于通过指定的URL获取一个代码库。</p><h3 id="git-config"><a href="#git-config" class="headerlink" title="git config"></a>git config</h3><blockquote><p>用法：git config –global user.name “[name]”</p></blockquote><blockquote><p>用法：git config –global user.email “[email address]”</p></blockquote><p>该命令将分别设置提交代码的用户名和电子邮件地址。</p><h3 id="git-add"><a href="#git-add" class="headerlink" title="git add"></a>git add</h3><blockquote><p>用法：git add [file]</p></blockquote><p>该命令可以将一个文件添加至stage(暂存区)。</p><blockquote><p>用法：git add *</p></blockquote><p>该命令可以将多个文件添加至stage(暂存区)。</p><h3 id="git-commit"><a href="#git-commit" class="headerlink" title="git commit"></a>git commit</h3><blockquote><p>用法：git commit -m “[ Type in the commit message]”</p></blockquote><p>该命令可以在版本历史记录中永久记录文件。</p><blockquote><p>用法：git commit -a</p></blockquote><p>该命令将提交git add命令添加的所有文件，并提交git add命令之后更改的所有文件。 </p><h3 id="git-diff"><a href="#git-diff" class="headerlink" title="git diff"></a>git diff</h3><blockquote><p>用法：git diff</p></blockquote><p>该命令可以显示尚未添加到stage的文件的变更。</p><blockquote><p>用法：git diff –staged</p></blockquote><p>该命令可以显示添加到stage的文件与当前最新版本之间的差异。</p><blockquote><p>用法：git diff [first branch] [second branch]</p></blockquote><p>该命令可以显示两个分支之间的差异。</p><h3 id="git-reset"><a href="#git-reset" class="headerlink" title="git reset"></a>git reset</h3><blockquote><p>用法：git reset [file]</p></blockquote><p>该命令将从stage中撤出指定的文件，但可以保留文件的内容。</p><blockquote><p>用法：git reset [commit]</p></blockquote><p>该命令可以撤销指定提交之后的所有提交，并在本地保留变更。</p><h3 id="git-status"><a href="#git-status" class="headerlink" title="git status"></a>git status</h3><blockquote><p>用法：git status</p></blockquote><p>该命令将显示所有需要提交的文件。</p><h3 id="git-rm"><a href="#git-rm" class="headerlink" title="git rm"></a>git rm</h3><blockquote><p>用法：git rm [file]</p></blockquote><p>该命令将删除工作目录中的文件，并将删除动作添加到stage。</p><h3 id="git-log"><a href="#git-log" class="headerlink" title="git log"></a>git log</h3><blockquote><p>用法：git log</p></blockquote><p>该命令可用于显示当前分支的版本历史记录。</p><blockquote><p>用法：git log –follow[file]</p></blockquote><p>该命令可用于显示某个文件的版本历史记录，包括文件的重命名。</p><h3 id="git-show"><a href="#git-show" class="headerlink" title="git show"></a>git show</h3><blockquote><p>用法：git show [commit]</p></blockquote><p>该命令经显示指定提交的元数据以及内容变更。</p><h3 id="git-tag"><a href="#git-tag" class="headerlink" title="git tag"></a>git tag</h3><blockquote><p>用法：git tag [commitID]</p></blockquote><p>该命令可以给指定的提交添加标签。</p><h3 id="git-branch"><a href="#git-branch" class="headerlink" title="git branch"></a>git branch</h3><blockquote><p>用法：git branch</p></blockquote><p>该命令将显示当前代码库中所有的本地分支。</p><blockquote><p>用法：git branch [branch name]</p></blockquote><p>该命令将创建一个分支。</p><blockquote><p>用法：git branch -d [branch name]</p></blockquote><p>该命令将删除指定的分支。</p><h3 id="git-checkout"><a href="#git-checkout" class="headerlink" title="git checkout"></a>git checkout</h3><blockquote><p>用法：git checkout [branch name]</p></blockquote><p>你可以通过该命令切换分支。</p><blockquote><p>用法：git checkout -b [branch name]</p></blockquote><p>你可以通过该命令创建一个分支，并切换到新分支上。</p><h3 id="git-merge"><a href="#git-merge" class="headerlink" title="git merge"></a>git merge</h3><blockquote><p>用法：git merge [branch name]</p></blockquote><p>该命令可以将指定分支的历史记录合并到当前分支。</p><h3 id="git-remote"><a href="#git-remote" class="headerlink" title="git remote"></a>git remote</h3><blockquote><p>用法：git remote add [variable name] [Remote Server Link]</p></blockquote><p>你可以通过该命令将本地的代码库连接到远程服务器。</p><h3 id="git-push"><a href="#git-push" class="headerlink" title="git push"></a>git push</h3><blockquote><p>用法：git push [variable name] master</p></blockquote><p>该命令可以将主分支上提交的变更发送到远程代码库。</p><blockquote><p>用法：git push [variable name] [branch]</p></blockquote><p>该命令可以将指定分支上的提交发送到远程代码库。</p><blockquote><p>用法：git push –all [variable name]</p></blockquote><p>该命令可以将所有分支发送到远程代码库。</p><blockquote><p>用法：git pull [Repository Link]</p></blockquote><p>该命令将获取远程服务器上的变更，并合并到你的工作目录。</p><h3 id="git-stash"><a href="#git-stash" class="headerlink" title="git stash"></a>git stash</h3><blockquote><p>用法：git stash save</p></blockquote><p>该命令将临时保存所有修改的文件。</p><blockquote><p>用法：git stash pop</p></blockquote><p>该命令将恢复最近一次stash（储藏）的文件。</p><blockquote><p>用法：git stash list</p></blockquote><p>该命令将显示stash的所有变更。</p><blockquote><p>用法：git stash drop</p></blockquote><p>该命令将丢弃最近一次stash的变更。</p><h2 id="Git小游戏"><a href="#Git小游戏" class="headerlink" title="Git小游戏"></a>Git小游戏</h2><p>游戏链接为<a href="https://oschina.gitee.io/learn-git-branching/">GIt小游戏</a></p><p>推荐一个图解答案<a href="https://blog.csdn.net/GDUT_xin/article/details/125537967">答案</a></p><pre class="line-numbers language-none"><code class="language-none">---基础篇1.git commitgit commitgit commit2.git branchgit branch bugFixgit checkout bugFix3.git mergegit branch bugFixgit checkout bugFixgit commit -m “commit bugFix”git checkout maingit commit -m “commit main”git merge bugFix4.git rebasegit branch bugFixgit checkout bugFixgit commit -m “bugFix”git checkout maingit commit -m “main commit”git checkout bugFixgit rebase main高级篇1.分离HEADgit checkout c42.相对引用(^)git checkout main^git checkout c33.相对引用2(~)git branch -f main c6git checkout HEAD~1git branch -f bugFix HEAD~14.撤销变更git reset HEAD~1git chekout pushedgit revert HEAD移动提交记录1.git cherry-pickgit cherry-pick c3 c4 c72.交互式rebasegit rebase -i overHere （打开控制面板）omit c2 （点击c2）c4 c5交换位置 （拉取）杂项1.只取一个提交记录git checkout maingit cherry-pick c42.提交的技巧#1git rebase -i HEAD~2 交换2和3的位置git commit --amendgit rebase -i HEAD~2 恢复2和3的位置git checkout miangit rebase caption main3.提交的技巧#2git checkout maingit cherry-pick c2git commit --amendgit cherry-pick c34.git taggit tag v0 c1git tag v1 c2git checkout c25. git descridegit commit高级话题1.多次rebasegit rebase main bugFixgit rebase bugFix sidegit rebase side anothergit rebase another main2.两个父节点git branch bugWork main^ ^ 2^3.纠缠不清的分支git checkout onegit cherry-pick c4 c3 c2git checkout twogit cherry-pick c5 c4 c3 c2git branch -f three c2Push &amp; Pull —— Git 远程仓库！1.git clonegit clone2.远程分支git commitgit checkout o&#x2F;maingit commit3.git fetchgit fetch4.git pullgit pull5.模拟团队合作git clonegit fakeTeamwork 2git commitgit pull6.git pushgit commitgit commitgit push7.偏离的提及历史git clonegit fakeTeamworkgit commitgit pull --rebasegit push8.锁定的main（locked main）git reset --hard o&#x2F;maingit checkout -b feature c2git push origin feature关于 origin 和它的周边 —— Git 远程仓库高级操作1.推送主分支git fetchgit rebase o&#x2F;main side1git rebase side1 side2git rebase side2 side3git rebase side3 maingit push2.合并远程仓库git checkout maingit pullgit merge side1git merge side2git merge side3git push3.远程追踪git checkout -b side o&#x2F;maingit commitgit pull --rebasegit push4.git push的参数git push origin maingit push origin foo5.git push的参数2git push origin main^:foogit push origin foo:main6.git fetch的参数git fetch origin main~1:foogit fetch origin foo:maingit checkout foogit merge main7.没有source的sourcegit fetch origin :bargit push origin :foo8.git pull的参数git pull origin bar:foogit pull origin main:side---<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>]]></content>
      
      
      <categories>
          
          <category> git </category>
          
      </categories>
      
      
        <tags>
            
            <tag> git基础 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>你好</title>
      <link href="/2023/03/25/chy/"/>
      <url>/2023/03/25/chy/</url>
      
        <content type="html"><![CDATA[<h1 id="hexo-theme-matery"><a href="#hexo-theme-matery" class="headerlink" title="hexo-theme-matery"></a>hexo-theme-matery</h1><p><a href="http://hits.dwyl.io/blinkfox/hexo-theme-matery"><img src="http://hits.dwyl.io/blinkfox/hexo-theme-matery.svg" alt="HitCount"></a> <a href="https://gitter.im/hexo-theme-matery/Lobby?utm_source=badge"><img src="https://img.shields.io/gitter/room/blinkfox/hexo-theme-matery.svg" alt="Gitter"></a> <a href="https://github.com/blinkfox/hexo-theme-matery/issues"><img src="https://img.shields.io/github/issues/blinkfox/hexo-theme-matery.svg" alt="GitHub issues"></a> <a href="https://github.com/blinkfox/hexo-theme-matery/blob/master/LICENSE"><img src="https://img.shields.io/github/license/blinkfox/hexo-theme-matery.svg" alt="GitHub license"></a> <a href="https://codeload.github.com/blinkfox/hexo-theme-matery/zip/master"><img src="https://img.shields.io/badge/downloads-master-green.svg" alt="Download"></a> <a href="http://hexo.io/"><img src="https://img.shields.io/badge/hexo-%3E%3D%205.0.0-blue.svg" alt="Hexo Version"></a> <a href="https://github.com/blinkfox/hexo-theme-matery/network"><img src="https://img.shields.io/github/forks/blinkfox/hexo-theme-matery.svg" alt="GitHub forks"></a> <a href="https://github.com/blinkfox/hexo-theme-matery/stargazers"><img src="https://img.shields.io/github/stars/blinkfox/hexo-theme-matery.svg" alt="GitHub stars"></a></p><p><a href="README.md">🇺🇸English Document</a> | <a href="http://blinkfox.com/">国内访问示例 (http://blinkfox.com)</a> | <a href="https://blinkfox.github.io/">Github 部署演示示例 (https://blinkfox.github.io)</a> </p><p>QQ 交流群1（已满）: <a href="https://jq.qq.com/?_wv=1027&k=5zMDYHT"><code>926552981</code></a> | QQ 交流群2（已满）: <a href="https://jq.qq.com/?_wv=1027&k=53q2Ayp"><code>971887688</code></a> | QQ 交流群3（推荐）: <a href="https://qm.qq.com/cgi-bin/qm/qr?k=fC1-kU-_aTn4q-JQq4GsYKr4WcKdgfGa&jump_from=webapi"><code>670694035</code></a></p><blockquote><p>这是一个采用 <code>Material Design</code> 和响应式设计的 Hexo 博客主题。</p></blockquote><h2 id="特性"><a href="#特性" class="headerlink" title="特性"></a>特性</h2><ul><li>简单漂亮，文章内容美观易读</li><li><a href="https://material.io/">Material Design</a> 设计</li><li>响应式设计，博客在桌面端、平板、手机等设备上均能很好的展现</li><li>首页轮播文章及每天动态切换 <code>Banner</code> 图片</li><li>瀑布流式的博客文章列表（文章无特色图片时会有 <code>24</code> 张漂亮的图片代替）</li><li>时间轴式的归档页</li><li><strong>词云</strong>的标签页和<strong>雷达图</strong>的分类页</li><li>丰富的关于我页面（包括关于我、文章统计图、我的项目、我的技能、相册等）</li><li>可自定义的数据的友情链接页面</li><li>支持文章置顶和文章打赏</li><li>支持 <code>MathJax</code></li><li>支持中文繁简转换</li><li><code>TOC</code> 目录</li><li>可设置复制文章内容时追加版权信息</li><li>可设置阅读文章时做密码验证</li><li><a href="https://gitalk.github.io/">Gitalk</a>、<a href="https://imsun.github.io/gitment/">Gitment</a>、<a href="https://valine.js.org/">Valine</a> 和 <a href="https://disqus.com/">Disqus</a> 评论模块（推荐使用 <code>Gitalk</code>）</li><li>集成了<a href="http://busuanzi.ibruce.info/">不蒜子统计</a>、谷歌分析（<code>Google Analytics</code>）和文章字数统计等功能</li><li>支持在首页的音乐播放和视频播放功能</li><li>支持<code>emoji</code>表情，用<code>markdown emoji</code>语法书写直接生成对应的能<strong>跳跃</strong>的表情。</li><li>支持 <a href="http://www.daovoice.io/">DaoVoice</a>、<a href="https://www.tidio.com/">Tidio</a> 在线聊天功能。</li></ul><table><thead><tr><th>表头</th><th>表头</th></tr></thead><tbody><tr><td>单元格</td><td>单元格</td></tr><tr><td>单元格</td><td>单元格</td></tr></tbody></table><table><thead><tr><th align="left">左对齐</th><th align="right">右对齐</th><th align="center">居中对齐</th></tr></thead><tbody><tr><td align="left">单元格</td><td align="right">单元格</td><td align="center">单元格</td></tr><tr><td align="left">单元格</td><td align="right">单元格</td><td align="center">单元格</td></tr></tbody></table><h2 id="贡献者"><a href="#贡献者" class="headerlink" title="贡献者"></a>贡献者</h2><p>感谢下面列出的贡献者，没有他们，hexo-theme-matery 不会这么完美。</p><ul><li><a href="https://github.com/HarborZeng">@HarborZeng</a></li><li><a href="https://github.com/shw2018">@shw2018</a></li><li><a href="https://github.com/L1cardo">@L1cardo</a></li><li><a href="https://github.com/Five-great">@Five-great</a></li></ul><h2 id="下载"><a href="#下载" class="headerlink" title="下载"></a>下载</h2><p>本主题<strong>推荐你使用 Hexo 5.0.0 及以上的版本</strong>。如果，你已经有一个自己的 <a href="https://hexo.io/zh-cn/">Hexo</a> 博客了，建议你将 Hexo 升级到最新稳定的版本。</p><p>点击 <a href="https://codeload.github.com/blinkfox/hexo-theme-matery/zip/master">这里</a> 下载 <code>master</code> 分支的最新稳定版的代码，解压缩后，将 <code>hexo-theme-matery</code> 的文件夹复制到你 Hexo 的 <code>themes</code> 文件夹中即可。</p><p>当然你也可以在你的 <code>themes</code> 文件夹下使用 <code>git clone</code> 命令来下载:</p><pre class="line-numbers language-bash" data-language="bash"><code class="language-bash"><span class="token function">git</span> clone https://github.com/blinkfox/hexo-theme-matery.git<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><h2 id="配置"><a href="#配置" class="headerlink" title="配置"></a>配置</h2><h3 id="切换主题"><a href="#切换主题" class="headerlink" title="切换主题"></a>切换主题</h3><p>修改 Hexo 根目录下的 <code>_config.yml</code> 的  <code>theme</code> 的值：<code>theme: hexo-theme-matery</code></p><h4 id="config-yml-文件的其它修改建议"><a href="#config-yml-文件的其它修改建议" class="headerlink" title="_config.yml 文件的其它修改建议:"></a><code>_config.yml</code> 文件的其它修改建议:</h4><ul><li>请修改 <code>_config.yml</code> 的 <code>url</code> 的值为你的网站主 <code>URL</code>（如：<code>http://xxx.github.io</code>）。</li><li>建议修改两个 <code>per_page</code> 的分页条数值为 <code>6</code> 的倍数，如：<code>12</code>、<code>18</code> 等，这样文章列表在各个屏幕下都能较好的显示。</li><li>如果你是中文用户，则建议修改 <code>language</code> 的值为 <code>zh-CN</code>。</li></ul><h3 id="新建分类-categories-页"><a href="#新建分类-categories-页" class="headerlink" title="新建分类 categories 页"></a>新建分类 categories 页</h3><p><code>categories</code> 页是用来展示所有分类的页面，如果在你的博客 <code>source</code> 目录下还没有 <code>categories/index.md</code> 文件，那么你就需要新建一个，命令如下：</p><pre class="line-numbers language-bash" data-language="bash"><code class="language-bash">hexo new page <span class="token string">"categories"</span><span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>编辑你刚刚新建的页面文件 <code>/source/categories/index.md</code>，至少需要以下内容：</p><pre class="line-numbers language-yaml" data-language="yaml"><code class="language-yaml"><span class="token punctuation">---</span><span class="token key atrule">title</span><span class="token punctuation">:</span> categories<span class="token key atrule">date</span><span class="token punctuation">:</span> <span class="token datetime number">2018-09-30 17:25:30</span><span class="token key atrule">type</span><span class="token punctuation">:</span> <span class="token string">"categories"</span><span class="token key atrule">layout</span><span class="token punctuation">:</span> <span class="token string">"categories"</span><span class="token punctuation">---</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title></title>
      <link href="/2023/03/23/hello-world/"/>
      <url>/2023/03/23/hello-world/</url>
      
        <content type="html"><![CDATA[<p>Welcome to <a href="https://hexo.io/">Hexo</a>! This is your very first post. Check <a href="https://hexo.io/docs/">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href="https://hexo.io/docs/troubleshooting.html">troubleshooting</a> or you can ask me on <a href="https://github.com/hexojs/hexo/issues">GitHub</a>.</p><h2 id="Quick-Start"><a href="#Quick-Start" class="headerlink" title="Quick Start"></a>Quick Start</h2><h3 id="Create-a-new-post"><a href="#Create-a-new-post" class="headerlink" title="Create a new post"></a>Create a new post</h3><pre class="line-numbers language-bash" data-language="bash"><code class="language-bash">$ hexo new <span class="token string">"My New Post"</span><span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>More info: <a href="https://hexo.io/docs/writing.html">Writing</a></p><h3 id="Run-server"><a href="#Run-server" class="headerlink" title="Run server"></a>Run server</h3><pre class="line-numbers language-bash" data-language="bash"><code class="language-bash">$ hexo server<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>More info: <a href="https://hexo.io/docs/server.html">Server</a></p><h3 id="Generate-static-files"><a href="#Generate-static-files" class="headerlink" title="Generate static files"></a>Generate static files</h3><pre class="line-numbers language-bash" data-language="bash"><code class="language-bash">$ hexo generate<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>More info: <a href="https://hexo.io/docs/generating.html">Generating</a></p><h3 id="Deploy-to-remote-sites"><a href="#Deploy-to-remote-sites" class="headerlink" title="Deploy to remote sites"></a>Deploy to remote sites</h3><pre class="line-numbers language-bash" data-language="bash"><code class="language-bash">$ hexo deploy<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>More info: <a href="https://hexo.io/docs/one-command-deployment.html">Deployment</a></p>]]></content>
      
      
      
    </entry>
    
    
  
  
</search>
