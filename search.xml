<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>语义场景补全（SSC）代码篇（二）</title>
      <link href="/2023/11/05/code-ssc2/"/>
      <url>/2023/11/05/code-ssc2/</url>
      
        <content type="html"><![CDATA[<h1 id="主要记录SSC中的理论与代码实现（第二章）"><a href="#主要记录SSC中的理论与代码实现（第二章）" class="headerlink" title="主要记录SSC中的理论与代码实现（第二章）"></a>主要记录SSC中的理论与代码实现（第二章）</h1><p>主要从开篇之作SSCNet（2017）开始讲起，然后按激光雷达点云输入（LMSCNet，2020；S3CNet，2021a；JS3C-Net，2021）,单双目以及两者融合的顺序</p><p>紧接上回，双目视觉的解决方案</p><h2 id="OccDepth"><a href="#OccDepth" class="headerlink" title="OccDepth"></a>OccDepth</h2><p>第一个只输入视觉的立体3D SSC方法。为了有效地利用深度信息，提出了一种立体软特征分配（Stereo-SFA）模块，通过隐式学习立体图像之间的相关性来更好地融合3D深度感知特征。占用感知深度（OAD）模块将知识提取与预先训练的深度模型明确地用于生成感知深度的3D特征。此外，为了更有效地验证立体输入场景，我们提出了基于TartanAir的SemanticTartanAir数据集。这是一个虚拟场景，可以使用真实的地面实况来更好地评估该方法的有效性。</p><p>代码链接如下: <a href="https://github.com/megvii-research/OccDepth">OccDepth: A Depth-aware Method for 3D Semantic Occupancy Network</a>, arXiv 2023.</p><p><img src="/pic/OccDepth.png"><br>3D SSC是通过桥接立体SFA模块来从立体图像中推断出来的，该模块用于将特征提升到3D空间，OAD模块用于增强深度预测，3D U-Net用于提取几何结构和语义。立体深度网络仅在训练中用于提供深度监督。</p><h3 id="代码介绍"><a href="#代码介绍" class="headerlink" title="代码介绍"></a>代码介绍</h3><p>代码方面类似monoscene，都使用了使用PyTorch Lightning，训练代码<a href="https://github.com/megvii-research/OccDepth/blob/main/occdepth/scripts/train.py">train.py</a>介绍如下：</p><ol><li><p><strong>导入模块</strong>: 脚本以各种导入语句开始，包括数据模块、模型、实用程序和配置。</p></li><li><p><strong>Hydra配置</strong>: <code>main</code> 函数被 <code>@hydra.main</code> 装饰，这表明可以使用Hydra进行配置，Hydra是一个强大的配置管理工具。配置路径根据 <code>config_path</code> 环境变量确定。</p></li><li><p><strong>主要函数</strong>: <code>main</code> 函数是脚本的入口点。它加载使用Hydra指定的配置。</p></li><li><p><strong>实验名称</strong>: 基于各种配置设置，创建了 <code>exp_name</code> 变量，用于命名实验。</p></li><li><p><strong>数据模块设置</strong>: 根据选择的数据集（例如 “kitti”、”NYU” 或 “tartanair”），实例化了相应的数据模块。数据模块处理数据加载和预处理。</p></li><li><p><strong>模型初始化</strong>: 创建了“OccDepth”模型的实例。模型接受各种参数，包括类别名称、类别权重和配置设置。</p></li><li><p><strong>日志记录和回调</strong>: 根据配置，脚本设置了使用TensorBoard的日志记录，并定义了各种回调，包括模型检查点和学习率监视器。</p></li><li><p><strong>从上次继续或从头开始训练</strong>: 脚本检查模型检查点文件（例如 “last.ckpt”）是否存在。如果存在，则继续从那个点训练。否则，从头开始训练模型。</p></li><li><p><strong>训练</strong>: 脚本初始化PyTorch Lightning Trainer，并调用 <code>fit</code> 方法，使用指定的数据模块来训练模型。训练过程由各种配置设置控制，包括GPU数量、梯度裁剪和其他超参数。</p></li><li><p><strong>随机种子初始化</strong>: 使用 <code>seed_everything</code> 设置随机种子。</p></li><li><p><strong>执行</strong>: 当运行脚本时，将调用主函数。</p></li></ol><p>总之，该脚本旨在训练”OccDepth”模型，可用于不同的数据集，并允许使用Hydra进行灵活的配置。它设置数据加载、模型和训练流程，并且如果有可用的检查点，可以从那个点继续训练。训练结果和日志将保存在配置中指定的目录中。</p><p><strong>接着看一下<a href="https://github.com/megvii-research/OccDepth/blob/main/occdepth/models/OccDepth.py">OccDepth.py</a>的模型</strong></p><p>“OccDepth” 是一个PyTorch模型，用于语义分割和深度预测的任务。这个模型主要由以下组件构成：</p><ol><li><p>**构造函数 <code>__init__</code>**：构造函数初始化模型的各种参数和组件。以下是一些重要的参数和配置项：</p><ul><li><code>class_names</code>：类别名称列表，用于分类问题。</li><li><code>class_weights</code>：类别权重，用于处理类别不平衡的情况。</li><li><code>class_weights_occ</code>：用于处理类别不平衡的另一组类别权重。</li><li><code>full_scene_size</code>：场景的完整尺寸。</li><li><code>project_res</code>：2D特征到3D特征的投影分辨率。</li><li><code>config</code>：模型的配置参数，包括超参数等。</li><li><code>infer_mode</code>：是否为推理模式，如果是，则不使用上下文先验（context prior）。</li></ul></li><li><p><strong>特征提取器</strong>：使用UNet结构进行特征提取。这些提取到的特征被用于语义分割。</p></li><li><p><strong>2D-3D投影层</strong>：这部分用于将2D特征映射到3D特征。包括两种投影方法：”flosp” 和 “flosp_depth”。</p><ul><li>“flosp” 模型对2D特征进行投影以生成3D场景特征。</li><li>“flosp_depth” 模型不仅对2D特征进行投影，还用深度信息进行额外的处理。</li></ul></li><li><p><strong>3D语义分割头部</strong>：这一部分负责将3D特征用于语义分割。具体的网络结构可能取决于不同的数据集（”NYU” 或 “kitti”）。</p></li><li><p><strong>2D语义分割头部</strong>：用于2D语义分割的头部网络。</p></li><li><p><strong>深度预测头部</strong>：根据需求，模型可以生成深度预测。</p></li><li><p><strong>训练、验证和测试步骤</strong>：这些步骤在不同的数据集和任务上运行，计算损失、评估性能，并记录指标。这包括分类损失、语义分割IoU、精度和召回。</p></li><li><p><strong>优化器和学习率调度器</strong>：在 <code>configure_optimizers</code> 方法中配置了优化器和学习率调度器。通常使用AdamW优化器和学习率衰减策略。</p></li><li><p><strong>模型的输入数据预处理</strong>：输入数据是图像，通过 <code>process_rgbs</code> 方法处理，生成特征图。</p></li><li><p><strong>模型的前向传播</strong>：通过 <code>forward</code> 方法执行模型的前向传播操作，包括特征提取、2D-3D投影、3D语义分割等。</p></li><li><p><strong>损失计算</strong>：根据任务类型和模型预测，计算不同的损失，包括分类损失、语义分割损失、深度损失等。</p></li><li><p><strong>评估指标</strong>：评估模型性能，包括IoU、精度、召回等。</p></li><li><p><strong>超参数配置</strong>：模型的超参数（如学习率、权重衰减等）在构造函数中进行了设置。</p></li><li><p><strong>导出模型和计算FLOPs和参数数量</strong>：通过条件选择，模型可以导出为ONNX格式，并计算模型的浮点运算数（FLOPs）和参数数量。</p></li></ol><p>总体而言，”OccDepth” 模型用于处理多视图数据的语义分割和深度预测任务，具有丰富的配置选项和评估指标，以满足不同数据集和任务的需求。这个模型的复杂性和功能强大，适用于一系列3D场景理解任务。</p><p><strong><a href="https://github.com/megvii-research/OccDepth/blob/main/occdepth/models/SFA.py">SFA.py</a>用于执行2D到3D的稀疏特征聚合。以下是该模块的功能和工作原理的简要说明：</strong></p><ul><li><p><code>SFA</code> 类继承自<code>nn.Module</code>，它包含了一个用于2D到3D投影的聚合过程。</p></li><li><p><code>__init__</code> 函数接受以下参数：</p><ul><li><code>scene_size</code>：3D场景的大小，通常表示为一个包含三个维度大小的元组或列表。</li><li><code>dataset</code>：表示使用的数据集的名称，例如”NYU”或”kitti”。</li><li><code>project_scale</code>：投影尺度，用于将2D特征映射到3D场景中。</li></ul></li><li><p><code>forward</code> 函数执行2D到3D的稀疏特征聚合操作：</p><ul><li>输入 <code>x2d</code> 是包含多个视图的2D特征的张量。</li><li><code>projected_pix</code> 是每个像素在3D场景中的投影坐标。</li><li><code>fov_mask</code> 是表示视野范围的掩码。</li></ul><p>  主要的工作步骤包括：</p><ol><li>对于每个视图，将2D特征映射到3D场景中。这是通过计算权重和聚合来实现的。</li><li>对不同视图之间的特征进行加权平均。</li><li>根据数据集类型（”NYU”或”kitti”）重塑3D场景特征的形状。</li></ol></li></ul><p>这个模块的关键思想是将多个2D视图的特征信息聚合到3D场景中，以增强3D场景的表示。这对于处理多视图或多帧输入的问题（如3D物体检测或重建）非常有用。模块的实现中包含了一系列的数学运算，包括加权平均和角度余弦相似度等。这有助于捕捉不同视图之间的相关性和信息融合。</p><p><a href="https://github.com/megvii-research/OccDepth/blob/main/occdepth/models/flosp_depth/flosp_depth.py">flosp_depth.py</a></p><p>这是一个名为 <code>FlospDepth</code> 的PyTorch模块，通常用于深度预测和点云处理。以下是该模块的功能和工作原理的简要说明：</p><ol><li><p>初始化和配置：</p><ul><li><code>__init__</code> 函数接受多个参数，包括场景边界、深度范围、输出通道数等，用于初始化模型的各个部分和配置。</li><li>可以选择不同的深度网络（<code>DepthNet</code>）配置。</li></ul></li><li><p>深度网络：</p><ul><li><code>DepthNet</code> 是一个用于预测深度图的子模块。</li><li>它接受输入特征图，相机内参矩阵等信息，并返回深度图。</li><li>深度图是在该模块中预测的，并且经过 softmax 处理。</li></ul></li><li><p>Voxel 特征聚合：</p><ul><li>从不同视角的深度图生成体素特征，这是通过采样视锥体积并将视锥体积中的深度信息投影到3D体素网格中实现的。</li><li>选择了不同的体素聚合方式，包括”mean”和”sum”，以聚合多个视角的信息。</li></ul></li><li><p>配置和参数：</p><ul><li>一些初始化参数，如体素大小、坐标、数量等，被存储在模块的缓冲区中，以便后续使用。</li><li>还有一些配置参数，如深度范围和体素网格大小等。</li></ul></li><li><p>推理模式和训练模式：</p><ul><li>模块支持两种模式，即推理模式和训练模式。</li><li>在推理模式中，模块可以接受缩放后的像素大小（<code>scaled_pixel_size</code>），以适应不同的尺度。</li><li>在训练模式中，模块接受相机内参矩阵、相机到世界坐标的变换矩阵等信息，用于生成体素网格。</li></ul></li></ol><p>这个模块的关键思想是从多个视角的深度图生成3D体素网格，然后通过合适的聚合方式将这些体素特征合并在一起。这对于许多3D场景理解任务，如点云分割、物体检测和语义分割，都是有用的。</p><h2 id="VoxFormer"><a href="#VoxFormer" class="headerlink" title="VoxFormer"></a>VoxFormer</h2><p>代码链接如下：<a href="https://github.com/NVlabs/VoxFormer">VoxFormer: a Cutting-edge Baseline for 3D Semantic Occupancy Prediction</a>, CVPR 2023</p><p>一种基于 Transformer 的语义场景完成框架，可以<strong>仅从 2D 图像输出完整的 3D 体积语义</strong>。我们的框架采用两阶段设计，从深度估计中一组稀疏的可见和占用体素查询开始，然后是从稀疏体素生成密集 3D 体素的致密化阶段。这种设计的一个关键思想是，2D 图像上的视觉特征仅对应于可见的场景结构，而不对应于被遮挡或空白的空间。因此，从可见结构的特征化和预测开始更为可靠。一旦我们获得了稀疏查询集，我们就应用屏蔽自动编码器设计，通过自注意力将信息传播到所有体素。</p><p><img src="/pic/VoxFormer.png"></p><p>VoxFormer的总体框架如上图所示。给定 RGB 图像，由 ResNet50 提取 2D 特征，并由现成的深度预测器估计深度。校正后的估计深度启用了与类别无关的查询建议阶段：将选择位于占用位置的查询来与图像特征进行可变形交叉注意。之后，将添加掩模标记以通过可变形自注意力来完成体素特征。精炼后的体素特征将被上采样并投影到输出空间以进行每体素语义分割。请注意，我们的框架支持单个或多个图像的输入。</p><p>VoxFormer由类不可知的查询建议（阶段-1）和类特定的语义分割（阶段-2）组成，其中阶段-1提出了一组稀疏的占用体素，阶段-2完成了从阶段-1给出的建议开始的场景表示。具体而言，阶段1具有轻量级的基于2D CNN的查询建议网络，该网络使用图像深度来重建场景几何结构。然后，它从整个视场上预定义的可学习体素查询中提出了一组稀疏的体素。</p><h3 id="代码分析"><a href="#代码分析" class="headerlink" title="代码分析"></a>代码分析</h3><p>第一阶段的查询建议网络(QPN)，根据深度决定要查询哪些体素:被占用的体素值得仔细关注，而空的体素可以从组中分离出来。给出二维RGB观测，首先基于深度估计得到场景的2.5D表示。然后，通过占用率预测获得三维查询位置，从而纠正图像深度不准确的问题。</p><p>代码<a href="https://github.com/NVlabs/VoxFormer/blob/main/projects/configs/voxformer/qpn.py">qpn.py</a>，基于mmdet3d</p><p>以下是对配置文件的更详细分析：</p><ol><li><p><code>_gamma_</code> 和 <code>_alpha_</code>：这些参数可能是用于模型的超参数调整，但配置文件中没有提供它们的具体用途。通常，<code>_gamma_</code> 和 <code>_alpha_</code> 可能会在训练过程中被动态地调整以提高模型性能。</p></li><li><p><code>_nsweep_</code>：指定了数据集中采样的扫描次数，这可能与数据集的采样策略和数据增强有关。在一些场景下，多次扫描可以提供更多的信息，例如在激光雷达数据中。</p></li><li><p><code>_depthmodel_</code>：这个参数指定了深度模型的名称或配置，定义了一个名为 MSNet3D 的三维立体匹配网络模型。这个模型主要用于深度估计和立体匹配任务。</p></li><li><p><code>point_cloud_range</code> 和 <code>voxel_size</code>：定义了点云的范围和体素的大小。这些参数用于将点云数据转化为体素表示，以便于模型的处理。<code>point_cloud_range</code> 指定了点云数据的范围，<code>voxel_size</code> 指定了体素的大小。</p></li><li><p><code>img_norm_cfg</code>：定义了图像的归一化配置，包括均值、标准差和是否将图像转化为RGB格式。这些配置通常用于对图像进行预处理，以便输入到深度学习模型中。</p></li><li><p><code>class_names</code>：包含目标类别的列表，用于指定模型需要检测或分类的物体类别。在这个配置中，包含了诸如’car’、’truck’、’pedestrian’等物体类别。</p></li><li><p><code>input_modality</code>：定义了数据输入模态，包括激光雷达、摄像机、雷达、地图和外部信息。这些模态可以用于训练和测试模型，根据任务需要选择合适的输入模态。</p></li><li><p><code>model</code>：指定了要训练的深度学习模型的配置。这个模型被命名为 <code>LMSCNet_SS</code>，并包括类别数、输入维度等信息。</p></li><li><p><code>train_cfg</code>：包含了训练配置，如点云的网格大小、体素大小、分配器配置等。这些配置参数影响了训练过程中的数据处理和损失计算。</p></li><li><p><code>dataset_type</code> 和 <code>data_root</code>：定义了数据集的类型和根目录，这将用于加载训练、验证和测试数据。</p></li><li><p><code>test_pipeline</code>：定义了测试数据的预处理管道，这里加载了多视角图像。预处理管道用于对输入数据进行处理以供模型使用。</p></li><li><p><code>data</code>：包含了数据集的设置，包括训练、验证和测试数据的类型、数据根目录、预处理根目录等。数据采样器也在这里配置。</p></li><li><p><code>optimizer</code> 和 <code>optimizer_config</code>：定义了优化器的类型、学习率、权重衰减等参数。<code>AdamW</code> 优化器用于模型的权重更新。</p></li><li><p><code>lr_config</code>：指定了学习率的调整策略，包括余弦退火、预热等。这些策略用于在训练过程中调整学习率。</p></li><li><p><code>total_epochs</code>：指定了总的训练轮数，模型将在这些轮数内进行训练。</p></li><li><p><code>evaluation</code>：定义了评估的间隔和评估的管道，用于在训练过程中定期评估模型的性能。</p></li><li><p><code>runner</code>：指定了训练器的类型和最大训练轮数，以及其他训练参数。</p></li><li><p><code>log_config</code>：包含了日志的配置，包括日志的记录间隔和日志类型。这些日志用于跟踪训练过程中的性能。</p></li><li><p><code>checkpoint_config</code>：定义了模型检查点的保存间隔，即模型在训练过程中的保存频率。</p></li></ol><p>分析一下_depthmodel_指定的MSNet3D和model指定的LMSCNet_SS模型</p><p>首先逐步分析代码<code>MSNet3D</code>的关键部分：</p><ol><li><p><code>hourglass3D</code> 类：这是一个用于定义三维卷积层的类，由多个 MobileV2_Residual_3D 模块组成。MobileV2_Residual_3D 模块用于构建深度神经网络的基本构建块。这个类定义了前向传播方法，通过堆叠卷积和反卷积层来构建一个”U”形网络。</p></li><li><p><code>MSNet3D</code> 类：这是主要的网络模型，包含了特征提取、特征匹配、立体匹配和深度估计等部分。它包括以下关键组件：</p><ul><li><code>feature_extraction</code>：特征提取模块。</li><li><code>dres0</code> 和 <code>dres1</code>：用于处理立体匹配代价体积的 MobileV2_Residual_3D 模块。</li><li><code>encoder_decoder1</code>、<code>encoder_decoder2</code> 和 <code>encoder_decoder3</code>：使用 <code>hourglass3D</code> 模块实现的编码器-解码器结构。</li><li><code>classif0</code>、<code>classif1</code>、<code>classif2</code> 和 <code>classif3</code>：用于执行立体匹配和深度估计的模块。</li></ul><p>在前向传播方法中，通过输入左视图和右视图的图像数据 <code>L</code> 和 <code>R</code>，首先提取特征，然后构建立体匹配代价体积。接下来，对代价体积进行多尺度编码器-解码器处理，最终产生深度估计。</p></li></ol><p>再好好分析一下<code>LMSCNet_SS</code>这个模型的主要组件和架构摘要：</p><ol><li><p>这个模型在<code>mmdet3d</code>框架中注册为自定义检测器。</p></li><li><p>它继承自<code>MVXTwoStageDetector</code>，这是<code>mmdet3d</code>框架中用于3D物体检测的基础类。</p></li><li><p>模型包括多个组件，包括编码器、解码器和分割头。它以3D占用网格数据作为输入，用于预测语义标签。</p></li><li><p>编码器通过多个2D卷积层处理输入数据，将特征图下采样到较小的尺寸。</p></li><li><p>解码器由一系列反卷积层组成，将特征图上采样到所需的输出比例。</p></li><li><p>分割头进一步处理特征图，以生成语义分割预测。</p></li><li><p>训练时计算损失，其中包括二元交叉熵损失（BCE）和其他特定于语义分割的损失。</p></li><li><p><code>forward</code> 方法用于处理训练和测试模式，训练时返回损失，测试时返回预测。</p></li><li><p>代码包括用于存储的二进制数据的打包和解包函数。</p></li><li><p>使用<code>auto_fp16</code>装饰器可以自动应用混合精度训练。</p></li></ol><p>第2阶段基于一种新颖的稀疏到密集类MAE架构，如图（a）所示。它首先通过允许所提出的体素关注图像观察来加强其特征化。接下来，未提出的体素将与可学习的掩码标记相关联，并且将通过自注意来处理全套体素，以完成每体素语义分割的场景表示。</p><p><img src="/pic/VoxFormer2.png"></p><p>请注意，这里提供了两个版本的VoxFormer，一个只以当前图像作为输入(<a href="https://github.com/NVlabs/VoxFormer/blob/main/projects/configs/voxformer/voxformer-S.py">VoxFormer- S</a>)，另一个以当前图像和前4个图像作为输入(<a href="https://github.com/NVlabs/VoxFormer/blob/main/projects/configs/voxformer/voxformer-T.py">VoxFormer- T</a>)。</p><p><strong>VoxFormer-S是一个用于3D物体检测的模型，该模型使用多通道LiDAR数据和图像数据进行训练。以下是对这个配置文件的主要部分进行的详细分析：</strong></p><ol><li><p><code>work_dir</code> 定义了模型训练期间的工作目录，训练日志和模型检查点将在此目录中保存。在这里，工作目录被设置为’result&#x2F;voxformer-S’。</p></li><li><p><code>_base_</code> 包含了其他配置文件的路径，这些文件会被合并到当前配置中。在这里，使用了默认的运行时配置文件’default_runtime.py’。</p></li><li><p><code>plugin</code> 和 <code>plugin_dir</code> 是用于配置MMDet3D插件的选项。<code>plugin</code> 设置为True，表示启用插件，而<code>plugin_dir</code> 指定了插件的目录。</p></li><li><p><code>_num_layers_cross_</code> 和 <code>_num_points_cross_</code> 分别定义了交叉变换器（cross_transformer）中的层数和点数。这些参数用于模型的 Transformer 层设置。</p></li><li><p><code>_dim_</code> 定义了模型的嵌入维度。在这里，它被设置为128。</p></li><li><p><code>_pos_dim_</code> 定义了位置编码的维度，通常是嵌入维度的一半。这里的值由_dim_除以2计算。</p></li><li><p><code>_ffn_dim_</code> 定义了Feedforward网络的维度，通常是嵌入维度的两倍。在这里，它被设置为256。</p></li><li><p><code>_num_levels_</code> 定义了输出级别的数量。这里设置为1，表示只有一个输出级别。</p></li><li><p><code>_labels_tag_</code> 和 <code>_num_cams_</code> 用于配置数据集标签和相机的数量。这些参数是与数据集相关的配置。</p></li><li><p><code>point_cloud_range</code> 和 <code>voxel_size</code> 分别定义了点云范围和体素大小。这些参数也是与数据集相关的配置。</p></li><li><p><code>_sem_scal_loss_</code> 和 <code>_geo_scal_loss_</code> 用于配置语义分割损失和几何损失的选项。这些参数控制是否启用语义分割损失和几何损失。</p></li><li><p><code>_depthmodel_</code> 是深度模型的选择。它在数据集中用于深度估计。</p></li><li><p><code>_nsweep_</code> 定义了查询建议网络的扫描次数。这是一个与数据集和模型训练相关的参数。</p></li><li><p><code>model</code> 部分定义了VoxFormer模型的结构，包括图像骨干网络、特征金字塔网络、以及交叉和自注意力变换器。模型结构的配置包括不同的组件和层，用于处理图像和点云数据。</p></li><li><p><code>dataset_type</code> 和 <code>data</code> 部分配置了数据集的类型、数据根目录以及训练、验证和测试数据集的设置。这些设置与数据集的加载和处理相关。</p></li><li><p><code>optimizer</code>、<code>optimizer_config</code> 和 <code>lr_config</code> 用于配置优化器、学习率衰减策略以及其他优化选项。这些设置用于模型训练的优化配置。</p></li><li><p><code>total_epochs</code> 定义了总的训练周期数。在这里，设置为20个周期。</p></li><li><p><code>evaluation</code> 部分配置了评估的间隔。这指定了多少个周期后进行一次模型评估。</p></li><li><p><code>runner</code> 部分定义了训练过程的运行器。这包括如何管理训练循环以及模型的保存和加载。</p></li><li><p><code>log_config</code> 部分配置了训练日志的记录方式。在这里，设置了文本记录和Tensorboard记录。</p></li><li><p><code>checkpoint_config</code> 用于配置模型检查点的保存间隔。在这里，设置为None，表示没有指定检查点的保存间隔。</p></li></ol><p><strong>VoxFormer- T一个用于3D物体检测的模型，结合了LiDAR点云数据和图像数据。以下是对这个配置文件的主要部分进行的详细分析：</strong></p><ol><li><p><code>work_dir</code> 定义了模型训练期间的工作目录，训练日志和模型检查点将在此目录中保存。在这里，工作目录被设置为’result&#x2F;voxformer-T’。</p></li><li><p><code>_base_</code> 包含了其他配置文件的路径，这些文件会被合并到当前配置中。在这里，使用了默认的运行时配置文件’default_runtime.py’。</p></li><li><p><code>plugin</code> 和 <code>plugin_dir</code> 是用于配置MMDet3D插件的选项。<code>plugin</code> 设置为True，表示启用插件，而<code>plugin_dir</code> 指定了插件的目录。</p></li><li><p><code>_num_layers_cross_</code> 和 <code>_num_points_cross_</code> 分别定义了交叉变换器（cross_transformer）中的层数和点数。这些参数用于模型的 Transformer 层设置。</p></li><li><p><code>_num_layers_self_</code> 和 <code>_num_points_self_</code> 分别定义了自注意力变换器（self_transformer）中的层数和点数。这些参数也用于模型的 Transformer 层设置。</p></li><li><p><code>_dim_</code> 定义了模型的嵌入维度。在这里，它被设置为128。</p></li><li><p><code>_pos_dim_</code> 定义了位置编码的维度，通常是嵌入维度的一半。这里的值由_dim_除以2计算。</p></li><li><p><code>_ffn_dim_</code> 定义了Feedforward网络的维度，通常是嵌入维度的两倍。在这里，它被设置为256。</p></li><li><p><code>_num_levels_</code> 定义了输出级别的数量。这里设置为1，表示只有一个输出级别。</p></li><li><p><code>_labels_tag_</code> 和 <code>_num_cams_</code> 用于配置数据集标签和相机的数量。这些参数是与数据集相关的配置。</p></li><li><p><code>_temporal_</code> 定义了用于数据集的时间轴信息。在这里，使用了一个包含-12、-9、-6和-3的列表，这表示了不同时间步长的输入数据。</p></li><li><p><code>point_cloud_range</code> 和 <code>voxel_size</code> 分别定义了点云范围和体素大小。这些参数也是与数据集相关的配置。</p></li><li><p><code>_sem_scal_loss_</code> 和 <code>_geo_scal_loss_</code> 用于配置语义分割损失和几何损失的选项。这些参数控制是否启用语义分割损失和几何损失。</p></li><li><p><code>_depthmodel_</code> 是深度模型的选择。它在数据集中用于深度估计。</p></li><li><p><code>_nsweep_</code> 定义了查询建议网络的扫描次数。这是一个与数据集和模型训练相关的参数。</p></li><li><p><code>model</code> 部分定义了VoxFormer-T模型的结构，包括图像骨干网络、特征金字塔网络、以及交叉和自注意力变换器。模型结构的配置包括不同的组件和层，用于处理图像和点云数据。</p></li><li><p><code>dataset_type</code> 和 <code>data</code> 部分配置了数据集的类型、数据根目录以及训练、验证和测试数据集的设置。这些设置与数据集的加载和处理相关。</p></li><li><p><code>optimizer</code>、<code>optimizer_config</code> 和 <code>lr_config</code> 用于配置优化器、学习率衰减策略以及其他优化选项。这些设置用于模型训练的优化配置。</p></li><li><p><code>total_epochs</code> 定义了总的训练周期数。在这里，设置为20个周期。</p></li><li><p><code>evaluation</code> 部分配置了评估的间隔。这指定了多少个周期后进行一次模型评估。</p></li><li><p><code>runner</code> 部分定义了训练过程的运行器。这包括如何管理训练循环以及模型的保存和加载。</p></li><li><p><code>log_config</code> 部分配置了训练日志的记录方式。在这里，设置了文本记录和Tensorboard记录。</p></li><li><p><code>checkpoint_config</code> 用于配置模型检查点的保存间隔。在这里，设置为None，表示没有指定检查点的保存间隔。</p></li></ol><p><strong>这两份代码之间的主要区别：</strong></p><ol><li><p><strong>时间轴差异</strong>:</p><ul><li><code>voxformer-S.py</code> 使用了一个空的时间轴，<code>_temporal_</code> 参数为空列表。</li><li><code>voxformer-T.py</code> 使用了包含多个时间步长的时间轴，<code>_temporal_</code> 参数设置为包含了[-12, -9, -6, -3]的列表。</li></ul></li><li><p><strong>相机数量差异</strong>:</p><ul><li><code>voxformer-S.py</code> 中的 <code>_num_cams_</code> 参数设置为1，表示只使用单个相机。</li><li><code>voxformer-T.py</code> 中的 <code>_num_cams_</code> 参数设置为5，表示使用5个相机。</li></ul></li><li><p><strong>输出级别的数量</strong>:</p><ul><li>两者中的 <code>model</code> 部分都设置了 <code>num_outs</code> 参数 <code>_num_levels_</code>，但两者都将其设置为1，表示只有一个输出级别。</li></ul></li><li><p>**<a href="https://github.com/zhangyp15/OccFormer">OccFormer: Dual-path Transformer for Vision-based 3D Semantic Occupancy Prediction</a>, ICCV 2023</p><ul><li><code>voxformer-S.py</code> 和 <code>voxformer-T.py</code> 都配置了相同的数据集类型 <code>dataset_type</code>，数据根目录 <code>data_root</code>，以及训练、验证和测试数据集的相关设置。</li></ul></li></ol><p>总的来说，主要的区别在于时间轴配置、相机数量以及数据集的数据加载。<code>voxformer-S.py</code> 使用了一个静态的时间轴和单个相机，而 <code>voxformer-T.py</code> 使用了多个时间步长和多个相机。这些差异可能是用于处理不同类型数据集或任务的配置。要选择合适的配置，需要考虑你的数据集和任务的特定需求。</p><h2 id="OccFormer"><a href="#OccFormer" class="headerlink" title="OccFormer"></a>OccFormer</h2><p>代码链接：<a href="https://github.com/zhangyp15/OccFormer">OccFormer: Dual-path Transformer for Vision-based 3D Semantic Occupancy Prediction</a>, ICCV 2023</p><p>自动驾驶的视觉感知经历了从鸟瞰图（BEV）表示到 3D 语义占用的转变。与BEV平面相比，3D语义占用进一步提供了沿垂直方向的结构信息。本文提出了 OccFormer，一种双路径transformer网络，可有效处理 3D 体积以进行语义占用预测。OccFormer 实现了对相机生成的 3D 体素特征的远程、动态且高效的编码。它是通过将繁重的 3D 处理分解为沿水平面的局部和全局transformer路径而获得的。对于占用解码器，我们通过提出保留池和类引导采样来适应 3D 语义占用的普通 Mask2Former，这显着减轻了稀疏性和类不平衡。实验结果表明，OccFormer 显着优于 SemanticKITTI 数据集上的语义场景完成和 nuScenes 数据集上的 LiDAR 语义分割的现有方法。</p><p><img src="/pic/OccFormer.png"></p><p>该Pipeline由用于提取多尺度2D特征的图像编码器、用于将2D特征提升到3D体积的图像到3D变换以及用于获得3D语义特征并预测3D语义占用的基于变换器的编码器-解码器组成。</p><h3 id="代码分析-1"><a href="#代码分析-1" class="headerlink" title="代码分析"></a>代码分析</h3><p><a href="https://github.com/zhangyp15/OccFormer/blob/main/projects/configs/occformer_kitti/occformer_kitti.py">occformer_kitti.py</a>这段代码是关于 <code>OccFormer</code> 模型的kitti数据集的配置，以下是对主要部分的详细分析：</p><ol><li><p><strong>基础配置</strong>:</p><ul><li><code>_base_</code> 参数包括两个配置文件，一个是 <code>custom_nus-3d.py</code>，另一个是 <code>default_runtime.py</code>。</li><li><code>sync_bn</code> 和 <code>plugin</code> 分别设置为 <code>True</code>，启用了同步批处理规范化（Batch Normalization）和插件（plugin）功能。</li></ul></li><li><p><strong>数据设置</strong>:</p><ul><li><code>class_names</code> 定义了类别的名称，包括一些物体类别（如汽车、自行车等）和一些特殊类别（如未标记、道路、人等），总共有20个类别。</li><li><code>point_cloud_range</code> 指定了点云的范围。</li><li><code>occ_size</code> 定义了3D体素的尺寸。</li><li><code>lss_downsample</code> 指定了生成3D体素（Local Semantic Segmentation, LSS）时的下采样比率。</li><li><code>voxel_size</code> 是通过计算 <code>point_cloud_range</code> 和 <code>occ_size</code> 得出的体素大小。</li></ul></li><li><p><strong>模型配置</strong>:</p><ul><li><code>model</code> 部分配置了 <code>OccFormer</code> 模型的架构，包括以下组件：<ul><li><code>img_backbone</code> 使用自定义的EfficientNet骨干网络。</li><li><code>img_neck</code> 使用SECONDFPN的颈部结构。</li><li><code>img_view_transformer</code> 使用 <code>ViewTransformerLiftSplatShootVoxel</code> 进行图像到点云的转换。</li><li><code>img_bev_encoder_backbone</code> 使用 <code>OccupancyEncoder</code> 进行BEV编码。</li><li><code>img_bev_encoder_neck</code> 使用多层Transformer解码器。</li><li><code>pts_bbox_head</code> 配置了 <code>Mask2FormerOccHead</code> 头部，用于形状转换、损失计算等。</li></ul></li></ul></li><li><p><strong>数据集配置</strong>:</p><ul><li>数据集类型 <code>dataset_type</code> 设置为 <code>CustomSemanticKITTILssDataset</code>。</li><li><code>data</code> 部分配置了数据加载和处理的设置，包括训练、验证和测试数据集。</li></ul></li><li><p><strong>优化器和学习率策略</strong>:</p><ul><li><code>optimizer</code> 部分配置了AdamW优化器，包括学习率、权重衰减等设置。</li><li><code>lr_config</code> 配置了学习率策略，采用阶段性的学习率衰减策略。</li></ul></li><li><p><strong>检查点和日志配置</strong>:</p><ul><li><code>checkpoint_config</code> 配置了检查点的保存策略。</li><li><code>runner</code> 部分定义了运行的类型和最大训练轮数。</li><li><code>evaluation</code> 部分配置了模型的评估策略。</li></ul></li></ol><p>总结，这段代码配置了 <code>OccFormer</code> 模型，包括图像和点云的处理、模型架构、数据加载、优化器、学习率策略和训练设置等。这个模型用于语义分割任务，能够将点云数据映射到3D体素空间，然后使用Transformer结构进行处理，最终输出语义分割结果。这是一个复杂的3D视觉模型，适用于点云数据的语义分割任务。</p><p><a href="https://github.com/zhangyp15/OccFormer/blob/main/projects/configs/occformer_nusc/occformer_nusc_r50_256x704.py">occformer_nusc_r50_256x704.py</a>这段代码是关于 <code>OccFormer</code> 模型的nuScenes数据集的配置，以下是对主要部分的详细分析：</p><ol><li><p><strong>基础配置</strong>:</p><ul><li><code>_base_</code> 参数包括两个配置文件：<code>custom_nus-3d.py</code> 和 <code>default_runtime.py</code>。</li><li><code>sync_bn</code> 和 <code>plugin</code> 分别设置为 <code>True</code>，启用了同步批处理规范化（Batch Normalization）和插件（plugin）功能。</li><li><code>plugin_dir</code> 指定插件的目录路径。</li><li><code>img_norm_cfg</code> 包含了图像的标准化配置，包括均值、标准差和是否转换为RGB格式。</li></ul></li><li><p><strong>类别和点云范围</strong>:</p><ul><li><code>class_names</code> 包含了物体类别的名称，共17个类别。</li><li><code>num_class</code> 表示类别的总数。</li><li><code>point_cloud_range</code> 定义了点云的范围。</li><li><code>occ_size</code> 指定了3D体素的尺寸。</li><li><code>lss_downsample</code> 是用于生成3D体素（Local Semantic Segmentation, LSS）时的下采样比率。</li><li><code>voxel_size</code> 是通过计算 <code>point_cloud_range</code> 和 <code>occ_size</code> 得出的体素大小。</li></ul></li><li><p><strong>数据配置</strong>:</p><ul><li><code>data_config</code> 包含了数据加载和处理的设置，包括相机信息、输入图像大小、数据增强参数等。</li><li><code>grid_config</code> 包含了3D卷积网格的配置，定义了xyz范围和体素大小等信息。</li></ul></li><li><p><strong>模型配置</strong>:</p><ul><li><code>model</code> 部分配置了 <code>OccupancyFormer</code> 模型，包括以下组件：<ul><li><code>img_backbone</code> 使用了ResNet-50骨干网络。</li><li><code>img_neck</code> 使用SECONDFPN的颈部结构。</li><li><code>img_view_transformer</code> 用于将图像信息转换为3D体素。</li><li><code>img_bev_encoder_backbone</code> 使用 <code>OccupancyEncoder</code> 进行BEV编码。</li><li><code>img_bev_encoder_neck</code> 使用多层Transformer解码器。</li><li><code>pts_bbox_head</code> 配置了 <code>Mask2FormerNuscOccHead</code> 头部，用于形状转换、损失计算等。</li></ul></li></ul></li><li><p><strong>数据集配置</strong>:</p><ul><li><code>dataset_type</code> 设置为 <code>CustomNuScenesOccLSSDataset</code>，指定了数据集类型。</li><li><code>data_root</code> 指定了数据集的根目录。</li><li><code>train_pipeline</code> 和 <code>test_pipeline</code> 分别配置了训练和测试数据的处理流程。</li></ul></li><li><p><strong>优化器和学习率策略</strong>:</p><ul><li><code>optimizer</code> 部分配置了AdamW优化器，包括学习率、权重衰减等设置。</li><li><code>optimizer_config</code> 包含了梯度剪切的设置。</li><li><code>lr_config</code> 配置了学习率策略，采用阶段性的学习率衰减策略。</li></ul></li><li><p><strong>检查点和日志配置</strong>:</p><ul><li><code>checkpoint_config</code> 配置了检查点的保存策略。</li><li><code>runner</code> 部分定义了运行的类型和最大训练轮数。</li><li><code>evaluation</code> 部分配置了模型评估的策略，包括评估间隔、评估指标等。</li></ul></li></ol><p><strong>这两段代码是针对不同的任务和数据集的配置文件，它们的主要区别在于以下几个方面：</strong></p><ol><li><p><strong>任务和数据集</strong>:</p><ul><li>第一个代码段是针对SemanticKITTI数据集的，主要用于3D语义分割任务。</li><li>第二个代码段是为了处理NuScenes数据集的，用于3D物体检测和语义分割任务。</li></ul></li><li><p><strong>类别和数据范围</strong>:</p><ul><li>第一个代码段中的 <code>class_names</code> 包含了SemanticKITTI数据集的类别，而第二个代码段的 <code>class_names</code> 包含了NuScenes数据集的类别，因此它们的类别列表是不同的。</li><li><code>point_cloud_range</code> 和 <code>occ_size</code> 也在两个代码段中有所不同，因为它们对应不同数据集的点云范围和3D体素尺寸。</li></ul></li><li><p><strong>数据处理和数据加载</strong>:</p><ul><li>两个代码段中的数据加载和处理流程是不同的。第一个代码段包含了用于加载SemanticKITTI数据的处理流程，而第二个代码段包含了用于加载NuScenes数据的处理流程。</li></ul></li><li><p><strong>模型配置</strong>:</p><ul><li>模型配置在两个代码段中也存在差异。虽然它们都使用了Transformer结构，但底层的图像骨干网络、特征提取过程、解码器结构等都可能有所不同。</li></ul></li><li><p><strong>数据路径和检查点路径</strong>:</p><ul><li>数据路径和检查点路径在两个配置中也不同。它们指定了数据集的根目录和用于预训练模型的检查点文件。</li></ul></li></ol><p>总的来说，这两段代码主要区别在于它们的应用领域、数据集和任务的不同。第一个代码段适用于SemanticKITTI数据集的3D语义分割任务，而第二个代码段则适用于NuScenes数据集的3D物体检测和语义分割任务。每个配置都经过仔细调整，以满足其特定的数据和任务需求。</p><h2 id="TPVFormer"><a href="#TPVFormer" class="headerlink" title="TPVFormer"></a>TPVFormer</h2><p>代码链接： <a href="https://github.com/wzzheng/TPVFormer">TPVFormer: An academic alternative to Tesla’s Occupancy Network</a>, CVPR2023</p><p>以视觉为中心的自动驾驶感知的现代方法广泛采用鸟瞰图（BEV）表示来描述 3D 场景。尽管它比体素表示效率更高，但它很难用单个平面描述场景的细粒度 3D 结构。为了解决这个问题，我们提出了一种三透视图（TPV）表示法，它与 BEV 一起提供了两个额外的垂直平面。我们通过对三个平面上的投影特征求和来对 3D 空间中的每个点进行建模。为了将图像特征提升到3D TPV空间，我们进一步提出了一种基于transformer的TPV编码器（TPVFormer）以有效地获得TPV特征。我们采用注意力机制来聚合每个 TPV 平面中每个查询对应的图像特征。实验表明，我们用稀疏监督训练的模型可以有效地预测所有体素的语义占用率。我们首次证明，在 nuScenes 上的 LiDAR 分割任务中，仅使用相机输入就可以实现与基于 LiDAR 的方法相当的性能。</p><p><img src="/pic/TPVFormer.png"></p><p>三维语义占用预测TPVFormer框架。我们采用一个图像骨干网来提取多摄像机图像的多尺度特征。然后通过交叉注意自适应提升二维特征到TPV空间，并利用交叉视图混合注意实现TPV平面之间的交互。为了预测三维空间中一个点的语义占用情况，我们在三个TPV平面上的投影特征的总和上应用一个轻量级预测头。</p><h3 id="代码分析-2"><a href="#代码分析-2" class="headerlink" title="代码分析"></a>代码分析</h3><p>这里我们只看TPVFormer 进行 3D 语义占用预测任务代码<a href="https://github.com/wzzheng/TPVFormer/blob/main/config/tpv04_occupancy.py">tpv04_occupancy.py</a></p><p>这段代码是一个配置文件，用于定义一个名为 “TPVFormer” 的模型，它的主要组成部分和参数如下：</p><ol><li><p><strong>基础配置</strong>:</p><ul><li><code>_base_</code> 中包含了用于数据集、优化器和学习率策略的配置文件路径。</li></ul></li><li><p><strong>数据集参数</strong>:</p><ul><li><code>dataset_params</code> 包含了数据集相关的参数，如数据版本、标签映射、数据空间的范围等。</li></ul></li><li><p><strong>模型类型</strong>:</p><ul><li><code>TPVFormer</code> 是所使用的模型类型。</li></ul></li><li><p><strong>TPV Aggregator</strong>:</p><ul><li><code>tpv_aggregator</code> 部分定义了一个名为 “TPVAggregator” 的模块，用于聚合TPV（Top-View Pillar）特征。</li><li>这个模块包括了输入和输出维度、TPV空间的尺寸、类别数等参数。</li></ul></li><li><p><strong>图像骨干网络</strong>:</p><ul><li><code>img_backbone</code> 定义了一个ResNet类型的图像骨干网络，用于从输入图像中提取特征。</li><li>这个网络的参数包括网络深度、输出特征层级、冻结的阶段、Batch Normalization等。</li></ul></li><li><p><strong>图像颈部（FPN）</strong>:</p><ul><li><code>img_neck</code> 定义了一个FPN类型的颈部网络，用于生成不同层级的特征金字塔。</li><li>这个网络将图像骨干网络的输出特征进行特征金字塔处理。</li></ul></li><li><p><strong>TPV Head</strong>:</p><ul><li><code>tpv_head</code> 定义了模型的头部，用于处理TPV特征。</li><li>这个部分包括了TPV空间的参数、特征维度、位置编码等。</li><li>它还包含了一个编码器，用于处理TPV特征。编码器中包括了多层的TPVFormerLayer，每一层包括了自注意力、跨视图注意力等组件。</li></ul></li></ol><p>这里我们主要看一下使用的<a href="https://github.com/wzzheng/TPVFormer/blob/main/tpvformer04/tpvformer.py">TPVFormer模型</a>的代码以及为 <a href="https://github.com/wzzheng/TPVFormer/blob/main/tpvformer04/tpv_aggregator.py">TPVAggregator</a>的模块，用于聚合TPV（Top-View Pillar）特征</p><p><strong>下面是对 TPVFormer 模型的代码的分析：</strong></p><ol><li><p>TPVFormer 继承了 <code>BaseModule</code>，这是一个基础的模型类，它是 mmcv（MMLab Computer Vision）和 mmseg（MMSegmentation）框架中的模型定义类。</p></li><li><p><code>__init__</code> 函数初始化 TPVFormer 模型，接受多个参数：</p><ul><li><code>use_grid_mask</code>: 控制是否使用 Grid Mask 数据增强。</li><li><code>img_backbone</code>: 图像骨干网络，用于提取图像特征。</li><li><code>img_neck</code>: 图像颈部网络，可选的，用于进一步处理图像特征。</li><li><code>tpv_head</code>: TPVFormer 头部网络，用于执行语义分割任务。</li><li><code>pretrained</code>: 预训练模型的配置。</li><li><code>tpv_aggregator</code>: 用于聚合 TPV 特征的头部网络。</li></ul></li><li><p>在 <code>__init__</code> 函数中，根据传入的参数初始化 TPVFormer 模型的各个组件，包括图像骨干网络、图像颈部网络、TPVFormer 头部网络以及 TPV 聚合器。如果提供了预训练模型，则将其配置传递给图像骨干网络。</p></li><li><p>TPVFormer 模型支持使用 Grid Mask 数据增强。Grid Mask 是一种数据增强方法，可以随机遮挡输入图像的一部分，从而增加模型的鲁棒性。</p></li><li><p><code>extract_img_feat</code> 函数用于提取图像特征，接受图像数据 <code>img</code> 作为输入。在函数内部，它首先对输入的图像进行形状变换，然后应用 Grid Mask 数据增强（如果启用），接着通过图像骨干网络提取图像特征，最后通过图像颈部网络进行进一步处理。</p></li><li><p><code>forward</code> 函数是 TPVFormer 模型的前向传播函数。它接受图像数据 <code>img</code> 和其他输入参数，调用 <code>extract_img_feat</code> 函数提取图像特征，然后将这些特征传递给 TPVFormer 头部网络 <code>tpv_head</code> 执行语义分割任务。最后，模型将输出结果传递给 TPV 聚合器 <code>tpv_aggregator</code> 进行进一步处理。</p></li></ol><p>总之，TPVFormer 是一个用于图像语义分割任务的模型，它包括图像骨干网络、图像颈部网络以及 TPVFormer 头部网络，可以处理输入的图像数据和点云数据，通过 TPV 聚合器聚合特征，以生成最终的语义分割结果。模型还支持 Grid Mask 数据增强以提高模型的鲁棒性。</p><p><strong>下面是对 TPVAggregator 模块的代码的分析：</strong></p><ol><li><p>TPVAggregator 继承了 <code>BaseModule</code>，这是 mmseg（MMSegmentation）框架中的头部模块类。</p></li><li><p><code>__init__</code> 函数初始化 TPVAggregator 模块，接受多个参数：</p><ul><li><code>tpv_h</code>, <code>tpv_w</code>, <code>tpv_z</code>: TPV 的高度、宽度和深度。</li><li><code>nbr_classes</code>: 类别数，即目标类别的数量。</li><li><code>in_dims</code>, <code>hidden_dims</code>, <code>out_dims</code>: 输入、隐藏和输出维度。</li><li><code>scale_h</code>, <code>scale_w</code>, <code>scale_z</code>: 高度、宽度和深度的缩放因子。</li><li><code>use_checkpoint</code>: 是否使用 PyTorch 的 checkpoint 功能。</li></ul></li><li><p>TPVAggregator 模块包括一个简单的神经网络，其中包含线性层（Linear）和 Softplus 激活函数用于从输入特征中提取特征，然后通过线性分类层进行目标分类。</p></li><li><p><code>forward</code> 函数用于执行前向传播操作，接受 TPV 特征列表 <code>tpv_list</code> 和点云数据 <code>points</code>（可选）。在函数内部，首先对 TPV 特征进行形状变换和插值操作，以匹配点云数据的尺寸和位置。然后，如果提供了点云数据，将点云数据映射到 TPV 特征上，执行点云与 TPV 特征的融合。最后，将融合后的特征传递给线性层和分类器进行分类，并返回分类结果。</p></li><li><p>如果未提供点云数据，则仅将 TPV 特征进行插值和融合，并执行分类。最终的分类结果以形状 <code>(batch_size, num_classes, scale_w * tpv_w, scale_h * tpv_h, scale_z * tpv_z)</code> 返回。</p></li></ol><p>总之，TPVAggregator 模块用于聚合 TPV 特征和点云数据，以执行语义分割任务。它包含线性层和分类器，用于将输入特征映射到目标类别的概率分布。这个模块的设计旨在结合点云数据和 TPV 特征，以提高语义分割的性能。</p>]]></content>
      
      
      <categories>
          
          <category> DL </category>
          
      </categories>
      
      
        <tags>
            
            <tag> ssc </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>OCC-VO:生成面向自动驾驶的基于3D占用栅格的视觉里程计稠密地图</title>
      <link href="/2023/11/04/occ-vo/"/>
      <url>/2023/11/04/occ-vo/</url>
      
        <content type="html"><![CDATA[<h1 id="OCC-VO-Dense-Mapping-via-3D-Occupancy-Based-Visual-Odometry-for-Autonomous-Driving"><a href="#OCC-VO-Dense-Mapping-via-3D-Occupancy-Based-Visual-Odometry-for-Autonomous-Driving" class="headerlink" title="OCC-VO: Dense Mapping via 3D Occupancy-Based Visual Odometry for Autonomous Driving"></a>OCC-VO: Dense Mapping via 3D Occupancy-Based Visual Odometry for Autonomous Driving</h1><p>论文链接：<a href="https://arxiv.org/pdf/2309.11011.pdf">OCC-VO-pdf</a><br>代码链接：<a href="https://github.com/USTCLH/OCC-VO">OCC-VO-py</a></p><p>生成面向自动驾驶的基于3D占用栅格的视觉里程计稠密地图</p><p>主要贡献：</p><ol><li><p><strong>OCC-VO框架的设计和开发：</strong> 作者设计并开发了OCC-VO框架，该框架接受环视相机图像作为输入，并生成密集语义地图，有助于增强场景理解，从而支持感知和导航等下游任务。</p></li><li><p><strong>3D语义占用预测模块：</strong> 在OCC-VO框架中，使用了名为TPV-Former的开源3D语义占用预测模块，用于将环视相机图像转化为3D语义占用栅格，实现对环境的语义理解。</p></li><li><p><strong>位姿估计和地图算法的定制：</strong> 为了解决3D语义占用地图的配准问题，作者设计了一种专为此任务定制的位姿估计和地图算法。该算法以GICP算法为基础，结合语义约束，以更好地对齐点云数据，特别适用于处理具有相似几何结构但不同语义的场景，如自动驾驶道路表面。</p></li><li><p><strong>全局语义地图的构建：</strong> 在地图创建阶段，借鉴了PFilter中的思想，消除了不可靠的点，从而创建了一个更稳健的全局语义地图。</p></li><li><p><strong>精准的地图和姿态估计：</strong> 最终的成果是经过精心调整的姿态估计和高度准确的地图，为感知和导航任务提供了可靠的基础。</p></li></ol><p>这些贡献共同构成了这项工作的核心，旨在提高环境理解和导航系统的性能，特别是在自动驾驶等领域中。</p><p>这段文本提供了关于OCC-VO框架的详细信息，包括作者、代码地址以及主要贡献。以下是以Markdown形式的总结：</p><h2 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h2><p>OCC-VO是一个新颖的框架，充分利用深度学习技术，将2D相机图像转化为3D语义占用，以解决自主系统中的视觉里程计（VO）挑战。它使用TPV-Former模块将环视相机图像转化为3D语义占用，经过特定设计的位姿估计和地图算法，包括语义标签滤波器、动态对象滤波器以及Voxel PFilter，生成密集的全局语义地图。在Occ3D-nuScenes数据集上的评估结果表明，OCC-VO相较于ORB-SLAM3取得了更高的成功率和轨迹准确性，成功率提高了20.6%，轨迹准确性提高了29.6%。</p><h2 id="主要贡献"><a href="#主要贡献" class="headerlink" title="主要贡献"></a>主要贡献</h2><ol><li><p>设计和开发OCC-VO框架，将环视相机图像转化为密集语义地图，用于增强场景理解和支持感知和导航任务。</p></li><li><p>使用TPV-Former模块，将环视相机图像转化为3D语义占用栅格。</p></li><li><p>定制位姿估计和地图算法，包括语义标签滤波器、动态对象滤波器，以及Voxel PFilter，以提高配准和全局地图一致性。</p></li><li><p>在Occ3D-nuScenes数据集上的评估，显示OCC-VO在自动驾驶场景中取得了较高的准确性和稳健性，尤其相较于ORB-SLAM3。</p></li></ol><h2 id="内容概述"><a href="#内容概述" class="headerlink" title="内容概述"></a>内容概述</h2><ul><li><p>系统概述：介绍了OCC-VO的工作流程，包括图像转化、姿态估计、过滤器应用，以及全局地图维护。</p></li><li><p>语义标签过滤器：介绍了基于语义标签的对象过滤方法，以提高配准准确性。</p></li><li><p>动态对象过滤器：解释了如何处理动态对象，以平衡准确性和场景恶化。</p></li><li><p>Voxel PFilter：介绍了引入Voxel PFilter来维护全局地图的一致性，校正网络干扰噪音。</p></li><li><p>实验：使用Occ3D-nuScenes数据集评估OCC-VO，比较其性能和执行时间分析。</p></li></ul><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>OCC-VO是一个新颖的VO框架，结合3D语义占用栅格，用于自动驾驶场景中的密集地图生成。通过设计的滤波器，OCC-VO在自动驾驶环境中取得了更高的准确性和稳健性。未来的工作将包括将环路闭合检测等模块整合到OCC-VO中，以发展成为一个完整的SLAM系统。</p><p><strong><a href="https://mp.weixin.qq.com/s/q_YpZSS2mDoa2SNhUCAOTg">来源</a></strong></p>]]></content>
      
      
      <categories>
          
          <category> DL </category>
          
          <category> slam </category>
          
      </categories>
      
      
        <tags>
            
            <tag> ssc </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>语义场景补全（SSC）代码篇（一）</title>
      <link href="/2023/11/03/code-ssc/"/>
      <url>/2023/11/03/code-ssc/</url>
      
        <content type="html"><![CDATA[<h1 id="主要记录SSC中的理论与代码实现（第一章）"><a href="#主要记录SSC中的理论与代码实现（第一章）" class="headerlink" title="主要记录SSC中的理论与代码实现（第一章）"></a>主要记录SSC中的理论与代码实现（第一章）</h1><p>主要从开篇之作SSCNet（2017）开始讲起，然后按激光雷达点云输入（LMSCNet，2020；S3CNet，2021a；JS3C-Net，2021）,单双目以及两者融合的顺序</p><h2 id="首先是开篇之作SSCNet"><a href="#首先是开篇之作SSCNet" class="headerlink" title="首先是开篇之作SSCNet"></a>首先是开篇之作SSCNet</h2><p>官方代码实在caffe框架上搭建的，第一个将语义分割和场景完成与 3D CNN 端到端结合的工作，从单视图深度地图观察生成一个完整的三维体素表示的体积占用和语义标签的场景，利用这两个任务的耦合特性，引入了语义场景完成网络(SSCNet)</p><p>代码链接： <a href="https://github.com/shurans/sscnet">SSCNet: Semantic Scene Completion from a Single Depth Image</a>, 2017</p><p><img src="/pic/SSCNet.png"></p><p>代码：<br>使用Caffe框架的网络定义txt文件,主要包括下面几个部分:</p><ul><li>数据层 (layer { name: “data” … })： 这是数据输入层，它从SUNCG数据集中加载数据。数据集包括深度信息（可能是三维数据）以及标签信息（seg_label）。还包括一些参数，如体素大小、裁剪大小、类别映射等。</li><li>卷积层 (layer { name: “conv1_1” … })： 这是一个卷积层，用于提取特征。它定义了卷积核的数量、大小、填充等参数。</li><li>ReLU 层 (layer { name: “relu1_1” … })： 这是激活函数层，通常用于引入非线性性。ReLU（Rectified Linear Unit）是一种常见的激活函数。</li><li>池化层 (layer { name: “pool2” … })： 这是一个池化层，用于降低特征图的分辨率，通常用于减少计算复杂度。</li><li>Eltwise 层 (layer { name: “res3_2” … })： 这是一个元素级操作层，通常用于残差网络（ResNet）中，用于连接不同层的输出。</li><li>全连接层 (layer { name: “fc12” … })： 这是一个全连接层，用于将卷积层的输出映射到最终的分类标签或预测。</li><li>SoftmaxWithLoss 层 (layer { name: “loss” … })： 这是损失层，通常与Softmax函数一起使用，用于计算损失函数，用于模型训练。</li></ul><h3 id="数据层配置"><a href="#数据层配置" class="headerlink" title="数据层配置"></a>数据层配置</h3><p>数据层在深度学习模型中是一个关键组成部分，用于加载和处理输入数据。在你提供的代码中，数据层的定义如下：</p><pre class="line-numbers language-protobuf" data-language="protobuf"><code class="language-protobuf">layer <span class="token punctuation">&#123;</span>  name<span class="token punctuation">:</span> <span class="token string">"data"</span>  type<span class="token punctuation">:</span> <span class="token string">"SuncgData"</span>  top<span class="token punctuation">:</span> <span class="token string">"data"</span>  top<span class="token punctuation">:</span> <span class="token string">"seg_label"</span>  top<span class="token punctuation">:</span> <span class="token string">"seg_weight"</span>  suncg_data_param <span class="token punctuation">&#123;</span>    file_data<span class="token punctuation">:</span> <span class="token string">"../../../../data/depthbin/SUNCGtrain_1_500"</span>    file_data<span class="token punctuation">:</span> <span class="token string">"../../../../data/depthbin/SUNCGtrain_501_1000"</span>    file_data<span class="token punctuation">:</span> <span class="token string">"../../../../data/depthbin/SUNCGtrain_1001_2000"</span>    file_data<span class="token punctuation">:</span> <span class="token string">"../../../../data/depthbin/SUNCGtrain_1001_3000"</span>    file_data<span class="token punctuation">:</span> <span class="token string">"../../../../data/depthbin/SUNCGtrain_3001_5000"</span>    file_data<span class="token punctuation">:</span> <span class="token string">"../../../../data/depthbin/SUNCGtrain_5001_7000"</span>    file_list<span class="token punctuation">:</span> <span class="token string">""</span>    vox_unit<span class="token punctuation">:</span> <span class="token number">0.02</span>    vox_margin<span class="token punctuation">:</span> <span class="token number">0.24</span>    vox_size<span class="token punctuation">:</span> <span class="token number">240</span>    vox_size<span class="token punctuation">:</span> <span class="token number">144</span>    vox_size<span class="token punctuation">:</span> <span class="token number">240</span>    crop_size<span class="token punctuation">:</span> <span class="token number">240</span>    crop_size<span class="token punctuation">:</span> <span class="token number">144</span>    crop_size<span class="token punctuation">:</span> <span class="token number">240</span>    label_size<span class="token punctuation">:</span> <span class="token number">60</span>    label_size<span class="token punctuation">:</span> <span class="token number">36</span>    label_size<span class="token punctuation">:</span> <span class="token number">60</span>    seg_classes<span class="token punctuation">:</span> <span class="token number">11</span>    shuffle<span class="token punctuation">:</span> <span class="token boolean">true</span>    occ_empty_only<span class="token punctuation">:</span> <span class="token boolean">true</span>    neg_obj_sample_ratio<span class="token punctuation">:</span> <span class="token number">2</span>    seg_class_map<span class="token punctuation">:</span> <span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span>    seg_class_weight<span class="token punctuation">:</span> <span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span>    occ_class_weight<span class="token punctuation">:</span> <span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span>    with_projection_tsdf<span class="token punctuation">:</span> <span class="token boolean">false</span>    batch_size<span class="token punctuation">:</span> <span class="token number">1</span>    tsdf_type<span class="token punctuation">:</span> <span class="token number">1</span>    data_type<span class="token punctuation">:</span> TSDF    surf_only<span class="token punctuation">:</span> <span class="token boolean">false</span>  <span class="token punctuation">&#125;</span><span class="token punctuation">&#125;</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>主要参数如下</p><ul><li><p><strong>name: “data”：</strong> 数据层的名称，用于在模型中引用该层。</p></li><li><p><strong>type: “SuncgData”：</strong> 数据层的类型，指定了如何处理数据。</p></li><li><p><strong>top: “data”, top: “seg_label”, top: “seg_weight”：</strong> 数据层的输出，模型中其他层可以引用这些输出。</p></li><li><p><strong>suncg_data_param：</strong> 数据层的参数配置块。</p></li><li><p><strong>file_data：</strong> 数据文件的路径，用于加载数据。这里看起来是从SUNCG数据集中加载深度信息。</p></li><li><p><strong>vox_unit, vox_margin, vox_size：</strong> 与体素（Voxel）的设置相关。<code>vox_unit</code>是体素的大小，<code>vox_margin</code>是体素的边缘大小，<code>vox_size</code>是体素在每个维度的数量。这些参数定义了体素网格的离散化空间。</p></li><li><p><strong>crop_size 和 label_size：</strong> 数据的裁剪大小和标签大小。<code>crop_size</code>表示输入数据的裁剪尺寸，<code>label_size</code>表示标签的大小。</p></li><li><p><strong>seg_classes：</strong> 指定了分类任务中的类别数量。</p></li><li><p><strong>shuffle：</strong> 一个布尔值，表示是否在加载数据时进行洗牌（打乱数据顺序）。</p></li><li><p><strong>occ_empty_only：</strong> 一个布尔值，可能表示只关注占用（occupied）和空闲（empty）之间的区别。</p></li><li><p><strong>neg_obj_sample_ratio：</strong> 负对象采样比例。</p></li><li><p><strong>seg_class_map, seg_class_weight, occ_class_weight：</strong> 用于定义不同类别的映射和权重。<code>seg_class_map</code>用于将一些类别映射到其他类别，<code>seg_class_weight</code>和<code>occ_class_weight</code>用于类别的权重设置。</p></li><li><p><strong>with_projection_tsdf 和 surf_only：</strong> 控制是否使用投影TSDF和是否仅考虑表面信息。投影TSDF通常用于三维重建，而<code>surf_only</code>可能用于指示仅关注表面的信息。</p></li><li><p><strong>batch_size：</strong> 训练时的批处理大小，影响每次模型权重更新时使用的样本数量。</p></li><li><p><strong>tsdf_type 和 data_type：</strong> 与数据类型相关的参数，例如TSDF数据类型。</p></li><li><p><strong>surf_only：</strong> 一个布尔值，可能表示是否仅考虑表面数据。</p></li></ul><p>这些参数共同定义了数据层的行为，确保数据被正确加载和预处理，以供深度学习模型使用。</p><h2 id="LMSCNet"><a href="#LMSCNet" class="headerlink" title="LMSCNet"></a>LMSCNet</h2><p>代码链接： <a href="https://github.com/astra-vision/LMSCNet">LMSCNet: Lightweight Multiscale 3D Semantic Completion</a>, 3DV 2020</p><p>一种从体素化稀疏三维激光雷达扫描中完成多尺度三维语义场景的新方法,使用具有全面多尺度跳跃连接的2D UNet主干来增强特征流，以及3D分割头</p><p><img src="/pic/LMSCNet.png"></p><p>首先看配置文件<a href="https://github.com/astra-vision/LMSCNet/blob/main/SSC_configs/examples/LMSCNet.yaml">LMSCNet.yaml</a>这段代码是一个用于训练深度学习模型的Python脚本，基于LMSCNet网络。以下是关于训练设置以及代码的关键部分的解释：</p><h3 id="训练设置"><a href="#训练设置" class="headerlink" title="训练设置"></a>训练设置</h3><ul><li><p><strong>数据加载器设置：</strong></p><ul><li><code>NUM_WORKERS: 4</code>：用于数据加载的工作线程数。</li></ul></li><li><p><strong>数据集设置：</strong></p><ul><li>数据增强：<ul><li><code>FLIPS: true</code>：表示是否进行数据增强，包括翻转等。</li></ul></li><li>模态设置：<ul><li><code>3D_LABEL: true</code>：表示是否使用3D标签数据。</li><li><code>3D_OCCLUDED: true</code>：表示是否使用3D遮挡数据。</li><li><code>3D_OCCUPANCY: true</code>：表示是否使用3D占用数据。</li></ul></li><li><code>ROOT_DIR: /datasets_local/datasets_lroldaoj/semantic_kitti_v1.0/</code>：数据集的根目录。</li><li><code>TYPE: SemanticKITTI</code>：数据集的类型，这里是SemanticKITTI。</li></ul></li><li><p><strong>模型设置：</strong></p><ul><li><code>TYPE: LMSCNet</code>：使用的模型类型为LMSCNet。</li></ul></li><li><p><strong>优化器设置：</strong></p><ul><li><code>BASE_LR: 0.001</code>：学习率的初始值。</li><li><code>BETA1: 0.9</code>：Adam优化器的beta1参数。</li><li><code>BETA2: 0.999</code>：Adam优化器的beta2参数。</li><li><code>MOMENTUM: NA</code>：动量参数（未设置）。</li><li><code>TYPE: Adam</code>：优化器的类型。</li><li><code>WEIGHT_DECAY: NA</code>：权重衰减参数（未设置）。</li></ul></li><li><p><strong>输出设置：</strong></p><ul><li><code>OUT_ROOT: ../SSC_out/</code>：输出结果的根目录。</li></ul></li><li><p><strong>调度器设置：</strong></p><ul><li><code>FREQUENCY: epoch</code>：学习率调度的频率为每个epoch。</li><li><code>LR_POWER: 0.98</code>：学习率调度的幂次方。</li><li><code>TYPE: power_iteration</code>：学习率调度类型。</li></ul></li><li><p><strong>状态设置：</strong></p><ul><li><code>RESUME: false</code>：是否从之前的检查点中恢复训练（未设置为恢复）。</li></ul></li><li><p><strong>训练设置：</strong></p><ul><li><code>BATCH_SIZE: 4</code>：每个批次的样本数量。</li><li><code>CHECKPOINT_PERIOD: 15</code>：保存检查点的周期。</li><li><code>EPOCHS: 80</code>：总共训练的轮数。</li><li><code>SUMMARY_PERIOD: 50</code>：汇总损失的周期。</li></ul></li><li><p><strong>验证设置：</strong></p><ul><li><code>BATCH_SIZE: 8</code>：验证时的批次大小。</li><li><code>SUMMARY_PERIOD: 20</code>：验证损失的周期。</li></ul></li></ul><h3 id="LMSCNet网络代码解释"><a href="#LMSCNet网络代码解释" class="headerlink" title="LMSCNet网络代码解释"></a>LMSCNet网络代码解释</h3><p><a href="https://github.com/astra-vision/LMSCNet/blob/main/LMSCNet/models/LMSCNet.py">LMSCNet.py</a></p><p>当分析这个神经网络模型时，我们可以将其分解为以下几个关键组件和部分。以下是对这些组件和部分的更详细解释：</p><h4 id="SegmentationHead（分割头部）"><a href="#SegmentationHead（分割头部）" class="headerlink" title="SegmentationHead（分割头部）"></a>SegmentationHead（分割头部）</h4><p><code>SegmentationHead</code> 类用于处理单一尺度的语义分割任务。它包括以下组件：</p><ul><li><p><strong>First Convolution (第一个卷积层)</strong>: <code>conv0</code> 定义了一个3D卷积层，用于将输入的特征从一个尺度（inplanes）转换到 <code>planes</code>。这是模型的初始特征处理步骤。</p></li><li><p><strong>ASPP Block (空间金字塔池化块)</strong>: <code>ASPP</code> 是”空间金字塔池化块”的缩写，它由多个卷积层和扩张卷积层（dilated convolution）组成。这些层用于捕捉不同感受野（receptive field）下的特征。这有助于模型更好地理解图像中的上下文信息。<code>conv1</code> 包括多个扩张卷积层，<code>bn1</code> 是相应的批归一化层，然后再通过 <code>conv2</code> 进行进一步处理。这些层通过 ReLU 激活函数进行激活。</p></li><li><p><strong>Convolution for Output (用于输出的卷积)</strong>: <code>conv_classes</code> 是用于生成最终语义分割预测的卷积层，其输出通道数等于目标类别数 <code>nbr_classes</code>。</p></li></ul><h4 id="LMSCNet（语义分割网络）"><a href="#LMSCNet（语义分割网络）" class="headerlink" title="LMSCNet（语义分割网络）"></a>LMSCNet（语义分割网络）</h4><p><code>LMSCNet</code> 类定义了整个语义分割网络，它的结构包括：</p><ul><li><p><strong>Encoder Blocks (编码块)</strong>: 这些块包括卷积层和激活函数，它们用于从输入数据提取特征。编码块分层堆叠，逐渐降低分辨率。其中 <code>Encoder_block1</code> 处理输入数据，然后 <code>Encoder_block2</code>、<code>Encoder_block3</code> 和 <code>Encoder_block4</code> 分别进行更多的特征提取。</p></li><li><p><strong>Output Scales (输出尺度)</strong>: 模型产生多个尺度的语义分割输出，包括1:8、1:4、1:2和1:1。每个输出尺度都有相应的卷积层和<code>SegmentationHead</code>。这些层用于生成不同分辨率下的语义分割预测。</p></li><li><p><strong>反卷积层 (Deconvolution Layers)</strong>: 这些层包括<code>deconv_1_8__1_2</code>、<code>deconv_1_8__1_1</code>、<code>deconv1_4</code>、<code>deconv1_2</code> 和 <code>deconv1_1</code>，用于上采样，将较低分辨率的输出转换为高分辨率。这些层用于与高分辨率的编码块特征进行连接。</p></li><li><p><strong>权重初始化和损失计算</strong>: 类中还包括了初始化权重的函数 <code>weights_initializer</code>，以及用于计算损失的函数 <code>compute_loss</code>。损失计算使用交叉熵损失，针对每个尺度的输出都会计算相应的损失。</p></li><li><p><strong>类别权重 (Class Weights)</strong>: 模型使用 <code>get_class_weights</code> 函数计算类别权重，以便处理不平衡的类别分布。</p></li><li><p><strong>其他辅助函数</strong>: 类还包括其他用于获取目标数据、设置训练尺度等的辅助函数。</p></li></ul><p>这个模型的核心思想是从低分辨率到高分辨率逐步提取特征，并生成多尺度的语义分割预测。模型的损失函数考虑了所有尺度的预测，以便综合多尺度信息来进行训练。这有助于提高语义分割任务的性能，特别是在处理不同尺度的对象时。模型的训练和验证过程通常需要提供训练数据、优化器和学习率调度器等组件。</p><h2 id="JS3CNet"><a href="#JS3CNet" class="headerlink" title="JS3CNet"></a>JS3CNet</h2><p>一种增强的联合单扫LiDAR点云语义分割方法，该方法利用学习的形状先验形式场景完成网络，即JS3C-Net</p><p>代码链接： <a href="https://github.com/yanx27/JS3C-Net">JS3CNet: Sparse Single Sweep LiDAR Point Cloud Segmentation via Learning Contextual Shape Priors from Scene Completion</a>, AAAI 2021</p><p><img src="/pic/JS3CNet.png" alt="在给定稀疏不完全单扫描点云的情况下，首先使用稀疏卷积U-Net对Fenc进行点特征编码。基于初始编码，MLP1用于生成形状嵌入（SE）FSE，该FSE与通过MLP2传递的初始编码一起流入MLP3，以生成用于点云语义分割的Fout。然后，来自SE的不完整细粒度点特征和来自语义场景完成（SSC）模块的完整体素特征流入点-体素交互（PVI）模块，以实现细化特征，最终在监督下输出完成体素。请注意，SSC和PVI模块可以在推理过程中丢弃。"></p><h3 id="代码分析"><a href="#代码分析" class="headerlink" title="代码分析"></a>代码分析</h3><p><a href="https://github.com/yanx27/JS3C-Net/blob/main/opt/JS3C_default_kitti.yaml">JS3C_default_kitti.yaml</a>这是一个JS3CNet网络的配置文件。配置文件用于定义训练和测试JS3CNet网络时的各种参数和设置。以下是关于配置文件的详细解释：</p><pre class="line-numbers language-yaml" data-language="yaml"><code class="language-yaml"><span class="token key atrule">GENERAL</span><span class="token punctuation">:</span>  <span class="token key atrule">task</span><span class="token punctuation">:</span> train  <span class="token key atrule">manual_seed</span><span class="token punctuation">:</span> <span class="token number">123</span>  <span class="token key atrule">dataset_dir</span><span class="token punctuation">:</span> /home/yxu/data/semantic_kitti/dataset/  <span class="token key atrule">debug</span><span class="token punctuation">:</span> <span class="token boolean important">False</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span></span></code></pre><ul><li><code>task</code>: 定义任务类型，这里设置为”train”表示进行训练。</li><li><code>manual_seed</code>: 随机数种子，用于确保实验的可复现性。</li><li><code>dataset_dir</code>: 数据集的目录路径。</li><li><code>debug</code>: 是否启用调试模式，设置为”False”表示不启用。</li></ul><pre class="line-numbers language-yaml" data-language="yaml"><code class="language-yaml"><span class="token key atrule">DATA</span><span class="token punctuation">:</span>  <span class="token key atrule">dataset</span><span class="token punctuation">:</span> SemanticKITTI  <span class="token key atrule">classes_seg</span><span class="token punctuation">:</span> <span class="token number">19</span>  <span class="token key atrule">classes_completion</span><span class="token punctuation">:</span> <span class="token number">20</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span></span></code></pre><ul><li><code>dataset</code>: 数据集的名称，这里使用SemanticKITTI数据集。</li><li><code>classes_seg</code>: 语义分割任务的类别数，这里设置为19。</li><li><code>classes_completion</code>: 完备性任务的类别数，这里设置为20。</li></ul><pre class="line-numbers language-yaml" data-language="yaml"><code class="language-yaml"><span class="token key atrule">Segmentation</span><span class="token punctuation">:</span>  <span class="token key atrule">model_name</span><span class="token punctuation">:</span> SubSparseConv  <span class="token key atrule">m</span><span class="token punctuation">:</span> <span class="token number">16</span>  <span class="token key atrule">block_residual</span><span class="token punctuation">:</span> <span class="token boolean important">False</span>  <span class="token key atrule">block_reps</span><span class="token punctuation">:</span> <span class="token number">1</span>  <span class="token key atrule">seg_groups</span><span class="token punctuation">:</span> <span class="token number">1</span>  <span class="token key atrule">use_coords</span><span class="token punctuation">:</span> <span class="token boolean important">False</span>  <span class="token key atrule">feature_dims</span><span class="token punctuation">:</span> <span class="token punctuation">[</span><span class="token number">16</span><span class="token punctuation">,</span><span class="token number">32</span><span class="token punctuation">,</span><span class="token number">48</span><span class="token punctuation">,</span><span class="token number">64</span><span class="token punctuation">,</span><span class="token number">80</span><span class="token punctuation">,</span><span class="token number">96</span><span class="token punctuation">,</span><span class="token number">112</span><span class="token punctuation">]</span>  <span class="token key atrule">input_channel</span><span class="token punctuation">:</span> <span class="token number">3</span>  <span class="token key atrule">scale</span><span class="token punctuation">:</span> <span class="token number">10</span>  <span class="token key atrule">full_scale</span><span class="token punctuation">:</span> <span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">2048</span><span class="token punctuation">]</span>  <span class="token key atrule">max_npoint</span><span class="token punctuation">:</span> <span class="token number">250000</span>  <span class="token key atrule">mode</span><span class="token punctuation">:</span> <span class="token number">4</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><ul><li><code>Segmentation</code>部分包含了与语义分割任务相关的参数设置。</li><li><code>model_name</code>: 语义分割模型的名称，这里使用SubSparseConv。</li><li><code>m</code>: SubSparseConv模型的参数，这里设置为16。</li><li><code>block_residual</code>: 是否使用块残差，设置为”False”表示不使用。</li><li><code>block_reps</code>: 块的重复次数，这里设置为1。</li><li><code>seg_groups</code>: 分割组的数量，这里设置为1。</li><li><code>use_coords</code>: 是否使用坐标信息，设置为”False”表示不使用。</li><li><code>feature_dims</code>: 特征维度的设置。</li><li><code>input_channel</code>: 输入通道的数量，这里设置为3。</li><li><code>scale</code>: 数据集的尺度。</li><li><code>full_scale</code>: 数据集的完全尺度范围。</li><li><code>max_npoint</code>: 最大点数，这里设置为250,000。</li><li><code>mode</code>: 模式设置，这里设置为4。</li></ul><pre class="line-numbers language-yaml" data-language="yaml"><code class="language-yaml"><span class="token key atrule">Completion</span><span class="token punctuation">:</span>  <span class="token key atrule">model_name</span><span class="token punctuation">:</span> SSCNet  <span class="token key atrule">m</span><span class="token punctuation">:</span> <span class="token number">32</span>  <span class="token key atrule">feeding</span><span class="token punctuation">:</span> both  <span class="token key atrule">no_fuse_feat</span><span class="token punctuation">:</span> <span class="token boolean important">False</span>  <span class="token key atrule">block_residual</span><span class="token punctuation">:</span> <span class="token boolean important">True</span>  <span class="token key atrule">block_reps</span><span class="token punctuation">:</span> <span class="token number">2</span>  <span class="token key atrule">use_coords</span><span class="token punctuation">:</span> <span class="token boolean important">False</span>  <span class="token key atrule">mode</span><span class="token punctuation">:</span> <span class="token number">0</span>  <span class="token key atrule">full_scale</span><span class="token punctuation">:</span> <span class="token punctuation">[</span><span class="token number">256</span><span class="token punctuation">,</span> <span class="token number">256</span><span class="token punctuation">,</span> <span class="token number">32</span><span class="token punctuation">]</span>  <span class="token key atrule">interaction</span><span class="token punctuation">:</span> <span class="token boolean important">True</span>  <span class="token key atrule">pooling_type</span><span class="token punctuation">:</span> mean  <span class="token key atrule">fuse_k</span><span class="token punctuation">:</span> <span class="token number">5</span>  <span class="token key atrule">point_cloud_range</span><span class="token punctuation">:</span> <span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">-25.6</span><span class="token punctuation">,</span> <span class="token number">-2</span><span class="token punctuation">,</span> <span class="token number">51.2</span><span class="token punctuation">,</span> <span class="token number">25.6</span><span class="token punctuation">,</span> <span class="token number">4.4</span><span class="token punctuation">]</span>  <span class="token key atrule">voxel_size</span><span class="token punctuation">:</span> <span class="token number">0.2</span>  <span class="token key atrule">search_k</span><span class="token punctuation">:</span> <span class="token number">8</span>  <span class="token key atrule">feat_relation</span><span class="token punctuation">:</span> <span class="token boolean important">False</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><ul><li><code>Completion</code>部分包含了与完备性任务相关的参数设置。</li><li><code>model_name</code>: 完备性模型的名称，这里使用SSCNet。</li><li><code>m</code>: SSCNet模型的参数，这里设置为32。</li><li><code>feeding</code>: 输入数据的类型，这里设置为”both”，表示同时输入特征和概率。</li><li><code>no_fuse_feat</code>: 是否不融合特征，设置为”False”表示融合特征。</li><li><code>block_residual</code>: 是否使用块残差，设置为”True”表示使用。</li><li><code>block_reps</code>: 块的重复次数，这里设置为2。</li><li><code>use_coords</code>: 是否使用坐标信息，设置为”False”表示不使用。</li><li><code>mode</code>: 模式设置，这里设置为0。</li><li><code>full_scale</code>: 完全尺度范围的设置。</li><li><code>interaction</code>: 是否启用交互，设置为”True”表示启用。</li><li><code>pooling_type</code>: 池化类型，这里设置为”mean”。</li><li><code>fuse_k</code>: 融合参数k的设置。</li><li><code>point_cloud_range</code>: 点云范围的设置。</li><li><code>voxel_size</code>: 体素大小的设置。</li><li><code>search_k</code>: 搜索参数k的设置。</li><li><code>feat_relation</code>: 特征关系的设置，这里设置为”False”。</li></ul><pre class="line-numbers language-yaml" data-language="yaml"><code class="language-yaml"><span class="token key atrule">TRAIN</span><span class="token punctuation">:</span>  <span class="token key atrule">epochs</span><span class="token punctuation">:</span> <span class="token number">100</span>  <span class="token key atrule">train_workers</span><span class="token punctuation">:</span> <span class="token number">10</span>  <span class="token key atrule">optim</span><span class="token punctuation">:</span> Adam  <span class="token key atrule">batch_size</span><span class="token punctuation">:</span> <span class="token number">2</span>  <span class="token key atrule">learning_rate</span><span class="token punctuation">:</span> <span class="token number">0.001</span>  <span class="token key atrule">lr_decay</span><span class="token punctuation">:</span> <span class="token number">0.7</span>  <span class="token key atrule">decay_step</span><span class="token punctuation">:</span> <span class="token number">10</span>  <span class="token key atrule">momentum</span><span class="token punctuation">:</span> <span class="token number">0.9</span>  <span class="token key atrule">weight_decay</span><span class="token punctuation">:</span> <span class="token number">0.0001</span>  <span class="token key atrule">save_freq</span><span class="token punctuation">:</span> <span class="token number">16</span>  <span class="token key atrule">uncertainty_loss</span><span class="token punctuation">:</span> <span class="token boolean important">True</span>  <span class="token key atrule">loss_weight</span><span class="token punctuation">:</span> <span class="token punctuation">[</span><span class="token number">1.0</span><span class="token punctuation">,</span> <span class="token number">0.8</span><span class="token punctuation">]</span>  <span class="token key atrule">pretrain_path</span><span class="token punctuation">:</span>  <span class="token key atrule">train_from</span><span class="token punctuation">:</span> <span class="token number">0</span>  <span class="token key atrule">seg_num_per_class</span><span class="token punctuation">:</span> <span class="token punctuation">[</span><span class="token number">55437630</span><span class="token punctuation">,</span> <span class="token number">320797</span><span class="token punctuation">,</span> <span class="token number">541736</span><span class="token punctuation">,</span> <span class="token number">2578735</span><span class="token punctuation">,</span> <span class="token number">3274484</span><span class="token punctuation">,</span> <span class="token number">552662</span><span class="token punctuation">,</span> <span class="token number">184064</span><span class="token punctuation">,</span> <span class="token number">78858</span><span class="token punctuation">,</span> <span class="token number">240942562</span><span class="token punctuation">,</span> <span class="token number">17294618</span><span class="token punctuation">,</span> <span class="token number">170599734</span><span class="token punctuation">,</span> <span class="token number">6369672</span><span class="token punctuation">,</span> <span class="token number">230413074</span><span class="token punctuation">,</span> <span class="token number">101130274</span><span class="token punctuation">,</span> <span class="token number">476491114</span><span class="token punctuation">,</span> <span class="token number">9833174</span><span class="token punctuation">,</span> <span class="token number">129609852</span><span class="token punctuation">,</span> <span class="token number">4506626</span><span class="token punctuation">,</span> <span class="token number">1168181</span><span class="token punctuation">]</span>  <span class="token key atrule">complt_num_per_class</span><span class="token punctuation">:</span> <span class="token punctuation">[</span><span class="token number">7632350044</span><span class="token punctuation">,</span> <span class="token number">15783539</span><span class="token punctuation">,</span>  <span class="token number">125136</span><span class="token punctuation">,</span> <span class="token number">118809</span><span class="token punctuation">,</span> <span class="token number">646799</span><span class="token punctuation">,</span> <span class="token number">821951</span><span class="token punctuation">,</span> <span class="token number">262978</span><span class="token punctuation">,</span> <span class="token number">283696</span><span class="token punctuation">,</span> <span class="token number">204750</span><span class="token punctuation">,</span> <span class="token number">61688703</span><span class="token punctuation">,</span> <span class="token number">4502961</span><span class="token punctuation">,</span> <span class="token number">44883650</span><span class="token punctuation">,</span> <span class="token number">2269923</span><span class="token punctuation">,</span> <span class="token number">56840218</span><span class="token punctuation">,</span> <span class="token number">15719652</span><span class="token punctuation">,</span> <span class="token number">158442623</span><span class="token punctuation">,</span> <span class="token number">2061623</span><span class="token punctuation">,</span> <span class="token number">36970522</span><span class="token punctuation">,</span> <span class="token number">1151988</span><span class="token punctuation">,</span> <span class="token number">334146</span><span class="token punctuation">]</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><ul><li><code>TRAIN</code>部分包含了训练相关的参数设置。</li><li><code>epochs</code>: 训练的轮数，这里设置为100。</li><li><code>train_workers</code>: 数据加载器的工作进程数，这里设置为10。</li><li><code>optim</code>: 优化器的选择，这里使用Adam。</li><li><code>batch_size</code>: 批量大小，这里设置为2。</li><li><code>learning_rate</code>: 学习率，这里设置为0.001。</li><li><code>lr_decay</code>: 学习率的衰减率，这里设置为0.7。</li><li><code>decay_step</code>: 学习率衰减的步数，这里设置为10。</li><li><code>momentum</code>: 优化器的动量，这里设置为0.9。</li><li><code>weight_decay</code>: 权重衰减，这里设置为0.0001。</li><li><code>save_freq</code>: 模型保存的频率，这里设置为16。</li><li><code>uncertainty_loss</code>: 是否启用不确定性损失，设置为”True”表示启用。</li><li><code>loss_weight</code>: 损失的权重，这里是一个列表，包括语义损失和完备性损失的权重。</li><li><code>pretrain_path</code>: 预训练模型的路径。</li><li><code>train_from</code>: 从哪一轮训练开始，这里设置为0。</li><li><code>seg_num_per_class</code>: 每个类别的语义分割样本数量。</li><li><code>complt_num_per_class</code>: 每个类别的完备性样本数量。</li></ul><p>这个配置文件包含了JS3CNet网络的各种参数和设置，用于训练和测试语义分割和完备性任务。</p><p><a href="https://github.com/yanx27/JS3C-Net/blob/main/train.py">J3SC_Net </a>是 JS3CNet 网络的核心部分，主要用于同时处理语义分割和点云完备性分割任务。它是一个继承自 PyTorch 的 <code>nn.Module</code> 的模型，下面将对其进行详细介绍：</p><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">class</span> <span class="token class-name">J3SC_Net</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> args<span class="token punctuation">)</span><span class="token punctuation">:</span>        <span class="token builtin">super</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>        self<span class="token punctuation">.</span>args <span class="token operator">=</span> args        self<span class="token punctuation">.</span>seg_head <span class="token operator">=</span> seg_model<span class="token punctuation">(</span>args<span class="token punctuation">)</span>        self<span class="token punctuation">.</span>complet_head <span class="token operator">=</span> complet_model<span class="token punctuation">(</span>args<span class="token punctuation">)</span>        self<span class="token punctuation">.</span>voxelpool <span class="token operator">=</span> model_utils<span class="token punctuation">.</span>VoxelPooling<span class="token punctuation">(</span>args<span class="token punctuation">)</span>        self<span class="token punctuation">.</span>seg_sigma <span class="token operator">=</span> nn<span class="token punctuation">.</span>Parameter<span class="token punctuation">(</span>torch<span class="token punctuation">.</span>Tensor<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">.</span>uniform_<span class="token punctuation">(</span><span class="token number">0.2</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">,</span> requires_grad<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>        self<span class="token punctuation">.</span>complet_sigma <span class="token operator">=</span> nn<span class="token punctuation">.</span>Parameter<span class="token punctuation">(</span>torch<span class="token punctuation">.</span>Tensor<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">.</span>uniform_<span class="token punctuation">(</span><span class="token number">0.2</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">,</span> requires_grad<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> x<span class="token punctuation">)</span><span class="token punctuation">:</span>        seg_inputs<span class="token punctuation">,</span> complet_inputs<span class="token punctuation">,</span> _ <span class="token operator">=</span> x        <span class="token comment"># 分割头部分</span>        seg_output<span class="token punctuation">,</span> feat <span class="token operator">=</span> self<span class="token punctuation">.</span>seg_head<span class="token punctuation">(</span>seg_inputs<span class="token punctuation">)</span>        <span class="token comment"># 完备性头部分</span>        coords <span class="token operator">=</span> complet_inputs<span class="token punctuation">[</span><span class="token string">'complet_coords'</span><span class="token punctuation">]</span>        coords <span class="token operator">=</span> coords<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span> <span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">]</span>        <span class="token keyword">if</span> args<span class="token punctuation">[</span><span class="token string">'DATA'</span><span class="token punctuation">]</span><span class="token punctuation">[</span><span class="token string">'dataset'</span><span class="token punctuation">]</span> <span class="token operator">==</span> <span class="token string">'SemanticKITTI'</span><span class="token punctuation">:</span>            coords<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">]</span> <span class="token operator">+=</span> <span class="token number">1</span>        <span class="token keyword">elif</span> args<span class="token punctuation">[</span><span class="token string">'DATA'</span><span class="token punctuation">]</span><span class="token punctuation">[</span><span class="token string">'dataset'</span><span class="token punctuation">]</span> <span class="token operator">==</span> <span class="token string">'SemanticPOSS'</span><span class="token punctuation">:</span>            coords<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">]</span><span class="token punctuation">[</span>coords<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">]</span> <span class="token operator">></span> <span class="token number">31</span><span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token number">31</span>        <span class="token keyword">if</span> args<span class="token punctuation">[</span><span class="token string">'Completion'</span><span class="token punctuation">]</span><span class="token punctuation">[</span><span class="token string">'feeding'</span><span class="token punctuation">]</span> <span class="token operator">==</span> <span class="token string">'both'</span><span class="token punctuation">:</span>            feeding <span class="token operator">=</span> torch<span class="token punctuation">.</span>cat<span class="token punctuation">(</span><span class="token punctuation">[</span>seg_output<span class="token punctuation">,</span> feat<span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span>        <span class="token keyword">elif</span> args<span class="token punctuation">[</span><span class="token string">'Completion'</span><span class="token punctuation">]</span><span class="token punctuation">[</span><span class="token string">'feeding'</span><span class="token punctuation">]</span> <span class="token operator">==</span> <span class="token string">'feat'</span><span class="token punctuation">:</span>            feeding <span class="token operator">=</span> feat        <span class="token keyword">else</span><span class="token punctuation">:</span>            feeding <span class="token operator">=</span> seg_output        <span class="token comment"># 使用Voxelpool模块进行特征池化操作</span>        features <span class="token operator">=</span> self<span class="token punctuation">.</span>voxelpool<span class="token punctuation">(</span>            invoxel_xyz<span class="token operator">=</span>complet_inputs<span class="token punctuation">[</span><span class="token string">'complet_invoxel_features'</span><span class="token punctuation">]</span><span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span> <span class="token punctuation">:</span><span class="token punctuation">,</span> <span class="token punctuation">:</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">,</span>            invoxel_map<span class="token operator">=</span>complet_inputs<span class="token punctuation">[</span><span class="token string">'complet_invoxel_features'</span><span class="token punctuation">]</span><span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span> <span class="token punctuation">:</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">.</span><span class="token builtin">long</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span>            src_feat<span class="token operator">=</span>feeding<span class="token punctuation">,</span>            voxel_center<span class="token operator">=</span>complet_inputs<span class="token punctuation">[</span><span class="token string">'voxel_centers'</span><span class="token punctuation">]</span>        <span class="token punctuation">)</span>        <span class="token keyword">if</span> self<span class="token punctuation">.</span>args<span class="token punctuation">[</span><span class="token string">'Completion'</span><span class="token punctuation">]</span><span class="token punctuation">[</span><span class="token string">'no_fuse_feat'</span><span class="token punctuation">]</span><span class="token punctuation">:</span>            features<span class="token punctuation">[</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token number">1</span>            features <span class="token operator">=</span> features<span class="token punctuation">.</span>detach<span class="token punctuation">(</span><span class="token punctuation">)</span>        <span class="token comment"># 创建SparseConvTensor，用于点云完备性分割</span>        batch_complet <span class="token operator">=</span> spconv<span class="token punctuation">.</span>SparseConvTensor<span class="token punctuation">(</span>            features<span class="token punctuation">.</span><span class="token builtin">float</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span> coords<span class="token punctuation">.</span><span class="token builtin">int</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span> args<span class="token punctuation">[</span><span class="token string">'Completion'</span><span class="token punctuation">]</span><span class="token punctuation">[</span><span class="token string">'full_scale'</span><span class="token punctuation">]</span><span class="token punctuation">,</span> args<span class="token punctuation">[</span><span class="token string">'TRAIN'</span><span class="token punctuation">]</span><span class="token punctuation">[</span><span class="token string">'batch_size'</span><span class="token punctuation">]</span>        <span class="token punctuation">)</span>        batch_complet <span class="token operator">=</span> dataset<span class="token punctuation">.</span>sparse_tensor_augmentation<span class="token punctuation">(</span>batch_complet<span class="token punctuation">,</span> complet_inputs<span class="token punctuation">[</span><span class="token string">'state'</span><span class="token punctuation">]</span><span class="token punctuation">)</span>        <span class="token comment"># 使用完备性头部分进行点云完备性分割</span>        complet_output <span class="token operator">=</span> self<span class="token punctuation">.</span>complet_head<span class="token punctuation">(</span>batch_complet<span class="token punctuation">)</span>        <span class="token keyword">return</span> seg_output<span class="token punctuation">,</span> complet_output<span class="token punctuation">,</span> <span class="token punctuation">[</span>self<span class="token punctuation">.</span>seg_sigma<span class="token punctuation">,</span> self<span class="token punctuation">.</span>complet_sigma<span class="token punctuation">]</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p><strong>主要组成部分</strong>：</p><ol><li><p><code>seg_head</code> 和 <code>complet_head</code>：这是分割头和完备性头的模型，它们通过 <code>args</code> 参数配置，用于执行分割任务和完备性任务。这些模型的初始化在 <code>__init__</code> 方法中进行。</p></li><li><p><code>voxelpool</code> 模块：用于对特征进行池化操作。根据输入参数，它将输入特征和相关的信息池化成新的特征。</p></li><li><p><code>seg_sigma</code> 和 <code>complet_sigma</code>：这是可学习的参数，分别表示语义分割和完备性分割任务的不确定性。模型在训练过程中会学习这些参数。</p></li></ol><p><strong>前向传播（<code>forward</code> 方法）</strong>：</p><p>在前向传播中，<code>J3SC_Net</code> 接受一个输入 <code>x</code>，其中 <code>x</code> 包含了分割和完备性任务的输入。首先，模型通过语义分割头 (<code>seg_head</code>) 处理分割任务的输入数据 <code>seg_inputs</code>，生成 <code>seg_output</code> 和 <code>feat</code>。</p><p>然后，对完备性任务的输入数据进行处理。首先调整坐标 <code>coords</code>，然后根据 <code>args</code> 的配置选择适当的输入数据 <code>feeding</code>。接下来，使用 <code>voxelpool</code> 模块对特征进行池化操作，生成 <code>features</code>。</p><p>最后，将生成的 <code>features</code> 传递给完备性头部分 (<code>complet_head</code>) 进行点云完备性分割。模型的输出包括语义分割结果 <code>seg_output</code> 和点云完备性分割结果 <code>complet_output</code>，以及模型学习的不确定性参数 <code>seg_sigma</code> 和 <code>complet_sigma</code>。</p><p><code>J3SC_Net</code> 模型将同时执行语义分割和点云完备性分割任务，使其成为 JS3CNet 网络的核心组件。这种多任务学习可以在处理点云数据时提高效率和性能。</p><p><code>SSCNet</code> 是一个用于点云完备性分割的神经网络模型，它是 JS3CNet 中完备性分割部分的模型。下面将详细介绍 <code>SSCNet</code> 模型的主要组成部分。</p><h4 id="SSCNet-Decoder"><a href="#SSCNet-Decoder" class="headerlink" title="SSCNet_Decoder"></a>SSCNet_Decoder</h4><p><code>SSCNet_Decoder</code> 是 <code>SSCNet</code> 的解码器部分。它主要负责将输入特征映射到点云的完备性分割结果。该解码器采用了类似 U-Net 架构的设计，分为不同的块（Block），其中包括卷积、批归一化和 ReLU 激活函数层。以下是 <code>SSCNet_Decoder</code> 中各个块的主要部分：</p><ul><li><p>Block 1：包括两个卷积层，每个卷积层后接批归一化和 ReLU 激活函数。该块的输出与一个残差块相加，并经过最大池化。</p></li><li><p>Block 2：与 Block 1 类似，包括两个卷积层，批归一化和 ReLU 激活函数。输出与残差块相加。</p></li><li><p>Block 3：包括两个卷积层，批归一化和 ReLU 激活函数。没有残差连接。</p></li><li><p>Block 4：包括两个卷积层，批归一化和 ReLU 激活函数。没有残差连接。</p></li><li><p>Block 5：包括两个卷积层，批归一化和 ReLU 激活函数。没有残差连接。</p></li><li><p>Prediction：该块用于生成最终的点云完备性分割结果。它包括两个卷积层，最终输出预测结果。</p></li></ul><p><code>SSCNet_Decoder</code> 的各个块逐渐提取并综合特征，最终生成点云完备性分割的预测结果。</p><h4 id="SSCNet"><a href="#SSCNet" class="headerlink" title="SSCNet"></a>SSCNet</h4><p><code>SSCNet</code> 是完备性分割网络的主要模型。它包含以下组件：</p><ul><li><p><code>Decoder</code>：前面介绍的 <code>SSCNet_Decoder</code>，用于点云完备性分割任务。它将输入的点云特征进行解码操作，最终生成点云完备性分割的预测结果。</p></li><li><p><code>upsample</code>：这是上采样模块，用于上采样点云完备性分割的结果，以便与语义分割结果相融合。</p></li><li><p><code>interaction_module</code>（可选）：如果配置中启用了交互模块（<code>args[&#39;Completion&#39;][&#39;interaction&#39;]</code> 为 <code>True</code>），则此组件用于执行点云之间的交互操作。这可以有助于改进点云完备性分割性能。</p></li></ul><p><code>SSCNet</code> 的前向传播过程首先将输入特征通过 <code>Decoder</code> 进行解码，然后根据配置选择是否执行交互操作，最后经过上采样模块生成最终的点云完备性分割结果。</p><p>这些组件一起构成了 <code>SSCNet</code> 模型，用于处理点云数据的完备性分割任务。</p><p><code>complet_head</code> 是用于点云语义完成任务的头部模块。它定义了一个 Unet 模型，用于处理 3D 点云数据。</p><h4 id="类-Unet"><a href="#类-Unet" class="headerlink" title="类 Unet"></a>类 <code>Unet</code></h4><p>这个类包含了 Unet 模型的构建和前向传播方法。以下是类 <code>Unet</code> 的详细分析：</p><ol><li><p><strong>构造方法 <code>__init__</code></strong>:</p><ul><li>这个方法接受一个配置参数 <code>config</code>，用于指定模型的各种参数和配置信息。</li><li><code>m</code> 是配置中指定的通道数。</li><li><code>input_dim</code> 是输入数据的维度，如果配置中指定了使用坐标信息 (<code>use_coords</code> 为 <code>True</code>)，则为 4，否则为 1。</li><li><code>sparseModel</code> 是一个 SparseConvNet 序列模型，它包含了 Unet 架构，包括编码器、解码器和跳跃连接部分。</li></ul></li><li><p><strong>前向传播方法 <code>forward</code></strong>:</p><ul><li>这个方法接受输入 <code>x</code>，其中 <code>x</code> 包含 <code>seg_coords</code> 和 <code>seg_features</code>。</li><li><code>x</code> 被传递给 <code>sparseModel</code>，这一步会进行 Unet 架构的前向传播。</li><li>如果在配置中启用了 <code>interaction</code>，则模型还会执行特征嵌入过程，以处理点云的交互信息。</li><li>最后，模型执行线性变换，将特征映射到类别预测空间，并返回预测结果以及特征信息。</li></ul></li></ol><h4 id="函数-Merge-tbl"><a href="#函数-Merge-tbl" class="headerlink" title="函数 Merge(tbl)"></a>函数 <code>Merge(tbl)</code></h4><p>这个函数是用于处理训练数据的工具函数，它接受一个包含多个样本数据的列表 <code>tbl</code>。</p><ol><li>函数通过迭代遍历每个样本数据，并从每个样本中提取出所需的数据，分别处理分割和完成任务的部分。</li><li>分割部分的数据包括坐标信息、标签和特征。</li><li>完成部分的数据包括点云坐标、输入数据、体素中心坐标、有效性信息、标签、统计信息以及点云体素特征。</li><li>这些数据被整理成适合输入到模型的格式，并以字典形式返回，包括 <code>seg_inputs</code> 和 <code>complet_inputs</code>。</li><li>还返回了一个包含样本文件名的列表，用于后续的分析和处理。</li></ol><p>总之，<code>complet_head</code> 中的代码定义了一个用于点云语义完成任务的 Unet 模型，同时提供了一个数据处理函数 <code>Merge(tbl)</code>，用于将原始数据整理成适合输入模型的格式。</p><p><code>model_utils.py</code> 包含了用于模型训练和实用工具的函数和类。下面是该文件中主要部分的详细分析：</p><ol><li><p><code>checkpoint_restore(model, exp_name, use_cuda=True, train_from=0)</code>:</p><ul><li>此函数用于从检查点文件中恢复模型的参数。</li><li><code>model</code> 是模型对象，<code>exp_name</code> 是实验名称，<code>use_cuda</code> 是是否使用 CUDA 运行模型，<code>train_from</code> 是指定的训练起始时期。</li><li>函数会查找以 “.pth” 结尾的模型检查点文件，找到最新的模型检查点并加载到模型中。</li><li>如果指定了 <code>train_from</code> 大于0，会从该时期开始训练。</li><li>函数返回模型训练的当前时期。</li></ul></li><li><p><code>VoxelPooling</code>:</p><ul><li>这是一个用于点云特征池化的自定义模块。</li><li>通过此模块，可以将点云体素汇聚为单一特征。</li><li>模块会对点云体素进行池化操作，并考虑了点云体素之间的关系。</li></ul></li><li><p><code>Loss</code>:</p><ul><li>这是一个自定义的损失函数模块，用于计算训练中的损失。</li><li>它计算分割损失和完成损失，支持带权重的损失计算。</li><li>如果 <code>uncertainty_loss</code> 被启用，还会计算损失的不确定性。</li></ul></li><li><p><code>interaction_module</code>:</p><ul><li>这是一个模块，用于模型中的点云交互。</li><li>它允许点云之间的特征交互，可以选择使用特征关系或直接的插值方法。</li><li>此模块通过模型的前向传播执行点云特征的交互操作。</li></ul></li><li><p><code>ResidualBlock</code>, <code>VGGBlock</code>, <code>UBlock</code>:</p><ul><li>这些模块用于构建模型的特定类型的块，如残差块、VGG 块和 U 块。</li><li>这些块用于构建点云处理模型的不同组件。</li></ul></li><li><p>其他工具函数:</p><ul><li>文件中还包括了其他用于文件读写、坐标转换、特征提取和其他辅助功能的函数。</li></ul></li></ol><p>总之，<code>model_utils.py</code> 包含了模型训练中的损失函数、模型恢复函数、自定义模块和工具函数，这些组成部分用于构建和训练点云处理模型。</p><h2 id="MonoScene"><a href="#MonoScene" class="headerlink" title="MonoScene"></a>MonoScene</h2><p>一种能在室内与室外场景均可使用的单目 SSC 方案：</p><ul><li>提出一种将 2D 特征投影到 3D 的方法： FLoSP </li><li>提出一种  3D Context Relation Prior （3D CRP） 提升网络的上下文意识</li><li>新的SSC损失函数，用于优化场景类亲和力（affinity）和局部锥体（frustum）比例</li></ul><p>代码链接： <a href="https://github.com/astra-vision/MonoScene">MonoScene: Monocular 3D Semantic Scene Completion</a>, CVPR 2022</p><p><img src="/pic/MonoScence.png"></p><p>Monoscene  网络流程</p><ul><li>输入 2d 图片，经过 2d unet 提取多层次的特征</li><li>Features Line of Sight Projection module (FLoSP) 用于将 2d 特征提升到 3d 位置上，增强信息流并实现2D-3D分离</li><li>3D Context Relation Prior （3D CRP） 用于增强长距离的上下文信息提取</li><li>loss 优化：Scene-Class Affinity Loss：提升类内和类间的场景方面度量；Frustum Proportion Loss：对齐局部截头体中的类分布，提供超越场景遮挡的监督信息</li></ul><p>网络结构</p><ul><li>2D unet：EfficientNetB7 用于提取图像特征</li><li>3D UNets：2层 encoder-decoder 结构，用于提取 3d 特征</li><li>completion head：3D ASPP 结构和 softmax 层，用于处理 3D UNet 输出得到3d场景 completion 结果</li></ul><h3 id="代码介绍"><a href="#代码介绍" class="headerlink" title="代码介绍"></a>代码介绍</h3><p><a href="https://github.com/astra-vision/MonoScene/blob/master/monoscene/models/monoscene.py">monoscene.py</a>使用PyTorch Lightning构建的一个名为<code>MonoScene</code>的模型类，用于单目深度估计和场景分割的任务。以下是该类的主要组件和功能的详细解释：</p><ol><li><p><strong>初始化方法</strong> (<code>__init__</code>)：这是类的构造方法，在创建<code>MonoScene</code>对象时被调用。它接受多个参数，包括类别数 (<code>n_classes</code>)、类别名称 (<code>class_names</code>)、特征数量 (<code>feature</code>)、类别权重 (<code>class_weights</code>) 等，用于初始化模型的各种参数和组件。其中的许多参数用于配置损失函数和训练过程中的各种选项。这里还创建了一个UNet 2D模型和多个FLoSP (Feature Learning from Single Point) 模型，用于3D场景估计。</p></li><li><p><strong>前向传播方法</strong> (<code>forward</code>)：<code>forward</code> 方法定义了如何计算模型的前向传播。它接受一个输入批次 (<code>batch</code>)，包括图像 (<code>img</code>)，并根据输入数据的尺寸进行前向传播计算。首先，通过UNet 2D模型 (<code>net_rgb</code>) 处理输入图像，然后根据尺度变化对图像进行3D投影，最终由UNet 3D模型 (<code>net_3d_decoder</code>) 进行3D场景估计。前向传播的结果保存在<code>out</code>变量中。</p></li><li><p><strong><code>step</code> 方法</strong>：<code>step</code> 方法执行了训练、验证和测试过程中的通用逻辑。它接受一个数据批次 (<code>batch</code>)、一个步骤类型 (<code>step_type</code>) 和一个度量指标 (<code>metric</code>)。在该方法中，对输入数据进行前向传播，计算并更新损失，同时也更新度量指标。这部分逻辑用于在训练、验证和测试时共享相同的操作。</p></li><li><p><strong>训练步骤</strong> (<code>training_step</code>)：<code>training_step</code> 方法是PyTorch Lightning中定义训练步骤的函数。它调用了<code>step</code> 方法，以及指定了步骤类型 (“train”) 和训练度量指标 (<code>self.train_metrics</code>)。</p></li><li><p><strong>验证步骤</strong> (<code>validation_step</code>) 和 <strong>验证结束方法</strong> (<code>validation_epoch_end</code>)：这两个方法用于在验证集上进行评估。<code>validation_step</code> 方法执行一个验证步骤，计算损失和更新验证度量指标 (<code>self.val_metrics</code>)。<code>validation_epoch_end</code> 方法在每个验证周期结束后，从度量指标中获取评估结果并进行记录。</p></li><li><p><strong>测试步骤</strong> (<code>test_step</code>) 和 <strong>测试结束方法</strong> (<code>test_epoch_end</code>)：这两个方法用于在测试集上进行评估，与验证过程类似。</p></li><li><p><strong>配置优化器</strong> (<code>configure_optimizers</code>)：在此方法中配置了模型的优化器和学习率调度器。根据数据集的不同（NYU或KITTI），选择了不同的优化器和学习率调度策略。</p></li><li><p><strong>指标记录和日志输出</strong>：在不同的步骤中，通过<code>self.log</code> 方法记录和输出训练、验证和测试的损失、IoU、精确度等度量指标。这些度量指标在每个epoch结束时会被重置，以便记录下一个epoch的结果。</p></li></ol><p>这段代码使用了PyTorch Lightning来规范化训练和评估流程，提供了清晰的组织结构，以及易于扩展和修改的接口，使其更容易适应不同的任务和数据集。根据您的需求，可以调整和扩展这个类来满足您的具体任务。</p><p>让我们逐行解释<code>MonoScene</code>类中的代码：</p><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">import</span> pytorch_lightning <span class="token keyword">as</span> pl<span class="token keyword">import</span> torch<span class="token keyword">import</span> torch<span class="token punctuation">.</span>nn <span class="token keyword">as</span> nn<span class="token keyword">from</span> monoscene<span class="token punctuation">.</span>models<span class="token punctuation">.</span>unet3d_nyu <span class="token keyword">import</span> UNet3D <span class="token keyword">as</span> UNet3DNYU<span class="token keyword">from</span> monoscene<span class="token punctuation">.</span>models<span class="token punctuation">.</span>unet3d_kitti <span class="token keyword">import</span> UNet3D <span class="token keyword">as</span> UNet3DKitti<span class="token keyword">from</span> monoscene<span class="token punctuation">.</span>loss<span class="token punctuation">.</span>sscMetrics <span class="token keyword">import</span> SSCMetrics<span class="token keyword">from</span> monoscene<span class="token punctuation">.</span>loss<span class="token punctuation">.</span>ssc_loss <span class="token keyword">import</span> sem_scal_loss<span class="token punctuation">,</span> CE_ssc_loss<span class="token punctuation">,</span> KL_sep<span class="token punctuation">,</span> geo_scal_loss<span class="token keyword">from</span> monoscene<span class="token punctuation">.</span>models<span class="token punctuation">.</span>flosp <span class="token keyword">import</span> FLoSP<span class="token keyword">from</span> monoscene<span class="token punctuation">.</span>loss<span class="token punctuation">.</span>CRP_loss <span class="token keyword">import</span> compute_super_CP_multilabel_loss<span class="token keyword">import</span> numpy <span class="token keyword">as</span> np<span class="token keyword">import</span> torch<span class="token punctuation">.</span>nn<span class="token punctuation">.</span>functional <span class="token keyword">as</span> F<span class="token keyword">from</span> monoscene<span class="token punctuation">.</span>models<span class="token punctuation">.</span>unet2d <span class="token keyword">import</span> UNet2D<span class="token keyword">from</span> torch<span class="token punctuation">.</span>optim<span class="token punctuation">.</span>lr_scheduler <span class="token keyword">import</span> MultiStepLR<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>这些行导入所需的Python库和模块，包括PyTorch、PyTorch Lightning、模型类、损失函数和其他依赖项。</p><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">class</span> <span class="token class-name">MonoScene</span><span class="token punctuation">(</span>pl<span class="token punctuation">.</span>LightningModule<span class="token punctuation">)</span><span class="token punctuation">:</span><span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>这一行定义了一个名为<code>MonoScene</code>的PyTorch Lightning模型类，它继承自<code>pl.LightningModule</code>，这是PyTorch Lightning中定义自定义模型的基类。</p><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>    self<span class="token punctuation">,</span>    n_classes<span class="token punctuation">,</span>    class_names<span class="token punctuation">,</span>    feature<span class="token punctuation">,</span>    class_weights<span class="token punctuation">,</span>    project_scale<span class="token punctuation">,</span>    full_scene_size<span class="token punctuation">,</span>    dataset<span class="token punctuation">,</span>    n_relations<span class="token operator">=</span><span class="token number">4</span><span class="token punctuation">,</span>    context_prior<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span>    fp_loss<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span>    project_res<span class="token operator">=</span><span class="token punctuation">[</span><span class="token punctuation">]</span><span class="token punctuation">,</span>    frustum_size<span class="token operator">=</span><span class="token number">4</span><span class="token punctuation">,</span>    relation_loss<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">,</span>    CE_ssc_loss<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span>    geo_scal_loss<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span>    sem_scal_loss<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span>    lr<span class="token operator">=</span><span class="token number">1e-4</span><span class="token punctuation">,</span>    weight_decay<span class="token operator">=</span><span class="token number">1e-4</span><span class="token punctuation">,</span><span class="token punctuation">)</span><span class="token punctuation">:</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>这是<code>MonoScene</code>类的构造方法 (<code>__init__</code>)。它接受许多参数，这些参数用于配置模型的不同方面，如类别数、类别名称、特征数量、损失权重、训练选项等。这些参数被用于初始化模型的各个组件和超参数。</p><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token builtin">super</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>__init<span class="token punctuation">(</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>这一行调用了基类的构造方法，即<code>pl.LightningModule</code>的构造方法。</p><pre class="line-numbers language-python" data-language="python"><code class="language-python">self<span class="token punctuation">.</span>project_res <span class="token operator">=</span> project_resself<span class="token punctuation">.</span>fp_loss <span class="token operator">=</span> fp_lossself<span class="token punctuation">.</span>dataset <span class="token operator">=</span> datasetself<span class="token punctuation">.</span>context_prior <span class="token operator">=</span> context_priorself<span class="token punctuation">.</span>frustum_size <span class="token operator">=</span> frustum_sizeself<span class="token punctuation">.</span>class_names <span class="token operator">=</span> class_namesself<span class="token punctuation">.</span>relation_loss <span class="token operator">=</span> relation_lossself<span class="token punctuation">.</span>CE_ssc_loss <span class="token operator">=</span> CE_ssc_lossself<span class="token punctuation">.</span>sem_scal_loss <span class="token operator">=</span> sem_scal_lossself<span class="token punctuation">.</span>geo_scal_loss <span class="token operator">=</span> geo_scal_lossself<span class="token punctuation">.</span>project_scale <span class="token operator">=</span> project_scaleself<span class="token punctuation">.</span>class_weights <span class="token operator">=</span> class_weightsself<span class="token punctuation">.</span>lr <span class="token operator">=</span> lrself<span class="token punctuation">.</span>weight_decay <span class="token operator">=</span> weight_decay<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>这些行将构造方法中传入的参数赋值给对象的属性，以便它们可以在整个类中使用。</p><pre class="line-numbers language-python" data-language="python"><code class="language-python">self<span class="token punctuation">.</span>projects <span class="token operator">=</span> <span class="token punctuation">&#123;</span><span class="token punctuation">&#125;</span>self<span class="token punctuation">.</span>scale_2ds <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">4</span><span class="token punctuation">,</span> <span class="token number">8</span><span class="token punctuation">]</span>  <span class="token comment"># 2D scales</span><span class="token keyword">for</span> scale_2d <span class="token keyword">in</span> self<span class="token punctuation">.</span>scale_2ds<span class="token punctuation">:</span>    self<span class="token punctuation">.</span>projects<span class="token punctuation">[</span><span class="token builtin">str</span><span class="token punctuation">(</span>scale_2d<span class="token punctuation">)</span><span class="token punctuation">]</span> <span class="token operator">=</span> FLoSP<span class="token punctuation">(</span>        full_scene_size<span class="token punctuation">,</span> project_scale<span class="token operator">=</span>self<span class="token punctuation">.</span>project_scale<span class="token punctuation">,</span> dataset<span class="token operator">=</span>self<span class="token punctuation">.</span>dataset    <span class="token punctuation">)</span>self<span class="token punctuation">.</span>projects <span class="token operator">=</span> nn<span class="token punctuation">.</span>ModuleDict<span class="token punctuation">(</span>self<span class="token punctuation">.</span>projects<span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>这一部分初始化了模型的<code>projects</code>属性，其中包含多个FLoSP模型，用于将2D特征投影到3D场景。不同的投影比例 (<code>scale_2d</code>) 在这里被迭代，为每个比例创建一个FLoSP模型，并将它们存储在<code>projects</code>字典中。</p><pre class="line-numbers language-python" data-language="python"><code class="language-python">self<span class="token punctuation">.</span>n_classes <span class="token operator">=</span> n_classes<span class="token keyword">if</span> self<span class="token punctuation">.</span>dataset <span class="token operator">==</span> <span class="token string">"NYU"</span><span class="token punctuation">:</span>    self<span class="token punctuation">.</span>net_3d_decoder <span class="token operator">=</span> UNet3DNYU<span class="token punctuation">(</span>        self<span class="token punctuation">.</span>n_classes<span class="token punctuation">,</span>        nn<span class="token punctuation">.</span>BatchNorm3d<span class="token punctuation">,</span>        n_relations<span class="token operator">=</span>n_relations<span class="token punctuation">,</span>        feature<span class="token operator">=</span>feature<span class="token punctuation">,</span>        full_scene_size<span class="token operator">=</span>full_scene_size<span class="token punctuation">,</span>        context_prior<span class="token operator">=</span>context_prior<span class="token punctuation">,</span>    <span class="token punctuation">)</span><span class="token keyword">elif</span> self<span class="token punctuation">.</span>dataset <span class="token operator">==</span> <span class="token string">"kitti"</span><span class="token punctuation">:</span>    self<span class="token punctuation">.</span>net_3d_decoder <span class="token operator">=</span> UNet3DKitti<span class="token punctuation">(</span>        self<span class="token punctuation">.</span>n_classes<span class="token punctuation">,</span>        nn<span class="token punctuation">.</span>BatchNorm3d<span class="token punctuation">,</span>        project_scale<span class="token operator">=</span>project_scale<span class="token punctuation">,</span>        feature<span class="token operator">=</span>feature<span class="token punctuation">,</span>        full_scene_size<span class="token operator">=</span>full_scene_size<span class="token punctuation">,</span>        context_prior<span class="token operator">=</span>context_prior<span class="token punctuation">,</span>    <span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>这一部分初始化了模型的3D解码器 (<code>net_3d_decoder</code>)，根据数据集的类型 (“NYU” 或 “kitti”) 选择不同的UNet 3D模型。该解码器将用于对3D场景进行估计。</p><pre class="line-numbers language-python" data-language="python"><code class="language-python">self<span class="token punctuation">.</span>net_rgb <span class="token operator">=</span> UNet2D<span class="token punctuation">.</span>build<span class="token punctuation">(</span>out_feature<span class="token operator">=</span>feature<span class="token punctuation">,</span> use_decoder<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>这一行创建了模型的2D UNet模型 (<code>net_rgb</code>)，该模型用于处理输入图像，其中的<code>feature</code>参数指定了模型的特征数量。</p><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token comment"># log hyperparameters</span>self<span class="token punctuation">.</span>save_hyperparameters<span class="token punctuation">(</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><p>这一行记录了模型的超参数，以便后续可以查看它们。</p><pre class="line-numbers language-python" data-language="python"><code class="language-python">self<span class="token punctuation">.</span>train_metrics <span class="token operator">=</span> SSCMetrics<span class="token punctuation">(</span>self<span class="token punctuation">.</span>n_classes<span class="token punctuation">)</span>self<span class="token punctuation">.</span>val_metrics <span class="token operator">=</span> SSCMetrics<span class="token punctuation">(</span>self<span class="token punctuation">.</span>n_classes<span class="token punctuation">)</span>self<span class="token punctuation">.</span>test_metrics <span class="token operator">=</span> SSCMetrics<span class="token punctuation">(</span>self<span class="token punctuation">.</span>n_classes<span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre><p>这些行初始化了训练、验证和测试度量指标 (<code>train_metrics</code>, <code>val_metrics</code>, <code>test_metrics</code>)，用于跟踪模型性能。</p><p>这只是构造方法的设置部分。整个<code>MonoScene</code>类的构造方法用于初始化模型的各个组件和超参数。在下面的部分中，将解释类中的其他方法。</p><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> batch<span class="token punctuation">)</span><span class="token punctuation">:</span>    img <span class="token operator">=</span> batch<span class="token punctuation">[</span><span class="token string">"img"</span><span class="token punctuation">]</span>    bs <span class="token operator">=</span> <span class="token builtin">len</span><span class="token punctuation">(</span>img<span class="token punctuation">)</span>    out <span class="token operator">=</span> <span class="token punctuation">&#123;</span><span class="token punctuation">&#125;</span>    x_rgb <span class="token operator">=</span> self<span class="token punctuation">.</span>net_rgb<span class="token punctuation">(</span>img<span class="token punctuation">)</span>    x3ds <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>    <span class="token keyword">for</span> i <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span>bs<span class="token punctuation">)</span><span class="token punctuation">:</span>        x3d <span class="token operator">=</span> <span class="token boolean">None</span>        <span class="token keyword">for</span> scale_2d <span class="token keyword">in</span> self<span class="token punctuation">.</span>project_res<span class="token punctuation">:</span>            <span class="token comment"># project features at each 2D scale to target 3D scale</span>            scale_2d <span class="token operator">=</span> <span class="token builtin">int</span><span class="token punctuation">(</span>scale_2d<span class="token punctuation">)</span>            projected_pix <span class="token operator">=</span> batch<span class="token punctuation">[</span><span class="token string">"projected_pix_&#123;&#125;"</span><span class="token punctuation">.</span><span class="token builtin">format</span><span class="token punctuation">(</span>self<span class="token punctuation">.</span>project_scale<span class="token punctuation">)</span><span class="token punctuation">]</span><span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token punctuation">.</span>cuda<span class="token punctuation">(</span><span class="token punctuation">)</span>            fov_mask <span class="token operator">=</span> batch<span class="token punctuation">[</span><span class="token string">"fov_mask_&#123;&#125;"</span><span class="token punctuation">.</span><span class="token builtin">format</span><span class="token punctuation">(</span>self<span class="token punctuation">.</span>project_scale<span class="token punctuation">)</span><span class="token punctuation">]</span><span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token punctuation">.</span>cuda<span class="token punctuation">(</span><span class="token punctuation">)</span>            <span class="token keyword">if</span> x3d <span class="token keyword">is</span> <span class="token boolean">None</span><span class="token punctuation">:</span>                x3d <span class="token operator">=</span> self<span class="token punctuation">.</span>projects<span class="token punctuation">[</span><span class="token builtin">str</span><span class="token punctuation">(</span>scale_2d<span class="token punctuation">)</span><span class="token punctuation">]</span><span class="token punctuation">(</span>                    x_rgb<span class="token punctuation">[</span><span class="token string">"1_"</span> <span class="token operator">+</span> <span class="token builtin">str</span><span class="token punctuation">(</span>scale_2d<span class="token punctuation">)</span><span class="token punctuation">]</span><span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token punctuation">,</span>                    projected_pix <span class="token operator">//</span> scale_2d<span class="token punctuation">,</span>                    fov_mask<span class="token punctuation">,</span>                <span class="token punctuation">)</span>            <span class="token keyword">else</span><span class="token punctuation">:</span>                x3d <span class="token operator">+=</span> self<span class="token punctuation">.</span>projects<span class="token punctuation">[</span><span class="token builtin">str</span><span class="token punctuation">(</span>scale_2d<span class="token punctuation">)</span><span class="token punctuation">]</span><span class="token punctuation">(</span>                    x_rgb<span class="token punctuation">[</span><span class="token string">"1_"</span> <span class="token operator">+</span> <span class="token builtin">str</span><span class="token punctuation">(</span>scale_2d<span class="token punctuation">)</span><span class="token punctuation">]</span><span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token punctuation">,</span>                    projected_pix <span class="token operator">//</span> scale_2d<span class="token punctuation">,</span>                    fov_mask<span class="token punctuation">,</span>                <span class="token punctuation">)</span>        x3ds<span class="token punctuation">.</span>append<span class="token punctuation">(</span>x3d<span class="token punctuation">)</span>    input_dict <span class="token operator">=</span> <span class="token punctuation">&#123;</span><span class="token string">"x3d"</span><span class="token punctuation">:</span> torch<span class="token punctuation">.</span>stack<span class="token punctuation">(</span>x3ds<span class="token punctuation">)</span><span class="token punctuation">&#125;</span>    out <span class="token operator">=</span> self<span class="token punctuation">.</span>net_3d_decoder<span class="token punctuation">(</span>input_dict<span class="token punctuation">)</span>    <span class="token keyword">return</span> out<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>这是模型的前向传播函数 <code>forward</code>。在这个函数中，输入图像 <code>img</code> 被传递给 2D UNet 模型 <code>net_rgb</code> 以提取特征。然后，特征将根据指定的投影比例 (<code>self.project_res</code>) 投影到3D场景中。对于每个输入样本，它会循环遍历不同的投影比例，并调用FLoSP模型 (<code>self.projects</code>) 来执行特征投影。最终，投影的特征将被传递给3D解码器 <code>net_3d_decoder</code> 进行3D场景估计。</p><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">def</span> <span class="token function">step</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> batch<span class="token punctuation">,</span> step_type<span class="token punctuation">,</span> metric<span class="token punctuation">)</span><span class="token punctuation">:</span>    bs <span class="token operator">=</span> <span class="token builtin">len</span><span class="token punctuation">(</span>batch<span class="token punctuation">[</span><span class="token string">"img"</span><span class="token punctuation">]</span><span class="token punctuation">)</span>    loss <span class="token operator">=</span> <span class="token number">0</span>    out_dict <span class="token operator">=</span> self<span class="token punctuation">(</span>batch<span class="token punctuation">)</span>    ssc_pred <span class="token operator">=</span> out_dict<span class="token punctuation">[</span><span class="token string">"ssc_logit"</span><span class="token punctuation">]</span>    target <span class="token operator">=</span> batch<span class="token punctuation">[</span><span class="token string">"target"</span><span class="token punctuation">]</span>    <span class="token keyword">if</span> self<span class="token punctuation">.</span>context_prior<span class="token punctuation">:</span>        P_logits <span class="token operator">=</span> out_dict<span class="token punctuation">[</span><span class="token string">"P_logits"</span><span class="token punctuation">]</span>        CP_mega_matrices <span class="token operator">=</span> batch<span class="token punctuation">[</span><span class="token string">"CP_mega_matrices"</span><span class="token punctuation">]</span>        <span class="token keyword">if</span> self<span class="token punctuation">.</span>relation_loss<span class="token punctuation">:</span>            loss_rel_ce <span class="token operator">=</span> compute_super_CP_multilabel_loss<span class="token punctuation">(</span>                P_logits<span class="token punctuation">,</span> CP_mega_matrices            <span class="token punctuation">)</span>            loss <span class="token operator">+=</span> loss_rel_ce            self<span class="token punctuation">.</span>log<span class="token punctuation">(</span>                step_type <span class="token operator">+</span> <span class="token string">"/loss_relation_ce_super"</span><span class="token punctuation">,</span>                loss_rel_ce<span class="token punctuation">.</span>detach<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span>                on_epoch<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span>                sync_dist<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span>            <span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>这个方法 <code>step</code> 用于执行训练、验证和测试步骤的公共逻辑。它接受批量数据 <code>batch</code>、步骤类型 <code>step_type</code>（如 “train”、”val”、”test”）和度量对象 <code>metric</code> 作为参数。在这个方法中，模型的前向传播被调用，并计算损失 <code>loss</code>。具体的损失计算依赖于模型的配置和目标。在这里，根据模型的配置，可以计算包括关系损失 (<code>loss_rel_ce</code>) 的不同损失项。如果启用了关系损失 (<code>self.relation_loss</code>)，则将计算关系损失并将其添加到总损失中。</p><pre class="line-numbers language-python" data-language="python"><code class="language-python">class_weight <span class="token operator">=</span> self<span class="token punctuation">.</span>class_weights<span class="token punctuation">.</span>type_as<span class="token punctuation">(</span>batch<span class="token punctuation">[</span><span class="token string">"img"</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token keyword">if</span> self<span class="token punctuation">.</span>CE_ssc_loss<span class="token punctuation">:</span>    loss_ssc <span class="token operator">=</span> CE_ssc_loss<span class="token punctuation">(</span>ssc_pred<span class="token punctuation">,</span> target<span class="token punctuation">,</span> class_weight<span class="token punctuation">)</span>    loss <span class="token operator">+=</span> loss_ssc    self<span class="token punctuation">.</span>log<span class="token punctuation">(</span>        step_type <span class="token operator">+</span> <span class="token string">"/loss_ssc"</span><span class="token punctuation">,</span>        loss_ssc<span class="token punctuation">.</span>detach<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span>        on_epoch<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span>        sync_dist<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span>    <span class="token punctuation">)</span><span class="token keyword">if</span> self<span class="token punctuation">.</span>sem_scal_loss<span class="token punctuation">:</span>    loss_sem_scal <span class="token operator">=</span> sem_scal_loss<span class="token punctuation">(</span>ssc_pred<span class="token punctuation">,</span> target<span class="token punctuation">)</span>    loss <span class="token operator">+=</span> loss_sem_scal    self<span class="token punctuation">.</span>log<span class="token punctuation">(</span>        step_type <span class="token operator">+</span> <span class="token string">"/loss_sem_scal"</span><span class="token punctuation">,</span>        loss_sem_scal<span class="token punctuation">.</span>detach<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span>        on_epoch<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span>        sync_dist<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span>    <span class="token punctuation">)</span><span class="token keyword">if</span> self<span class="token punctuation">.</span>geo_scal_loss<span class="token punctuation">:</span>    loss_geo_scal <span class="token operator">=</span> geo_scal_loss<span class="token punctuation">(</span>ssc_pred<span class="token punctuation">,</span> target<span class="token punctuation">)</span>    loss <span class="token operator">+=</span> loss_geo_scal    self<span class="token punctuation">.</span>log<span class="token punctuation">(</span>        step_type <span class="token operator">+</span> <span class="token string">"/loss_geo_scal"</span><span class="token punctuation">,</span>        loss_geo_scal<span class="token punctuation">.</span>detach<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span>        on_epoch<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span>        sync_dist<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span>    <span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>在这一部分中，根据模型的配置，计算了交叉熵损失 (<code>loss_ssc</code>)、语义分割损失 (<code>loss_sem_scal</code>) 和几何尺度损失 (<code>loss_geo_scal</code>)。这些损失用于对模型进行监督训练，以便它能够学习适应输入数据并进行3D场景估计。</p><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">if</span> self<span class="token punctuation">.</span>fp_loss <span class="token keyword">and</span> step_type <span class="token operator">!=</span> <span class="token string">"test"</span><span class="token punctuation">:</span>    frustums_masks <span class="token operator">=</span> torch<span class="token punctuation">.</span>stack<span class="token punctuation">(</span>batch<span class="token punctuation">[</span><span class="token string">"frustums_masks"</span><span class="token punctuation">]</span><span class="token punctuation">)</span>    frustums_class_dists <span class="token operator">=</span> torch<span class="token punctuation">.</span>stack<span class="token punctuation">(</span>        batch<span class="token punctuation">[</span><span class="token string">"frustums_class_dists"</span><span class="token punctuation">]</span>    <span class="token punctuation">)</span><span class="token punctuation">.</span><span class="token builtin">float</span><span class="token punctuation">(</span><span class="token punctuation">)</span>  <span class="token comment"># (bs, n_frustums, n_classes)</span>    n_frustums <span class="token operator">=</span> frustums_class_dists<span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span>    pred_prob <span class="token operator">=</span> F<span class="token punctuation">.</span>softmax<span class="token punctuation">(</span>ssc_pred<span class="token punctuation">,</span> dim<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span>    batch_cnt <span class="token operator">=</span> frustums_class_dists<span class="token punctuation">.</span><span class="token builtin">sum</span><span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">)</span>  <span class="token comment"># (n_frustums, n_classes)</span>    frustum_loss <span class="token operator">=</span> <span class="token number">0</span>    frustum_nonempty <span class="token operator">=</span> <span class="token number">0</span>    <span class="token keyword">for</span> frus <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span>n_frustums<span class="token punctuation">)</span><span class="token punctuation">:</span>        frustum_mask <span class="token operator">=</span> frustums_masks<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span> frus<span class="token punctuation">,</span> <span class="token punctuation">:</span><span class="token punctuation">,</span> <span class="token punctuation">:</span><span class="token punctuation">,</span> <span class="token punctuation">:</span><span class="token punctuation">]</span><span class="token punctuation">.</span>unsqueeze<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">.</span><span class="token builtin">float</span><span class="token punctuation">(</span><span class="token punctuation">)</span>        prob <span class="token operator">=</span> frustum_mask <span class="token operator">*</span> pred_prob  <span class="token comment"># bs, n_classes, H, W, D</span>        prob <span class="token operator">=</span> prob<span class="token punctuation">.</span>reshape<span class="token punctuation">(</span>bs<span class="token punctuation">,</span> self<span class="token punctuation">.</span>n_classes<span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">.</span>permute<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">)</span>        prob <span class="token operator">=</span> prob<span class="token punctuation">.</span>reshape<span class="token punctuation">(</span>self<span class="token punctuation">.</span>n_classes<span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span>        cum_prob <span class="token operator">=</span> prob<span class="token punctuation">.</span><span class="token builtin">sum</span><span class="token punctuation">(</span>dim<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span>  <span class="token comment"># n_classes</span>        total_cnt <span class="token operator">=</span> torch<span class="token punctuation">.</span><span class="token builtin">sum</span><span class="token punctuation">(</span>batch_cnt<span class="token punctuation">[</span>frus<span class="token punctuation">]</span><span class="token punctuation">)</span>        total_prob <span class="token operator">=</span> prob<span class="token punctuation">.</span><span class="token builtin">sum</span><span class="token punctuation">(</span><span class="token punctuation">)</span>        <span class="token keyword">if</span> total_prob <span class="token operator">></span> <span class="token number">0</span> <span class="token keyword">and</span> total_cnt <span class="token operator">></span> <span class="token number">0</span><span class="token punctuation">:</span>            frustum_target_proportion <span class="token operator">=</span> batch_cnt<span class="token punctuation">[</span>frus<span class="token punctuation">]</span> <span class="token operator">/</span> total_cnt            cum_prob <span class="token operator">=</span> cum_prob <span class="token operator">/</span> total_prob  <span class="token comment"># n_classes</span>            frustum_loss_i <span class="token operator">=</span> KL_sep<span class="token punctuation">(</span>cum_prob<span class="token punctuation">,</span> frustum_target_proportion<span class="token punctuation">)</span>            frustum_loss <span class="token operator">+=</span> frustum_loss_i            frustum_nonempty <span class="token operator">+=</span> <span class="token number">1</span>    frustum_loss <span class="token operator">=</span> frustum_loss <span class="token operator">/</span> frustum_nonempty    loss <span class="token operator">+=</span> frustum_loss    self<span class="token punctuation">.</span>log<span class="token punctuation">(</span>        step_type <span class="token operator">+</span> <span class="token string">"/loss_frustums"</span><span class="token punctuation">,</span>        frustum_loss<span class="token punctuation">.</span>detach<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span>        on_epoch<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span>        sync_dist<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span>    <span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>在这部分代码中，计算</p><p>了与模型输出的SSC预测和输入数据中的frustum masks 相关的损失项。如果 <code>self.fp_loss</code> 被设置为 <code>True</code> 并且步骤类型不是 “test”，则将计算frustum损失。这个损失考虑了模型对视场的理解，以及模型的预测与输入数据的关系。</p><p>最后，损失项被添加到总损失 <code>loss</code> 中，并使用 <code>self.log</code> 方法记录损失，以便在训练期间进行监控。</p><pre class="line-numbers language-python" data-language="python"><code class="language-python">y_true <span class="token operator">=</span> target<span class="token punctuation">.</span>cpu<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>numpy<span class="token punctuation">(</span><span class="token punctuation">)</span>y_pred <span class="token operator">=</span> ssc_pred<span class="token punctuation">.</span>detach<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>cpu<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>numpy<span class="token punctuation">(</span><span class="token punctuation">)</span>y_pred <span class="token operator">=</span> np<span class="token punctuation">.</span>argmax<span class="token punctuation">(</span>y_pred<span class="token punctuation">,</span> axis<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span>metric<span class="token punctuation">.</span>add_batch<span class="token punctuation">(</span>y_pred<span class="token punctuation">,</span> y_true<span class="token punctuation">)</span>self<span class="token punctuation">.</span>log<span class="token punctuation">(</span>step_type <span class="token operator">+</span> <span class="token string">"/loss"</span><span class="token punctuation">,</span> loss<span class="token punctuation">.</span>detach<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span> on_epoch<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span> sync_dist<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>在这里，真实标签和模型预测的标签用于计算模型性能指标，并将它们传递给 <code>metric</code> 对象。度量对象用于跟踪模型性能，并使用 <code>self.log</code> 方法记录损失。</p><p>这部分代码覆盖了<code>step</code> 方法中的主要逻辑，它是用于训练、验证和测试的公共代码。</p><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">def</span> <span class="token function">training_step</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> batch<span class="token punctuation">,</span> batch_idx<span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token keyword">return</span> self<span class="token punctuation">.</span>step<span class="token punctuation">(</span>batch<span class="token punctuation">,</span> <span class="token string">"train"</span><span class="token punctuation">,</span> self<span class="token punctuation">.</span>train_metrics<span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><p>这个方法是用于训练的步骤。它调用了 <code>step</code> 方法，并传递了相应的参数。在训练期间，它还返回损失以供 PyTorch Lightning 进行后续的处理。</p><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">def</span> <span class="token function">validation_step</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> batch<span class="token punctuation">,</span> batch_idx<span class="token punctuation">)</span><span class="token punctuation">:</span>    self<span class="token punctuation">.</span>step<span class="token punctuation">(</span>batch<span class="token punctuation">,</span> <span class="token string">"val"</span><span class="token punctuation">,</span> self<span class="token punctuation">.</span>val_metrics<span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><p>这个方法是验证步骤。它也调用了 <code>step</code> 方法，但不返回损失，而是将性能指标记录到验证度量对象 <code>val_metrics</code> 中。</p><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">def</span> <span class="token function">validation_epoch_end</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> outputs<span class="token punctuation">)</span><span class="token punctuation">:</span>    metric_list <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">(</span><span class="token string">"train"</span><span class="token punctuation">,</span> self<span class="token punctuation">.</span>train_metrics<span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token punctuation">(</span><span class="token string">"val"</span><span class="token punctuation">,</span> self<span class="token punctuation">.</span>val_metrics<span class="token punctuation">)</span><span class="token punctuation">]</span>    <span class="token keyword">for</span> prefix<span class="token punctuation">,</span> metric <span class="token keyword">in</span> metric_list<span class="token punctuation">:</span>        stats <span class="token operator">=</span> metric<span class="token punctuation">.</span>get_stats<span class="token punctuation">(</span><span class="token punctuation">)</span>        <span class="token keyword">for</span> i<span class="token punctuation">,</span> class_name <span class="token keyword">in</span> <span class="token builtin">enumerate</span><span class="token punctuation">(</span>self<span class="token punctuation">.</span>class_names<span class="token punctuation">)</span><span class="token punctuation">:</span>            self<span class="token punctuation">.</span>log<span class="token punctuation">(</span>                <span class="token string">"&#123;&#125;_SemIoU/&#123;&#125;"</span><span class="token punctuation">.</span><span class="token builtin">format</span><span class="token punctuation">(</span>prefix<span class="token punctuation">,</span> class_name<span class="token punctuation">)</span><span class="token punctuation">,</span>                stats<span class="token punctuation">[</span><span class="token string">"iou_ssc"</span><span class="token punctuation">]</span><span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token punctuation">,</span>                sync_dist<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span>            <span class="token punctuation">)</span>        self<span class="token punctuation">.</span>log<span class="token punctuation">(</span><span class="token string">"&#123;&#125;/mIoU"</span><span class="token punctuation">.</span><span class="token builtin">format</span><span class="token punctuation">(</span>prefix<span class="token punctuation">)</span><span class="token punctuation">,</span> stats<span class="token punctuation">[</span><span class="token string">"iou_ssc_mean"</span><span class="token punctuation">]</span><span class="token punctuation">,</span> sync_dist<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>        self<span class="token punctuation">.</span>log<span class="token punctuation">(</span><span class="token string">"&#123;&#125;/IoU"</span><span class="token punctuation">.</span><span class="token builtin">format</span><span class="token punctuation">(</span>prefix<span class="token punctuation">)</span><span class="token punctuation">,</span> stats<span class="token punctuation">[</span><span class="token string">"iou"</span><span class="token punctuation">]</span><span class="token punctuation">,</span> sync_dist<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>        self<span class="token punctuation">.</span>log<span class="token punctuation">(</span><span class="token string">"&#123;&#125;/Precision"</span><span class="token punctuation">.</span><span class="token builtin">format</span><span class="token punctuation">(</span>prefix<span class="token punctuation">)</span><span class="token punctuation">,</span> stats<span class="token punctuation">[</span><span class="token string">"recall"</span><span class="token punctuation">]</span><span class="token punctuation">,</span> sync_dist<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>        self<span class="token punctuation">.</span>log<span class="token punctuation">(</span><span class="token string">"&#123;&#125;/Recall"</span><span class="token punctuation">.</span><span class="token builtin">format</span><span class="token punctuation">(</span>prefix<span class="token punctuation">)</span><span class="token punctuation">,</span> stats<span class="token punctuation">[</span><span class="token string">"recall"</span><span class="token punctuation">]</span><span class="token punctuation">,</span> sync_dist<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>        metric<span class="token punctuation">.</span>reset<span class="token punctuation">(</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>这个方法在验证周期结束时被调用，用于总结验证阶段的性能。它计算各个类别的语义分割IoU、平均IoU、IoU和精确度，并使用 <code>self.log</code> 方法记录这些度量值。</p><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">def</span> <span class="token function">test_step</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> batch<span class="token punctuation">,</span> batch_idx<span class="token punctuation">)</span><span class="token punctuation">:</span>    self<span class="token punctuation">.</span>step<span class="token punctuation">(</span>batch<span class="token punctuation">,</span> <span class="token string">"test"</span><span class="token punctuation">,</span> self<span class="token punctuation">.</span>test_metrics<span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><p>这个方法是测试步骤，类似于验证步骤，它调用 <code>step</code> 方法并将性能指标记录到测试度量对象 <code>test_metrics</code> 中。</p><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">def</span> <span class="token function">test_epoch_end</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> outputs<span class="token punctuation">)</span><span class="token punctuation">:</span>    classes <span class="token operator">=</span> self<span class="token punctuation">.</span>class_names    metric_list <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">(</span><span class="token string">"test"</span><span class="token punctuation">,</span> self<span class="token punctuation">.</span>test_metrics<span class="token punctuation">)</span><span class="token punctuation">]</span>    <span class="token keyword">for</span> prefix<span class="token punctuation">,</span> metric <span class="token keyword">in</span> metric_list<span class="token punctuation">:</span>        <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">"&#123;&#125;======"</span><span class="token punctuation">.</span><span class="token builtin">format</span><span class="token punctuation">(</span>prefix<span class="token punctuation">)</span><span class="token punctuation">)</span>        stats <span class="token operator">=</span> metric<span class="token punctuation">.</span>get_stats<span class="token punctuation">(</span><span class="token punctuation">)</span>        <span class="token keyword">print</span><span class="token punctuation">(</span>            <span class="token string">"Precision=&#123;:.4f&#125;, Recall=&#123;:.4f&#125;, IoU=&#123;:.4f&#125;"</span><span class="token punctuation">.</span><span class="token builtin">format</span><span class="token punctuation">(</span>                stats<span class="token punctuation">[</span><span class="token string">"precision"</span><span class="token punctuation">]</span> <span class="token operator">*</span> <span class="token number">100</span><span class="token punctuation">,</span> stats<span class="token punctuation">[</span><span class="token string">"recall"</span><span class="token punctuation">]</span> <span class="token operator">*</span> <span class="token number">100</span><span class="token punctuation">,</span> stats<span class="token punctuation">[</span><span class="token string">"iou"</span><span class="token punctuation">]</span> <span class="token operator">*</span> <span class="token number">100</span>            <span class="token punctuation">)</span>        <span class="token punctuation">)</span>        <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">"class IoU: &#123;&#125;, "</span><span class="token punctuation">.</span><span class="token builtin">format</span><span class="token punctuation">(</span>classes<span class="token punctuation">)</span><span class="token punctuation">)</span>        <span class="token keyword">print</span><span class="token punctuation">(</span>            <span class="token string">" "</span><span class="token punctuation">.</span>join<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token string">"&#123;:.4f&#125;, "</span><span class="token punctuation">]</span> <span class="token operator">*</span> <span class="token builtin">len</span><span class="token punctuation">(</span>classes<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">.</span><span class="token builtin">format</span><span class="token punctuation">(</span>                <span class="token operator">*</span><span class="token punctuation">(</span>stats<span class="token punctuation">[</span><span class="token string">"iou_ssc"</span><span class="token punctuation">]</span> <span class="token operator">*</span> <span class="token number">100</span><span class="token punctuation">)</span><span class="token punctuation">.</span>tolist<span class="token punctuation">(</span><span class="token punctuation">)</span>            <span class="token punctuation">)</span>        <span class="token punctuation">)</span>        <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">"mIoU=&#123;:.4f&#125;"</span><span class="token punctuation">.</span><span class="token builtin">format</span><span class="token punctuation">(</span>stats<span class="token punctuation">[</span><span class="token string">"iou_ssc_mean"</span><span class="token punctuation">]</span> <span class="token operator">*</span> <span class="token number">100</span><span class="token punctuation">)</span><span class="token punctuation">)</span>        metric<span class="token punctuation">.</span>reset<span class="token punctuation">(</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>这个方法在测试周期结束时被调用，用于总结测试阶段的性能。它打印了精确度、召回率、IoU 和类别IoU的信息，并将这些信息显示在控制台上。</p><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">def</span> <span class="token function">configure_optimizers</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token keyword">if</span> self<span class="token punctuation">.</span>dataset <span class="token operator">==</span> <span class="token string">"NYU"</span><span class="token punctuation">:</span>        optimizer <span class="token operator">=</span> torch<span class="token punctuation">.</span>optim<span class="token punctuation">.</span>AdamW<span class="token punctuation">(</span>            self<span class="token punctuation">.</span>parameters<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span> lr<span class="token operator">=</span>self<span class="token punctuation">.</span>lr<span class="token punctuation">,</span> weight_decay<span class="token operator">=</span>self<span class="token punctuation">.</span>weight_decay        <span class="token punctuation">)</span>        scheduler <span class="token operator">=</span> MultiStepLR<span class="token punctuation">(</span>optimizer<span class="token punctuation">,</span> milestones<span class="token operator">=</span><span class="token punctuation">[</span><span class="token number">20</span><span class="token punctuation">]</span><span class="token punctuation">,</span> gamma<span class="token operator">=</span><span class="token number">0.1</span><span class="token punctuation">)</span>        <span class="token keyword">return</span> <span class="token punctuation">[</span>optimizer<span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token punctuation">[</span>scheduler<span class="token punctuation">]</span>    <span class="token keyword">elif</span> self<span class="token punctuation">.</span>dataset <span class="token operator">==</span> <span class="token string">"kitti"</span><span class="token punctuation">:</span>        optimizer <span class="token operator">=</span> torch<span class="token punctuation">.</span>optim<span class="token punctuation">.</span>AdamW<span class="token punctuation">(</span>            self<span class="token punctuation">.</span>parameters<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span> lr<span class="token operator">=</span>self<span class="token punctuation">.</span>lr<span class="token punctuation">,</span> weight_decay<span class="token operator">=</span>self<span class="token punctuation">.</span>weight_decay        <span class="token punctuation">)</span>        scheduler <span class="token operator">=</span> MultiStepLR<span class="token punctuation">(</span>optimizer<span class="token punctuation">,</span> milestones<span class="token operator">=</span><span class="token punctuation">[</span><span class="token number">20</span><span class="token punctuation">]</span><span class="token punctuation">,</span> gamma<span class="token operator">=</span><span class="token number">0.1</span><span class="token punctuation">)</span>        <span class="token keyword">return</span> <span class="token punctuation">[</span>optimizer<span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token punctuation">[</span>scheduler<span class="token punctuation">]</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>这个方法用于配置优化器和学习率调度器。根据数据集类型 (<code>self.dataset</code>)，它初始化一个AdamW优化器并将其与一个学习率调度器一起返回。调度器将在训练期间按照指定的里程碑来调整学习率。<br>这就是<code>MonoScene</code>类中的所有方法和逻辑的解释。该类代表了一个用于场景语义分割的PyTorch Lightning模型，它包括了许多用于定义前向传播、计算损失和跟踪性能的方法。</p><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">import</span> torch<span class="token keyword">import</span> torch<span class="token punctuation">.</span>nn <span class="token keyword">as</span> nn<span class="token keyword">class</span> <span class="token class-name">FLoSP</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> scene_size<span class="token punctuation">,</span> dataset<span class="token punctuation">,</span> project_scale<span class="token punctuation">)</span><span class="token punctuation">:</span>        <span class="token builtin">super</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>        self<span class="token punctuation">.</span>scene_size <span class="token operator">=</span> scene_size        self<span class="token punctuation">.</span>dataset <span class="token operator">=</span> dataset        self<span class="token punctuation">.</span>project_scale <span class="token operator">=</span> project_scale    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> x2d<span class="token punctuation">,</span> projected_pix<span class="token punctuation">,</span> fov_mask<span class="token punctuation">)</span><span class="token punctuation">:</span>        c<span class="token punctuation">,</span> h<span class="token punctuation">,</span> w <span class="token operator">=</span> x2d<span class="token punctuation">.</span>shape        src <span class="token operator">=</span> x2d<span class="token punctuation">.</span>view<span class="token punctuation">(</span>c<span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span>        zeros_vec <span class="token operator">=</span> torch<span class="token punctuation">.</span>zeros<span class="token punctuation">(</span>c<span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">.</span>type_as<span class="token punctuation">(</span>src<span class="token punctuation">)</span>        src <span class="token operator">=</span> torch<span class="token punctuation">.</span>cat<span class="token punctuation">(</span><span class="token punctuation">[</span>src<span class="token punctuation">,</span> zeros_vec<span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span>        pix_x<span class="token punctuation">,</span> pix_y <span class="token operator">=</span> projected_pix<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">,</span> projected_pix<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">]</span>        img_indices <span class="token operator">=</span> pix_y <span class="token operator">*</span> w <span class="token operator">+</span> pix_x        img_indices<span class="token punctuation">[</span><span class="token operator">~</span>fov_mask<span class="token punctuation">]</span> <span class="token operator">=</span> h <span class="token operator">*</span> w        img_indices <span class="token operator">=</span> img_indices<span class="token punctuation">.</span>expand<span class="token punctuation">(</span>c<span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">.</span><span class="token builtin">long</span><span class="token punctuation">(</span><span class="token punctuation">)</span>  <span class="token comment"># c, HWD</span>        src_feature <span class="token operator">=</span> torch<span class="token punctuation">.</span>gather<span class="token punctuation">(</span>src<span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> img_indices<span class="token punctuation">)</span>        <span class="token keyword">if</span> self<span class="token punctuation">.</span>dataset <span class="token operator">==</span> <span class="token string">"NYU"</span><span class="token punctuation">:</span>            x3d <span class="token operator">=</span> src_feature<span class="token punctuation">.</span>reshape<span class="token punctuation">(</span>                c<span class="token punctuation">,</span>                self<span class="token punctuation">.</span>scene_size<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span> <span class="token operator">//</span> self<span class="token punctuation">.</span>project_scale<span class="token punctuation">,</span>                self<span class="token punctuation">.</span>scene_size<span class="token punctuation">[</span><span class="token number">2</span><span class="token punctuation">]</span> <span class="token operator">//</span> self<span class="token punctuation">.</span>project_scale<span class="token punctuation">,</span>                self<span class="token punctuation">.</span>scene_size<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span> <span class="token operator">//</span> self<span class="token punctuation">.</span>project_scale<span class="token punctuation">,</span>            <span class="token punctuation">)</span>            x3d <span class="token operator">=</span> x3d<span class="token punctuation">.</span>permute<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">)</span>        <span class="token keyword">elif</span> self<span class="token punctuation">.</span>dataset <span class="token operator">==</span> <span class="token string">"kitti"</span><span class="token punctuation">:</span>            x3d <span class="token operator">=</span> src_feature<span class="token punctuation">.</span>reshape<span class="token punctuation">(</span>                c<span class="token punctuation">,</span>                self<span class="token punctuation">.</span>scene_size<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span> <span class="token operator">//</span> self<span class="token punctuation">.</span>project_scale<span class="token punctuation">,</span>                self<span class="token punctuation">.</span>scene_size<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span> <span class="token operator">//</span> self<span class="token punctuation">.</span>project_scale<span class="token punctuation">,</span>                self<span class="token punctuation">.</span>scene_size<span class="token punctuation">[</span><span class="token number">2</span><span class="token punctuation">]</span> <span class="token operator">//</span> self<span class="token punctuation">.</span>project_scale<span class="token punctuation">,</span>            <span class="token punctuation">)</span>        <span class="token keyword">return</span> x3d<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>这段代码定义了一个名为 <code>FLoSP</code> 的 PyTorch 模块，它用于执行特征投影操作。FLoSP 的作用是将一个 2D 图像中的特征投影到 3D 空间中。让我们逐行解释这个模块的功能和实现：</p><ol><li><p><code>class FLoSP(nn.Module):</code>：定义了一个名为 <code>FLoSP</code> 的 PyTorch 模块，它继承自 <code>nn.Module</code>。</p></li><li><p><code>def __init__(self, scene_size, dataset, project_scale):</code>：初始化方法，接受三个参数：</p><ul><li><code>scene_size</code>：表示 3D 场景的大小（宽度、高度、深度）。</li><li><code>dataset</code>：表示数据集的名称，可以是 “NYU” 或 “kitti”。</li><li><code>project_scale</code>：表示投影的尺度，通常是 2 的幂次方，用于将 2D 特征图中的像素投影到 3D 空间。</li></ul></li><li><p><code>self.scene_size = scene_size</code>：将输入的 <code>scene_size</code> 存储为对象属性，以便在模块的其他方法中使用。</p></li><li><p><code>self.dataset = dataset</code>：将输入的 <code>dataset</code> 存储为对象属性，以便在模块的其他方法中使用。</p></li><li><p><code>self.project_scale = project_scale</code>：将输入的 <code>project_scale</code> 存储为对象属性，以便在模块的其他方法中使用。</p></li><li><p><code>def forward(self, x2d, projected_pix, fov_mask):</code>：前向传播方法，用于执行特征投影操作。接受三个输入参数：</p><ul><li><code>x2d</code>：2D 特征图，是一个张量，其形状为 <code>(c, h, w)</code>，其中 <code>c</code> 表示通道数，<code>h</code> 和 <code>w</code> 表示高度和宽度。</li><li><code>projected_pix</code>：包含 2D 像素坐标的张量，其形状为 <code>(bs, 2)</code>，其中 <code>bs</code> 表示批处理大小。</li><li><code>fov_mask</code>：表示感兴趣区域的遮罩，是一个布尔值张量，形状与 <code>projected_pix</code> 相同。</li></ul></li><li><p><code>c, h, w = x2d.shape</code>：获取输入 2D 特征图 <code>x2d</code> 的通道数、高度和宽度。</p></li><li><p><code>src = x2d.view(c, -1)</code>：将 2D 特征图 <code>x2d</code> 重新形状为 <code>(c, h * w)</code>，即将每个像素的特征连接到一个向量中。</p></li><li><p><code>zeros_vec = torch.zeros(c, 1).type_as(src)</code>：创建一个与 <code>src</code> 相同数据类型的零向量，用于在特征向量后添加一个零值，以匹配 3D 坐标。</p></li><li><p><code>src = torch.cat([src, zeros_vec], 1)</code>：将零向量添加到特征向量的末尾，以将特征向量的维度从 <code>(c, h * w)</code> 扩展到 <code>(c, h * w + 1)</code>。</p></li><li><p><code>pix_x, pix_y = projected_pix[:, 0], projected_pix[:, 1]</code>：从 <code>projected_pix</code> 中提取 2D 像素坐标的 x 和 y 值。</p></li><li><p><code>img_indices = pix_y * w + pix_x</code>：计算像素坐标对应的在扁平化 2D 特征图中的索引。</p></li><li><p><code>img_indices[~fov_mask] = h * w</code>：将不在感兴趣区域内的像素坐标对应的索引设置为 2D 特征图的最大索引值（类似于超出图像范围的像素索引）。</p></li><li><p><code>img_indices = img_indices.expand(c, -1).long()</code>：将计算得到的索引扩展为 <code>(c, h * w)</code> 的长整型张量。</p></li><li><p><code>src_feature = torch.gather(src, 1, img_indices)</code>：使用计算得到的索引从 <code>src</code> 中聚合出感兴趣的像素特征。</p></li><li><p><code>if self.dataset == &quot;NYU&quot;:</code>：如果数据集是 “NYU”，则执行以下操作。否则，执行 “kitti” 分支。</p><p>a. <code>x3d = src_feature.reshape(c, self.scene_size[0] // self.project_scale, self.scene_size[2] // self.project_scale, self.scene_size[1] // self.project_scale)</code>：将特征投影到 3D 空间，根据输入的 <code>scene_size</code> 和 <code>project_scale</code> 进行重新形状。这一步是根据 NYU 数据集的特定情况进行的形状转换。</p><p>b. <code>x3d = x3d.permute(0, 1, 3, 2)</code>：对投影得到的 3D 特征进行维度的置换。</p></li><li><p><code>elif self.dataset == &quot;kitti&quot;:</code>：如果数据集是 “kitti”，则执行以下操作。</p><p>a. <code>x3d = src_feature.reshape(c, self.scene_size[0] // self.project_scale, self.scene_size[1] // self.project_scale, self.scene_size[2] // self.project_scale)</code>：将特征投影到 3D 空间，根据输入的 <code>scene_size</code> 和 <code>project_scale</code> 进行重新形状。这一步是根据 kitti 数据集的特定情况进行的形状转换。</p></li><li><p><code>return x3d</code>：返回投影到 3D 空间的特征张量 <code>x3d</code>。</p></li></ol><p>这个模块的主要功能是将 2D 特征图中的像素特征投影到 3D 空间中，以便后续在 3D 空间中进行深度学习任务。具体的投影方法和维度变换取决于数据集的类型（”NYU” 或 “kitti”）和投影尺度（<code>project_scale</code>）。这个模块通常用于处理语义分割和深度估计等任务。</p><p>unet3d_kitti.py是一个3D U-Net模型的定义，用于语义分割任务。以下是代码的逐行解释：</p><ol><li><p><code>UNet3D</code> 类继承自 <code>nn.Module</code>，用于定义3D U-Net模型。</p></li><li><p><code>__init__</code> 函数用于初始化模型，它接受一些参数，如类别数 <code>class_num</code>、标准化层 <code>norm_layer</code>、全尺寸 <code>full_scene_size</code>、特征数 <code>feature</code>、项目尺度 <code>project_scale</code>、上下文先验 <code>context_prior</code> 和 Batch Normalization 层的动量 <code>bn_momentum</code>。</p></li><li><p><code>size_l1</code>、<code>size_l2</code> 和 <code>size_l3</code> 分别表示3D U-Net的3个不同尺寸层。</p></li><li><p><code>dilations</code> 是用于处理图像的卷积核的膨胀率。</p></li><li><p><code>self.process_l1</code> 和 <code>self.process_l2</code> 定义了两个处理层，用于处理输入3D数据，包括一系列的卷积、标准化和下采样操作。</p></li><li><p><code>self.up_13_l2</code>、<code>self.up_12_l1</code> 和 <code>self.up_l1_lfull</code> 定义了上采样层，将3D数据上采样到不同尺寸的层。</p></li><li><p><code>self.ssc_head</code> 定义了用于预测语义分割的头部，包括一系列卷积和输出层。</p></li><li><p><code>self.context_prior</code> 用于确定是否使用上下文先验。如果 <code>context_prior</code> 为真，将定义 <code>self.CP_mega_voxels</code>，它是上下文先验处理的一部分。</p></li><li><p><code>forward</code> 函数接受输入数据的字典 <code>input_dict</code>，包括 <code>x3d</code>，表示3D数据。</p></li><li><p>通过一系列处理步骤，如 <code>self.process_l1</code> 和 <code>self.process_l2</code>，输入数据被处理和下采样到不同尺寸层。</p></li><li><p>如果存在上下文先验，将使用 <code>self.CP_mega_voxels</code> 处理数据。</p></li><li><p>使用上采样层 <code>self.up_13_l2</code> 和 <code>self.up_12_l1</code> 将数据上采样到不同尺寸的层。</p></li><li><p>最终，通过 <code>self.ssc_head</code> 进行语义分割预测，并将结果存储在 <code>res</code> 字典中。</p></li></ol><p>这个模型的核心部分是3D U-Net结构，它包含编码器和解码器部分，用于从3D输入数据中提取特征并生成语义分割结果。根据输入数据的尺寸和具体任务，该模型可以适应不同的上下文先验处理。</p><h2 id="StereoScene"><a href="#StereoScene" class="headerlink" title="StereoScene"></a>StereoScene</h2><p>StereoScene用于3D语义场景补全（SSC），它充分利用了轻量级相机输入而无需使用任何外部3D传感器。研究者的关键想法是利用立体匹配来解决几何模糊性问题。为了提高其在不匹配区域的鲁棒性，论文引入了鸟瞰图（BEV）表示法，以激发具有丰富上下文信息的未知区域预测能力。在立体匹配和BEV表示法的基础上，论文精心设计了一个相互交互聚合（MIA）模块，以充分发挥两者的作用，促进它们互补聚合</p><p>代码链接：<a href="https://github.com/Arlo0o/StereoScene">StereoScene: BEV-Assisted Stereo Matching Empowers 3D Semantic Scene Completion</a>, arXiv 2023.</p><p><img src="/pic/StereoScene.png"></p><p>整体的StereoScene框架如上图所示。论文遵循使用连续的2D和3 UNetsf作为backbones。给定输入的双目图像，我们使用2D UNet来提取多尺度特征。BEV潜在体积和立体几何体积分别由BEV构造器和立体构造器构造。为了充分利用这两个卷的互补潜力，提出了一个相互交互聚合模块来相互引导和聚合它们。</p><h3 id="代码解释"><a href="#代码解释" class="headerlink" title="代码解释"></a>代码解释</h3><p><a href="https://github.com/Arlo0o/StereoScene/blob/main/projects/configs/occupancy/semantickitti/stereoscene.py">stereoscene.py</a>代码是一个配置文件，它定义了一个基于 MMDetection3D 的三维计算机视觉模型的配置。以下是代码中各部分的解释：</p><ol><li><p><code>_base_</code>：这是导入配置的基础文件的地方，<code>custom_nus-3d.py</code> 和 <code>default_runtime.py</code> 包含了一些共享的配置选项。这些基础配置文件通常包括数据集设置、运行时设置等。</p></li><li><p><code>plugin</code> 和 <code>plugin_dir</code>：这两个参数用于启用和指定 MMDetection3D 插件。插件是额外的功能模块，可以扩展 MMDetection3D 的功能。</p></li><li><p><code>img_norm_cfg</code>：这是图像归一化配置，指定了均值和标准差，以便对图像进行归一化。这通常用于图像预处理。</p></li><li><p><code>class_names</code>：定义了数据集中的类别名称。在这个例子中，有 20 个类别，包括车辆、行人、道路、建筑物等。</p></li><li><p><code>point_cloud_range</code>、<code>occ_size</code> 和 <code>lss_downsample</code>：这些参数定义了点云数据的范围、体素的大小和降采样率。它们是用于处理点云数据的重要参数。</p></li><li><p><code>model</code>：这是定义模型的部分。它指定了模型的各个组件，包括图像骨干网络、头部网络、点云处理头部等。这里使用的是 <code>BEVDepthOccupancy</code> 模型，该模型在三维计算机视觉中执行深度估计和语义分割任务。</p></li><li><p><code>dataset_type</code> 和 <code>data_root</code>：这些参数定义了数据集的类型和数据集的根目录。在这个例子中，使用的是 <code>CustomSemanticKITTILssDataset</code> 数据集，数据位于 <code>./data/occupancy/semanticKITTI/RGB/</code>。</p></li><li><p><code>train_pipeline</code> 和 <code>test_pipeline</code>：这些参数定义了数据预处理和增强的步骤。它们包括图像加载、语义分割标签加载、深度图生成等。</p></li><li><p><code>input_modality</code>：定义了输入的模态，包括激光雷达、相机、雷达、地图等。在这个例子中，仅使用相机数据。</p></li><li><p><code>test_config</code> 和 <code>data</code>：这些参数定义了训练和测试数据加载的配置，包括数据预处理、数据集类型、类别、点云范围等。</p></li><li><p><code>optimizer</code> 和 <code>optimizer_config</code>：这些参数定义了优化器的类型、学习率和权重衰减等配置。</p></li><li><p><code>lr_config</code>：定义了学习率调度策略，这里使用的是阶段性学习率下降。</p></li><li><p><code>checkpoint_config</code>：用于定义模型检查点的保存和保留策略。</p></li><li><p><code>runner</code>：指定了训练的运行方式，这里使用的是按照 epoch 训练。</p></li><li><p><code>evaluation</code>：定义了模型评估的配置，包括评估间隔和保存最佳模型的规则。</p></li></ol><p>这个配置文件的目的是定义了一个用于深度估计和语义分割的三维计算机视觉模型，并指定了数据加载和训练配置。模型的结构和数据集的具体细节在其他文件中定义。这个配置文件会被传递给 MMDetection3D 的训练和测试脚本，以实际执行训练和测试任务。</p><p>这个配置文件中定义了一个名为<code>model</code>的模型，其结构和配置包括以下几个主要组件：</p><ol><li><p><code>type</code>: 这里指定了模型的类型为<code>BEVDepthOccupancy</code>，这是一个自定义的三维计算机视觉模型，用于深度估计和语义分割任务。</p></li><li><p><code>img_backbone</code>: 定义了图像骨干网络的配置，该网络用于提取图像特征。在这个配置中，使用了<code>CustomEfficientNet</code>骨干网络，具体配置包括网络的类型、预训练模型的路径等。</p></li><li><p><code>img_neck</code>: 这部分定义了图像特征的“颈部”或附加处理，用于进一步处理骨干网络提取的特征。这里使用了<code>SECONDFPN</code>结构，对特征进行了上采样和融合。</p></li><li><p><code>img_view_transformer</code>: 这是一个自定义的视图变换器，用于将图像特征映射到点云视图，包括视角的转换和体素化。</p></li><li><p><code>img_bev_encoder_backbone</code> 和 <code>img_bev_encoder_neck</code>: 定义了用于处理点云的背景信息的网络。<code>img_bev_encoder_backbone</code>用于提取点云的背景特征，<code>img_bev_encoder_neck</code>用于进一步处理这些特征。</p></li><li><p><code>pts_bbox_head</code>: 这是点云处理头部的配置，用于执行深度估计和语义分割任务。它包括输出通道数、语义分割类别数、点云范围等配置。</p></li><li><p><code>train_cfg</code> 和 <code>test_cfg</code>: 分别定义了模型的训练和测试配置，这些配置包括损失函数、评价指标等。</p></li></ol><p>这个<code>model</code>定义了整个三维计算机视觉模型的结构，包括图像骨干网络、点云处理头部等组件。模型将通过这些组件来提取和处理图像和点云数据，以执行深度估计和语义分割任务。配置文件中还包括数据加载和训练策略等，用于训练和测试这个模型。</p><p>这个<code>model</code>配置是在使用MMDetection框架时的一种标准格式。MMDetection使用Python的字典格式来组织模型的配置。下面是对这个<code>model</code>配置的格式解释：</p><ol><li><p><code>type</code>: 这个字段指定了要使用的模型的类型。在这个配置中，<code>type</code>的值是<code>BEVDepthOccupancy</code>，表示要使用名为<code>BEVDepthOccupancy</code>的自定义模型。</p></li><li><p><code>img_backbone</code>, <code>img_neck</code>, <code>img_view_transformer</code>, <code>img_bev_encoder_backbone</code>, <code>img_bev_encoder_neck</code>, <code>pts_bbox_head</code>: 这些字段用于配置模型中不同组件的设置。每个组件的配置包含了该组件的类型、参数、超参数等。</p></li><li><p><code>train_cfg</code> 和 <code>test_cfg</code>: 这些字段定义了模型的训练和测试配置，包括损失函数、评价指标等。这些配置用于指导训练和测试过程。</p></li></ol><p>整个<code>model</code>配置是一个嵌套的字典，用于描述模型的结构和超参数设置。在MMDetection框架中，模型的配置采用了类似于YAML格式的字典结构，以便灵活配置不同的模型和任务。这种配置方式使得用户可以根据自己的需求轻松地定制和修改模型的设置。</p><p><a href="https://github.com/Arlo0o/StereoScene/blob/main/projects/mmdet3d_plugin/occupancy/detectors/bevdepth_occupancy.py">bevdepth_occupancy.py</a>在上述代码中，<code>model</code> 的配置与类 <code>BEVDepthOccupancy</code> 相关，特别是与 <code>BEVDepthOccupancy</code> 类中的不同函数和组件有关。</p><p><code>BEVDepthOccupancy</code> 类是一个自定义的模型类，它继承自 <code>BEVDepth</code> 类。以下是与 <code>model</code> 配置中的组件相关的一些重要功能和配置项：</p><ol><li><p><code>image_encoder</code> 和 <code>bev_encoder</code> 函数：这些函数是模型的核心组件，分别用于处理图像信息和点云信息。<code>image_encoder</code> 用于处理输入的图像信息，而 <code>bev_encoder</code> 用于处理点云信息。这些函数将输入数据转换为特征表示。</p></li><li><p><code>forward_pts_train</code> 函数：这个函数在模型的训练过程中使用，计算了与点云相关的训练损失。这包括点云的目标检测和语义分割任务。</p></li><li><p><code>extract_img_feat</code> 函数：这个函数用于从输入的图像中提取特征。它将输入的图像分别送入 <code>image_encoder</code> 和 <code>img_view_transformer</code> 中，然后将它们的输出特征合并。</p></li><li><p><code>simple_test</code> 函数：这个函数用于在测试阶段生成模型的输出。它调用了 <code>extract_feat</code> 函数来提取特征，然后将这些特征传递给点云检测头部 <code>pts_bbox_head</code>，最后生成了测试结果。</p></li><li><p>其他与损失函数、数据预处理、评估等相关的函数：代码中还包括了与训练和测试相关的其他函数，用于处理损失函数的计算、数据的预处理以及评估模型性能等任务。</p></li></ol><p>总的来说，<code>model</code> 配置中的组件是 <code>BEVDepthOccupancy</code> 类中的一部分，用于定义模型的结构和训练&#x2F;测试过程中的操作。这些组件协同工作以实现点云目标检测和语义分割的任务。不同的组件负责处理不同类型的输入数据（图像、点云等）并生成相应的特征表示，最终用于模型的训练和测试。这种模块化的设计使得模型可以方便地扩展和配置，以适应不同的应用场景。</p><p><a href="https://github.com/Arlo0o/StereoScene/blob/main/projects/mmdet3d_plugin/occupancy/dense_heads/occhead.py">occhead.py</a>这是一个名为<code>OccHead</code>的模型头，主要用于3D语义分割任务。以下是对该头部的关键要点：</p><ol><li><p><strong>输入和输出</strong>:</p><ul><li>输入：<code>OccHead</code>头部的输入包括体素特征（<code>voxel_feats</code>）以及点云信息（<code>points</code>）。</li><li>输出：它的输出通常有两部分，一部分是用于体素级别的语义分割（<code>output_voxels</code>），另一部分是用于点云级别的语义分割（<code>output_points</code>）。</li></ul></li><li><p><strong>监督方式</strong>:</p><ul><li>该头部支持两种不同级别的监督方式：体素级别和点云级别。</li><li>体素级别监督是指根据体素特征生成语义分割结果，用于离散的体素数据。</li><li>点云级别监督是指根据点云信息生成语义分割结果，用于稠密点云数据。</li></ul></li><li><p><strong>损失函数</strong>:</p><ul><li>对于体素级别监督，支持交叉熵（<code>voxel_ce</code>）、Lovasz-Softmax（<code>voxel_lovasz</code>）、Voxel Semantic Scaling（<code>voxel_sem_scal</code>）、Voxel Geometric Scaling（<code>voxel_geo_scal</code>）、IoU损失（<code>voxel_dice</code>）等损失函数。</li><li>对于点云级别监督，支持交叉熵（<code>point_ce</code>）和Lovasz-Softmax（<code>point_lovasz</code>）损失函数。</li></ul></li><li><p><strong>学习策略</strong>:</p><ul><li>对于体素级别监督，通过卷积操作生成语义分割结果。</li><li>对于点云级别监督，采样相关的体素特征和图像特征，然后使用多层感知机（Mlp）处理这些特征生成点云级别的语义分割结果。</li></ul></li><li><p><strong>语义Kitti</strong>:</p><ul><li>该模型头部支持语义Kitti数据集，并提供了相关的损失函数和度量指标。</li></ul></li><li><p><strong>损失权重</strong>:</p><ul><li>您可以设置不同损失函数的权重，以控制它们对最终损失的贡献。</li></ul></li><li><p><strong>可选功能</strong>:</p><ul><li>该模型头部支持不同的可选功能，如软权重（<code>soft_weights</code>）和图像特征采样（<code>sampling_img_feats</code>）。</li></ul></li><li><p><strong>其他</strong>:</p><ul><li>还包括一些图像处理操作，如上采样和网格采样。</li></ul></li></ol><p>总之，<code>OccHead</code>是一个用于语义分割任务的多功能头部，支持体素级别和点云级别的监督，提供了多种损失函数和度量指标以用于不同的数据集和任务。这些选择和配置可以根据具体的任务需求进行调整和优化。</p>]]></content>
      
      
      <categories>
          
          <category> DL </category>
          
      </categories>
      
      
        <tags>
            
            <tag> ssc </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>语义场景补全（SSC）</title>
      <link href="/2023/10/29/ssc/"/>
      <url>/2023/10/29/ssc/</url>
      
        <content type="html"><![CDATA[<h1 id="语义场景补全（SSC）相关文章与数据集总结"><a href="#语义场景补全（SSC）相关文章与数据集总结" class="headerlink" title="语义场景补全（SSC）相关文章与数据集总结"></a>语义场景补全（SSC）相关文章与数据集总结</h1><p><em><strong>首先附上链接</strong></em></p><h2 id="SSC"><a href="#SSC" class="headerlink" title="SSC"></a>SSC</h2><ul><li><a href="https://github.com/Jiawei-Yao0812/NDCScene">NDC-Scene: Boost Monocular 3D Semantic Scene Completion in Normalized Device Coordinates Space</a>, ICCV 2023.</li><li><a href="https://arxiv.org/abs/2307.05873">OG: Equip vision occupancy with instance segmentation and visual grounding</a>, arXiv 2023.</li><li><a href="https://github.com/NVlabs/FB-BEV">FB-OCC: 3D Occupancy Prediction based on Forward-Backward View Transformation</a>, CVPRW 2023.</li><li><a href="https://github.com/hustvl/Symphonies">Symphonize 3D Semantic Scene Completion with Contextual Instance Queries</a>, arXiv 2023.</li><li><a href="https://arxiv.org/pdf/2305.16133.pdf">OVO: Open-Vocabulary Occupancy</a>, arXiv 2023.</li><li><a href="https://github.com/opendrivelab/occnet">OccNet: Scene as Occupancy</a>, ICCV 2023.</li><li><a href="https://astra-vision.github.io/SceneRF/">SceneRF: Self-Supervised Monocular 3D Scene Reconstruction with Radiance Fields</a>, ICCV 2023.</li><li><a href="https://fwmb.github.io/bts/">Behind the Scenes: Density Fields for Single View Reconstruction</a>, CVPR 2023.</li><li><a href="https://github.com/shurans/sscnet">Semantic Scene Completion from a Single Depth Image</a>, CVPR 2017</li><li><a href="https://github.com/astra-vision/LMSCNet">LMSCNet: Lightweight Multiscale 3D Semantic Completion</a>, 3DV 2020</li><li><a href="https://github.com/astra-vision/MonoScene">MonoScene: Monocular 3D Semantic Scene Completion</a>, CVPR 2022</li><li><a href="https://github.com/Arlo0o/StereoScene">StereoScene: BEV-Assisted Stereo Matching Empowers 3D Semantic Scene Completion</a>, arXiv 2023.</li><li><a href="https://github.com/megvii-research/OccDepth">OccDepth: A Depth-aware Method for 3D Semantic Occupancy Network</a>, arXiv 2023.</li><li><a href="https://github.com/NVlabs/VoxFormer">VoxFormer: a Cutting-edge Baseline for 3D Semantic Occupancy Prediction</a>, CVPR 2023</li><li><a href="https://github.com/wzzheng/TPVFormer">TPVFormer: An academic alternative to Tesla’s Occupancy Network</a>, CVPR2023</li><li><a href="https://github.com/zhangyp15/OccFormer">OccFormer: Dual-path Transformer for Vision-based 3D Semantic Occupancy Prediction</a>, ICCV 2023</li><li><a href="https://github.com/weiyithu/SurroundOcc">SurroundOcc: Multi-Camera 3D Occupancy Prediction for Autonomous Driving</a>, ICCV 2023</li><li><a href="https://ahayler.github.io/publications/s4c/">S4C: Self-Supervised Semantic Scene Completion with Neural Fields</a>, arXiv 2023</li><li><a href="https://arxiv.org/abs/2306.10013">PanoOcc: Unified Occupancy Representation for Camera-based 3D Panoptic Segmentation</a>, arXiv 2023.</li><li><a href="https://github.com/wzzheng/PointOcc">PointOcc: Cylindrical Tri-Perspective View for Point-based 3D Semantic Occupancy Prediction</a>, arXiv 2023.</li><li><a href="https://arxiv.org/abs/2309.09502">RenderOcc: Vision-Centric 3D Occupancy Prediction with 2D Rendering Supervision</a>, arXiv 2023.</li></ul><h2 id="相关-Dataset-x2F-Benchmark"><a href="#相关-Dataset-x2F-Benchmark" class="headerlink" title="相关 Dataset&#x2F;Benchmark"></a>相关 Dataset&#x2F;Benchmark</h2><ul><li><a href="https://arxiv.org/abs/2309.12708">PointSSC: A Cooperative Vehicle-Infrastructure Point Cloud Benchmark for Semantic Scene Completion</a>, arXiv 2023.</li><li><a href="https://github.com/Tsinghua-MARS-Lab/Occ3D">Occ3D: A Large-Scale 3D Occupancy Prediction Benchmark for Autonomous Driving</a>, arXiv 2023</li><li><a href="https://github.com/JeffWang987/OpenOccupancy">OpenOccupancy: A Large Scale Benchmark for Surrounding Semantic Occupancy Perception</a>, ICCV 2023</li><li><a href="https://github.com/ai4ce/Occ4cast/">Occ4cast: LiDAR-based 4D Occupancy Completion and Forecasting</a>, arXiv 2023.</li><li><a href="https://github.com/opendrivelab/occnet">OccNet: Scene as Occupancy</a>, ICCV 2023.</li><li><a href="https://github.com/ai4ce/SSCBench">SSCBench: A Large-Scale 3D Semantic Scene Completion Benchmark for Autonomous Driving</a>, arXiv 2023.</li></ul><p><em><strong>我们从SSCBench数据集开始介绍起</strong></em></p><h2 id="SSCBench-A-Large-Scale-3D-Semantic-Scene-Completion-Benchmark-for-Autonomous-Driving"><a href="#SSCBench-A-Large-Scale-3D-Semantic-Scene-Completion-Benchmark-for-Autonomous-Driving" class="headerlink" title="SSCBench: A Large-Scale 3D Semantic Scene Completion Benchmark for Autonomous Driving"></a>SSCBench: A Large-Scale 3D Semantic Scene Completion Benchmark for Autonomous Driving</h2><p>SSCBench提供的数据集格式与SemanticKITTI兼容，这是一个综合了 KITTI-360 、nuScenes和Waymo 中的场景的全面基准，总体而言，我们的SSCBench由三个子集组成，包括38562帧用于训练，15798帧用于验证，12553帧用于测试，总计66913帧（～67K），大大超过了上述SemanticKITTI的规模～7.7倍。SSCBench 便于在各种实际场景中轻松探索基于摄像头和LiDAR的SSC。</p><h3 id="相关工作"><a href="#相关工作" class="headerlink" title="相关工作"></a>相关工作</h3><ul><li><strong>单眼感知和3D语义场景完成</strong>。单眼相机的简单性、效率、可负担性和可访问性使单眼感知成为视觉和机器人界关注的焦点。SSCNet（2017）引入了单目3D语义场景完成（SSC）的概念，该概念旨在从单个深度图像重建和完成3D体积内的语义和几何结构。然而，由于缺乏室外数据集，他们只考虑有界的室内场景。Semantickitti（2019）构建了第一个基于KITTI的户外数据集，用于街景中的3D语义场景完成。现有的方法通常依赖于3D输入，如激光雷达点云（Lmscnet，2020；S3CNet，2021a；JS3C-Net，2021），而最近的基于单目视觉的解决方案也出现了（Monoscene，2022；Voxformer，2023），双目视觉的解决方案也出现了（OccDepth，2021）。然而，户外SSC的发展受到数据集缺乏的阻碍，SemanticKITTI（Behley et al.，2019）是唯一支持街景SSC的数据集。构建多样化的数据集对于释放SSC在自主系统中的全部潜力至关重要。</li><li><strong>街景中的点云分割</strong>。3D激光雷达分割旨在为点云分配逐点语义标签，在这一领域，源于PointNet++（2017）的基于点的方法在小型合成点云上表现良好。基于体素的方法通过最初通过笛卡尔坐标将3D空间划分为体素来处理点云。注意，3D激光雷达分割旨在基于原始激光雷达扫描对场景进行分类和理解，而3D语义场景完成包括在相机或激光雷达的输入下完成遮挡区域。</li><li><strong>自动驾驶数据集和基准</strong>。自动驾驶研究在高质量数据集上蓬勃发展，这些数据集是训练和评估感知、预测和规划算法的生命线。2012年，开创性的KITTI数据集引发了自动驾驶研究的一场革命，开启了包括物体检测、跟踪、映射和光学&#x2F;深度估计在内的多项任务。从那时起，研究界接受了这一挑战，产生了丰富的数据集。这些数据集通过应对多模式融合、多任务学习、恶劣天气、协同驾驶、重复驾驶，以及密集交通场景等。有几个有影响力且广泛使用的驾驶数据集，如KITTI-360（2022）、nuScenes（2020）和Waymo（2017）。它们提供了激光雷达和相机记录以及点云语义和边界注释，因此，我们可以通过聚合多个语义点云并利用3D框来处理动态对象，为SSC创建准确的地面实况标签。</li></ul><h3 id="介绍SSCNet"><a href="#介绍SSCNet" class="headerlink" title="介绍SSCNet"></a>介绍SSCNet</h3><p>本文的重点是语义场景补全，这是一个任务，从单视图深度地图观察生成一个完整的三维体素表示的体积占用和语义标签的场景。之前的工作分别考虑了场景补全和深度地图的语义标记。然而，我们注意到这两个问题是紧密联系在一起的。为了利用这两个任务的耦合特性，我们引入了语义场景完成网络(SSCNet)，<strong>这是一个端到端3D卷积网络，以单个深度图像作为输入，并同时输出相机视图截锥中所有体素的占用率和语义标签</strong>。我们的网络使用了一个基于扩张的3D上下文模块，以有效地扩展接受域，使3D上下文学习成为可能。为了训练我们的网络，我们构建了SUNCG——一个人工创建的大规模合成3D场景数据集，包含密集的体积标注。我们的实验表明，联合模型在语义场景完成任务方面优于单独处理每个任务的方法和替代方法。数据集、代码和经过训练的模型将在接受后在线提供。</p><p><img src="/pic/SSCNet.png"></p><p>语义场景补全网络（SSCNet），SSCNet [36] 是第一个将语义分割和场景完成与 3D CNN 端到端结合的工作。这是一种端到端的3D卷积网络，它以单个深度图像为输入，同时输出相机视图截头体中所有<em>体素的占用和语义标签</em>。网络使用基于扩张的3D上下文模块来有效地扩展感受野并实现3D上下文学习。</p><h5 id="数据编码（第3-1节）、网络架构（第3-2节）和训练数据生成（第4节）。"><a href="#数据编码（第3-1节）、网络架构（第3-2节）和训练数据生成（第4节）。" class="headerlink" title="数据编码（第3.1节）、网络架构（第3.2节）和训练数据生成（第4节）。"></a>数据编码（第3.1节）、网络架构（第3.2节）和训练数据生成（第4节）。</h5><ul><li><strong>数据编码</strong>。采用截断有符号距离函数（TSDF）对三维空间进行编码，其中每个体素都存储到其最近表面的距离值d，该值的符号表示体素是在自由空间中还是在遮挡空间中。对标准TSDF进行了以下修改：（1）消除视图相关性：选择计算到整个观测表面上任何地方最近点的距离。（2）消除空位中的强梯度,使用flipped TSDF(翻转的TSDF)</li><li><strong>网络架构</strong>。网络以高分辨率三维体积为输入，首先使用几个三维卷积层来学习局部几何表示。我们使用带有步长和池化层的卷积层，将分辨率降低到原始输入的四分之一。然后，我们使用基于膨胀的3D上下文模块来捕获更高级别的对象间上下文信息。之后，来自不同尺度的网络响应被连接并馈送到另外两个卷积层中，以聚合来自多个尺度的信息。最后，使用体素方向的softmax层来预测最终的体素标签。</li><li><strong>训练数据生成</strong>。SUNCG：一个大型合成场景数据集。SUNCG数据集包含45622个不同的场景，这些场景具有通过Planner5D平台手动创建的逼真的房间和家具布局[25]。有49884个有效楼层，包含404058个房间和5697217个对象实例，这些实例来自2644个覆盖84个类别的唯一对象网格。为了生成模拟典型图像捕获过程的合成深度图，我们使用一组简单的启发式方法来拾取相机视点。给定一个3D场景，我们从地板上间隔1米且不被物体占据的位置的统一网格开始。然后，我们根据NYU Depth v2数据集的分布选择相机姿势。1然后，我们使用Kinect的内部特性和分辨率渲染深度图。之后，我们使用一组简单的启发式方法来排除不好的观点。SUNCG数据集中的3D场景由有限数量的对象实例组成，我们通过首先对库中的每个单独对象进行体素化，然后根据每个场景配置和视点变换标签来加快体素化过程</li></ul><p><img src="/pic/SSCNet2.png"></p><p>上图显示<strong>表面(a)的不同编码</strong>。投影TSDF (b)是根据相机计算的，因此是视景相关的。准确的TSDF (c)具有较少的视图依赖性，但在沿着遮挡边界的空空间中显示出强烈的梯度(用灰色圈出)。相反，翻转TSDF (d)在近地表有最强的梯度。</p><p><img src="/pic/SSCNet2.png"></p><p>上图显示<strong>合成训练数据</strong>。我们收集了一个大规模的合成三维场景数据集来训练我们的网络。对于每个3D场景，我们选择一组摄像机位置，并生成成对的渲染深度图像和体积地面真实作为训练示例。</p><h3 id="介绍LMSCNet"><a href="#介绍LMSCNet" class="headerlink" title="介绍LMSCNet"></a>介绍LMSCNet</h3><p>一种从体素化稀疏三维激光雷达扫描中完成多尺度三维语义场景的新方法。与文献相反，我们使用具有全面多尺度跳跃连接的2D UNet主干来增强特征流，以及3D分割头。在SemanticKITTI基准测试中，与所有其他已发布的方法相比，我们的方法在语义完成方面表现相当，在占用完成方面表现更好，同时明显更轻、更快。</p><h4 id="方法"><a href="#方法" class="headerlink" title="方法"></a>方法</h4><p>我们解决密集 3D 语义完成的问题，其中的任务是为每个单独的体素分配语义标签。给定稀疏 3D 体素网格，目标是预测 3D 语义场景表示，其中每个体素被分配一个语义标签 C &#x3D; [c0, c1, … 。 。 , cN]，其中 N 是语义类别的数量，c0 代表自由体素。我们的架构称为 LMSCNet，如下图所示，使用轻量级 UNet 风格架构来预测多个尺度的 3D 语义完成，允许快速粗略推理，有利于移动机器人应用。我们主要沿高度轴使用 2D 卷积，而不是贪婪的 3D 卷积；类似于鸟瞰图。下面我们详细介绍我们的定制轻量级 2D&#x2F;3D 架构、多尺度重建和整体训练流程</p><p><img src="/pic/LMSCNet.png" alt="&quot;LMSCNet：轻量级多尺度语义完成网络。我们的管道使用具有 2D 主干卷积（蓝色）和 3D 分割头（灰色）的 UNet 架构来执行不同尺度的 3D 语义分割和完成，同时保持低复杂性。卷积参数显示为：（滤波器数量、内核大小和步幅）。请注意，我们有意降低 2D 特征维度并使用 Atrous 3D 卷积（来自 [26] 的 ASPP 块）来保持较低的推理复杂度。&quot;"></p><h3 id="介绍S3CNet"><a href="#介绍S3CNet" class="headerlink" title="介绍S3CNet"></a>介绍S3CNet</h3><p>主要贡献如下：（a）一种基于稀疏张量的神经网络架构，该架构能够有效地从稀疏的三维点云数据中学习特征，并联合解决耦合的场景完成和语义分割问题；（b） 一种新颖的几何感知三维张量分割损失；（c） 一种多视图融合和语义后处理策略，解决了远距离或遮挡区域和小尺寸对象的挑战。给定单个稀疏点云帧，我们的模型预测了一个密集的3D占用长方体，其中为每个体素单元分配了语义标签（如图1所示），生成了原始输入中不包含的3D环境的丰富信息，如激光雷达扫描之间的间隙、遮挡区域和未来场景。</p><p>利用逐点法向量作为几何特征编码来指导我们的模型根据对象的局部曲面凸性来填充间隙。我们还利用从球面范围图像计算的基于LiDAR的翻转截断有符号距离函数（fTSDF[5]）作为空间编码，来区分场景的自由空间、占用空间和遮挡空间。至于未来的场景，由于这些区域远离车辆，主要是道路或其他形式的地形，我们提出了稀疏语义场景完成网络的2D变体，以支持通过与鸟瞰图（BEV）语义图预测的多视图融合来构建3D场景。为了解决稀疏性，我们利用Minkowski引擎[6]，一个用于稀疏张量的自动微分库来构建我们的2D和3D语义场景完成网络。我们还采用了组合的几何启发语义分割损失来提高语义标签预测的准确性。</p><p><img src="/pic/S3CNet.png"><br>整个系统管道如图所示。从一次激光雷达扫描中，我们构建了两个稀疏张量，将场景封装为内存高效的2D和3D表示。每个张量通过其对应的语义场景完成网络，2D S3CNet或3D S3CNet，以在相应维度上语义地完成场景。我们提出了一种动态体素融合方法，用预测的2D语义BEV图进一步加密重建场景（详细讨论见第3.3节）。这抵消了对3D网络的显著内存需求——3D空间中的指数稀疏性增长使在一定范围内完成类变得困难。使用稀疏张量空间传播网络[30]，我们细化融合2D-3D预测的噪声区域中的语义标签。</p><h3 id="介绍JS3C-Net"><a href="#介绍JS3C-Net" class="headerlink" title="介绍JS3C-Net"></a>介绍JS3C-Net</h3><p>提出了一种增强的联合单扫LiDAR点云语义分割方法，该方法利用学习的形状先验形式场景完成网络，即JS3C-Net。具体来说，通过在LiDAR序列中合并数十个连续帧，在没有额外标注的情况下，实现了一个大的完整点云作为语义场景完成(SSC)任务的地面真相。利用这些标注优化的SSC可以捕获引人注意的形状先验，使不完整的输入完整到带有语义标签的可接受形状(Song et al. 2017)。因此，完全端到端的训练策略使得完成的形状先验在本质上有利于语义分割(SS)。进一步提出了一个设计良好的点体素交互(PVI)模块，用于SS和SSC任务之间的隐式相互知识融合。具体来说，通过PVI模块，利用逐点分割和逐体补全来维护粗全局结构和细粒度局部几何。更重要的是，我们设计的SSC和PVI模块是一次性的。为了实现这一点，JS3C-Net以级联的方式将SS和SSC结合在一起，这意味着它不会影响SS的信息流，同时在推理阶段丢弃SSC和PVI模块。因此，它可以避免生成完整的高分辨率密集体而带来额外的计算负担。</p><p><img src="/pic/JS3CNet.png"></p><p>JS3C Net的总体管道。在给定稀疏不完全单扫描点云的情况下，首先使用稀疏卷积U-Net对Fenc进行点特征编码。基于初始编码，MLP1用于生成形状嵌入（SE）FSE，该FSE与通过MLP2传递的初始编码一起流入MLP3，以生成用于点云语义分割的Fout。然后，来自SE的不完整细粒度点特征和来自语义场景完成（SSC）模块的完整体素特征流入点-体素交互（PVI）模块，以实现细化特征，最终在监督下输出完成体素。请注意，SSC和PVI模块可以在推理过程中丢弃。</p><p><img src="/pic/JS3CNet2.png"></p><p>上图 (a)部分显示了SSC模块的内部结构，该模块以分割网络中的语义概率为输入，通过多个卷积块和密集的上样本生成完整体积。(b)部分演示了PVI模块的一个二维实例，该实例利用数字“5”的粗全局结构的中心点，从原始点云中查询k个最近邻，然后应用基于图的聚合，通过细粒度的局部几何实现完整的“5”。</p><h3 id="MonoScene介绍"><a href="#MonoScene介绍" class="headerlink" title="MonoScene介绍"></a>MonoScene介绍</h3><p>一种能在室内与室外场景均可使用的单目 SSC 方案：</p><ul><li>提出一种将 2D 特征投影到 3D 的方法： FLoSP </li><li>提出一种  3D Context Relation Prior （3D CRP） 提升网络的上下文意识</li><li>新的SSC损失函数，用于优化场景类亲和力（affinity）和局部锥体（frustum）比例</li></ul><p><img src="/pic/MonoScence.png"></p><p>Monoscene  网络流程</p><ul><li>输入 2d 图片，经过 2d unet 提取多层次的特征</li><li>Features Line of Sight Projection module (FLoSP) 用于将 2d 特征提升到 3d 位置上，增强信息流并实现2D-3D分离</li><li>3D Context Relation Prior （3D CRP） 用于增强长距离的上下文信息提取</li><li>loss 优化：Scene-Class Affinity Loss：提升类内和类间的场景方面度量；Frustum Proportion Loss：对齐局部截头体中的类分布，提供超越场景遮挡的监督信息</li></ul><p>网络结构</p><ul><li>2D unet：EfficientNetB7 用于提取图像特征</li><li>3D UNets：2层 encoder-decoder 结构，用于提取 3d 特征</li><li>completion head：3D ASPP 结构和 softmax 层，用于处理 3D UNet 输出得到3d场景 completion 结果</li></ul><p><strong>Features Line of Sight Projection (FLoSP)</strong><br>从3维网络处理后者将为来自2维特征的集合提供线索。整个过程如下图所示，实际上假设已知相机内参，将3维体素中心投影到2维，从2维的解码器特征图采样得到对应特征，在所有尺度集合上进行重复，最终的3维特征图可用如下表示：</p><p><img src="/pic/MonoScence2.png"></p><h3 id="StereoScene介绍"><a href="#StereoScene介绍" class="headerlink" title="StereoScene介绍"></a>StereoScene介绍</h3><p><strong>3D语义场景补全（SSC）是一个需要从不完整观测中推断出密集3D场景的不适定问题。</strong>以往的方法要么明确地融合3D几何信息，要么依赖于从单目RGB图像中学习到的3D先验知识。然而，像LiDAR这样的3D传感器昂贵且具有侵入性，而单目相机由于固有的尺度模糊性而难以建模精确的几何信息。在这项工作中，研究者提出了StereoScene用于3D语义场景补全（SSC），它充分利用了轻量级相机输入而无需使用任何外部3D传感器。研究者的关键想法是利用<strong>立体匹配来解决几何模糊性问题</strong>。为了提高其在不匹配区域的鲁棒性，论文引入了鸟瞰图（BEV）表示法，以激发具有丰富上下文信息的未知区域预测能力。在立体匹配和BEV表示法的基础上，论文精心设计了一个<strong>相互交互聚合（MIA）</strong>模块，以充分发挥两者的作用，促进它们互补聚合。</p><p><img src="/pic/StereoScene.png"></p><p>整体的StereoScene框架如上图所示。论文遵循使用连续的2D和3 UNetsf作为backbones。给定输入的双目图像，我们使用2D UNet来提取多尺度特征。BEV潜在体积和立体几何体积分别由BEV构造器和立体构造器构造。为了充分利用这两个卷的互补潜力，提出了一个相互交互聚合模块来相互引导和聚合它们。</p><h3 id="介绍OccDepth"><a href="#介绍OccDepth" class="headerlink" title="介绍OccDepth"></a>介绍OccDepth</h3><p>第一个只输入视觉的立体3D SSC方法。为了有效地利用深度信息，提出了一种立体软特征分配（Stereo-SFA）模块，通过隐式学习立体图像之间的相关性来更好地融合3D深度感知特征。占用感知深度（OAD）模块将知识提取与预先训练的深度模型明确地用于生成感知深度的3D特征。此外，为了更有效地验证立体输入场景，我们提出了基于TartanAir的SemanticTartanAir数据集。这是一个虚拟场景，可以使用真实的地面实况来更好地评估该方法的有效性。</p><p><img src="/pic/OccDepth.png"><br>3D SSC是通过桥接立体SFA模块来从立体图像中推断出来的，该模块用于将特征提升到3D空间，OAD模块用于增强深度预测，3D U-Net用于提取几何结构和语义。立体深度网络仅在训练中用于提供深度监督。</p><h3 id="介绍VoxFormer"><a href="#介绍VoxFormer" class="headerlink" title="介绍VoxFormer"></a>介绍VoxFormer</h3><p>一种基于 Transformer 的语义场景完成框架，可以<strong>仅从 2D 图像输出完整的 3D 体积语义</strong>。我们的框架采用两阶段设计，从深度估计中一组稀疏的可见和占用体素查询开始，然后是从稀疏体素生成密集 3D 体素的致密化阶段。这种设计的一个关键思想是，2D 图像上的视觉特征仅对应于可见的场景结构，而不对应于被遮挡或空白的空间。因此，从可见结构的特征化和预测开始更为可靠。一旦我们获得了稀疏查询集，我们就应用屏蔽自动编码器设计，通过自注意力将信息传播到所有体素。</p><p>VoxFormer由类不可知的查询建议（阶段-1）和类特定的语义分割（阶段-2）组成，其中阶段-1提出了一组稀疏的占用体素，阶段-2完成了从阶段-1给出的建议开始的场景表示。具体而言，阶段1具有轻量级的基于2D CNN的查询建议网络，该网络使用图像深度来重建场景几何结构。然后，它从整个视场上预定义的可学习体素查询中提出了一组稀疏的体素。第2阶段基于一种新颖的稀疏到密集类MAE架构，如图（a）所示。它首先通过允许所提出的体素关注图像观察来加强其特征化。接下来，未提出的体素将与可学习的掩码标记相关联，并且将通过自注意来处理全套体素，以完成每体素语义分割的场景表示。</p><p><img src="/pic/VoxFormer2.png"></p><p>VoxFormer的总体框架如下图所示，给定RGB图像，通过ResNet50[61]提取2D特征，并通过现成的深度预测器估计深度。校正后的估计深度启用了类不可知查询建议阶段：将选择位于占用位置的查询，以与图像特征进行可变形的交叉关注。之后，将添加掩码令牌，用于通过可变形的自我注意来完成体素特征。细化的体素特征将被上采样并投影到输出空间，用于每个体素的语义分割。请注意，我们的框架支持单个或多个图像的输入。<br><img src="/pic/VoxFormer.png"></p><h3 id="介绍OccFormer"><a href="#介绍OccFormer" class="headerlink" title="介绍OccFormer"></a>介绍OccFormer</h3><p>提出了一种双路变压器网络OccFormer，该网络可以有效地处理三维体积进行语义占用预测。OccFormer实现了摄像机生成的3D体素特性的长程、动态和高效编码。它是通过将繁重的三维处理分解为局部和全局的变压器路径沿水平面得到的。在占位解码器中，我们将传统的Mask2Former用于3D语义占位，提出了保留池化和类引导采样，显著缓解了稀疏性和类的不平衡。</p><p><img src="/pic/OccFormer.png"></p><p>上图显示的为所提出的用于基于相机的3D语义占用预测的OccFormer的框架。该流水线由用于提取多尺度2D特征的图像编码器、用于将2D特征提升到3D体积的图像到3D变换以及用于获得3D语义特征并预测3D语义占用的基于变换器的编码器-解码器组成。</p><h4 id="对于编码器部分："><a href="#对于编码器部分：" class="headerlink" title="对于编码器部分："></a>对于编码器部分：</h4><p>我们提出了 dual-path transformer 模块，以释放 self-attention 的能力，同时限制了二次复杂性 (quadratic complexity)。</p><p>具体来说，</p><ul><li>local path 沿每个二维BEV切片运行，并使用共享窗口注意力 ( shared windowed attention) 来捕捉细粒度的细节。</li><li>global path 对 collapsed 的BEV特征进行处理，以获得场景级的理解。</li><li>最后，双路径的输出被自适应地融合以生成输出的三维特征体 3D feature volume 。<br>双路径设计明显打破了三维特征体的挑战性处理，我们证明了它比传统的三维卷积有明显优势。</li></ul><h4 id="对于解码器部分："><a href="#对于解码器部分：" class="headerlink" title="对于解码器部分："></a>对于解码器部分：</h4><p>我们是第一个将最先进的 Mask2Former[9] 方法用于三维语义占用预测的。</p><p>我们进一步提出使用最大池化而不是默认的双线性来计算 attention的 masked regions，这可以更好地保留小类。此外，我们还提出了 class-guided 的采样方法，以捕获前景区域，从而进行更有效的优化。</p><h3 id="介绍S4CNet"><a href="#介绍S4CNet" class="headerlink" title="介绍S4CNet"></a>介绍S4CNet</h3><p><img src="/pic/S4C.png"></p><p>S4C是第一种完全自我监督的语义场景完成（SSC）方法，该方法仅基于图像数据（和相机姿势）进行训练。我们的方法从单个图像中预测体积场景表示，即使在遮挡区域中也能捕获几何和语义信息。与以前的SSC方法相比，我们不需要任何3D地面实况信息，允许使用仅图像的数据集进行训练。尽管缺乏基本事实数据，但我们的方法与性能差异很小的监督方法相比具有竞争力。</p><p><img src="/pic/S4C2.png"></p><p>上图显示网络概述a） 根据输入图像II，编码器-解码器网络预测描述图像的截头体中的语义场的像素对齐特征图F。像素ui的特征fui对从光学中心通过像素投射的光线上的语义和占用分布进行编码。b） 语义场允许通过体积渲染来渲染新颖的视图及其相应的语义分割。将3D点xi投影到输入图像中，并因此将F投影到采样fui中。结合xi的位置编码，两个MLP分别对点σi和语义标签li的密度进行解码。用于新颖视图合成的颜色ci是通过颜色采样从其他图像中获得的。c） 为了获得最佳效果，我们要求训练视图覆盖尽可能多的场景表面。因此，我们从随机的未来时间步长中采样侧视图，这些时间步长观察在输入帧中被遮挡的场景区域。</p><h4 id="语义多视图一致性训练"><a href="#语义多视图一致性训练" class="headerlink" title="语义多视图一致性训练"></a>语义多视图一致性训练</h4><p>SSC的所有现有方法都依赖于3D地面实况数据进行训练。这些数据通常是从带注释的激光雷达扫描中获得的，这是非常困难和昂贵的。与3D数据相比，具有语义标签的图像是大量可用的。我们建议利用这些可用的2D数据来训练用于3D语义场景完成的神经网络。为了使我们的方法尽可能通用，我们使用从预先训练的语义分割网络生成的伪语义标签。这使我们能够以完全自我监督的方式，仅从姿势图像中训练我们的架构，而不需要2D或3D地面实况数据。</p><p><strong>训练数据和目标</strong>：该方法的目标是进行语义场景完成，包括对3D场景的重建和分配语义标签。与传统方法不同，传统方法依赖于昂贵的从LiDAR扫描中获得的3D真实数据，这种方法旨在利用具有语义标签的2D图像数据的丰富性。作者建议使用从预训练的语义分割网络生成的伪语义标签来训练SSC的神经网络。这种方法允许在不需要2D或3D真实数据的情况下进行全面的自我监督训练。</p><p><strong>数据来源</strong>：在自动驾驶场景中，汽车上装有多个摄像头，包括前置摄像头和侧置摄像头。该方法在多个时间步的视频序列中使用这些摄像头拍摄的多个构图图像进行训练。</p><p><strong>训练目标</strong>：从这些帧中随机选择一部分作为新视图合成的重建目标。神经网络经过训练，可以根据其他帧的语义场和颜色样本来重建颜色信息和语义标签。</p><p><strong>训练信号</strong>：训练信号是通过测量伪语义真值语义掩模和重建图像之间的差异生成的。这种差异用作训练期间的损失函数。</p><p><strong>战略视图选择</strong>：为了确保有效的训练，选择训练视图是经过策略性考虑的。选择了带有与输入图像的随机偏移的侧置摄像视图。这些视图对于捕捉输入图像中被遮挡的区域提供了重要线索，为场景完成提供了重要线索。</p><p><strong>重建块</strong>：只从不同帧中随机选择的块进行颜色（ˆPi,k）和语义标签（ˆSi）的重建。这种方法旨在进行高效的训练，同时确保神经网络学习了一般场景几何和特定对象的语义。</p><p><strong>损失函数</strong>：进行SSC的训练涉及使用语义和光度重建损失的组合。光度损失有助于捕捉一般场景几何，而语义损失对于区分对象和学习粗略几何非常重要，并且引导模型学习物体周围更清晰的边缘。</p><p><img src="/pic/S4C3.png"></p><p>上图显示的是在SSCBench-KITTI-360预测体素网格。对占用图的定性评价表明，该方法能够准确地重建和标注场景。特别是与MonoScene等其他基于图像的方法相比，S4C能够恢复图像1中右侧车道等细节。S4C产生的体素占用显示出比基于激光雷达的训练更少的洞，后者再现了在地面上发现的洞。</p><h3 id="与Occ3D，OpenOccupancy以及Occ4cast对比。"><a href="#与Occ3D，OpenOccupancy以及Occ4cast对比。" class="headerlink" title="与Occ3D，OpenOccupancy以及Occ4cast对比。"></a>与Occ3D，OpenOccupancy以及Occ4cast对比。</h3><p> 我们将SSCBench与同时进行的相关工作Occ3D进行了比较（Tian et al.，2023）。差异在于：</p><ul><li>（a）设置：Occ3D使用周围视图图像作为输入，并且只考虑相机可见的3D体素的重建。SSCBench考虑了一个更具挑战性但更有意义的设置（也是一个公认的设置）：如何仅使用单眼视觉输入在可见和遮挡区域重建和完成3D语义。这项任务需要对时间信息和三维几何关系进行推理，以摆脱有限的视野；</li><li>（b） 规模：SSCBench提供了比Occ3D更多的数据集，由于单目驾驶记录丰富，计划增加更多数据集；</li><li>（c） 可访问性：我们继承了先驱KITTI广泛使用的设置，从而使SSCBench更容易被社区访问；</li><li>（d） 全面性：我们将SSC方法与单目、三目和点云输入进行比较，并为跨领域泛化测试提供统一的标签。另一个相关的基准，OpenOccupancy（Wang et al.，2023），也表现出类似的差异，特别是它只使用了nuScenes数据集（Caesar et al.，2020），这导致了多样性的限制。</li></ul><p>Occ4cast提出了一个新的LiDAR感知任务：将占用补全和预测Occupancy Completion and Forecasting（OCF）统一到一个框架中<br>主要贡献总结如下：</p><ul><li>我们提出了OCF任务，该任务要求从稀疏的3D输入中获得时空密集的4D感知。</li><li>我们利用公共自动驾驶数据生成了一个名为OCFBench的大规模数据集。</li><li>我们提出了基线方法来处理OCF任务，并在我们的数据集上提供了详细的基准。</li></ul><h3 id="数据集介绍："><a href="#数据集介绍：" class="headerlink" title="数据集介绍："></a>数据集介绍：</h3><ul><li>SemanticKITTI：大规模的 LiDAR 点云标注数据集 SemanticKITTI，标注 28 类语义，共 22 个 sequences，43000 scans，不仅支持3D语义分割，而且是第一个户外SSC基准。一个明显的局限性是它在生成真值时遗漏了动态物体，导致了标签的不准确，产生痕迹。其次，它受到规模有限和缺乏多样化地理覆盖范围的限制，数据收集仅限于一个城市。</li><li>SSCBench-KITTI-360。KITTI-360（Liao et al.，2022）在著名的KITTI数据集的基础上，引入了一个丰富的数据收集框架，具有不同的传感器模态和全景视点（一个透视立体相机和一对鱼眼相机），并提供了全面的注释，包括一致的2D和3D语义实例标签以及3D边界基元。密集和连贯的标签不仅支持分割和检测等既定任务，还支持语义SLAM（Bowman et al.，2017）和新视图合成（Zhang et al.，2023a）等新应用。虽然KITTI-360包括基于点云的语义场景完成，但SSC的流行方法仍然以体素化表示为中心（Roldao等人，2022），这在机器人技术中表现出更广泛的适用性。我们利用开源训练和验证集，我们构建了由9个长序列组成的SSCBench-KITTI-360。为了减少冗余，我们按照SemanticKITTI SSC基准，每5帧采样一次。训练集包括来自场景00、02-05、07和10的8487帧，而验证集包括来自情景06的1812帧。测试集包括来自场景09的2566帧。数据集总共包含12865（~13K）帧，比SemanticKITTI的规模高出约1.5倍。</li><li>SSCBench-nuScenes。与KITTI的前置摄像头不同，nuScenes（Caesar et al.，2020）捕捉到了自我车辆周围的360度全景。它提供了各种各样的多模式传感数据，包括在波士顿和新加坡收集的相机图像、激光雷达点云和雷达数据。nuScenes为复杂的城市驾驶场景提供了细致的注释，包括不同的天气条件、施工区域和不同的照明。全景nuScenes（Fong et al.，2022）用语义和实例标签扩展了原始nuScene数据集。凭借全面的指标和评估协议，nuScenes在自动驾驶研究中得到了广泛应用（Gu et al.，2023；胡等人，2023，李等人，2021；Huang等人，2023.）。nuScenes数据集由1K 20秒的场景组成，其中仅为训练和验证集提供标签，共850个场景。从可用的850个场景中，我们分配了500个场景用于训练，200个场景用于验证，150个场景用于测试。这种分布导致20064帧用于训练，8050帧用于验证，5949帧用于测试，总计34078帧（～34K）。这个规模大约是SemanticKITTI的四倍。由于“nuScenes”仅为频率为2Hz的关键帧提供注释，因此在SSCBench“nuScene”中没有下采样。</li><li>SSC Bench Waymo。Waymo数据集（Sun et al.，2020）收集自美国各地，提供了大规模的多模式传感器记录。Waymo提供了5台相机，其组合水平视场为～230度，略小于nuScenes。数据是在多个城市的不同条件下采集的，包括旧金山、凤凰城和山景城，确保了每个城市的广泛地理覆盖。它包括1000个用于训练和验证的场景，以及150个用于测试的场景，每个场景的时间跨度为20秒。评论为了构建SSCBench Waymo，我们利用开源训练和验证场景，并将它们重新分配到500、298和202个场景中，分别用于训练、验证和测试。为了减少基准测试的冗余和训练时间，我们将原始数据的样本减少了10倍。这种下采样产生了10011帧的训练集、5936帧的验证集和4038帧的测试集，总计19985帧（～20K）。</li></ul><h3 id="构建Pipeline"><a href="#构建Pipeline" class="headerlink" title="构建Pipeline"></a>构建Pipeline</h3><ul><li><strong>先决条件</strong>。为了建立SSCBench，基于激光雷达或基于相机的SSC需要具有多模式记录的驾驶数据集。数据集应包括顺序收集的3D激光雷达点云，具有用于完成几何图形的精确传感器姿态，用于理解语义场景的逐点语义注释，以及用于处理动态实例的3D边界注释。</li><li><strong>点云聚合</strong>。为了生成完整的表示，我们的方法包括在车辆前方的定义区域内叠加一组广泛的激光扫描。在像nuScenes和Waymo这样的短序列中，我们利用未来的扫描和相应区域的测量来创建密集的语义点云。在像KITTI-360这样具有多个循环闭包的长序列中，除了时间邻域之外，我们还合并了所有空间相邻点云。先进的SLAM系统（Bailey&amp;Durrant-White，2006）提供了精确的传感器姿态，极大地促进了静态环境中点云的聚集。对于动态对象，我们通过同步来避免时空管道。我们使用实例标签将动态对象转换为当前帧内的空间对齐。</li><li><strong>聚合点云的Voxeization</strong>。体素化是将连续的3D空间离散为由称为体素的体积元素组成的规则网格结构，使非结构化数据能够转换为可由卷积神经网络（CNNs）或视觉变换器（ViTs）有效处理的结构化格式。Voxelization引入了空间分辨率和内存消耗之间的权衡，并为3D感知提供了灵活和可扩展的表示（Maturana&amp;Scherer，2015；周和图泽尔，2018；李等人，2023）。为了便于集成，SSCBench遵循SemanticKITTI的设置，体积向前延伸51.2米，每侧延伸25.6米，高度为6.4米。体素分辨率为0.2m，产生256×256×32的体素体积。每个体素的标签由其内标记点的多数投票决定，而如果不存在点，则相应地标记空体素。</li><li><strong>排除未知Voxels</strong>。如果没有无处不在的场景感知，捕捉完整的3D户外动态场景几乎是不可能的。虽然可以利用空间或先验知识推理，但我们的意图是通过最小化这些步骤产生的误差来确保基本事实的保真度。因此，在训练和评估过程中，我们只考虑来自所有视点的可见和探测体素。具体来说，我们首先从不同的角度使用光线跟踪来识别和去除物体内或墙后的遮挡体素。此外，在具有稀疏感测的数据集中，其中许多体素仍然没有被标记，我们在训练和评估期间去除这些未知体素，以增强基本事实标签的可靠性</li></ul><p>讨论与分析</p><ul><li>点云密度的影响。我们的实验阐明了激光雷达输入密度对模型性能的影响。在SSCBench nuScenes数据集中，其特征是相对稀疏的激光雷达输入（32个通道），基于相机的方法在几何度量上优于基于激光雷达的方法。然而，在SSCBench Waymo数据集中，得益于密集的激光雷达输入（64个通道，5个激光雷达），基于激光雷达的方法大大优于基于相机的方法。基于激光雷达的方法对输入的敏感性变得明显，在密集输入中观察到优势，而在稀疏输入中则观察到显著的性能下降。这突出了未来研究开发强大的基于激光雷达的方法的必要性，该方法可以在利用效益的同时减轻退化。</li><li>单眼与三眼。表3显示了具有单目和三目输入的TPVFormer的性能。而三眼设置提供了更宽的视野，有助于提高IoU的整体性能（36.78→ 39.06）和mIoU（10.91→ 13.70），仅使用一台相机就能获得优异的结果仍然是一个引人注目的学术挑战。开发能够将模型的性能与全景图相匹配的单目方法仍然具有重要的研究价值，因为它们具有内存高效、计算高效和易于部署的特点。</li><li>与SemanticKITTI的比较。当将我们在SSCBench上的实验结果与SemanticKITTI的实验结果进行比较时，我们观察到了显著的差异（Behley et al.，2019）（有关更多细节，我们请读者参阅VoxFormer（Li et al.，2023））。虽然VoxFormer在IoU和mIoU等指标上在SemanticKITTI上表现出色，但它面临着SSCBench数据集多样性的挑战。这一挑战主要源于其深度估计模块无法超越SemanticKITTI进行推广。此外，LMSCNet在SemanticKITTI上通常表现出优于SSCNet的几何性能，而在SSCBench上则表现出相反的趋势。这些差异强调了两个要点。首先，他们强调了SSCBench的重要性，它为全面评估提供了多样化和苛刻的现实世界场景。其次，他们强调了在各种环境中保持高性能的稳健方法的必要性。</li></ul><h3 id="结论"><a href="#结论" class="headerlink" title="结论"></a>结论</h3><ul><li>限制和未来工作。SSCBench仅包含符合SSC问题惯例的3D数据。这限制了对具有时间维度的4D方法的评估。未来的工作将旨在扩大SSCBench，以包括时间信息。</li><li>总结。在本文中，我们介绍了SSCBench，这是一个由不同街景组成的大型基准，旨在促进稳健和可推广的语义场景完成模型的开发。通过细致的策划和全面的基准测试，我们发现了现有方法的瓶颈，并为未来的研究方向提供了宝贵的见解。我们的目标是让SSCBench刺激3D语义场景完成的进步，最终增强下一代自主系统的感知能力。</li></ul>]]></content>
      
      
      <categories>
          
          <category> DL </category>
          
      </categories>
      
      
        <tags>
            
            <tag> ssc </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>ubuntu学习记录</title>
      <link href="/2023/05/15/zhiling/"/>
      <url>/2023/05/15/zhiling/</url>
      
        <content type="html"><![CDATA[<p>&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;节点&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;<br><a href="https://subscribe.pronetworklink.com/api/v1/client/e5cea80c08c2367003269501e9cbea19">https://subscribe.pronetworklink.com/api/v1/client/e5cea80c08c2367003269501e9cbea19</a></p><p>&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;orbslam&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;</p><h1 id="运行摄像头，发布话题"><a href="#运行摄像头，发布话题" class="headerlink" title="运行摄像头，发布话题"></a>运行摄像头，发布话题</h1><p>roslaunch usb_cam usb_cam-test.launch</p><h1 id="新终端：开启ORBSLAM2"><a href="#新终端：开启ORBSLAM2" class="headerlink" title="新终端：开启ORBSLAM2"></a>新终端：开启ORBSLAM2</h1><p>rosrun ORB_SLAM2 Mono .&#x2F;Vocabulary&#x2F;ORBvoc.txt .&#x2F;Examples&#x2F;ROS&#x2F;ORB_SLAM2&#x2F;Asus.yaml</p><p>pcl_viewer的使用:pcl_viewer a.pcd</p><p>&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;vscode使用&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;</p><p>vscode查看函数列表Ctrl + Shift + O</p><p>markdown ctrl-shift-vOpen preview(打开新窗口预览该文件)</p><p>ctrl+c+c </p><p>pkg-config –modversion opencv</p><p>g++ -std&#x3D;c++11 test.cpp <code>pkg-config --libs --cflags opencv</code> -o result</p><p>sudo gedit ~&#x2F;.bashrc</p><p>source ~&#x2F;.bashrc</p><p>echo $ROS_PACKAGE_PATH</p><p>问题：liunx系统打开vscode之后代码和终端字体有的单词间距很奇怪，修改字体大小和间距后无效，修改了字体完成。</p><p>操作：打开设置，输入Editor:Font Family，</p><p>修改终端字体：在Terminal › Integrated: Font Family下输入monospace，</p><p>修改编辑器字体：在Editor:Font Family输入monospac</p><p>&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;服务器&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;</p><p>ssh -p 1938 <a href="mailto:&#x63;&#x68;&#101;&#x6e;&#x68;&#x61;&#105;&#x79;&#97;&#x6e;&#103;&#x40;&#49;&#48;&#x2e;&#54;&#x39;&#46;&#x34;&#x37;&#x2e;&#x38;&#x32;">&#x63;&#x68;&#101;&#x6e;&#x68;&#x61;&#105;&#x79;&#97;&#x6e;&#103;&#x40;&#49;&#48;&#x2e;&#54;&#x39;&#46;&#x34;&#x37;&#x2e;&#x38;&#x32;</a></p><p>nvidia-smi</p><p>gpustat -i 1</p><p>-i <a href="http://pypi.douban.com/simple/">http://pypi.douban.com/simple/</a> –trusted-host pypi.douban.com</p><p>vim ~&#x2F;.bashrc</p><p>source ~&#x2F;.bashrc</p><p>&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;cuda&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;</p><p>export PATH&#x3D;”$PATH:&#x2F;usr&#x2F;local&#x2F;cuda&#x2F;bin”<br>export LD_LIBRARY_PATH&#x3D;”$LD_LIBRARY_PATH:&#x2F;usr&#x2F;local&#x2F;cuda&#x2F;lib64&#x2F;“<br>export LIBRARY_PATH&#x3D;”$LIBRARY_PATH:&#x2F;usr&#x2F;local&#x2F;cuda&#x2F;lib64”<br>export CUDA_HOME&#x3D;$CUDA_HOME:”&#x2F;usr&#x2F;local&#x2F;cuda”</p><p>chmod 777 file</p><p>&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;docker&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;</p><p>查看容器运行现状:docker ps。<br>停止容器:docker stop 容器id，<br>查看容器标准输出：docker logs<br>获取镜像：docker pull 命令来载入 ubuntu 镜像<br>启动容器：docker run -it ubuntu &#x2F;bin&#x2F;bash<br>查看容器：docker ps -a<br>退出容器：exit或着CTRL+D<br>启动已停止的容器：docker start 容器id<br>后台运行：-d指定容器运行模式，如：docker run -itd ubuntu-test ubuntu &#x2F;bin&#x2F;bash<br>注：-d参数默认不会进入容器<br>进入容器：docker attach容器id（从容器最初会导致容器停止）以及docker exec容器id(推荐使用，从容器退出不会导致容器停止)<br>导出容器：docker export 如：docker export 容器id  &gt;  ubuntu.tar<br>导入容器：docker import  如：cat docker&#x2F;ubuntu.tar | docker import - test&#x2F;ubuntu:v1<br>将快照文件 ubuntu.tar 导入到镜像 test&#x2F;ubuntu:v1:<br>删除容器：docker rm -f 容器id<br>删除所有终止状态的容器：docker container prune</p><p>sudo docker ps -a</p><p>sudo docker container ls -a</p><p>docker stop 容器id</p><p>sudo docker start 容器id          # 启动容器</p><p>sudo docker attach 容器id  进入容器正在执行的终端</p><p>sudo docker stats</p><p>sudo docker images</p><p>&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;conda源&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;</p><p>channels:</p><ul><li>defaults<br>show_channel_urls: true<br>channel_alias: <a href="https://mirrors.bfsu.edu.cn/anaconda">https://mirrors.bfsu.edu.cn/anaconda</a><br>default_channels:</li><li><a href="https://mirrors.bfsu.edu.cn/anaconda/pkgs/main">https://mirrors.bfsu.edu.cn/anaconda/pkgs/main</a></li><li><a href="https://mirrors.bfsu.edu.cn/anaconda/pkgs/free">https://mirrors.bfsu.edu.cn/anaconda/pkgs/free</a></li><li><a href="https://mirrors.bfsu.edu.cn/anaconda/pkgs/r">https://mirrors.bfsu.edu.cn/anaconda/pkgs/r</a></li><li><a href="https://mirrors.bfsu.edu.cn/anaconda/pkgs/pro">https://mirrors.bfsu.edu.cn/anaconda/pkgs/pro</a></li><li><a href="https://mirrors.bfsu.edu.cn/anaconda/pkgs/msys2">https://mirrors.bfsu.edu.cn/anaconda/pkgs/msys2</a><br>custom_channels:<br>  conda-forge: <a href="https://mirrors.bfsu.edu.cn/anaconda/cloud">https://mirrors.bfsu.edu.cn/anaconda/cloud</a><br>  msys2: <a href="https://mirrors.bfsu.edu.cn/anaconda/cloud">https://mirrors.bfsu.edu.cn/anaconda/cloud</a><br>  bioconda: <a href="https://mirrors.bfsu.edu.cn/anaconda/cloud">https://mirrors.bfsu.edu.cn/anaconda/cloud</a><br>  menpo: <a href="https://mirrors.bfsu.edu.cn/anaconda/cloud">https://mirrors.bfsu.edu.cn/anaconda/cloud</a><br>  pytorch: <a href="https://mirrors.bfsu.edu.cn/anaconda/cloud">https://mirrors.bfsu.edu.cn/anaconda/cloud</a><br>  simpleitk: <a href="https://mirrors.bfsu.edu.cn/anaconda/cloud">https://mirrors.bfsu.edu.cn/anaconda/cloud</a></li></ul><p>channels:</p><ul><li>defaults<br>show_channel_urls: true<br>channel_alias: <a href="https://mirrors.tuna.tsinghua.edu.cn/anaconda">https://mirrors.tuna.tsinghua.edu.cn/anaconda</a><br>default_channels:</li><li><a href="https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/main">https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/main</a></li><li><a href="https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/free">https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/free</a></li><li><a href="https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/r">https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/r</a></li><li><a href="https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/pro">https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/pro</a></li><li><a href="https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/msys2">https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/msys2</a><br>custom_channels:<br>  conda-forge: <a href="https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud">https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud</a><br>  msys2: <a href="https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud">https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud</a><br>  bioconda: <a href="https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud">https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud</a><br>  menpo: <a href="https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud">https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud</a><br>  pytorch: <a href="https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud">https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud</a><br>  simpleitk: <a href="https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud">https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud</a></li></ul><p>&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;opencv安装&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;</p><p>cmake -D CMAKE_BUILD_TYPE&#x3D;RELEASE <br>      -D CMAKE_INSTALL_PREFIX&#x3D;&#x2F;usr&#x2F;local&#x2F;opencv_2.4.11 <br>      -D WITH_CUDA&#x3D;OFF <br>      -D WITH_OPENGL&#x3D;OFF <br>      -D WITH_OPENCL&#x3D;OFF <br>      -D BUILD_JPEG&#x3D;OFF <br>      -D BUILD_PNG&#x3D;OFF <br>      -D BUILD_JASPER&#x3D;OFF <br>      -DBUILD_OPENEXR&#x3D;OFF <br>      -D BUILD_TIFF&#x3D;OFF <br>      -D BUILD_ZLIB&#x3D;OFF <br>      -D WITH_FFMPEG&#x3D;OFF <br>    ..</p><p>cmake -D CMAKE_BUILD_TYPE&#x3D;RELEASE -D CMAKE_INSTALL_PREFIX&#x3D;&#x2F;usr&#x2F;local&#x2F;opencv2411 -D CUDA_GENERATION&#x3D;Kepler -D WITH_TBB&#x3D;ON -D BUILD_NEW_PYTHON_SUPPORT&#x3D;ON -D WITH_V4L&#x3D;ON -D INSTALL_C_EXAMPLES&#x3D;ON -D INSTALL_PYTHON_EXAMPLES&#x3D;ON -D BUILD_EXAMPLES&#x3D;ON -D WITH_QT&#x3D;OFF -D WITH_OPENGL&#x3D;ON -D BUILD_TIFF&#x3D;ON ..&#x2F;local&#x2F;opencv2411 ..</p><p>sudo cmake -D CMAKE_BUILD_TYPE&#x3D;RELEASE \ -D CMAKE_INSTALL_PREFIX&#x3D;&#x2F;usr&#x2F;local \ -D OPENCV_EXTRA_MODULES_PATH&#x3D;~&#x2F;opencv_contrib-3.4.3&#x2F;modules -D INSTALL_PYTHON_EXAMPLES&#x3D;ON \ -D INSTALL_C_EXAMPLES&#x3D;ON -D OPENCV_ENABLE_NONFREE:BOOL&#x3D;ON -D BUILD_opencv_world:BOOL&#x3D;ON -D BUILD_EXAMPLES&#x3D;ON .. </p><p>—————————-ROS—————————————</p><p>rostopic list</p><p>rostopic type &#x2F;tianbot_mini&#x2F;odom    输出：类型</p><p>rosmsg show 类型    输出：类型包含的结构</p><p>rosrun  turtlesim turtlesim_node 运行节点</p><p>rosrun turtlesim turtle_teleop_key 键盘控制运动</p><p>查看当前所有的topic：rostopic list<br>查看某个topic的输出：rostopic echo [topic_name]<br>查看某个topic的发布频率：rostopic hz [topic_name]<br>查看某个topic的数据格式：rostopic echo [topic_name]&#x2F;encoding</p><p>&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;slam-yolov5&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;</p><p>.&#x2F;Examples&#x2F;RGB-D&#x2F;rgbd_tum  Vocabulary&#x2F;ORBvoc.txt Examples&#x2F;RGB-D&#x2F;TUM3.yaml &#x2F;home&#x2F;chy&#x2F;dataset&#x2F;rgbd_dataset_freiburg3_walking_halfsphere &#x2F;home&#x2F;chy&#x2F;dataset&#x2F;rgbd_dataset_freiburg3_walking_halfsphere&#x2F;associations.txt   </p><p>&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;segment-anything&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;</p><p>python scripts&#x2F;amg.py –input &#x2F;home&#x2F;chy&#x2F;code_chy&#x2F;segment-anything&#x2F;99.jpg –output &#x2F;home&#x2F;chy&#x2F;code_chy&#x2F;segment-anything&#x2F;outs –model-type vit_b –checkpoint &#x2F;home&#x2F;chy&#x2F;code_chy&#x2F;segment-anything&#x2F;sam_vit_b_01ec64.pth</p><p>&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;解压&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;<br>tgz格式解压到当前文件夹：<br>tar -zxvf xxx.tar.gz<br>zip格式解压到当前文件夹：<br>unzip xxx.zip<br>tar.xz格式解压<br>首先：xz -d xxx.tar.xz 解压得到tar文件 ;<br>其次：tar -xvf xxx.tar得到完整解压文件。</p><p>&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;git&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;</p><p>在git的repo中，可能会有子项目的代码，也就是”git中的git”</p><p> –recursive是递归的意思，不仅会git clone当前项目中的代码，也会clone项目中子项目的代码。</p><p><em><strong>未完待续</strong></em></p>]]></content>
      
      
      <categories>
          
          <category> linux </category>
          
      </categories>
      
      
        <tags>
            
            <tag> linux </tag>
            
            <tag> ubuntu </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>docker学习记录</title>
      <link href="/2023/05/07/docker1/"/>
      <url>/2023/05/07/docker1/</url>
      
        <content type="html"><![CDATA[<h1 id="docker-的基本使用"><a href="#docker-的基本使用" class="headerlink" title="docker 的基本使用"></a>docker 的基本使用</h1><h2 id="docker镜像命令和容器命令"><a href="#docker镜像命令和容器命令" class="headerlink" title="docker镜像命令和容器命令"></a>docker镜像命令和容器命令</h2><p><img src="/pic/e59c326db4bece61c8e5916822302408.png" alt="uTools_1683184235571.png"></p><p><img src="/pic/1bed533c0452fc72ba67462e37e5d850.png" alt="uTools_1683184257569.png"></p><br/><h2 id="x3D-x3D-docker-作业练习-x3D-x3D"><a href="#x3D-x3D-docker-作业练习-x3D-x3D" class="headerlink" title="&#x3D;&#x3D;docker 作业练习&#x3D;&#x3D;"></a><em><strong>&#x3D;&#x3D;docker 作业练习&#x3D;&#x3D;</strong></em></h2><blockquote><p><strong>&#x3D;&#x3D;Docker 安装 Nginx&#x3D;&#x3D;</strong></p></blockquote><pre class="line-numbers language-powershell" data-language="powershell"><code class="language-powershell"><span class="token comment"># -d后台运行</span><span class="token comment">#--name   给容器起名字</span><span class="token comment"># -p 宿主机端口：容器内部端口</span>chy@ocean:~$ docker run <span class="token operator">-</span>d <span class="token operator">--</span>name nginx01 <span class="token operator">-</span>p 8000:80 nginxc83728c3f1d2d20cba7571a6fd08c506cb5bc14b1fbb9385b8fca0e687615e4dchy@ocean:~$ docker <span class="token function">ps</span>CONTAINER ID   IMAGE     COMMAND                   CREATED         STATUS         PORTS                                   NAMESc83728c3f1d2   nginx     <span class="token string">"/docker-entrypoint.…"</span>   6 seconds ago   Up 5 seconds   0<span class="token punctuation">.</span>0<span class="token punctuation">.</span>0<span class="token punctuation">.</span>0:8000->80/tcp<span class="token punctuation">,</span> :::8000->80/tcp   nginx01chy@ocean:~$ curl localhost:8000&lt;<span class="token operator">!</span>DOCTYPE html>&lt;html>&lt;head>&lt;title>Welcome to nginx!&lt;<span class="token operator">/</span>title>&lt;style>html <span class="token punctuation">&#123;</span> color-scheme: light dark<span class="token punctuation">;</span> <span class="token punctuation">&#125;</span>body <span class="token punctuation">&#123;</span> width: 35em<span class="token punctuation">;</span> margin: 0 auto<span class="token punctuation">;</span>font-family: Tahoma<span class="token punctuation">,</span> Verdana<span class="token punctuation">,</span> Arial<span class="token punctuation">,</span> sans-serif<span class="token punctuation">;</span> <span class="token punctuation">&#125;</span>&lt;<span class="token operator">/</span>style>&lt;<span class="token operator">/</span>head>&lt;body>&lt;h1>Welcome to nginx!&lt;<span class="token operator">/</span>h1><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p><img src="/pic/27c501e8902ecff60cf6e1b31b56641e.png" alt="截图"></p><blockquote><p><strong>&#x3D;&#x3D;作业：docker 来装一个tomcat&#x3D;&#x3D;</strong></p></blockquote><pre class="line-numbers language-powershell" data-language="powershell"><code class="language-powershell">docker run <span class="token operator">-</span>it <span class="token operator">--</span><span class="token function">rm</span> tomcat:9<span class="token punctuation">.</span>0<span class="token comment"># --rm 一般用来测试，用完就删 </span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre><br/><blockquote><p>作业三：部署es+kibana</p></blockquote><pre class="line-numbers language-powershell" data-language="powershell"><code class="language-powershell"><span class="token comment"># es暴露端口多</span><span class="token comment">#es 十分的耗内存</span><span class="token comment">#es的数据一般挂在到安全目录！挂载</span><span class="token comment"># --net somenetwork  网络配置</span>docker stats查看docker所用内存<span class="token operator">-</span>e环境配置修改docker run <span class="token operator">-</span>d <span class="token operator">--</span>name elasticsearch <span class="token operator">-</span>p 9200:9200 <span class="token operator">-</span>p 9300:9300 <span class="token operator">-</span>e <span class="token string">"discovery.type=single-node"</span> elasticsearch:7<span class="token punctuation">.</span>6<span class="token punctuation">.</span>2chy@ocean:~$ curl localhost:9200<span class="token punctuation">&#123;</span>  <span class="token string">"name"</span> : <span class="token string">"c41cc3475bc3"</span><span class="token punctuation">,</span>  <span class="token string">"cluster_name"</span> : <span class="token string">"docker-cluster"</span><span class="token punctuation">,</span>  <span class="token string">"cluster_uuid"</span> : <span class="token string">"bhypk5Q-RvC4x_FOMoFF-g"</span><span class="token punctuation">,</span>  <span class="token string">"version"</span> : <span class="token punctuation">&#123;</span>    <span class="token string">"number"</span> : <span class="token string">"7.6.2"</span><span class="token punctuation">,</span>    <span class="token string">"build_flavor"</span> : <span class="token string">"default"</span><span class="token punctuation">,</span>    <span class="token string">"build_type"</span> : <span class="token string">"docker"</span><span class="token punctuation">,</span>    <span class="token string">"build_hash"</span> : <span class="token string">"ef48eb35cf30adf4db14086e8aabd07ef6fb113f"</span><span class="token punctuation">,</span>    <span class="token string">"build_date"</span> : <span class="token string">"2020-03-26T06:34:37.794943Z"</span><span class="token punctuation">,</span>    <span class="token string">"build_snapshot"</span> : false<span class="token punctuation">,</span>    <span class="token string">"lucene_version"</span> : <span class="token string">"8.4.0"</span><span class="token punctuation">,</span>    <span class="token string">"minimum_wire_compatibility_version"</span> : <span class="token string">"6.8.0"</span><span class="token punctuation">,</span>    <span class="token string">"minimum_index_compatibility_version"</span> : <span class="token string">"6.0.0-beta1"</span>  <span class="token punctuation">&#125;</span><span class="token punctuation">,</span>  <span class="token string">"tagline"</span> : <span class="token string">"You Know, for Search"</span><span class="token punctuation">&#125;</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p><strong>&#x3D;&#x3D;可视化&#x3D;&#x3D;</strong></p><ul><li>portainer(先用这个)</li></ul><pre class="line-numbers language-powershell" data-language="powershell"><code class="language-powershell">docker run <span class="token operator">-</span>d <span class="token operator">-</span>p 8080:9000\<span class="token operator">--</span>restart=always <span class="token operator">-</span>v <span class="token operator">/</span><span class="token keyword">var</span><span class="token operator">/</span>run/docker<span class="token punctuation">.</span>sock:<span class="token operator">/</span><span class="token keyword">var</span><span class="token operator">/</span>run/docker<span class="token punctuation">.</span>sock <span class="token operator">--</span>privileged=true portainer/portainer<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre><br/><p><strong>&#x3D;&#x3D;什么是portainer&#x3D;&#x3D;</strong></p><p>Docker 图像化界面管理工具，提供一个后台面板供我们操作</p><p>访问测试：<a href="http://192.168.1.40:8088/">http://192.168.1.40:8088</a></p><br/><h2 id="docker-镜像"><a href="#docker-镜像" class="headerlink" title="docker  镜像"></a>docker  镜像</h2><p><img src="/pic/c2cccd377124402c404b71f338287944.png" alt="截图"></p><p><strong>&#x3D;&#x3D;commit镜像&#x3D;&#x3D;</strong></p><br/><pre class="line-numbers language-powershell" data-language="powershell"><code class="language-powershell">docker commit 提交容器成为一个新的副本docker commit <span class="token operator">-</span>m=提交的描述信息  <span class="token operator">-</span>a=“作者 ” 容器 id 目标镜像名:<span class="token namespace">[TAG]</span>docker commit <span class="token operator">-</span>a=<span class="token string">"chy"</span> <span class="token operator">-</span>m=<span class="token string">"add webapps application"</span> 8f9660706542 tomcat_chy:1<span class="token punctuation">.</span>0<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span></span></code></pre><h2 id="x3D-x3D-容器数据卷-x3D-x3D"><a href="#x3D-x3D-容器数据卷-x3D-x3D" class="headerlink" title="&#x3D;&#x3D;容器数据卷&#x3D;&#x3D;"></a><strong>&#x3D;&#x3D;容器数据卷&#x3D;&#x3D;</strong></h2><h3 id="什么是容器数据卷"><a href="#什么是容器数据卷" class="headerlink" title="什么是容器数据卷"></a>什么是容器数据卷</h3><p><img src="/pic/2eb6ed69ff73865ee2e352c59cfa3263.png" alt="截图"></p><p><strong>总结一句话:容器的持久化和同步操作！容器间也可以进行数据共享的！</strong></p><br/><h3 id="使用数据卷"><a href="#使用数据卷" class="headerlink" title="使用数据卷"></a>使用数据卷</h3><br/><blockquote><p>方式一：直接使用命令来挂载 -v</p><p>docker run -it -v  主机目录：容器内目录    类似于-p</p><p> docker run -it -v &#x2F;home&#x2F;chy&#x2F;docker_ubuntu18_home:&#x2F;home ubuntu:18.04 &#x2F;bin&#x2F;bash</p></blockquote><pre class="line-numbers language-c_cpp" data-language="c_cpp"><code class="language-c_cpp">&quot;Mounts&quot;: [            &#123;                &quot;Type&quot;: &quot;bind&quot;,                &quot;Source&quot;: &quot;&#x2F;home&#x2F;chy&#x2F;docker_ubuntu18_home&quot;,                &quot;Destination&quot;: &quot;&#x2F;home&quot;,                &quot;Mode&quot;: &quot;&quot;,                &quot;RW&quot;: true,                &quot;Propagation&quot;: &quot;rprivate&quot;            &#125;        ],<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><br/><pre class="line-numbers language-powershell" data-language="powershell"><code class="language-powershell">chy@ocean:~<span class="token operator">/</span>docker_ubuntu18_home$ vim ceshi<span class="token punctuation">.</span>py <span class="token punctuation">[</span>2<span class="token punctuation">]</span><span class="token operator">+</span>  已停止               vim ceshi<span class="token punctuation">.</span>pychy@ocean:~<span class="token operator">/</span>docker_ubuntu18_home$ docker <span class="token function">ps</span>CONTAINER ID   IMAGE                 COMMAND        CREATED       STATUS       PORTS                                       NAMESd11b0f59e8b1   portainer/portainer   <span class="token string">"/portainer"</span>   5 hours ago   Up 5 hours   0<span class="token punctuation">.</span>0<span class="token punctuation">.</span>0<span class="token punctuation">.</span>0:8088->9000/tcp<span class="token punctuation">,</span> :::8088->9000/tcp   elated_davincichy@ocean:~<span class="token operator">/</span>docker_ubuntu18_home$ docker <span class="token function">ps</span> <span class="token operator">-</span>aCONTAINER ID   IMAGE                 COMMAND                   CREATED          STATUS                      PORTS                                       NAMESd591bed7171e   ubuntu:18<span class="token punctuation">.</span>04          <span class="token string">"/bin/bash"</span>               22 minutes ago   Exited <span class="token punctuation">(</span>0<span class="token punctuation">)</span> 21 minutes ago                                               wizardly_spenced11b0f59e8b1   portainer/portainer   <span class="token string">"/portainer"</span>              5 hours ago      Up 5 hours                  0<span class="token punctuation">.</span>0<span class="token punctuation">.</span>0<span class="token punctuation">.</span>0:8088->9000/tcp<span class="token punctuation">,</span> :::8088->9000/tcp   elated_davincic83728c3f1d2   nginx                 <span class="token string">"/docker-entrypoint.…"</span>   6 hours ago      Exited <span class="token punctuation">(</span>0<span class="token punctuation">)</span> 6 hours ago                                                  nginx01ff3cf19a0b79   ubuntu:18<span class="token punctuation">.</span>04          <span class="token string">"/bin/bash"</span>               7 hours ago      Exited <span class="token punctuation">(</span>127<span class="token punctuation">)</span> 7 hours ago                                                lucid_chatterjeee19f4782e1c5   ubuntu:18<span class="token punctuation">.</span>04          <span class="token string">"bash"</span>                    7 hours ago      Exited <span class="token punctuation">(</span>130<span class="token punctuation">)</span> 7 hours ago                                                pedantic_paninic29921e3edcf   composetest_web       <span class="token string">"flask run"</span>               2 weeks ago      Exited <span class="token punctuation">(</span>137<span class="token punctuation">)</span> 2 weeks ago                                                composetest-web-10a446f730c1f   redis:alpine          <span class="token string">"docker-entrypoint.s…"</span>   2 weeks ago      Exited <span class="token punctuation">(</span>0<span class="token punctuation">)</span> 6 days ago                                                   composetest-redis-1chy@ocean:~<span class="token operator">/</span>docker_ubuntu18_home$ docker <span class="token function">start</span> d591bed7171ed591bed7171echy@ocean:~<span class="token operator">/</span>docker_ubuntu18_home$ docker <span class="token function">ps</span> CONTAINER ID   IMAGE                 COMMAND        CREATED          STATUS         PORTS                                       NAMESd591bed7171e   ubuntu:18<span class="token punctuation">.</span>04          <span class="token string">"/bin/bash"</span>    22 minutes ago   Up 3 seconds                                               wizardly_spenced11b0f59e8b1   portainer/portainer   <span class="token string">"/portainer"</span>   5 hours ago      Up 5 hours     0<span class="token punctuation">.</span>0<span class="token punctuation">.</span>0<span class="token punctuation">.</span>0:8088->9000/tcp<span class="token punctuation">,</span> :::8088->9000/tcp   elated_davincichy@ocean:~<span class="token operator">/</span>docker_ubuntu18_home$ docker attach d591bed7171eroot@d591bed7171e:<span class="token operator">/</span><span class="token comment"># ls</span>bin  boot  dev  etc  home  lib  lib64  media  mnt  opt  proc  root  run  sbin  srv  sys  tmp  usr  <span class="token keyword">var</span>root@d591bed7171e:<span class="token operator">/</span><span class="token comment"># cd home/</span>root@d591bed7171e:<span class="token operator">/</span>home<span class="token comment"># ls</span>ceshi<span class="token punctuation">.</span>pyroot@d591bed7171e:<span class="token operator">/</span>home<span class="token comment"># cat ceshi.py </span>hello<span class="token punctuation">,</span> linux updata<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h3 id="好处：我们以后修改只需要在本地修改即可，容器内会自动同步"><a href="#好处：我们以后修改只需要在本地修改即可，容器内会自动同步" class="headerlink" title="好处：我们以后修改只需要在本地修改即可，容器内会自动同步"></a>好处：我们以后修改只需要在本地修改即可，容器内会自动同步</h3><br/><h3 id="实战：安装MySQL"><a href="#实战：安装MySQL" class="headerlink" title="实战：安装MySQL"></a>实战：安装MySQL</h3><p>思考：MySQL的数据持久化的问题</p><pre class="line-numbers language-powershell" data-language="powershell"><code class="language-powershell"><span class="token comment"># 官方的运行，有配置密码</span>docker run <span class="token operator">--</span>name some-mysql <span class="token operator">-</span>e MYSQL_ROOT_PASSWORD=my-secret-pw <span class="token operator">-</span>d mysql:tag<span class="token operator">-</span>e 环境配置<span class="token comment">#本主机上测试</span>docker run <span class="token operator">-</span>d <span class="token operator">-</span>p 3310:3306 <span class="token operator">-</span>v <span class="token operator">/</span>home/chy/docker_volume/mysql/conf:<span class="token operator">/</span>etc/mysql/conf<span class="token punctuation">.</span>d <span class="token operator">-</span>v <span class="token operator">/</span>home/chy/docker_volume/mysql/<span class="token keyword">data</span>:<span class="token operator">/</span><span class="token keyword">var</span><span class="token operator">/</span>lib/mysql <span class="token operator">-</span>e MYSQL_ROOT_PASSWOR=chy <span class="token operator">--</span>name mysql_chy mysql:5<span class="token punctuation">.</span>7<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><br/><h3 id="具名和匿名挂载"><a href="#具名和匿名挂载" class="headerlink" title="具名和匿名挂载"></a>具名和匿名挂载</h3><pre class="line-numbers language-powershell" data-language="powershell"><code class="language-powershell"><span class="token comment"># 匿名挂载</span><span class="token operator">-</span>v 容器内路径docker run <span class="token operator">-</span>d <span class="token operator">-</span>P <span class="token operator">--</span>name nginx <span class="token operator">-</span>v <span class="token operator">/</span>etc/nginx nginx<span class="token comment"># 查看所有volume 的情况</span>chy@ocean:~$ docker volume <span class="token function">ls</span>DRIVER    VOLUME NAMElocal     2198da5e7d86ba5a3de24d1aa71ab791bcb22e94e9f84e3d2f1fef73dc54d6b7local     a361a22ba8ef6ffffd78f233128054bd668b3e5315adffab8cfa79b5921ea720local     ba8715be0d242c0a4d004e38640b10c5dd5833f021920f8bd6fc0e830e012ebb<span class="token comment"># 这里的乱码号就是匿名容器名</span>chy@ocean:~$ docker run <span class="token operator">-</span>d <span class="token operator">-</span>P <span class="token operator">--</span>name nginx01 <span class="token operator">-</span>v juming-nginx:<span class="token operator">/</span>etc/nginx nginxa221eae136c83f19fa5f87944efa8eb1e9ea7841b8b982c0b2a943626be6842dchy@ocean:~$ docker volume <span class="token function">ls</span>DRIVER    VOLUME NAMElocal     2198da5e7d86ba5a3de24d1aa71ab791bcb22e94e9f84e3d2f1fef73dc54d6b7local     a361a22ba8ef6ffffd78f233128054bd668b3e5315adffab8cfa79b5921ea720local     ba8715be0d242c0a4d004e38640b10c5dd5833f021920f8bd6fc0e830e012ebblocal     juming-nginx<span class="token comment">#这里的juming-nginx就是具名挂载</span><span class="token comment"># 通过 -v 卷名：容器内路径</span><span class="token comment"># 查看一下这个卷</span>chy@ocean:~$ docker volume inspect juming-nginx<span class="token punctuation">[</span>    <span class="token punctuation">&#123;</span>        <span class="token string">"CreatedAt"</span>: <span class="token string">"2023-05-05T10:03:11+08:00"</span><span class="token punctuation">,</span>        <span class="token string">"Driver"</span>: <span class="token string">"local"</span><span class="token punctuation">,</span>        <span class="token string">"Labels"</span>: null<span class="token punctuation">,</span>        <span class="token string">"Mountpoint"</span>: <span class="token string">"/var/lib/docker/volumes/juming-nginx/_data"</span><span class="token punctuation">,</span>        <span class="token string">"Name"</span>: <span class="token string">"juming-nginx"</span><span class="token punctuation">,</span>        <span class="token string">"Options"</span>: null<span class="token punctuation">,</span>        <span class="token string">"Scope"</span>: <span class="token string">"local"</span>    <span class="token punctuation">&#125;</span><span class="token punctuation">]</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>挂载位置在&#x2F;var&#x2F;lib&#x2F;docker&#x2F;volumes&#x2F;XXXX&#x2F;_data内</p><br/><pre class="line-numbers language-powershell" data-language="powershell"><code class="language-powershell"><span class="token comment">#如何确定是具名挂载还是匿名挂载，还是指定路径挂载！</span><span class="token operator">-</span>V容器内路径  <span class="token comment">#匿名挂载</span><span class="token operator">-</span>V卷名：容器内路径  <span class="token comment">#具名挂载</span><span class="token operator">-</span>V/宿主机路径：：容器内路径   <span class="token comment">#指定路径挂载！</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span></span></code></pre><p>&#x3D;&#x3D;扩展&#x3D;&#x3D;</p><pre class="line-numbers language-powershell" data-language="powershell"><code class="language-powershell"><span class="token comment">#通过-V容器内路径：rorw改变读写权限</span>ro    readonly   <span class="token comment">#只读</span>rw   readwrite  <span class="token comment">#可读可写</span><span class="token comment">#一旦这个了设置了容器权限，容器对我们挂载出来的内容就有限定了！</span>docker run <span class="token operator">-</span>d <span class="token operator">-</span>P-<span class="token operator">-</span>name nginx02 <span class="token operator">-</span>v juming-nginx:<span class="token operator">/</span>etc/nginx:ro nginxdocker run <span class="token operator">-</span>d <span class="token operator">-</span>P <span class="token operator">--</span>name nginx02 <span class="token operator">-</span>v juming-nginx:<span class="token operator">/</span>etc/nginx:rw nginx<span class="token comment">#ro只要看到ro就说明这个路径只能通过宿主机来操作，容器内部是无法操作</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><br/><h2 id="x3D-x3D-Docker-File-x3D-x3D"><a href="#x3D-x3D-Docker-File-x3D-x3D" class="headerlink" title="&#x3D;&#x3D;Docker File&#x3D;&#x3D;"></a><strong>&#x3D;&#x3D;Docker File&#x3D;&#x3D;</strong></h2><h3 id="dockerfile介绍"><a href="#dockerfile介绍" class="headerlink" title="dockerfile介绍"></a>dockerfile介绍</h3><br/><p>dockerfile是用来构建dokcer镜像的文件！命令参数脚本I</p><h3 id="构建步骤："><a href="#构建步骤：" class="headerlink" title="构建步骤："></a>构建步骤：</h3><p>1、编写一个dockerfile文件<br>2、docker build构建成为一个镜像<br>3、docker run运行镜像<br>4、docker push发布镜像(DockerHub、阿里云镜像仓库！)</p><p><img src="/pic/4a105decfbf4a49ac1c660091b87418e.png" alt="截图"></p><p><img src="/pic/f49f57bbfe2c62d2a5c54b898d94ce24.png" alt="截图"></p><br/><p><strong>dockerfile是面向开发的，我们以后要发布项目，做镜像，就需要编写dockerfile文件，这个文件十分简单！</strong></p><p>&#x3D;&#x3D;Docker镜像逐渐成为企业交付的标准，必须要掌握！&#x3D;&#x3D;</p><br/><p><strong>步骤：开发，部署，运维。。。缺一不可</strong></p><ul><li>DockerFile:构建文件，定义了一切的步骤，源代码</li><li>Dockerlmages:通过DockerFile构建生成的镜像，最终发布和运行的产品！</li><li>Docker容器：容器就是镜像运行起来提供服务器</li></ul><br/><p><img src="/pic/83ae3813ab76ef7f521bbbb61aa89f30.png" alt="截图"></p><p><img src="/pic/fe5789bef1d222aaed7c55d0d3a99709.png" alt="截图"></p><pre class="line-numbers language-powershell" data-language="powershell"><code class="language-powershell"><span class="token keyword">FROM</span>    <span class="token comment">#基础镜镜像，一切从这里开始构建</span>MAINTAINER    <span class="token comment">#镜像是谁写的，姓名+邮箱</span>RUN   <span class="token comment">#镜像构建的时候需要运行的命令</span>ADD   <span class="token comment">#步骤：tomcat镜像，这个tomcat压缩包！添加内容</span>WORKDIR       <span class="token comment">#镜像的工作目录</span>VOLUME      <span class="token comment">#挂载的目录</span>EXPOST    <span class="token comment">#保留端口配置</span>ENTRYPOINT  <span class="token comment">#指定这个容器启动的时候要运行的命令，可以追加命令</span>ONBUILD   <span class="token comment">#当构建一个被继承DockerFile这个时候就会运行ONBUILD的指令。触发指令。</span><span class="token function">COPY</span>    <span class="token comment">#类似ADD，将我们文件拷贝到镜像中</span>ENV   <span class="token comment">#构建的时候设置环境变量！</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><br/><br/><br/><br/><br/><br/><br/><br/><br/><p>Dockerfile就是用来构建docker镜像的构建文件！命令脚本！先体验一下！<br>通过这个脚本可以生成镜像，镜像是一层一层的，脚本一个个的命令，每个命令都是一层！</p><pre class="line-numbers language-powershell" data-language="powershell"><code class="language-powershell"><span class="token comment">#创建一个dockerfi1e文件，名字可以随机建议Dockerfi1e</span><span class="token comment">#文件中的内容指令（大写）参数</span><span class="token keyword">FROM</span> ubuntu:18<span class="token punctuation">.</span>04VOLUME <span class="token punctuation">[</span><span class="token string">"volume01"</span><span class="token punctuation">,</span><span class="token string">"volume02"</span><span class="token punctuation">]</span><span class="token comment">#匿名挂载，未来使用特别多</span>CMD <span class="token function">echo</span> <span class="token string">"-------end-------"</span>CMD <span class="token operator">/</span>bin/bash<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p><img src="/pic/9082fb7a18429cafcfa617e90b675cd1.png" alt="截图"></p><br/><pre class="line-numbers language-powershell" data-language="powershell"><code class="language-powershell"><span class="token string">"Mounts"</span>: <span class="token punctuation">[</span>    <span class="token punctuation">&#123;</span>        <span class="token string">"Type"</span>: <span class="token string">"volume"</span><span class="token punctuation">,</span>        <span class="token string">"Name"</span>: <span class="token string">"22c524e180d59166fb95b5012971cfe1e5566bca1ceb5ecabd26fbe20d6371c0"</span><span class="token punctuation">,</span>        <span class="token string">"Source"</span>: <span class="token string">"/var/lib/docker/volumes/22c524e180d59166fb95b5012971cfe1e5566bca1ceb5ecabd26fbe20d6371c0/_data"</span><span class="token punctuation">,</span>        <span class="token string">"Destination"</span>: <span class="token string">"volume01"</span><span class="token punctuation">,</span>        <span class="token string">"Driver"</span>: <span class="token string">"local"</span><span class="token punctuation">,</span>        <span class="token string">"Mode"</span>: <span class="token string">""</span><span class="token punctuation">,</span>        <span class="token string">"RW"</span>: true<span class="token punctuation">,</span>        <span class="token string">"Propagation"</span>: <span class="token string">""</span>    <span class="token punctuation">&#125;</span><span class="token punctuation">,</span>    <span class="token punctuation">&#123;</span>        <span class="token string">"Type"</span>: <span class="token string">"volume"</span><span class="token punctuation">,</span>        <span class="token string">"Name"</span>: <span class="token string">"20a0e49e357f110e9452991d152ec1a8ac34d48a8c77b0fed84a1d8232f6282b"</span><span class="token punctuation">,</span>        <span class="token string">"Source"</span>: <span class="token string">"/var/lib/docker/volumes/20a0e49e357f110e9452991d152ec1a8ac34d48a8c77b0fed84a1d8232f6282b/_data"</span><span class="token punctuation">,</span>        <span class="token string">"Destination"</span>: <span class="token string">"volume02"</span><span class="token punctuation">,</span>        <span class="token string">"Driver"</span>: <span class="token string">"local"</span><span class="token punctuation">,</span>        <span class="token string">"Mode"</span>: <span class="token string">""</span><span class="token punctuation">,</span>        <span class="token string">"RW"</span>: true<span class="token punctuation">,</span>        <span class="token string">"Propagation"</span>: <span class="token string">""</span>    <span class="token punctuation">&#125;</span><span class="token punctuation">]</span><span class="token punctuation">,</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>测试一下刚才的文件是否同步出去了！</p><br/><p>这种方式我们未来使用的十分多，因为我们通常会构建自己的镜像！</p><br/><p>假设构建镜像时候没有挂载卷，要手动镜像挂载V卷名：容器内路径！</p><br/><h3 id="数据卷挂载"><a href="#数据卷挂载" class="headerlink" title="数据卷挂载"></a>数据卷挂载</h3><p>多个容器实现数据同步</p><p><img src="/pic/eb4af3f4a09cdc70786469482529743b.png" alt="截图"></p><pre class="line-numbers language-powershell" data-language="powershell"><code class="language-powershell">chy@ocean:~<span class="token operator">/</span>docker_volume$ docker run <span class="token operator">-</span>it <span class="token operator">--</span>name chy_ubuntu02 <span class="token operator">--</span>volumes-<span class="token keyword">from</span> chy_ubuntu01 chy_ubuntu:1<span class="token punctuation">.</span>0  <span class="token operator">/</span>bin/bashroot@bd9d9eddee71:<span class="token operator">/</span><span class="token comment"># ls</span>bin  boot  dev  etc  home  lib  lib64  media  mnt  opt  proc  root  run  sbin  srv  sys  tmp  usr  <span class="token keyword">var</span>  volume01  volume02root@bd9d9eddee71:<span class="token operator">/</span><span class="token comment"># cd volume01</span>root@bd9d9eddee71:<span class="token operator">/</span>volume01<span class="token comment"># ls</span>docker01<span class="token punctuation">.</span>txtroot@bd9d9eddee71:<span class="token operator">/</span>volume01<span class="token comment"># touch docker02.txt</span>root@bd9d9eddee71:<span class="token operator">/</span>volume01<span class="token comment"># ls</span>docker01<span class="token punctuation">.</span>txt  docker02<span class="token punctuation">.</span>txtroot@bd9d9eddee71:<span class="token operator">/</span>volume01<span class="token comment"># ls</span>docker01<span class="token punctuation">.</span>txt  docker02<span class="token punctuation">.</span>txt  docker03<span class="token punctuation">.</span>txtroot@bd9d9eddee71:<span class="token operator">/</span>volume01<span class="token comment"># </span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><pre class="line-numbers language-powershell" data-language="powershell"><code class="language-powershell">chy@ocean:~$ docker  run <span class="token operator">-</span>it <span class="token operator">--</span>name chy_ubuntu03 <span class="token operator">--</span>volumes-<span class="token keyword">from</span> chy_ubuntu02 chy_ubuntu:1<span class="token punctuation">.</span>0 <span class="token operator">/</span>bin/bashroot@0872aa038cfb:<span class="token operator">/</span><span class="token comment"># ls</span>bin   dev  home  lib64  mnt  proc  run   srv  tmp  <span class="token keyword">var</span>       volume02boot  etc  lib   media  opt  root  sbin  sys  usr  volume01root@0872aa038cfb:<span class="token operator">/</span><span class="token comment"># cd volume01</span>root@0872aa038cfb:<span class="token operator">/</span>volume01<span class="token comment"># ls</span>docker01<span class="token punctuation">.</span>txt  docker02<span class="token punctuation">.</span>txtroot@0872aa038cfb:<span class="token operator">/</span>volume01<span class="token comment"># touch docker03.txt</span>root@0872aa038cfb:<span class="token operator">/</span>volume01<span class="token comment"># ls</span>docker01<span class="token punctuation">.</span>txt  docker02<span class="token punctuation">.</span>txt  docker03<span class="token punctuation">.</span>txtroot@0872aa038cfb:<span class="token operator">/</span>volume01<span class="token comment"># </span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>&#x3D;&#x3D;主要使用的是 –volumes-from 实现容器间的数据共享 &#x3D;&#x3D;</p><p><strong>结论：</strong></p><p>容器之间配置信息的传递，数据卷容器的生命周期一直持续到没有容器使用为止。<br>但是一旦你持久化到了本地，这个时候，本地的数据是不会删除的！</p><h3 id="docker-compose常用命令"><a href="#docker-compose常用命令" class="headerlink" title="docker-compose常用命令"></a>docker-compose常用命令</h3><br/><pre class="line-numbers language-powershell" data-language="powershell"><code class="language-powershell">docker-compose up <span class="token operator">-</span>d nginx                     构建建启动nignx容器docker-compose exec nginx bash            登录到nginx容器中docker-compose down                              删除所有nginx容器<span class="token punctuation">,</span>镜像docker-compose <span class="token function">ps</span>                                   显示所有容器docker-compose restart nginx                   重新启动nginx容器docker-compose run <span class="token operator">--</span>no-deps <span class="token operator">--</span><span class="token function">rm</span> php-fpm php <span class="token operator">-</span>v  在php-fpm中不启动关联容器，并容器执行php <span class="token operator">-</span>v 执行完成后删除容器docker-compose build nginx                     构建镜像 。        docker-compose build <span class="token operator">--</span>no-cache nginx   不带缓存的构建。docker-compose logs  nginx                     查看nginx的日志 docker-compose logs <span class="token operator">-</span>f nginx                   查看nginx的实时日志docker-compose config  <span class="token operator">-</span>q                        验证（docker-compose<span class="token punctuation">.</span>yml）文件配置，当配置正确时，不输出任何内容，当文件配置错误，输出错误信息。 docker-compose events <span class="token operator">--</span>json nginx       以json的形式输出nginx的docker日志docker-compose pause nginx                 暂停nignx容器docker-compose unpause nginx             恢复ningx容器docker-compose <span class="token function">rm</span> nginx                       删除容器（删除前必须关闭容器）docker-compose stop nginx                    停止nignx容器docker-compose <span class="token function">start</span> nginx                    启动nignx容器<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h2 id="Docker-小结"><a href="#Docker-小结" class="headerlink" title="Docker 小结"></a>Docker 小结</h2><p><img src="/pic/82819ea9bbc4474aba79ad5353b2f288.png" alt="截图"></p><br/><h2 id="x3D-x3D-Docker-网络（未来安排，集成部署）-x3D-x3D"><a href="#x3D-x3D-Docker-网络（未来安排，集成部署）-x3D-x3D" class="headerlink" title="&#x3D;&#x3D;Docker 网络（未来安排，集成部署）&#x3D;&#x3D;"></a><strong>&#x3D;&#x3D;Docker 网络（未来安排，集成部署）&#x3D;&#x3D;</strong></h2><h3 id="理解Docker0"><a href="#理解Docker0" class="headerlink" title="理解Docker0"></a>理解Docker0</h3><p>清空所有环境</p><blockquote><p> 测试</p></blockquote><pre class="line-numbers language-powershell" data-language="powershell"><code class="language-powershell">1: lo: &lt;LOOPBACK<span class="token punctuation">,</span>UP<span class="token punctuation">,</span>LOWER_UP> mtu 65536 qdisc noqueue state UNKNOWN <span class="token function">group</span> default qlen 1000    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00    inet 127<span class="token punctuation">.</span>0<span class="token punctuation">.</span>0<span class="token punctuation">.</span>1/8 scope host lo<span class="token comment">#本机回环地址</span>       valid_lft forever preferred_lft forever    inet6 ::1/128 scope host        valid_lft forever preferred_lft forever2: enp5s0: &lt;BROADCAST<span class="token punctuation">,</span>MULTICAST<span class="token punctuation">,</span>UP<span class="token punctuation">,</span>LOWER_UP> mtu 1500 qdisc fq_codel state UP <span class="token function">group</span> default qlen 1000    link/ether c8:7f:54:57:48:62 brd ff:ff:ff:ff:ff:ff    inet 192<span class="token punctuation">.</span>168<span class="token punctuation">.</span>1<span class="token punctuation">.</span>40/24 brd 192<span class="token punctuation">.</span>168<span class="token punctuation">.</span>1<span class="token punctuation">.</span>255 scope global dynamic noprefixroute enp5s0       valid_lft 5419sec preferred_lft 5419sec<span class="token comment">#阿里云内网地址</span>    inet6 fe80::f8aa:d5c6:44bb:4a07/64 scope link noprefixroute        valid_lft forever preferred_lft forever       4: docker0: &lt;BROADCAST<span class="token punctuation">,</span>MULTICAST<span class="token punctuation">,</span>UP<span class="token punctuation">,</span>LOWER_UP> mtu 1500 qdisc noqueue state UP <span class="token function">group</span> default     link/ether 02:42:87:11:cb:17 brd ff:ff:ff:ff:ff:ff    inet 172<span class="token punctuation">.</span>17<span class="token punctuation">.</span>0<span class="token punctuation">.</span>1/16 brd 172<span class="token punctuation">.</span>17<span class="token punctuation">.</span>255<span class="token punctuation">.</span>255 scope global docker0       valid_lft forever preferred_lft forever<span class="token comment">#docker地址</span>    inet6 fe80::42:87ff:fe11:cb17/64 scope link        valid_lft forever preferred_lft forever<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>三个网络</p><br/><pre class="line-numbers language-powershell" data-language="powershell"><code class="language-powershell">root@e75dea24bd59:~<span class="token operator">/</span>catkin_ws<span class="token comment"># ifconfig</span>eth0: flags=4163&lt;UP<span class="token punctuation">,</span>BROADCAST<span class="token punctuation">,</span>RUNNING<span class="token punctuation">,</span>MULTICAST>  mtu 1500        inet 172<span class="token punctuation">.</span>17<span class="token punctuation">.</span>0<span class="token punctuation">.</span>4  netmask 255<span class="token punctuation">.</span>255<span class="token punctuation">.</span>0<span class="token punctuation">.</span>0  broadcast 172<span class="token punctuation">.</span>17<span class="token punctuation">.</span>255<span class="token punctuation">.</span>255        ether 02:42:<span class="token function">ac</span>:11:00:04  txqueuelen 0  <span class="token punctuation">(</span>Ethernet<span class="token punctuation">)</span>        RX packets 35  bytes 4999 <span class="token punctuation">(</span>4<span class="token punctuation">.</span>9 KB<span class="token punctuation">)</span>        RX errors 0  dropped 0  overruns 0  frame 0        TX packets 0  bytes 0 <span class="token punctuation">(</span>0<span class="token punctuation">.</span>0 B<span class="token punctuation">)</span>        TX errors 0  dropped 0 overruns 0  carrier 0  collisions 0lo: flags=73&lt;UP<span class="token punctuation">,</span>LOOPBACK<span class="token punctuation">,</span>RUNNING>  mtu 65536        inet 127<span class="token punctuation">.</span>0<span class="token punctuation">.</span>0<span class="token punctuation">.</span>1  netmask 255<span class="token punctuation">.</span>0<span class="token punctuation">.</span>0<span class="token punctuation">.</span>0        loop  txqueuelen 1000  <span class="token punctuation">(</span>Local Loopback<span class="token punctuation">)</span>        RX packets 0  bytes 0 <span class="token punctuation">(</span>0<span class="token punctuation">.</span>0 B<span class="token punctuation">)</span>        RX errors 0  dropped 0  overruns 0  frame 0        TX packets 0  bytes 0 <span class="token punctuation">(</span>0<span class="token punctuation">.</span>0 B<span class="token punctuation">)</span>        TX errors 0  dropped 0 overruns 0  carrier 0  collisions 0root@e75dea24bd59:~<span class="token operator">/</span>catkin_ws<span class="token comment"># read escape sequence</span>chy@ocean:~$ docker <span class="token function">ps</span> CONTAINER ID   IMAGE                 COMMAND                   CREATED        STATUS          PORTS                                       NAMESe75dea24bd59   4483fce64730          <span class="token string">"/entrypoint-melodic…"</span>   46 hours ago   Up 2 minutes                                                elastic_brownd11b0f59e8b1   portainer/portainer   <span class="token string">"/portainer"</span>              2 days ago     Up 11 minutes   0<span class="token punctuation">.</span>0<span class="token punctuation">.</span>0<span class="token punctuation">.</span>0:8088->9000/tcp<span class="token punctuation">,</span> :::8088->9000/tcp   elated_davincichy@ocean:~$ ping 172<span class="token punctuation">.</span>17<span class="token punctuation">.</span>0<span class="token punctuation">.</span>4PING 172<span class="token punctuation">.</span>17<span class="token punctuation">.</span>0<span class="token punctuation">.</span>4 <span class="token punctuation">(</span>172<span class="token punctuation">.</span>17<span class="token punctuation">.</span>0<span class="token punctuation">.</span>4<span class="token punctuation">)</span> 56<span class="token punctuation">(</span>84<span class="token punctuation">)</span> bytes of <span class="token keyword">data</span><span class="token punctuation">.</span>64 bytes <span class="token keyword">from</span> 172<span class="token punctuation">.</span>17<span class="token punctuation">.</span>0<span class="token punctuation">.</span>4: icmp_seq=1 ttl=64 time=0<span class="token punctuation">.</span>120 ms64 bytes <span class="token keyword">from</span> 172<span class="token punctuation">.</span>17<span class="token punctuation">.</span>0<span class="token punctuation">.</span>4: icmp_seq=2 ttl=64 time=0<span class="token punctuation">.</span>078 ms64 bytes <span class="token keyword">from</span> 172<span class="token punctuation">.</span>17<span class="token punctuation">.</span>0<span class="token punctuation">.</span>4: icmp_seq=3 ttl=64 time=0<span class="token punctuation">.</span>086 ms<span class="token comment">#Linux可以ping通docker容器</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><blockquote><p>原理</p></blockquote><br/><p>1、我们每启动一个docker容器，docker就会给docker容器分配一个ip,我们只要安装了docker,就会有一个网卡docker0<br>桥接模式，使用的技术是evth-pair技术！</p><br/><pre class="line-numbers language-powershell" data-language="powershell"><code class="language-powershell"><span class="token comment">#我们发现这个容器带来网卡，都是一对对的</span><span class="token comment"># evth-pair就是一对的虚拟设备接口，他们都是成对出现的，一段连着协议，一段彼此相连</span><span class="token comment">#正因为有这个特性，evth-pair充当一个桥梁，连接各种虚拟网络设备的</span><span class="token comment">#Openstac,Docker容器之间的连接，OVS的连接，都是使用evth-pair技术</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span></span></code></pre><p><img src="/pic/9bffa4485e6f6dbfc14beeb7a8fd9dd1.png" alt="截图"></p><br/><p>结论：tomcat01和tomcat(02是公用的一个路由器，docker0.<br>所有的容器不指定网络的情况下，都是docker0路由的，docker:会给我们的容器分配一个默认的可用IP<br>255.255.0.1&#x2F;16域局域网<br>0000000.000000.000000.000000<br>255.255.255.255</p><blockquote><p>结论</p></blockquote><br/><p>Docker 使用的是Linux的桥接，宿主机中是IGDocker容器的网桥 docker0</p><br/><br/><p><img src="/pic/09cd8423a8f4a1175f54bfa430c0ac5a.png" alt="截图"></p><p>Docker 中所哟的网络接口都是虚拟的，虚拟的转发效率高（内网传递文件）</p><br/><p>只要容器删除，对应网桥一对也没了</p><br/><h3 id="自定义网络"><a href="#自定义网络" class="headerlink" title="自定义网络"></a>自定义网络</h3><br/><blockquote><p> 查看所有的docker网络</p></blockquote><pre class="line-numbers language-powershell" data-language="powershell"><code class="language-powershell">chy@ocean:~$ docker network <span class="token function">ls</span>NETWORK ID     NAME                  DRIVER    SCOPE0e0cf6ea89e8   bridge                bridge    local2a0cbafc7650   composetest_default   bridge    local9a4f385414eb   host                  host      local7772e34bde04   none                  null      local<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h3 id="网络模式"><a href="#网络模式" class="headerlink" title="网络模式"></a>网络模式</h3><p>bridge:桥接docker(默认，自己创建也是使用桥接模式)</p><p>none:不配置网络</p><p>host:和宿主机共享网络</p><p>container:容器网络连通！（用的少！局限很大）</p><br/><pre class="line-numbers language-powershell" data-language="powershell"><code class="language-powershell">Usage:  docker network COMMANDManage networksCommands:  connect     Connect a container to a network  create      Create a network  disconnect  Disconnect a container <span class="token keyword">from</span> a network  inspect     Display detailed information on one or more networks  <span class="token function">ls</span>          List networks  prune       Remove all unused networks  <span class="token function">rm</span>          Remove one or more networksRun <span class="token string">'docker network COMMAND --help'</span> <span class="token keyword">for</span> more information on a command<span class="token punctuation">.</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><pre class="line-numbers language-powershell" data-language="powershell"><code class="language-powershell"><span class="token comment">#docker0特点：默认，域名不能访问，--1ink可以打通连接！</span><span class="token comment">#我们可以自定义一个网络！</span><span class="token comment">#--driver bridge</span><span class="token comment">#--subnet192.168.0.0/16</span><span class="token comment">#--gateway192.168.0.1</span><span class="token namespace">[root@kuangshen /]</span>docker network create <span class="token operator">--</span>driver bridge <span class="token operator">--</span>subnet 192<span class="token punctuation">.</span>168<span class="token punctuation">.</span>0<span class="token punctuation">.</span>0/16 <span class="token operator">--</span>gateway 192<span class="token punctuation">.</span>168<span class="token punctuation">.</span>0<span class="token punctuation">.</span>1myneteb21272b3a35 ceaba11b4aa5bbff131c3fb09c4790f0852ed4540707438db052<span class="token namespace">[root@kuangshen /]</span>docker network 1s<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>]]></content>
      
      
      <categories>
          
          <category> docker </category>
          
      </categories>
      
      
        <tags>
            
            <tag> docker </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>c++核心编程</title>
      <link href="/2023/04/11/c-base2/"/>
      <url>/2023/04/11/c-base2/</url>
      
        <content type="html"><![CDATA[<h1 id="内存分区模型"><a href="#内存分区模型" class="headerlink" title="内存分区模型"></a>内存分区模型</h1><p>c++程序执行时，将内存大方向分为4个区域：</p><ul><li>代码区：存放函数体的<strong>二进制代码</strong>，由操作系统进行管理的</li><li>全局区：存放全局变量和静态变量以及常量</li><li>栈区：由编译器自动分配释放，存放函数的参数值，局部变量等</li><li>堆区：由程序员分配和释放，若程序员不释放，程序结束时由操作系统回收</li></ul><p>内存四区的意义：</p><p><em><strong>不同区域存放的数据，赋予不同声明周期给我们更大的灵活编程</strong></em></p><h2 id="程序运行前"><a href="#程序运行前" class="headerlink" title="程序运行前"></a>程序运行前</h2><p>在程序编译后，生成了exe可执行程序，<strong>未执行该程序前</strong>分为两个区域</p><p><strong>代码区：</strong></p><ul><li>存放CPU执行的机器指令</li><li>代码区是共享的，共享的目的是对于频繁被执行的程序，只需要在内存中有一份代码即可</li><li>代码区是只读的，使其只读的原因是防止程序意外的修改了它的指令</li></ul><p><strong>全局区：</strong></p><ul><li><strong>全局变量</strong>和<strong>静态变量</strong>存放在此</li><li>全局区还包含**常量区，字符串常量和其他常量(const修饰的全局变量)**也存在于此</li><li>该区域的数据在程序执行结束后由<strong>操作系统</strong>释放</li></ul><blockquote><p>注意：局部变量和const修饰的局部变量不在全局区里</p></blockquote><p>总结：</p><ul><li>c++中程序运行前分为全局区和代码区</li><li>代码区特点是共享和只读</li><li>全局区中存放全局变量、静态变量、常量 </li><li>常量区存放const修饰的全局常量和字符串常量</li></ul><p>示例</p><pre class="line-numbers language-c++" data-language="c++"><code class="language-c++">#include&lt;iostream&gt;using namespace std;&#x2F;&#x2F;全局变量int g_a&#x3D;10;int g_b&#x3D;20;const int c_g_a&#x3D;10;const int c_g_b&#x3D;20;int main()&#123;    &#x2F;&#x2F;全局区    &#x2F;&#x2F;全局变量、静态变量，常量    &#x2F;&#x2F;创建普通局部变量    int a&#x3D;10;    int b &#x3D;20;    cout&lt;&lt;&quot;局部变量a的地址:&quot;&lt;&lt;(long long)&amp;a&lt;&lt;endl;    cout&lt;&lt;&quot;局部变量b的地址:&quot;&lt;&lt;(long long)&amp;b&lt;&lt;endl;&#x2F;&#x2F;     局部变量a的地址:140723942533960&#x2F;&#x2F; 局部变量b的地址:140723942533964    cout&lt;&lt;&quot;全局变量g_a的地址:&quot;&lt;&lt;(long long)&amp;g_a&lt;&lt;endl;    cout&lt;&lt;&quot;全局变量g_b的地址:&quot;&lt;&lt;(long long)&amp;g_b&lt;&lt;endl;&#x2F;&#x2F;     全局变量g_a的地址:94603780857872&#x2F;&#x2F; 全局变量g_b的地址:94603780857876    &#x2F;&#x2F;静态变量    static int s_a&#x3D;10;    static int s_b&#x3D;20;    cout&lt;&lt;&quot;静态变量s_a的地址:&quot;&lt;&lt;(long long)&amp;s_a&lt;&lt;endl;    cout&lt;&lt;&quot;静态变量s_b的地址:&quot;&lt;&lt;(long long)&amp;s_b&lt;&lt;endl;&#x2F;&#x2F;     静态变量s_a的地址:94603780857880&#x2F;&#x2F; 静态变量s_b的地址:94603780857884    &#x2F;&#x2F;常量    &#x2F;&#x2F;字符串常量    cout&lt;&lt;&quot;字符串常量的地址：&quot;&lt;&lt;(long long)&amp;&quot;hello world&quot;&lt;&lt;endl;&#x2F;&#x2F; 字符串常量的地址：94603778756100    &#x2F;&#x2F;const修饰的变量：const修饰的全局变量以及const修饰的局部变量    cout&lt;&lt;&quot;全局变量c_g_a的地址:&quot;&lt;&lt;(long long)&amp;c_g_a&lt;&lt;endl;    cout&lt;&lt;&quot;全局变量c_g_b的地址:&quot;&lt;&lt;(long long)&amp;c_g_b&lt;&lt;endl;&#x2F;&#x2F;     全局变量c_g_a的地址:94603778755912&#x2F;&#x2F; 全局变量c_g_b的地址:94603778755916    const int c_l_a&#x3D;10;    const int c_l_b&#x3D;20;    cout&lt;&lt;&quot;局部变量c_l_a的地址:&quot;&lt;&lt;(long long)&amp;c_l_a&lt;&lt;endl;    cout&lt;&lt;&quot;局部变量c_l_b的地址:&quot;&lt;&lt;(long long)&amp;c_l_a&lt;&lt;endl;&#x2F;&#x2F;     局部变量c_l_a的地址:140723942533968&#x2F;&#x2F; 局部变量c_l_b的地址:140723942533968    return 0;&#125;<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h2 id="程序运行后"><a href="#程序运行后" class="headerlink" title="程序运行后"></a>程序运行后</h2><p><strong>栈区：</strong></p><ul><li>由编译器自动释放，存放函数的参数值，局部变量等</li></ul><blockquote><p>注意事项：不要返回局部变量的地址，栈区开辟的数据由编译器自动释放</p></blockquote><p>示例</p><pre class="line-numbers language-c++" data-language="c++"><code class="language-c++">#include&lt;iostream&gt;using namespace std;&#x2F;&#x2F;栈区数据的注意事项 ----不要返回局部变量的地址&#x2F;&#x2F;栈区的数据由编译器管理开辟与释放int* func(int b)&#123;&#x2F;&#x2F;形参数据也会放在栈区    b&#x3D;100;    int a &#x3D;10;&#x2F;&#x2F;局部变量存放栈区，栈区数据在函数执行完后自动释放    return &amp;a;&#125;int main()&#123;    int *p&#x3D;func(1);    cout&lt;&lt;*p&lt;&lt;endl;    system(&quot;read -p &#39;Press Enter to continue...&#39; var&quot;);    return 0;&#125;<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p><strong>堆区：</strong></p><p>由程序员分配释放，若程序员不释放，程序结束时由系统回收</p><p>在c++中主要利用new在堆区开辟内存</p><p>示例</p><pre class="line-numbers language-c++" data-language="c++"><code class="language-c++">#include&lt;iostream&gt;using namespace std;int *func()&#123;    &#x2F;&#x2F;利用new关键字可以将数据开辟到堆区    &#x2F;&#x2F;指针本质也是局部变量，放在栈上。指针保存的数据放在堆区    int *p&#x3D;new int(10);    return p;&#125;int main()&#123;    &#x2F;&#x2F;在堆区开辟数据    int *p&#x3D;func();    cout&lt;&lt;*p&lt;&lt;endl;    system(&quot;read -p &#39;Press Enter to continue...&#39; var&quot;);    return 0;&#125;<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h2 id="new操作符"><a href="#new操作符" class="headerlink" title="new操作符"></a>new操作符</h2><p>c++中利用new操作符在堆区开辟数据</p><p>堆区开辟的数据由程序员手动开辟与释放，释放利用操作符delect</p><p>语法：new 数据类型</p><p>利用new创建的数据，会返回该数据对应的类型的指针</p><h1 id="引用"><a href="#引用" class="headerlink" title="引用"></a>引用</h1><h2 id="引用的基本能使用"><a href="#引用的基本能使用" class="headerlink" title="引用的基本能使用"></a>引用的基本能使用</h2><p>作用：给变量起别名</p><p>语法：数据类型 &amp;别名&#x3D;原名</p><p>示例：</p><pre class="line-numbers language-c++" data-language="c++"><code class="language-c++">#include&lt;iostream&gt;using namespace std;int main()&#123;    int a&#x3D;10;    int &amp;b&#x3D;a;    b&#x3D;20;    cout&lt;&lt;a&lt;&lt;endl;    &#x2F;&#x2F;20<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p><a href="https://blog.csdn.net/qq_58325487/article/details/124691945">c++引用入门</a></p><h2 id="引用的注意事项"><a href="#引用的注意事项" class="headerlink" title="引用的注意事项"></a>引用的注意事项</h2><ul><li>引用必须初始化</li><li>引用初始化后，不可以改变</li></ul><p>示例：</p><pre class="line-numbers language-c++" data-language="c++"><code class="language-c++">#include&lt;iostream&gt;using namespace std;int main()&#123;    int a&#x3D;10;    &#x2F;&#x2F;int&amp; b; &#x2F;&#x2F;错误，必须要初始化    int &amp;b&#x3D;a;    int c&#x3D;20;    b&#x3D;c;&#x2F;&#x2F;赋值操作不是更改引用    cout&lt;&lt;a&lt;&lt;endl;    &#x2F;&#x2F;20&#125;<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h2 id="y引用做函数参数"><a href="#y引用做函数参数" class="headerlink" title="y引用做函数参数"></a>y引用做函数参数</h2><p>作用：函数传参时，可以利用引用的技术让形参修饰实参</p><p>有点：可以简化指针修改实参</p><p>示例：</p><pre class="line-numbers language-c++" data-language="c++"><code class="language-c++"><span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre>]]></content>
      
      
      <categories>
          
          <category> c++ </category>
          
      </categories>
      
      
        <tags>
            
            <tag> c++ </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>linux环境下c++实现通讯录系统</title>
      <link href="/2023/04/11/c-base/"/>
      <url>/2023/04/11/c-base/</url>
      
        <content type="html"><![CDATA[<h1 id="通讯录系统"><a href="#通讯录系统" class="headerlink" title="通讯录系统"></a>通讯录系统</h1><p>该系统具有下面7种操作：</p><hr><p>***** 1、添加联系人*****</p><p>***** 2、显示联系人*****</p><p>***** 3、删除联系人*****</p><p>***** 4、查找联系人*****</p><p>***** 5、修改联系人*****</p><p>***** 6、清空联系人*****</p><p>***** 0、退出通讯录 ****</p><hr><p>源代码如下：</p><pre class="line-numbers language-c++" data-language="c++"><code class="language-c++">&#x2F;*封装函数显示界面，如void showMenu()*&#x2F;#include&lt;iostream&gt;#include &lt;unistd.h&gt;#include &lt;string&gt;using namespace std;#define MAX 1000&#x2F;&#x2F;联系人结构体struct Person&#123;    string m_Name;    int m_Sex;    int m_Age;    string m_Phone;    string m_Addr;&#125;;&#x2F;&#x2F;通讯录结构体struct Addressbooks&#123;    Person personArray [MAX];    int m_Size;&#x2F;&#x2F;通讯录中人员个数&#125;;&#x2F;&#x2F;1.添加联系人void addPerson(Addressbooks *abs)&#123;        if(abs-&gt;m_Size&#x3D;&#x3D;MAX)&#123;            cout&lt;&lt;&quot;通讯录已满，无法添加&quot;&lt;&lt; endl;            return;        &#125;else&#123;            string name;            cout&lt;&lt;&quot;请输入姓名：&quot;&lt;&lt;endl;            cin&gt;&gt;name;            abs-&gt;personArray[abs-&gt;m_Size].m_Name&#x3D;name;            cout&lt;&lt;&quot;1---男&quot;&lt;&lt;endl;            cout&lt;&lt;&quot;2---女&quot;&lt;&lt;endl;            int sex&#x3D;0;            while(true)&#123;                cin&gt;&gt;sex;                    if(sex&#x3D;&#x3D;1||sex&#x3D;&#x3D;2)&#123;                        abs-&gt;personArray[abs-&gt;m_Size].m_Sex&#x3D;sex;                        break;         &#125;            cout&lt;&lt;&quot;输入有误，请重新输入&quot;&lt;&lt;endl;        &#125;        cout&lt;&lt;&quot;请输入年龄&quot;&lt;&lt;endl;        int age &#x3D;0;        cin&gt;&gt;age;        abs-&gt;personArray[abs-&gt;m_Size].m_Age&#x3D;age;        cout&lt;&lt;&quot;请输入电话号码&quot;&lt;&lt;endl;        string iphone;        cin&gt;&gt;iphone;        abs-&gt;personArray[abs-&gt;m_Size].m_Phone&#x3D;iphone;        cout&lt;&lt;&quot;请输入家庭住址&quot;&lt;&lt;endl;        string address;        cin&gt;&gt;address;        abs-&gt;personArray[abs-&gt;m_Size].m_Addr&#x3D;address;        abs-&gt;m_Size++;        cout&lt;&lt;&quot;添加成功&quot;&lt;&lt;endl;        &#125;                        &#x2F;&#x2F;linux按任意键继续命令        system(&quot;read -p &#39;Press Enter to continue...&#39; var&quot;);        system(&quot;clear&quot;);&#125;&#x2F;&#x2F;2.示所有的联系人void showPerson(Addressbooks *abs)&#123;    if(abs-&gt;m_Size&#x3D;&#x3D;0)&#123;        cout&lt;&lt;&quot;当前记录为空&quot;&lt;&lt;endl;    &#125;else&#123;        for(int i&#x3D;0;i&lt;abs-&gt;m_Size;i++)&#123;            cout&lt;&lt;&quot;姓名：&quot;&lt;&lt;abs-&gt;personArray[i].m_Name&lt;&lt;&quot;\t&quot;;            cout&lt;&lt;&quot;性别：&quot;&lt;&lt;(abs-&gt;personArray[i].m_Sex &#x3D;&#x3D;1 ?&quot;男&quot;:&quot;女&quot;)&lt;&lt;&quot;\t&quot;;            cout&lt;&lt;&quot;年龄：&quot;&lt;&lt;abs-&gt;personArray[i].m_Age&lt;&lt;&quot;\t&quot;;            cout&lt;&lt;&quot;电话：&quot;&lt;&lt;abs-&gt;personArray[i].m_Phone&lt;&lt;&quot;\t&quot;;            cout&lt;&lt;&quot;地址：&quot;&lt;&lt;abs-&gt;personArray[i].m_Addr&lt;&lt;endl;        &#125;    &#125;                &#x2F;&#x2F;linux按任意键继续命令            system(&quot;read -p &#39;Press Enter to continue...&#39; var&quot;);            system(&quot;clear&quot;);&#x2F;&#x2F;清屏&#125;    &#x2F;&#x2F;检测联系人是否存在，如果存在返回数组的具体位置，不存在返回-1int isExist(Addressbooks *abs,string name)&#123;        for(int i&#x3D;0;i&lt;abs-&gt;m_Size;i++)&#123;            if(abs-&gt;personArray[i].m_Name&#x3D;&#x3D;name)&#123;                    return i;            &#125;        &#125;        return -1;&#125;&#x2F;&#x2F;3.删除指定联系人void deletePerson(Addressbooks *abs)&#123;    cout&lt;&lt;&quot;请输入您要删除的联系人&quot;&lt;&lt;endl;    string name;    cin&gt;&gt;name;    int ret&#x3D;isExist(abs,name);    if(ret&#x3D;&#x3D;-1)&#123;        cout&lt;&lt;&quot;查无此人&quot;&lt;&lt;endl;    &#125;else&#123;            for(int i&#x3D;ret;i&lt;abs-&gt;m_Size;i++)&#123;                &#x2F;&#x2F;数据迁移                abs-&gt;personArray[i]&#x3D;abs-&gt;personArray[i+1];            &#125;            abs-&gt;m_Size--;            cout&lt;&lt;&quot;删除成功&quot;&lt;&lt;endl;    &#125;                &#x2F;&#x2F;linux按任意键继续命令            system(&quot;read -p &#39;Press Enter to continue...&#39; var&quot;);            system(&quot;clear&quot;);&#x2F;&#x2F;清屏&#125;&#x2F;&#x2F;4.查找联系人信息void findPerson(Addressbooks *abs)&#123;    cout&lt;&lt;&quot;请输入您要查找的联系人&quot;&lt;&lt;endl;    string name;    cin&gt;&gt;name;    int ret &#x3D;isExist(abs,name);    if(ret!&#x3D;-1)&#123;            cout&lt;&lt;&quot;姓名：&quot;&lt;&lt;abs-&gt;personArray[ret].m_Name&lt;&lt;&quot;\t&quot;;            cout&lt;&lt;&quot;性别：&quot;&lt;&lt;(abs-&gt;personArray[ret].m_Sex &#x3D;&#x3D;1 ?&quot;男&quot;:&quot;女&quot;)&lt;&lt;&quot;\t&quot;;            cout&lt;&lt;&quot;年龄：&quot;&lt;&lt;abs-&gt;personArray[ret].m_Age&lt;&lt;&quot;\t&quot;;            cout&lt;&lt;&quot;电话：&quot;&lt;&lt;abs-&gt;personArray[ret].m_Phone&lt;&lt;&quot;\t&quot;;            cout&lt;&lt;&quot;地址：&quot;&lt;&lt;abs-&gt;personArray[ret].m_Addr&lt;&lt;endl;    &#125;else&#123;        cout&lt;&lt;&quot;查无此人&quot;&lt;&lt;endl;    &#125;                &#x2F;&#x2F;linux按任意键继续命令            system(&quot;read -p &#39;Press Enter to continue...&#39; var&quot;);            system(&quot;clear&quot;);&#x2F;&#x2F;清屏&#125;&#x2F;&#x2F;5.修改联系人void modifyPerson(Addressbooks* abs)&#123;    cout&lt;&lt;&quot;请输入你要修改的联系人&quot;&lt;&lt;endl;    string name;    cin&gt;&gt;name;    int ret &#x3D; isExist(abs,name);    if(ret!&#x3D;-1)&#123;        string name;        cout&lt;&lt;&quot;请输入姓名：&quot;&lt;&lt;endl;        cin&gt;&gt;name;        abs-&gt;personArray[ret].m_Name&#x3D;name;        cout&lt;&lt;&quot;请输入年龄：&quot;&lt;&lt;endl;        cout&lt;&lt;&quot;1---男&quot;&lt;&lt;endl;        cout&lt;&lt;&quot;2---女&quot;&lt;&lt;endl;        int sex&#x3D;0;        while(true)&#123;            cin&gt;&gt;sex;            if(sex&#x3D;&#x3D;1||sex&#x3D;&#x3D;2)&#123;                abs-&gt;personArray[ret].m_Sex&#x3D;sex;                break;            &#125;            cout&lt;&lt;&quot;输入有误，请重新输入&quot;&lt;&lt;endl;        &#125;        cout&lt;&lt;&quot;请输入年龄：&quot;&lt;&lt;endl;        int age&#x3D;0;        cin&gt;&gt;age;        abs-&gt;personArray[ret].m_Age&#x3D;age;        cout&lt;&lt;&quot;请输入联系电话：&quot;&lt;&lt;endl;        string iphone;        cin&gt;&gt;iphone;        abs-&gt;personArray[ret].m_Phone&#x3D;iphone;        cout&lt;&lt;&quot;请输入家庭住址：&quot;&lt;&lt;endl;        string address;        cin&gt;&gt;address;        abs-&gt;personArray[ret].m_Addr&#x3D;address;        cout&lt;&lt;&quot;修改成功&quot;&lt;&lt;endl;    &#125;else&#123;        cout&lt;&lt;&quot;查无此人&quot;&lt;&lt;endl;    &#125;                    &#x2F;&#x2F;linux按任意键继续命令            system(&quot;read -p &#39;Press Enter to continue...&#39; var&quot;);            system(&quot;clear&quot;);&#x2F;&#x2F;清屏&#125;&#x2F;&#x2F;6.清空联系人void cleanPerson(Addressbooks *abs)&#123;    abs-&gt;m_Size&#x3D;0;    cout&lt;&lt;&quot;通讯录已清空&quot;&lt;&lt;endl;    system(&quot;read -p &#39;Press Enter to continue...&#39; var&quot;);    system(&quot;clear&quot;);&#x2F;&#x2F;清屏&#125;void showMenu()&#123;    cout&lt;&lt;&quot;*************************&quot;&lt;&lt;endl;    cout&lt;&lt;&quot;***** 1、添加联系人*****&quot; &lt;&lt;endl;    cout&lt;&lt;&quot;***** 2、显示联系人*****&quot;&lt;&lt;endl;    cout&lt;&lt;&quot;***** 3、删除联系人*****&quot;&lt;&lt;endl;    cout&lt;&lt;&quot;***** 4、查找联系人*****&quot;&lt;&lt;endl;    cout&lt;&lt;&quot;***** 5、修改联系人*****&quot;&lt;&lt;endl;    cout&lt;&lt;&quot;***** 6、清空联系人*****&quot;&lt;&lt;endl;    cout&lt;&lt;&quot;***** 0、退出通讯录 ****&quot;&lt;&lt;endl;    cout&lt;&lt;&quot;*************************&quot;&lt;&lt;endl;&#125;int main()&#123;    Addressbooks abs;    abs.m_Size&#x3D;0;    int select &#x3D;0;    &#x2F;&#x2F;菜单调用    while(true)&#123;        showMenu();        cin &gt;&gt;select;        switch (select)&#123;            case 1:     &#x2F;&#x2F;添加联系人            addPerson(&amp;abs);                break;            case 2:     &#x2F;&#x2F;显示联系人                showPerson(&amp;abs);                break;            case 3: &#x2F;&#x2F;删除联系人                deletePerson(&amp;abs);            &#x2F;&#x2F; &#123;            &#x2F;&#x2F;     cout&lt;&lt;&quot; 请输入要删除联系人的姓名&quot;&lt;&lt;endl;            &#x2F;&#x2F;     string name;            &#x2F;&#x2F;     cin&gt;&gt;name;            &#x2F;&#x2F;     cout&lt;&lt;(isExist(&amp;abs,name)&#x3D;&#x3D;-1?&quot;查无此人&quot;:&quot;查有此人&quot;)&lt;&lt;endl;            &#x2F;&#x2F; &#125;                break;            case 4:&#x2F;&#x2F;查找联系人                findPerson(&amp;abs);                break;            case 5:&#x2F;&#x2F;修改联系人                modifyPerson(&amp;abs);                break;            case 6: &#x2F;&#x2F;清空联系人                cleanPerson(&amp;abs);                break;            case 0:                cout&lt;&lt;&quot;欢迎下次使用&quot;&lt;&lt;endl;                &#x2F;&#x2F;linux按任意键继续命令                system(&quot;read -p &#39;Press Enter to continue...&#39; var&quot;);                return 0;                break;             default:                break;        &#125;    &#125;&#x2F;&#x2F;linux按任意键继续命令    system(&quot;read -p &#39;Press Enter to continue...&#39; var&quot;);    return 0;&#125;<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>]]></content>
      
      
      <categories>
          
          <category> c++ </category>
          
      </categories>
      
      
        <tags>
            
            <tag> c++ </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>slam14讲源代码的记录（后九讲）</title>
      <link href="/2023/04/08/slam14-3/"/>
      <url>/2023/04/08/slam14-3/</url>
      
        <content type="html"><![CDATA[<h2 id="第六讲"><a href="#第六讲" class="headerlink" title="第六讲"></a>第六讲</h2><p>本讲只要讲解最小二乘法的含义以及处理方式，如高斯牛顿（GN）、列文伯格-马夸尔特法(L-M)等下降法策略</p><h3 id="在ch6-x2F-gaussNewton-cpp中，手写GN法，最小二乘估计曲线参数"><a href="#在ch6-x2F-gaussNewton-cpp中，手写GN法，最小二乘估计曲线参数" class="headerlink" title="在ch6&#x2F;gaussNewton.cpp中，手写GN法，最小二乘估计曲线参数"></a>在ch6&#x2F;gaussNewton.cpp中，手写GN法，最小二乘估计曲线参数</h3><pre class="line-numbers language-c++" data-language="c++"><code class="language-c++">&#x2F;&#x2F;求解误差项    for (int i &#x3D; 0; i &lt; N; i++) &#123;  double xi &#x3D; x_data[i], yi &#x3D; y_data[i];  &#x2F;&#x2F; 第i个数据点  double error &#x3D; yi - exp(ae * xi * xi + be * xi + ce);  Vector3d J; &#x2F;&#x2F; 雅可比矩阵  J[0] &#x3D; -xi * xi * exp(ae * xi * xi + be * xi + ce);  &#x2F;&#x2F; de&#x2F;da  J[1] &#x3D; -xi * exp(ae * xi * xi + be * xi + ce);  &#x2F;&#x2F; de&#x2F;db  J[2] &#x3D; -exp(ae * xi * xi + be * xi + ce);  &#x2F;&#x2F; de&#x2F;dc  H +&#x3D; inv_sigma * inv_sigma * J * J.transpose();  b +&#x3D; -inv_sigma * inv_sigma * error * J;  cost +&#x3D; error * error;&#125;&#x2F;&#x2F; 求解线性方程 Hx&#x3D;b&#x2F;&#x2F;对于正定矩阵，可以使用cholesky分解来解方程Vector3d dx &#x3D; H.ldlt().solve(b);&#x2F;&#x2F;也可以使用QR分解Vector3d dx &#x3D; H.colPivHouseholderQr().solve(b);<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h3 id="ch6-x2F-ceresCurveFitting-cpp"><a href="#ch6-x2F-ceresCurveFitting-cpp" class="headerlink" title="ch6&#x2F;ceresCurveFitting.cpp"></a>ch6&#x2F;ceresCurveFitting.cpp</h3><p>使用ceres拟合曲线</p><p><img src="/pic/%E9%80%89%E5%8C%BA_131.png" alt="Ceres简介"></p><pre class="line-numbers language-c++" data-language="c++"><code class="language-c++">&#x2F;&#x2F; 代价函数的计算模型,结构体struct CURVE_FITTING_COST &#123;  CURVE_FITTING_COST(double x, double y) : _x(x), _y(y) &#123;&#125;&#x2F;&#x2F;构造函数初始化方式，给_x赋值x,_y赋值y  &#x2F;&#x2F; 残差的计算  template&lt;typename T&gt;&#x2F;&#x2F;函数模板  bool operator()(          &#x2F;&#x2F;括号运算符重载    const T *const abc, &#x2F;&#x2F; 模型参数，有3维    T *residual) const &#123;    residual[0] &#x3D; T(_y) - ceres::exp(abc[0] * T(_x) * T(_x) + abc[1] * T(_x) + abc[2]); &#x2F;&#x2F; y-exp(ax^2+bx+c)    return true;  &#125;  const double _x, _y;    &#x2F;&#x2F; x,y数据&#125;;  &#x2F;&#x2F; 构建最小二乘问题  ceres::Problem problem;  for (int i &#x3D; 0; i &lt; N; i++) &#123;    problem.AddResidualBlock(     &#x2F;&#x2F; 向问题中添加误差项      &#x2F;&#x2F; 使用自动求导，模板参数：误差类型，输出维度，输入维度，维数要与前面struct中一致      new ceres::AutoDiffCostFunction&lt;CURVE_FITTING_COST, 1, 3&gt;(        new CURVE_FITTING_COST(x_data[i], y_data[i])      ),      nullptr,            &#x2F;&#x2F; 核函数，这里不使用，为空      abc                 &#x2F;&#x2F; 待估计参数    );  &#125;  &#x2F;&#x2F; 配置求解器  ceres::Solver::Options options;     &#x2F;&#x2F; 这里有很多配置项可以填  options.linear_solver_type &#x3D; ceres::DENSE_NORMAL_CHOLESKY;  &#x2F;&#x2F; 增量方程如何求解  options.minimizer_progress_to_stdout &#x3D; true;   &#x2F;&#x2F; 输出到cout  ceres::Solver::Summary summary;                &#x2F;&#x2F; 优化信息  chrono::steady_clock::time_point t1 &#x3D; chrono::steady_clock::now();  ceres::Solve(options, &amp;problem, &amp;summary);  &#x2F;&#x2F; 开始优化  chrono::steady_clock::time_point t2 &#x3D; chrono::steady_clock::now();  chrono::duration&lt;double&gt; time_used &#x3D; chrono::duration_cast&lt;chrono::duration&lt;double&gt;&gt;(t2 - t1);  cout &lt;&lt; &quot;solve time cost &#x3D; &quot; &lt;&lt; time_used.count() &lt;&lt; &quot; seconds. &quot; &lt;&lt; endl;<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p><a href="http://www.ceres-solver.org/tutorial.html">ceres官方教程</a></p><h3 id="ch6-x2F-g2oCurveFitting-cpp"><a href="#ch6-x2F-g2oCurveFitting-cpp" class="headerlink" title="ch6&#x2F;g2oCurveFitting.cpp"></a>ch6&#x2F;g2oCurveFitting.cpp</h3><p>待更新</p><h2 id="第七讲"><a href="#第七讲" class="headerlink" title="第七讲"></a>第七讲</h2><p>视觉里程计1,vo,特征提取与匹配，对极几何，PnP，ICP,三角化,BA,SVD,直接法，光流法，光度误差</p><h3 id="ch7-x2F-orb-cv-cpp"><a href="#ch7-x2F-orb-cv-cpp" class="headerlink" title="ch7&#x2F;orb_cv.cpp"></a>ch7&#x2F;orb_cv.cpp</h3><p><a href="https://zhuanlan.zhihu.com/p/345482379">ORB特征匹配、手写ORB特征代码详解</a></p><pre class="line-numbers language-c++" data-language="c++"><code class="language-c++">#include &lt;iostream&gt;#include &lt;opencv2&#x2F;core&#x2F;core.hpp&gt;#include &lt;opencv2&#x2F;features2d&#x2F;features2d.hpp&gt;#include &lt;opencv2&#x2F;highgui&#x2F;highgui.hpp&gt;#include &lt;chrono&gt;using namespace std;using namespace cv;int main(int argc, char **argv) &#123;  if (argc !&#x3D; 3) &#123;    cout &lt;&lt; &quot;usage: feature_extraction img1 img2&quot; &lt;&lt; endl;    return 1;  &#125;  &#x2F;&#x2F;-- 读取图像  Mat img_1 &#x3D; imread(argv[1], CV_LOAD_IMAGE_COLOR);  Mat img_2 &#x3D; imread(argv[2], CV_LOAD_IMAGE_COLOR);  assert(img_1.data !&#x3D; nullptr &amp;&amp; img_2.data !&#x3D; nullptr);  &#x2F;&#x2F;-- 初始化  std::vector&lt;KeyPoint&gt; keypoints_1, keypoints_2;  Mat descriptors_1, descriptors_2;  Ptr&lt;FeatureDetector&gt; detector &#x3D; ORB::create();        &#x2F;&#x2F;ORB  Ptr&lt;DescriptorExtractor&gt; descriptor &#x3D; ORB::create();  Ptr&lt;DescriptorMatcher&gt; matcher &#x3D; DescriptorMatcher::create(&quot;BruteForce-Hamming&quot;);  &#x2F;&#x2F;-- 第一步:检测 Oriented FAST 角点位置  chrono::steady_clock::time_point t1 &#x3D; chrono::steady_clock::now();  detector-&gt;detect(img_1, keypoints_1);  detector-&gt;detect(img_2, keypoints_2);  &#x2F;&#x2F;-- 第二步:根据角点位置计算 BRIEF 描述子  descriptor-&gt;compute(img_1, keypoints_1, descriptors_1);  descriptor-&gt;compute(img_2, keypoints_2, descriptors_2);  chrono::steady_clock::time_point t2 &#x3D; chrono::steady_clock::now();  chrono::duration&lt;double&gt; time_used &#x3D; chrono::duration_cast&lt;chrono::duration&lt;double&gt;&gt;(t2 - t1);  cout &lt;&lt; &quot;extract ORB cost &#x3D; &quot; &lt;&lt; time_used.count() &lt;&lt; &quot; seconds. &quot; &lt;&lt; endl;  Mat outimg1,outimg2;  drawKeypoints(img_1, keypoints_1, outimg1, Scalar::all(-1), DrawMatchesFlags::DEFAULT);  drawKeypoints(img_2, keypoints_2, outimg2, Scalar::all(-1), DrawMatchesFlags::DEFAULT);  imshow(&quot;pic1_ORB features&quot;, outimg1);  imshow(&quot;pic2_ORB features&quot;, outimg2);  &#x2F;&#x2F;-- 第三步:对两幅图像中的BRIEF描述子进行匹配，使用 Hamming 距离  vector&lt;DMatch&gt; matches;  t1 &#x3D; chrono::steady_clock::now();  matcher-&gt;match(descriptors_1, descriptors_2, matches);  t2 &#x3D; chrono::steady_clock::now();  time_used &#x3D; chrono::duration_cast&lt;chrono::duration&lt;double&gt;&gt;(t2 - t1);  cout &lt;&lt; &quot;match ORB cost &#x3D; &quot; &lt;&lt; time_used.count() &lt;&lt; &quot; seconds. &quot; &lt;&lt; endl;  &#x2F;&#x2F;-- 第四步:匹配点对筛选  &#x2F;&#x2F; 计算最小距离和最大距离  auto min_max &#x3D; minmax_element(matches.begin(), matches.end(),                                [](const DMatch &amp;m1, const DMatch &amp;m2) &#123; return m1.distance &lt; m2.distance; &#125;);  double min_dist &#x3D; min_max.first-&gt;distance;  double max_dist &#x3D; min_max.second-&gt;distance;  printf(&quot;-- Max dist : %f \n&quot;, max_dist);  printf(&quot;-- Min dist : %f \n&quot;, min_dist);  &#x2F;&#x2F;当描述子之间的距离大于两倍的最小距离时,即认为匹配有误.但有时候最小距离会非常小,设置一个经验值30作为下限.  std::vector&lt;DMatch&gt; good_matches;  for (int i &#x3D; 0; i &lt; descriptors_1.rows; i++) &#123;    if (matches[i].distance &lt;&#x3D; max(2 * min_dist, 30.0)) &#123;      good_matches.push_back(matches[i]);    &#125;  &#125;  &#x2F;&#x2F;-- 第五步:绘制匹配结果  Mat img_match;  Mat img_goodmatch;  drawMatches(img_1, keypoints_1, img_2, keypoints_2, matches, img_match);  drawMatches(img_1, keypoints_1, img_2, keypoints_2, good_matches, img_goodmatch);  imshow(&quot;all matches&quot;, img_match);  imshow(&quot;good matches&quot;, img_goodmatch);  waitKey(0);  return 0;&#125;<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p><a href="https://vimsky.com/examples/usage/cpp-algorithm-minmax_element-function-01.html">C++ Algorithm minmax_element()用法及代码示例</a></p><p><img src="/pic/%E9%80%89%E5%8C%BA_133.png" alt="两图特征点"></p><p><img src="/pic/%E9%80%89%E5%8C%BA_134.png" alt="匹配情况"></p><h3 id="ch7-x2F-orb-self-cpp"><a href="#ch7-x2F-orb-self-cpp" class="headerlink" title="ch7&#x2F;orb_self.cpp"></a>ch7&#x2F;orb_self.cpp</h3><p>手写ORB特征</p><p><strong>(1)FAST角点检测</strong></p><p>opencv库函数：利用ORB特征检测器detector里的detect函数。</p><p>手写：利用改进的FAST算法，增加了中心像素和围绕该像素的圆的像素之间的强度差阈值以及非最大值抑制。</p><p>其中的FAST（）函数结构如下：</p><pre class="line-numbers language-C++" data-language="C++"><code class="language-C++">CV_EXPORTS void FAST( InputArray image, CV_OUT std::vector&lt;KeyPoint&gt;&amp; keypoints,                      int threshold, bool nonmaxSuppression&#x3D;true );&#x2F;*image： 检测的灰度图像keypoints: 在图像上检测到的关键点threshold: 中心像素和围绕该像素的圆的像素之间的强度差阈值nonmaxSuppression: 参数非最大值抑制,默认为真，对检测到的角点应用非最大值抑制。*&#x2F;<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p><strong>（2）描述子计算</strong></p><p>opencv库函数：构建ORB特征描述器descriptor里的compute函数</p><p>手写：自定义函数ComputeORB()来计算描述子，在这个函数里面首先会排除一些靠近边缘的特征点。排除的坏点的判断方式如下：</p><ul><li><p>当kp.pt.x &lt; half_boundary时，边长为boundary的图像块将在-x轴上超出图像。</p></li><li><p>当kp.pt.y &lt; half_boundary时，边长为boundary的图像块将在-y轴上超出图像。</p></li><li><p>当kp.pt.x &gt;&#x3D; img.cols - half_boundary（kp.pt.x&gt;&#x3D; 640-16）时，边长为boundary(32)的图像块将在+x轴上超出图像。</p></li><li><p>当kp.pt.y &gt;&#x3D; img.rows - half_boundary（kp.pt.y&gt;&#x3D; 480-16）时，边长为boundary(32)的图像块将在+y轴上超出图像。</p></li></ul><p>同时，在手写代码中，按照灰度质心法定义了图像块的矩和质心，最后求出了特征点的角度。</p><p>在求描述子时，事先准备了256*4个数据集，这些数据集表示以关键点为中心，[-13,12]的范围内,随机选点对p,q。选取两个点p,q，这两个点的坐标从数据集选取，然后乘上之前求的角度再加上关键点，以此找到关键点附近的两个随机像素，然后比较像素值。最终形成描述子。</p><p><strong>（3）BRIEF描述子匹配函数</strong></p><p>opencv下：利用自带的match函数，比较两副图像的描述子的汉明距离，并从小到大排序在matches容器中，然后在容器中挑选好的描述子，这些描述子满足描述子之间的距离小于两倍的最小距离和经验阈值的最小值，因为最小距离可能是0；</p><p>手写：描述子是采用256位二进制描述，对应到8个32位的unsigned int 数据，并利用SSE指令集计算每个unsigned int变量中1的个数，从而计算汉明距离。手写的暴力匹配代码中输入三个参数，分别是第一副和第二副图像的描述子，和存放输出匹配对的容器；这里的暴力匹配的思路为：取第一副图片中的一个描述子，分别计算与第二副图片每个描述子的汉明距离，然后选取最近的距离以及所对应的匹配对，然后多次选取图片1中的描述子重复上述操作，分别找到最短距离和相应的匹配对。最后再将比较得到的最小距离与设定的经验阈值作比较，如果小于经验阈值则保留并输出该匹配对。</p><p>具体代码如下：</p><pre class="line-numbers language-C++" data-language="C++"><code class="language-C++"> &#x2F;&#x2F; compute the descriptor&#x2F;&#x2F;(1)计算角点的方向；(2)计算描述子。void ComputeORB(const cv::Mat &amp;img, vector&lt;cv::KeyPoint&gt; &amp;keypoints, vector&lt;DescType&gt; &amp;descriptors) &#123;  const int half_patch_size &#x3D; 8; &#x2F;&#x2F;计算特征点方向时，选取的图像块，16*16  const int half_boundary &#x3D; 16;&#x2F;&#x2F;计算描述子时在32*32的图像块中选点  int bad_points &#x3D; 0; &#x2F;&#x2F;计算描述子时，在32*32的区域块选择两个点比较，所选择的点超出图像范围的。出现这种情况下的FAST角点的数目。    &#x2F;&#x2F;遍历所有FAST角点  for (auto &amp;kp: keypoints)   &#123;    &#x2F;&#x2F;超出图像边界的角点的描述子设为空    if (kp.pt.x &lt; half_boundary || kp.pt.y &lt; half_boundary ||        kp.pt.x &gt;&#x3D; img.cols - half_boundary || kp.pt.y &gt;&#x3D; img.rows - half_boundary) &#123;      &#x2F;&#x2F; outside      bad_points++; &#x2F;&#x2F;bad_points的描述子设为空      descriptors.push_back(&#123;&#125;);      continue;    &#125;    &#x2F;&#x2F;计算16*16图像块的灰度质心    &#x2F;&#x2F;可参照下面的图片帮助理解    float m01 &#x3D; 0, m10 &#x3D; 0;&#x2F;&#x2F;图像块的矩 视觉slam十四讲中p157    for (int dx &#x3D; -half_patch_size; dx &lt; half_patch_size; ++dx)     &#123;      for (int dy &#x3D; -half_patch_size; dy &lt; half_patch_size; ++dy)       &#123;        uchar pixel &#x3D; img.at&lt;uchar&gt;(kp.pt.y + dy, kp.pt.x + dx);        m10 +&#x3D; dx * pixel; &#x2F;&#x2F;pixel表示灰度值 视觉slam十四讲中p157最上面的公式        m01 +&#x3D; dy * pixel;&#x2F;&#x2F;pixel表示灰度值 视觉slam十四讲中p157最上面的公式      &#125;    &#125;     &#x2F;&#x2F; angle should be arc tan(m01&#x2F;m10);参照下面第三章图片帮助理解    float m_sqrt &#x3D; sqrt(m01 * m01 + m10 * m10) + 1e-18; &#x2F;&#x2F; avoid divide by zero  1e-18避免了m_sqrt的值为0（图像块全黑）将m01 * m01 + m10 * m10进行开根    float sin_theta &#x3D; m01 &#x2F; m_sqrt;&#x2F;&#x2F;sin_theta &#x3D; m01 &#x2F; 根号下（m01 * m01 + m10 * m10））    float cos_theta &#x3D; m10 &#x2F; m_sqrt;&#x2F;&#x2F;cos_theta &#x3D; m10 &#x2F; 根号下（m01 * m01 + m10 * m10））    &#x2F;&#x2F;因为tan_theta &#x3D; m01&#x2F;m10 即为tan_theta &#x3D; sin_theta &#x2F; cos_theta &#x3D; [m01 &#x2F; 根号下（m01 * m01 + m10 * m10] &#x2F; [m10 &#x2F; 根号下（m01 * m01 + m10 * m10]    &#x2F;&#x2F;目的是求出特征点的方向  视觉slam十四讲中p157第三个公式    &#x2F;&#x2F; compute the angle of this point    DescType desc(8, 0); &#x2F;&#x2F;8个元素，它们的值初始化为0        for (int i &#x3D; 0; i &lt; 8; i++) &#123;      uint32_t d &#x3D; 0;      for (int k &#x3D; 0; k &lt; 32; k++)       &#123;        int idx_pq &#x3D; i * 32 + k;&#x2F;&#x2F;idx_pq表示二进制描述子中的第几位        cv::Point2f p(ORB_pattern[idx_pq * 4], ORB_pattern[idx_pq * 4 + 1]);        cv::Point2f q(ORB_pattern[idx_pq * 4 + 2], ORB_pattern[idx_pq * 4 + 3]);         &#x2F;&#x2F; rotate with theta        &#x2F;&#x2F;p,q绕原点旋转theta得到pp,qq        cv::Point2f pp &#x3D; cv::Point2f(cos_theta * p.x - sin_theta * p.y, sin_theta * p.x + cos_theta * p.y)                         + kp.pt;        cv::Point2f qq &#x3D; cv::Point2f(cos_theta * q.x - sin_theta * q.y, sin_theta * q.x + cos_theta * q.y)                         + kp.pt;        if (img.at&lt;uchar&gt;(pp.y, pp.x) &lt; img.at&lt;uchar&gt;(qq.y, qq.x)) &#123;          d |&#x3D; 1 &lt;&lt; k;        &#125;      &#125;      desc[i] &#x3D; d;    &#125;    descriptors.push_back(desc);&#x2F;&#x2F;desc表示该Oriented_FAST角点的描述子  &#125;   cout &lt;&lt; &quot;bad&#x2F;total: &quot; &lt;&lt; bad_points &lt;&lt; &quot;&#x2F;&quot; &lt;&lt; keypoints.size() &lt;&lt; endl;&#125; &#x2F;&#x2F; brute-force matchingvoid BfMatch(const vector&lt;DescType&gt; &amp;desc1, const vector&lt;DescType&gt; &amp;desc2, vector&lt;cv::DMatch&gt; &amp;matches) &#123;  const int d_max &#x3D; 40;&#x2F;&#x2F;描述子之间的距离小于这个值，才被认为是正确匹配   for (size_t i1 &#x3D; 0; i1 &lt; desc1.size(); ++i1)  &#x2F;&#x2F;size_t相当于int，便于代码移植  &#123;    if (desc1[i1].empty()) continue;    cv::DMatch m&#123;i1, 0, 256&#125;; &#x2F;&#x2F;定义了一个匹配对m    for (size_t i2 &#x3D; 0; i2 &lt; desc2.size(); ++i2)&#x2F;&#x2F;计算描述子desc1[i1]和描述子desc2[i2]的距离，即不同位数的数目     &#123;      if (desc2[i2].empty()) continue;      int distance &#x3D; 0;      for (int k &#x3D; 0; k &lt; 8; k++) &#123;        distance +&#x3D; _mm_popcnt_u32(desc1[i1][k] ^ desc2[i2][k]);      &#125;      if (distance &lt; d_max &amp;&amp; distance &lt; m.distance) &#123;        m.distance &#x3D; distance;        m.trainIdx &#x3D; i2;      &#125;    &#125;    if (m.distance &lt; d_max) &#123;      matches.push_back(m);    &#125;  &#125;<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><blockquote><p>参考<a href="https://blog.csdn.net/weixin_53660567/article/details/121095677">视觉SLAM十四讲CH7代码解析及课后习题详解</a></p></blockquote><h3 id="ch7-x2F-pose-estimation-2d2d-cpp"><a href="#ch7-x2F-pose-estimation-2d2d-cpp" class="headerlink" title="ch7&#x2F;pose_estimation_2d2d.cpp"></a>ch7&#x2F;pose_estimation_2d2d.cpp</h3><p>本程序演示了如何使用2D-2D的特征匹配估计相机运动,对极约束求解相机运动</p><pre class="line-numbers language-c++" data-language="c++"><code class="language-c++">&#x2F;&#x2F;&#x2F;&#x2F;#include &lt;iostream&gt;#include &lt;opencv2&#x2F;core&#x2F;core.hpp&gt;#include &lt;opencv2&#x2F;features2d&#x2F;features2d.hpp&gt;#include &lt;opencv2&#x2F;highgui&#x2F;highgui.hpp&gt;#include &lt;opencv2&#x2F;calib3d&#x2F;calib3d.hpp&gt;using namespace std;using namespace cv;&#x2F;&#x2F;声明特征匹配函数void find_feature_matches(const Mat &amp;img1,const Mat &amp;img2,                          std::vector&lt;KeyPoint&gt; &amp;keypoints_1,                          std::vector&lt;KeyPoint&gt; &amp;keypoints_2,                          std::vector&lt;DMatch&gt; &amp;matches);&#x2F;&#x2F;声明位姿估计函数void pose_estimated_2d2d( std::vector&lt;KeyPoint&gt; keypoints_1,                          std::vector&lt;KeyPoint&gt; keypoints_2,                          std::vector&lt;DMatch&gt; matches,                          Mat &amp;R, Mat &amp;t);&#x2F;&#x2F;声明 像素坐标转归一化坐标 p99页 公式5.5 变换一下就好啦！&#x2F;&#x2F;这里的函数我改了一下，书上的只是得到二维点，主程序中，还把二维点转换为三维点，我觉得多此一举，一个函数实现不就好了么&#x2F;&#x2F;下面是我实现的坐标变换函数 返回的是Mat类型，其实照片也是矩阵Mat pixel2cam(const Point2d &amp;p,const Mat &amp;K);&#x2F;&#x2F;p是像素坐标，K是相机内参矩阵int main(int argc,char** argv)&#123;    &#x2F;&#x2F;运行可执行文件+图片1的路径+图片2的路径    if(argc!&#x3D;3)    &#123;        cout&lt;&lt;&quot;usage: .&#x2F;pose_estimated_2d2d img1 img2&quot;&lt;&lt;endl;        return 1;    &#125;    &#x2F;*argv[1]&#x3D;&quot;&#x2F;home&#x2F;nnz&#x2F;data&#x2F;slam_pratice&#x2F;pratice_pose_estimated&#x2F;1.png&quot;;    argv[2]&#x3D;&quot;&#x2F;home&#x2F;nnz&#x2F;data&#x2F;slam_pratice&#x2F;pratice_pose_estimated&#x2F;2.png&quot;;*&#x2F;    &#x2F;&#x2F;读取图片    Mat img1&#x3D;imread(argv[1],CV_LOAD_IMAGE_COLOR);&#x2F;&#x2F;彩色图    Mat img2&#x3D;imread(argv[2],CV_LOAD_IMAGE_COLOR);    &#x2F;&#x2F;定义要用到的变量啊，比如 keypoints_1,keypoints_2,matches    vector&lt;KeyPoint&gt; keypoints_1, keypoints_2;    vector&lt;DMatch&gt; matches;    &#x2F;&#x2F;计算两个keypoints的matches    find_feature_matches(img1,img2,keypoints_1,keypoints_2,matches);    &#x2F;&#x2F;这样就得到matche啦！    cout &lt;&lt; &quot;一共找到了&quot; &lt;&lt; matches.size() &lt;&lt; &quot;组匹配点&quot; &lt;&lt; endl;    &#x2F;&#x2F;重点来了    &#x2F;&#x2F;估计两组图片之间的运动    Mat R,t;&#x2F;&#x2F;定义一下旋转和平移    pose_estimated_2d2d(keypoints_1,keypoints_2,matches,R,t);    &#x2F;&#x2F;这样就算出R ,t啦    &#x2F;&#x2F;当然我们还需要验证一下对极约束 准不准了    &#x2F;&#x2F;验证 E&#x3D;t^R*scale;    &#x2F;&#x2F;t_x是t的反对称矩阵    Mat t_x &#x3D;            (Mat_&lt;double&gt;(3, 3) &lt;&lt; 0, -t.at&lt;double&gt;(2, 0), t.at&lt;double&gt;(1, 0),                    t.at&lt;double&gt;(2, 0), 0, -t.at&lt;double&gt;(0, 0),                    -t.at&lt;double&gt;(1, 0), t.at&lt;double&gt;(0, 0), 0);    cout &lt;&lt; &quot;t^R&#x3D;&quot; &lt;&lt; endl &lt;&lt; t_x * R &lt;&lt; endl;    &#x2F;&#x2F;验证对极约束 对应P167 页公式7.10 x2TEx1&#x3D;0  E&#x3D;t^*R    &#x2F;&#x2F;定义内参矩阵    Mat K &#x3D; (Mat_&lt;double&gt;(3, 3) &lt;&lt; 520.9, 0, 325.1,                                                 0, 521.0,249.7,                                                 0, 0, 1);    for(DMatch m: matches)    &#123;        Mat x1&#x3D;pixel2cam(keypoints_1[m.queryIdx].pt,K);        Mat x2&#x3D;pixel2cam(keypoints_2[m.queryIdx].pt,K);        Mat d&#x3D;x2.t()*t_x*R*x1;&#x2F;&#x2F;若d很趋近于零，则说明没啥问题        cout &lt;&lt; &quot;epipolar constraint &#x3D; &quot; &lt;&lt; d &lt;&lt; endl;    &#125;    return 0;&#125;&#x2F;&#x2F;最复杂的地方来咯&#x2F;&#x2F;函数的实现&#x2F;&#x2F;匹配函数void find_feature_matches(const Mat &amp;img1,const Mat &amp;img2,                          std::vector&lt;KeyPoint&gt; &amp;keypoints_1,                          std::vector&lt;KeyPoint&gt; &amp;keypoints_2,                          std::vector&lt;DMatch&gt; &amp;matches)&#123;    &#x2F;&#x2F;先初始化，创建咱们要用到的对象    &#x2F;&#x2F;定义两个关键点对应的描述子，同时创建检测keypoints的检测器    Mat descriptors_1, descriptors_2;    vector&lt;DMatch&gt; match;&#x2F;&#x2F;暂时存放匹配点,因为后面还要进行筛选    Ptr&lt;FeatureDetector&gt; detector&#x3D;ORB::create();&#x2F;&#x2F;keypoints检测器    Ptr&lt;DescriptorExtractor&gt; descriptor &#x3D;ORB::create();&#x2F;&#x2F;描述子提取器    Ptr&lt;DescriptorMatcher&gt; matcher &#x3D;DescriptorMatcher::create(            &quot;BruteForce-Hamming&quot;);&#x2F;&#x2F;描述子匹配器（方法：暴力匹配）    &#x2F;&#x2F;step1:找到角点    detector-&gt;detect(img1,keypoints_1);&#x2F;&#x2F;得到图1的关键点（keypoints_1）    detector-&gt;detect(img2,keypoints_2);&#x2F;&#x2F;得到图2的关键点（keypoints_2）    &#x2F;&#x2F;step2:计算关键点所对应的描述子    descriptor-&gt;compute(img1,keypoints_1,descriptors_1);&#x2F;&#x2F;得到descriptors_1    descriptor-&gt;compute(img2,keypoints_2,descriptors_2);&#x2F;&#x2F;得到descriptors_2    &#x2F;&#x2F;step3:进行暴力匹配    matcher-&gt;match(descriptors_1,descriptors_2,match);    &#x2F;&#x2F;step4:对match进行筛选，得到好的匹配点，把好的匹配点放在matches中    &#x2F;&#x2F;先定义两个变量，一个是最大距离，一个是最小距离    double min_dist&#x3D;1000, max_dist&#x3D;0;    &#x2F;&#x2F;找出所有匹配之间的最小距离和最大距离, 即是最相似的和最不相似的两组点之间的距离    for(int i&#x3D;0;i&lt;descriptors_1.rows;i++) &#x2F;&#x2F;描述子本质是由 0,1 组成的向量    &#123;        double dist &#x3D;match[i].distance;        &#x2F;&#x2F;还记得orb_cv中如何找最大距离和最远距离的吗，那里面的程序是用下面的函数实现的，下面的函数得到的是pair first 里面是最小距离，second里面是最大距离        &#x2F;&#x2F; minmax_element(matches.begin(),matched.end,[](const DMatch &amp;m1,const DMatch &amp;m2)&#123;return m1.distance&lt;m2.distance;&#125;);        &#x2F;&#x2F;本程序用下面的if语句得到距离        if (dist &lt; min_dist) min_dist &#x3D; dist;        if (dist &gt; max_dist) max_dist &#x3D; dist;    &#125;    printf(&quot;-- Max dist : %f \n&quot;, max_dist);    printf(&quot;-- Min dist : %f \n&quot;, min_dist);    &#x2F;&#x2F;当描述子之间的距离大于两倍的最小距离时,即认为匹配有误.    &#x2F;&#x2F; 但有时候最小距离会非常小,设置一个经验值30作为下限.    for(int i&#x3D;0;i&lt;descriptors_1.rows;i++)    &#123;        if(match[i].distance&lt;&#x3D;max(2*min_dist,30.0))        &#123;            matches.push_back(match[i]);        &#125;    &#125;&#125;&#x2F;&#x2F;像素到归一化坐标Mat pixel2cam(const Point2d &amp;p,const Mat &amp;K)&#x2F;&#x2F;p是像素坐标，K是相机内参矩阵&#123;    Mat t;    t&#x3D;(Mat_&lt;double&gt;(3,1)&lt;&lt;(p.x - K.at&lt;double&gt;(0, 2)) &#x2F; K.at&lt;double&gt;(0, 0),            (p.y - K.at&lt;double&gt;(1, 2)) &#x2F; K.at&lt;double&gt;(1, 1),1);    return t;&#125;&#x2F;&#x2F;实现位姿估计函数void pose_estimated_2d2d( std::vector&lt;KeyPoint&gt; keypoints_1,                          std::vector&lt;KeyPoint&gt; keypoints_2,                          std::vector&lt;DMatch&gt; matches,                          Mat &amp;R, Mat &amp;t)&#123;    &#x2F;&#x2F; 相机内参来源于 TUM Freiburg2    Mat K &#x3D; (Mat_&lt;double&gt;(3, 3) &lt;&lt; 520.9, 0, 325.1,                                                 0, 521.0, 249.7,                                                 0, 0, 1);    &#x2F;&#x2F;咱们要把关键点的像素点拿出来 ,定义两个容器接受两个图关键点的像素位置    vector&lt;Point2d&gt; points1;    vector&lt;Point2d&gt; points2;    for(int i&#x3D;0;i&lt;(int)matches.size();i++)    &#123;        &#x2F;&#x2F;queryIdx是图1中匹配的关键点的对应编号        &#x2F;&#x2F;trainIdx是图2中匹配的关键点的对应编号        &#x2F;&#x2F;pt可以把关键点的像素位置取出来        points1.push_back(keypoints_1[matches[i].queryIdx].pt);        points2.push_back(keypoints_2[matches[i].trainIdx].pt);    &#125;    &#x2F;&#x2F;-- 计算基础矩阵    Mat fundamental_matrix;    fundamental_matrix &#x3D; findFundamentalMat(points1, points2, CV_FM_8POINT);    cout &lt;&lt; &quot;fundamental_matrix is &quot; &lt;&lt; endl &lt;&lt; fundamental_matrix &lt;&lt; endl;    &#x2F;&#x2F;计算本质矩阵 E    &#x2F;&#x2F;把cx ,cy放进一个向量里面 &#x3D;相机的光心    Point2d principal_point(325.1, 249.7);    double focal_length&#x3D;521;&#x2F;&#x2F;相机的焦距    &#x2F;&#x2F;之所以取上面的principal_point、focal_length是因为计算本质矩阵的函数要用    &#x2F;&#x2F;得到本质矩阵essential_matrix    Mat essential_matrix&#x3D;findEssentialMat(points1,points2,focal_length,principal_point);    cout&lt;&lt;&quot; essential_matrix &#x3D;\n&quot;&lt;&lt; essential_matrix &lt;&lt;endl;    &#x2F;&#x2F;-- 计算单应矩阵 homography_matrix    &#x2F;&#x2F;-- 但是本例中场景不是平面，单应矩阵意义不大    Mat homography_matrix;    homography_matrix &#x3D; findHomography(points1, points2, RANSAC, 3);    cout &lt;&lt; &quot;homography_matrix is &quot; &lt;&lt; endl &lt;&lt; homography_matrix &lt;&lt; endl;    &#x2F;&#x2F;通过本质矩阵恢复咱们的 R  t    recoverPose(essential_matrix,points1,points2,R,t,focal_length,principal_point);    &#x2F;&#x2F;输出咱们的 R t    cout&lt;&lt;&quot; 得到图1到图2 的位姿变换:\n &quot;&lt;&lt;endl;    cout&lt;&lt;&quot;R&#x3D; \n&quot;&lt;&lt; R &lt;&lt;endl;    cout&lt;&lt;&quot;t&#x3D; \n&quot;&lt;&lt; t &lt;&lt;endl;&#125;<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p><a href="https://blog.csdn.net/joun772/article/details/109466153?utm_medium=distribute.pc_relevant.none-task-blog-2~default~baidujs_baidulandingword~default-12-109466153-blog-116243451.235%5Ev28%5Epc_relevant_recovery_v2&spm=1001.2101.3001.4242.7&utm_relevant_index=15">SLAM十四讲-ch7(2)-位姿估计(包含2d-2d、3d-2d、3d-3d、以及三角化实现代码的注释)</a></p><h3 id="ch7-x2F-triangulation-cpp"><a href="#ch7-x2F-triangulation-cpp" class="headerlink" title="ch7&#x2F;triangulation.cpp"></a>ch7&#x2F;triangulation.cpp</h3><p>在上面的2d2d位姿估计的基础上，利用三角化来获得特征匹配点的深度信息(通过画图，验证三维点与特征点的重投影关系)</p><pre class="line-numbers language-c++" data-language="c++"><code class="language-c++">&#x2F;&#x2F;&#x2F;&#x2F; Created by wenbo on 2020&#x2F;11&#x2F;3.&#x2F;&#x2F;&#x2F;&#x2F;该程序在pose_estimated_2d2d的基础上加上三角化，以求得匹配的特征点在世界下的三维点&#x2F;&#x2F;&#x2F;&#x2F;#include &lt;iostream&gt;#include &lt;opencv2&#x2F;opencv.hpp&gt;&#x2F;&#x2F; #include &quot;extra.h&quot; &#x2F;&#x2F; used in opencv2using namespace std;using namespace cv;&#x2F;&#x2F;声明特征匹配函数void find_feature_matches(const Mat &amp;img1,const Mat &amp;img2,                          std::vector&lt;KeyPoint&gt; &amp;keypoints_1,                          std::vector&lt;KeyPoint&gt; &amp;keypoints_2,                          std::vector&lt;DMatch&gt; &amp;matches);&#x2F;&#x2F;声明位姿估计函数void pose_estimated_2d2d( std::vector&lt;KeyPoint&gt; &amp;keypoints_1,                          std::vector&lt;KeyPoint&gt; &amp;keypoints_2,                          std::vector&lt;DMatch&gt; &amp;matches,                          Mat &amp;R, Mat &amp;t);&#x2F;&#x2F;声明 像素坐标转归一化坐标 p99页 公式5.5 变换一下就好啦！&#x2F;&#x2F;这里的函数我改了一下，书上的只是得到二维点，主程序中，还把二维点转换为三维点，我觉得多此一举，一个函数实现不就好了么&#x2F;&#x2F;下面是我实现的坐标变换函数 返回的是Mat类型，其实照片也是矩阵Point2f pixel2cam(const Point2d &amp;p, const Mat &amp;K);&#x2F;&#x2F;p是像素坐标，K是相机内参矩阵&#x2F;&#x2F;声明三角化函数void triangulation(        const vector&lt;KeyPoint&gt; &amp;keypoint_1,        const vector&lt;KeyPoint&gt; &amp;keypoint_2,        const std::vector&lt;DMatch&gt; &amp;matches,        const Mat &amp;R, const Mat &amp;t,        vector&lt;Point3d&gt; &amp;points);&#x2F;&#x2F;&#x2F; 作图用inline cv::Scalar get_color(float depth) &#123;    float up_th &#x3D; 50, low_th &#x3D; 10, th_range &#x3D; up_th - low_th;    if (depth &gt; up_th) depth &#x3D; up_th;    if (depth &lt; low_th) depth &#x3D; low_th;    return cv::Scalar(255 * depth &#x2F; th_range, 0, 255 * (1 - depth &#x2F; th_range));&#125;int main(int argc,char** argv)&#123;    &#x2F;&#x2F;运行可执行文件+图片1的路径+图片2的路径    if(argc!&#x3D;3)    &#123;        cout&lt;&lt;&quot;usage: .&#x2F;triangulation img1 img2&quot;&lt;&lt;endl;        return 1;    &#125;    &#x2F;&#x2F;读取图片    Mat img1&#x3D;imread(argv[1],CV_LOAD_IMAGE_COLOR);&#x2F;&#x2F;彩色图    Mat img2&#x3D;imread(argv[2],CV_LOAD_IMAGE_COLOR);    &#x2F;&#x2F;定义要用到的变量啊，比如 keypoints_1,keypoints_2,matches    vector&lt;KeyPoint&gt; keypoints_1, keypoints_2;    vector&lt;DMatch&gt; matches;    &#x2F;&#x2F;计算两个keypoints的matches    find_feature_matches(img1,img2,keypoints_1,keypoints_2,matches);    &#x2F;&#x2F;这样就得到matches啦！    cout &lt;&lt; &quot;一共找到了&quot; &lt;&lt; matches.size() &lt;&lt; &quot;组匹配点&quot; &lt;&lt; endl;    &#x2F;&#x2F;重点来了    &#x2F;&#x2F;估计两组图片之间的运动    Mat R,t;&#x2F;&#x2F;定义一下旋转和平移    pose_estimated_2d2d(keypoints_1,keypoints_2,matches,R,t);    &#x2F;&#x2F;这样就算出R ,t啦    &#x2F;&#x2F;三角化    &#x2F;&#x2F;定义一个容器 points 用来存放特征匹配点在世界坐标系下的3d点    vector&lt;Point3d&gt; points;    triangulation(keypoints_1,keypoints_2,matches,R,t,points);    &#x2F;&#x2F;得到三维点    &#x2F;&#x2F;验证三维点与特征点的重投影关系    Mat K &#x3D; (Mat_&lt;double&gt;(3, 3) &lt;&lt; 520.9, 0, 325.1,                                                 0, 521.0, 249.7,                                                 0, 0, 1);    Mat img_plot1&#x3D;img1.clone();    Mat img_plot2&#x3D;img2.clone();    &#x2F;&#x2F;利用循环找到图1和图2特征点在图上的位置，并圈出来    for(int i&#x3D;0;i&lt;matches.size();i++)    &#123;        &#x2F;&#x2F;先画图1中特征点        &#x2F;&#x2F;在这里，为什么从一个世界坐标系下的3d点，就可以得到，图1相机坐标下的深度点呢？        &#x2F;&#x2F;我觉得是因为 图1的位姿: R是单位矩阵，t为0（在三角化函数中有写到） 所以可以把图1的相机坐标看成是世界坐标        float  depth1&#x3D;points[i].z;&#x2F;&#x2F;取出图1各个特征点的深度信息        cout&lt;&lt;&quot;depth: &quot;&lt;&lt;depth1&lt;&lt;endl;        Point2d pt1_cam&#x3D;pixel2cam(keypoints_1[matches[i].queryIdx].pt,K);        cv::circle(img_plot1, keypoints_1[matches[i].queryIdx].pt, 2, get_color(depth1), 2);        &#x2F;&#x2F;画图2        &#x2F;&#x2F;得到图2坐标系下的3d点，得到图2的深度信息        Mat pt2_trans&#x3D;R*(Mat_&lt;double&gt;(3, 1) &lt;&lt;points[i].x,points[i].y,points[i].z)+t;        float depth2 &#x3D; pt2_trans.at&lt;double&gt;(2, 0);        cv::circle(img_plot2, keypoints_2[matches[i].trainIdx].pt, 2, get_color(depth2), 2);    &#125;&#x2F;&#x2F;画图    cv::imshow(&quot;img 1&quot;, img_plot1);    cv::imshow(&quot;img 2&quot;, img_plot2);    cv::waitKey();    return 0;&#125;void triangulation(        const vector&lt;KeyPoint&gt; &amp;keypoint_1,        const vector&lt;KeyPoint&gt; &amp;keypoint_2,        const std::vector&lt;DMatch&gt; &amp;matches,        const Mat &amp;R, const Mat &amp;t,        vector&lt;Point3d&gt; &amp;points)&#123;    &#x2F;&#x2F;定义图1在世界坐标系下的位姿    Mat T1 &#x3D; (Mat_&lt;float&gt;(3, 4) &lt;&lt;                                1, 0, 0, 0,                                0, 1, 0, 0,                                0, 0, 1, 0);    &#x2F;&#x2F;定义图2在世界坐标系下的位姿    Mat T2 &#x3D; (Mat_&lt;float&gt;(3, 4) &lt;&lt;                                R.at&lt;double&gt;(0, 0), R.at&lt;double&gt;(0, 1), R.at&lt;double&gt;(0, 2), t.at&lt;double&gt;(0, 0),            R.at&lt;double&gt;(1, 0), R.at&lt;double&gt;(1, 1), R.at&lt;double&gt;(1, 2), t.at&lt;double&gt;(1, 0),            R.at&lt;double&gt;(2, 0), R.at&lt;double&gt;(2, 1), R.at&lt;double&gt;(2, 2), t.at&lt;double&gt;(2, 0)    );    Mat K &#x3D; (Mat_&lt;double&gt;(3, 3) &lt;&lt; 520.9, 0, 325.1, 0, 521.0, 249.7, 0, 0, 1);    &#x2F;&#x2F;容器 pts_1、pts_2分别存放图1和图2中特征点对应的自己相机归一化坐标中的 x与 y    vector&lt;Point2f&gt; pts_1,pts_2;    for(DMatch m:matches)&#x2F;&#x2F;这样的遍历写起来比较快    &#123;        &#x2F;&#x2F;将像素坐标变为相机下的归一化坐标        pts_1.push_back(pixel2cam(keypoint_1[m.queryIdx].pt,K));        pts_2.push_back(pixel2cam(keypoint_2[m.trainIdx].pt,K));    &#125;    Mat pts_4d;    cv::triangulatePoints(T1,T2,pts_1,pts_2,pts_4d);    &#x2F;*    传入两个图像对应相机的变化矩阵，各自相机坐标系下归一化相机坐标，    输出的3D坐标是齐次坐标，共四个维度，因此需要将前三个维度除以第四个维度以得到非齐次坐标xyz    *&#x2F;    &#x2F;&#x2F;转换为非齐次坐标    for(int i&#x3D;0;i&lt;pts_4d.cols;i++)&#x2F;&#x2F;遍历所有的点，列数表述点的数量    &#123;        &#x2F;&#x2F;定义x来接收每一个三维点        Mat x&#x3D;pts_4d.col(i); &#x2F;&#x2F;x为4x1维度        x&#x2F;&#x3D;x.at&lt;float&gt;(3,0);&#x2F;&#x2F;归一化        Point3d p(x.at&lt;float&gt;(0, 0),                  x.at&lt;float&gt;(1, 0),                  x.at&lt;float&gt;(2, 0));        points.push_back(p);&#x2F;&#x2F;将图1测得的目标相对相机实际位置（Xc,Yc,Zc）存入points    &#125;&#125;<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p><img src="/pic/%E9%80%89%E5%8C%BA_135.png"></p><p><a href="https://www.codenong.com/cs105088833/">对极约束和三角测量</a></p><h3 id="ch7-x2F-pose-estimation-3d2d-cpp"><a href="#ch7-x2F-pose-estimation-3d2d-cpp" class="headerlink" title="ch7&#x2F;pose_estimation_3d2d.cpp"></a>ch7&#x2F;pose_estimation_3d2d.cpp</h3><p>本程序使用Opencv的EPnP求解pnp问题，并手写了一个高斯牛顿法的PnP,然后调用g2o来求解</p><pre class="line-numbers language-c++" data-language="c++"><code class="language-c++">&#x2F;&#x2F;&#x2F;&#x2F; Created by nnz on 2020&#x2F;11&#x2F;5.&#x2F;&#x2F;&#x2F;&#x2F;&#x2F;&#x2F; Created by nnz on 2020&#x2F;11&#x2F;4.&#x2F;&#x2F;#include &lt;iostream&gt;#include &lt;opencv2&#x2F;core&#x2F;core.hpp&gt;#include &lt;opencv2&#x2F;features2d&#x2F;features2d.hpp&gt;#include &lt;opencv2&#x2F;highgui&#x2F;highgui.hpp&gt;#include &lt;opencv2&#x2F;calib3d&#x2F;calib3d.hpp&gt;#include &lt;Eigen&#x2F;Core&gt;#include &lt;g2o&#x2F;core&#x2F;base_vertex.h&gt;#include &lt;g2o&#x2F;core&#x2F;base_unary_edge.h&gt;#include &lt;g2o&#x2F;core&#x2F;sparse_optimizer.h&gt;#include &lt;g2o&#x2F;core&#x2F;block_solver.h&gt;#include &lt;g2o&#x2F;core&#x2F;solver.h&gt;#include &lt;g2o&#x2F;core&#x2F;optimization_algorithm_gauss_newton.h&gt;#include &lt;g2o&#x2F;solvers&#x2F;dense&#x2F;linear_solver_dense.h&gt;#include &lt;sophus&#x2F;se3.hpp&gt;#include &lt;chrono&gt;&#x2F;&#x2F;该程序用了三种方法实现位姿估计&#x2F;&#x2F;第一种，调用cv的函数pnp求解 R ,t&#x2F;&#x2F;第二种，手写高斯牛顿进行位姿优化&#x2F;&#x2F;第三种，利用g2o进行位姿优化using namespace std;using namespace cv;typedef vector&lt;Eigen::Vector2d, Eigen::aligned_allocator&lt;Eigen::Vector2d&gt;&gt; VecVector2d;&#x2F;&#x2F;VecVector2d可以定义存放二维向量的容器typedef vector&lt;Eigen::Vector3d, Eigen::aligned_allocator&lt;Eigen::Vector3d&gt;&gt; VecVector3d;&#x2F;&#x2F;VecVector3d可以定义存放三维向量的容器void find_feature_matches(        const Mat &amp;img_1, const Mat &amp;img_2,        std::vector&lt;KeyPoint&gt; &amp;keypoints_1,        std::vector&lt;KeyPoint&gt; &amp;keypoints_2,        std::vector&lt;DMatch&gt; &amp;matches);&#x2F;&#x2F; 像素坐标转相机归一化坐标Point2d pixel2cam(const Point2d &amp;p, const Mat &amp;K);&#x2F;&#x2F; BA by gauss-newton 手写高斯牛顿进行位姿优化void bundleAdjustmentGaussNewton(        const VecVector3d &amp;points_3d,        const VecVector2d &amp;points_2d,        const Mat &amp;K,        Sophus::SE3d &amp;pose);&#x2F;&#x2F;利用g2o优化posevoid bundleAdjustmentG2O(        const VecVector3d &amp;points_3d,        const VecVector2d &amp;points_2d,        const Mat &amp;K,        Sophus::SE3d &amp;pose);int main(int argc ,char** argv)&#123;    &#x2F;&#x2F;读取图片    if (argc !&#x3D; 5) &#123;        cout &lt;&lt; &quot;usage: pose_estimation_3d2d img1 img2 depth1 depth2&quot; &lt;&lt; endl;        return 1;    &#125;    &#x2F;&#x2F;读取图片    Mat img_1&#x3D;imread(argv[1],CV_LOAD_IMAGE_COLOR);&#x2F;&#x2F;读取彩色图    Mat img_2&#x3D;imread(argv[2],CV_LOAD_IMAGE_COLOR);    assert(img_1.data &amp;&amp; img_2.data &amp;&amp; &quot;Can Not load images!&quot;);&#x2F;&#x2F;若读取的图片没有内容，就终止程序    vector&lt;KeyPoint&gt; keypoints_1, keypoints_2;    vector&lt;DMatch&gt; matches;    find_feature_matches(img_1,img_2,keypoints_1,keypoints_2,matches);&#x2F;&#x2F;得到两个图片的特征匹配点    cout &lt;&lt; &quot;一共找到了&quot; &lt;&lt; matches.size() &lt;&lt; &quot;组匹配点&quot; &lt;&lt; endl;    &#x2F;&#x2F;建立3d点，把深度图信息读进来，构造三维点    Mat d1 &#x3D; imread(argv[3], CV_LOAD_IMAGE_UNCHANGED);       &#x2F;&#x2F; 深度图为16位无符号数，单通道图像    Mat K &#x3D; (Mat_&lt;double&gt;(3, 3) &lt;&lt; 520.9, 0, 325.1, 0, 521.0, 249.7, 0, 0, 1);    vector&lt;Point3d&gt; pts_3d;&#x2F;&#x2F;创建容器pts_3d存放3d点（图1对应的特征点的相机坐标下的3d点）    vector&lt;Point2d&gt; pts_2d;&#x2F;&#x2F;创建容器pts_2d存放图2的特征点    for(DMatch m:matches)    &#123;        &#x2F;&#x2F;把对应的图1的特征点的深度信息拿出来        ushort d &#x3D; d1.ptr&lt;unsigned short&gt;(int(keypoints_1[m.queryIdx].pt.y))[int(keypoints_1[m.queryIdx].pt.x)];        if(d&#x3D;&#x3D;0) &#x2F;&#x2F;深度有问题            continue;        float dd&#x3D;d&#x2F;5000.0;&#x2F;&#x2F;用dd存放换算过尺度的深度信息        Point2d p1&#x3D;pixel2cam(keypoints_1[m.queryIdx].pt,K);&#x2F;&#x2F;p1里面放的是图1特征点在相机坐标下的归一化坐标（只包含 x,y）        pts_3d.push_back(Point3d(p1.x*dd,p1.y*dd,dd));&#x2F;&#x2F;得到图1特征点在相机坐标下的3d坐标        pts_2d.push_back(keypoints_2[m.trainIdx].pt);&#x2F;&#x2F;得到图2特张点的像素坐标    &#125;    cout&lt;&lt;&quot;3d-2d pairs:&quot;&lt;&lt; pts_3d.size() &lt;&lt;endl;&#x2F;&#x2F;3d-2d配对个数得用pts_3d的size    cout&lt;&lt;&quot;使用cv求解 位姿&quot;&lt;&lt;endl;    cout&lt;&lt;&quot;***********************************opencv***********************************&quot;&lt;&lt;endl;    chrono::steady_clock::time_point t1 &#x3D; chrono::steady_clock::now();    Mat r, t;    &#x2F;&#x2F;Mat()这个参数指的是畸变系数向量？    solvePnP(pts_3d, pts_2d, K, Mat(), r, t, false); &#x2F;&#x2F; 调用OpenCV 的 PnP 求解，可选择EPNP，DLS等方法    Mat R;    cv::Rodrigues(r,R);&#x2F;&#x2F;r是旋转向量，利用cv的Rodrigues()函数将旋转向量转换为旋转矩阵    chrono::steady_clock::time_point t2 &#x3D; chrono::steady_clock::now();    chrono::duration&lt;double&gt; time_used &#x3D; chrono::duration_cast&lt;chrono::duration&lt;double&gt;&gt;(t2 - t1);    cout &lt;&lt; &quot;solve pnp in opencv cost time: &quot; &lt;&lt; time_used.count() &lt;&lt; &quot; seconds.&quot; &lt;&lt; endl;    cout &lt;&lt; &quot;R&#x3D;&quot; &lt;&lt; endl &lt;&lt; R &lt;&lt; endl;    cout &lt;&lt; &quot;t&#x3D;&quot; &lt;&lt; endl &lt;&lt; t &lt;&lt; endl;    cout&lt;&lt;&quot;***********************************opencv***********************************&quot;&lt;&lt;endl;    cout&lt;&lt;&quot;手写高斯牛顿优化位姿&quot;&lt;&lt;endl;    cout&lt;&lt;&quot;***********************************手写高斯牛顿***********************************&quot;&lt;&lt;endl;    VecVector3d pts_3d_eigen;&#x2F;&#x2F;存放3d点（图1对应的特征点的相机坐标下的3d点）    VecVector2d pts_2d_eigen;&#x2F;&#x2F;存放图2的特征点    for(size_t i&#x3D;0;i&lt;pts_3d.size();i++)&#x2F;&#x2F;size_t    &#123;        pts_3d_eigen.push_back(Eigen::Vector3d(pts_3d[i].x,pts_3d[i].y,pts_3d[i].z));        pts_2d_eigen.push_back(Eigen::Vector2d(pts_2d[i].x,pts_2d[i].y));    &#125;    Sophus::SE3d pose_gn;&#x2F;&#x2F;位姿（李群）    t1 &#x3D; chrono::steady_clock::now();    bundleAdjustmentGaussNewton(pts_3d_eigen, pts_2d_eigen, K, pose_gn);    t2 &#x3D; chrono::steady_clock::now();    time_used &#x3D; chrono::duration_cast&lt;chrono::duration&lt;double&gt;&gt;(t2 - t1);    cout &lt;&lt; &quot;solve pnp by gauss newton cost time: &quot; &lt;&lt; time_used.count() &lt;&lt; &quot; seconds.&quot; &lt;&lt; endl;    cout&lt;&lt;&quot;R &#x3D; \n&quot;&lt;&lt;pose_gn.rotationMatrix()&lt;&lt;endl;    cout&lt;&lt;&quot;t &#x3D; &quot;&lt;&lt;pose_gn.translation().transpose()&lt;&lt;endl;    cout&lt;&lt;&quot;***********************************手写高斯牛顿***********************************&quot;&lt;&lt;endl;    cout&lt;&lt;&quot;g2o优化位姿&quot;&lt;&lt;endl;    cout &lt;&lt; &quot;***********************************g2o***********************************&quot; &lt;&lt; endl;    Sophus::SE3d pose_g2o;    t1 &#x3D; chrono::steady_clock::now();    bundleAdjustmentG2O(pts_3d_eigen, pts_2d_eigen, K, pose_g2o);    t2 &#x3D; chrono::steady_clock::now();    time_used &#x3D; chrono::duration_cast&lt;chrono::duration&lt;double&gt;&gt;(t2 - t1);    cout &lt;&lt; &quot;solve pnp by g2o cost time: &quot; &lt;&lt; time_used.count() &lt;&lt; &quot; seconds.&quot; &lt;&lt; endl;    cout &lt;&lt; &quot;***********************************g2o***********************************&quot; &lt;&lt; endl;    return 0;&#125;&#x2F;&#x2F;实现特征匹配void find_feature_matches(const Mat &amp;img_1, const Mat &amp;img_2,                          std::vector&lt;KeyPoint&gt; &amp;keypoints_1,                          std::vector&lt;KeyPoint&gt; &amp;keypoints_2,                          std::vector&lt;DMatch&gt; &amp;matches) &#123;    &#x2F;&#x2F;-- 初始化    Mat descriptors_1, descriptors_2;    &#x2F;&#x2F; used in OpenCV3    Ptr&lt;FeatureDetector&gt; detector &#x3D; ORB::create();    Ptr&lt;DescriptorExtractor&gt; descriptor &#x3D; ORB::create();    &#x2F;&#x2F; use this if you are in OpenCV2    &#x2F;&#x2F; Ptr&lt;FeatureDetector&gt; detector &#x3D; FeatureDetector::create ( &quot;ORB&quot; );    &#x2F;&#x2F; Ptr&lt;DescriptorExtractor&gt; descriptor &#x3D; DescriptorExtractor::create ( &quot;ORB&quot; );    Ptr&lt;DescriptorMatcher&gt; matcher &#x3D; DescriptorMatcher::create(&quot;BruteForce-Hamming&quot;);    &#x2F;&#x2F;-- 第一步:检测 Oriented FAST 角点位置    detector-&gt;detect(img_1, keypoints_1);    detector-&gt;detect(img_2, keypoints_2);    &#x2F;&#x2F;-- 第二步:根据角点位置计算 BRIEF 描述子    descriptor-&gt;compute(img_1, keypoints_1, descriptors_1);    descriptor-&gt;compute(img_2, keypoints_2, descriptors_2);    &#x2F;&#x2F;-- 第三步:对两幅图像中的BRIEF描述子进行匹配，使用 Hamming 距离    vector&lt;DMatch&gt; match;    &#x2F;&#x2F; BFMatcher matcher ( NORM_HAMMING );    matcher-&gt;match(descriptors_1, descriptors_2, match);    &#x2F;&#x2F;-- 第四步:匹配点对筛选    double min_dist &#x3D; 10000, max_dist &#x3D; 0;    &#x2F;&#x2F;找出所有匹配之间的最小距离和最大距离, 即是最相似的和最不相似的两组点之间的距离    for (int i &#x3D; 0; i &lt; descriptors_1.rows; i++) &#123;        double dist &#x3D; match[i].distance;        if (dist &lt; min_dist) min_dist &#x3D; dist;        if (dist &gt; max_dist) max_dist &#x3D; dist;    &#125;    printf(&quot;-- Max dist : %f \n&quot;, max_dist);    printf(&quot;-- Min dist : %f \n&quot;, min_dist);    &#x2F;&#x2F;当描述子之间的距离大于两倍的最小距离时,即认为匹配有误.但有时候最小距离会非常小,设置一个经验值30作为下限.    for (int i &#x3D; 0; i &lt; descriptors_1.rows; i++) &#123;        if (match[i].distance &lt;&#x3D; max(2 * min_dist, 30.0)) &#123;            matches.push_back(match[i]);        &#125;    &#125;&#125;&#x2F;&#x2F;实现像素坐标到相机坐标的转换（求出来的只是包含相机坐标下的x,y的二维点）Point2d pixel2cam(const Point2d &amp;p, const Mat &amp;K) &#123;    return Point2d            (                    (p.x - K.at&lt;double&gt;(0, 2)) &#x2F; K.at&lt;double&gt;(0, 0),                    (p.y - K.at&lt;double&gt;(1, 2)) &#x2F; K.at&lt;double&gt;(1, 1)            );&#125;&#x2F;&#x2F;手写高斯牛顿void bundleAdjustmentGaussNewton(        const VecVector3d &amp;points_3d,        const VecVector2d &amp;points_2d,        const Mat &amp;K,        Sophus::SE3d &amp;pose)&#123;    typedef Eigen::Matrix&lt;double,6,1&gt; Vector6d;    const int iters&#x3D;10;&#x2F;&#x2F;迭代次数    double cost&#x3D;0,lastcost&#x3D;0;&#x2F;&#x2F;代价函数（目标函数）    &#x2F;&#x2F;拿出内参    double fx &#x3D; K.at&lt;double&gt;(0, 0);    double fy &#x3D; K.at&lt;double&gt;(1, 1);    double cx &#x3D; K.at&lt;double&gt;(0, 2);    double cy &#x3D; K.at&lt;double&gt;(1, 2);    &#x2F;&#x2F;进入迭代    for (int iter &#x3D; 0; iter &lt;iters ; iter++)    &#123;        Eigen::Matrix&lt;double,6,6&gt; H &#x3D; Eigen::Matrix&lt;double,6,6&gt;::Zero();&#x2F;&#x2F;初始化H矩阵        Vector6d b &#x3D; Vector6d::Zero();&#x2F;&#x2F;对b矩阵初始化        cost &#x3D; 0;        &#x2F;&#x2F; 遍历所有的特征点  计算cost        for(int i&#x3D;0;i&lt;points_3d.size();i++)        &#123;            Eigen::Vector3d pc&#x3D;pose*points_3d[i];&#x2F;&#x2F;利用待优化的pose得到图2的相机坐标下的3d点            double inv_z&#x3D;1.0&#x2F;pc[2];&#x2F;&#x2F;得到图2的相机坐标下的3d点的z的倒数，也就是1&#x2F;z            double inv_z2 &#x3D; inv_z * inv_z;&#x2F;&#x2F;(1&#x2F;z)^2            &#x2F;&#x2F;定义投影            Eigen::Vector2d proj(fx * pc[0] &#x2F; pc[2] + cx, fy * pc[1] &#x2F; pc[2] + cy);            &#x2F;&#x2F;定义误差            Eigen::Vector2d e&#x3D;points_2d[i]-proj;            cost +&#x3D; e.squaredNorm();&#x2F;&#x2F;cost&#x3D;e*e            &#x2F;&#x2F;定义雅克比矩阵J            Eigen::Matrix&lt;double, 2, 6&gt; J;            J &lt;&lt; -fx * inv_z,                    0,                    fx * pc[0] * inv_z2,                    fx * pc[0] * pc[1] * inv_z2,                    -fx - fx * pc[0] * pc[0] * inv_z2,                    fx * pc[1] * inv_z,                    0,                    -fy * inv_z,                    fy * pc[1] * inv_z2,                    fy + fy * pc[1] * pc[1] * inv_z2,                    -fy * pc[0] * pc[1] * inv_z2,                    -fy * pc[0] * inv_z;            H +&#x3D; J.transpose() * J;            b +&#x3D; -J.transpose() * e;        &#125;        &#x2F;&#x2F;出了这个内循环，表述结束一次迭代的计算，接下来，要求pose了        Vector6d dx;&#x2F;&#x2F;P129页 公式6.33 计算增量方程 Hdx&#x3D;b        dx &#x3D; H.ldlt().solve(b);&#x2F;&#x2F;算出增量dx        &#x2F;&#x2F;判断dx这个数是否有效        if (isnan(dx[0]))        &#123;            cout &lt;&lt; &quot;result is nan!&quot; &lt;&lt; endl;            break;        &#125;        &#x2F;&#x2F;如果我们进行了迭代，且最后的cost&gt;&#x3D;lastcost的话，那就表明满足要求了，可以停止迭代了        if (iter &gt; 0 &amp;&amp; cost &gt;&#x3D; lastcost)        &#123;            &#x2F;&#x2F; cost increase, update is not good            cout &lt;&lt; &quot;cost: &quot; &lt;&lt; cost &lt;&lt; &quot;, last cost: &quot; &lt;&lt; lastcost &lt;&lt; endl;            break;        &#125;        &#x2F;&#x2F;优化pose 也就是用dx更新pose        pose&#x3D;Sophus::SE3d::exp(dx) * pose;&#x2F;&#x2F;dx是李代数，要转换为李群        lastcost&#x3D;cost;        cout &lt;&lt; &quot;iteration &quot; &lt;&lt; iter &lt;&lt; &quot; cost&#x3D;&quot;&lt;&lt; std::setprecision(12) &lt;&lt; cost &lt;&lt; endl;        &#x2F;&#x2F;std::setprecision(12)浮点数控制位数为12位        &#x2F;&#x2F;如果误差特别小了，也结束迭代        if (dx.norm() &lt; 1e-6)        &#123;            &#x2F;&#x2F; converge            break;        &#125;    &#125;    cout&lt;&lt;&quot;pose by g-n \n&quot;&lt;&lt;pose.matrix()&lt;&lt;endl;&#125;&#x2F;&#x2F;对于用g2o来进行优化的话，首先要定义顶点和边的模板&#x2F;&#x2F;顶点，也就是咱们要优化的pose 用李代数表示它 6维class Vertexpose: public g2o::BaseVertex&lt;6,Sophus::SE3d&gt;L&#123;public:    EIGEN_MAKE_ALIGNED_OPERATOR_NEW;&#x2F;&#x2F;必须写，我也不知道为什么    &#x2F;&#x2F;重载setToOriginImpl函数 这个应该就是把刚开的待优化的pose放进去    virtual void setToOriginImpl() override    &#123;        _estimate &#x3D; Sophus::SE3d();    &#125;    &#x2F;&#x2F;重载oplusImpl函数，用来更新pose（待优化的系数）    virtual void oplusImpl(const double *update) override    &#123;        Eigen::Matrix&lt;double,6,1&gt; update_eigen;&#x2F;&#x2F;更新的量，就是增量呗，dx        update_eigen &lt;&lt; update[0], update[1], update[2], update[3], update[4], update[5];        _estimate&#x3D;Sophus::SE3d::exp(update_eigen)* _estimate;&#x2F;&#x2F;更新pose 李代数要转换为李群，这样才可以左乘    &#125;    &#x2F;&#x2F;存盘 读盘 ：留空    virtual bool read(istream &amp;in) override &#123;&#125;    virtual bool write(ostream &amp;out) const override &#123;&#125;&#125;;&#x2F;&#x2F;定义边模板 边也就是误差，二维 并且把顶点也放进去class EdgeProjection : public g2o::BaseUnaryEdge&lt;2,Eigen::Vector2d,Vertexpose&gt;&#123;public:    EIGEN_MAKE_ALIGNED_OPERATOR_NEW;&#x2F;&#x2F;必须写，我也不知道为什么    &#x2F;&#x2F;有参构造，初始化 图1中的3d点 以及相机内参K    EdgeProjection(const Eigen::Vector3d &amp;pos, const Eigen::Matrix3d &amp;K) : _pos3d(pos),_K(K) &#123;&#125;    &#x2F;&#x2F;计算误差    virtual void computeError() override    &#123;        const Vertexpose *v&#x3D;static_cast&lt;const Vertexpose *&gt;(_vertices[0]);        Sophus::SE3d T&#x3D;v-&gt;estimate();        Eigen::Vector3d pos_pixel &#x3D; _K * (T * _pos3d);&#x2F;&#x2F;T * _pos3d是图2的相机坐标下的3d点        pos_pixel &#x2F;&#x3D; pos_pixel[2];&#x2F;&#x2F;得到了像素坐标的齐次形式        _error &#x3D; _measurement - pos_pixel.head&lt;2&gt;();    &#125;    &#x2F;&#x2F;计算雅克比矩阵    virtual void linearizeOplus() override    &#123;        const Vertexpose *v &#x3D; static_cast&lt;Vertexpose *&gt; (_vertices[0]);        Sophus::SE3d T &#x3D; v-&gt;estimate();        Eigen::Vector3d pos_cam&#x3D;T*_pos3d;&#x2F;&#x2F;图2的相机坐标下的3d点        double fx &#x3D; _K(0, 0);        double fy &#x3D; _K(1, 1);        double cx &#x3D; _K(0, 2);        double cy &#x3D; _K(1, 2);        double X &#x3D; pos_cam[0];        double Y &#x3D; pos_cam[1];        double Z &#x3D; pos_cam[2];        double Z2 &#x3D; Z * Z;        &#x2F;&#x2F;雅克比矩阵见 书 p187 公式7.46        _jacobianOplusXi                &lt;&lt; -fx &#x2F; Z, 0, fx * X &#x2F; Z2, fx * X * Y &#x2F; Z2, -fx - fx * X * X &#x2F; Z2, fx * Y &#x2F; Z,                0, -fy &#x2F; Z, fy * Y &#x2F; (Z * Z), fy + fy * Y * Y &#x2F; Z2, -fy * X * Y &#x2F; Z2, -fy * X &#x2F; Z;    &#125;    &#x2F;&#x2F;存盘 读盘 ：留空    virtual bool read(istream &amp;in) override &#123;&#125;    virtual bool write(ostream &amp;out) const override &#123;&#125;private:    Eigen::Vector3d _pos3d;    Eigen::Matrix3d _K;&#125;;&#x2F;&#x2F;利用g2o优化posevoid bundleAdjustmentG2O(        const VecVector3d &amp;points_3d,        const VecVector2d &amp;points_2d,        const Mat &amp;K,        Sophus::SE3d &amp;pose)&#123;    &#x2F;&#x2F; 构建图优化，先设定g2o    typedef g2o::BlockSolver&lt;g2o::BlockSolverTraits&lt;6, 3&gt;&gt; BlockSolverType;  &#x2F;&#x2F;  优化系数pose is 6, 数据点 landmark is 3    typedef g2o::LinearSolverDense&lt;BlockSolverType::PoseMatrixType&gt; LinearSolverType; &#x2F;&#x2F; 线性求解器类型    &#x2F;&#x2F; 梯度下降方法，可以从GN, LM, DogLeg 中选    auto solver &#x3D; new g2o::OptimizationAlgorithmGaussNewton(            g2o::make_unique&lt;BlockSolverType&gt;                    (g2o::make_unique&lt;LinearSolverType&gt;()));&#x2F;&#x2F;把设定的类型都放进求解器    g2o::SparseOptimizer optimizer;     &#x2F;&#x2F; 图模型    optimizer.setAlgorithm(solver);   &#x2F;&#x2F; 设置求解器 算法g-n    optimizer.setVerbose(true);       &#x2F;&#x2F; 打开调试输出    &#x2F;&#x2F;加入顶点    Vertexpose *v&#x3D;new Vertexpose();    v-&gt;setEstimate(Sophus::SE3d());    v-&gt;setId(0);    optimizer.addVertex(v);    &#x2F;&#x2F;K    &#x2F;&#x2F; K    Eigen::Matrix3d K_eigen;    K_eigen &lt;&lt;            K.at&lt;double&gt;(0, 0), K.at&lt;double&gt;(0, 1), K.at&lt;double&gt;(0, 2),            K.at&lt;double&gt;(1, 0), K.at&lt;double&gt;(1, 1), K.at&lt;double&gt;(1, 2),            K.at&lt;double&gt;(2, 0), K.at&lt;double&gt;(2, 1), K.at&lt;double&gt;(2, 2);    &#x2F;&#x2F;加入边    int index&#x3D;1;    for(size_t i&#x3D;0;i&lt;points_2d.size();++i)    &#123;        &#x2F;&#x2F;遍历 把3d点和像素点拿出来        auto p2d &#x3D; points_2d[i];        auto p3d &#x3D; points_3d[i];        EdgeProjection *edge &#x3D; new EdgeProjection(p3d, K_eigen);&#x2F;&#x2F;有参构造        edge-&gt;setId(index);        edge-&gt;setVertex(0,v);        edge-&gt;setMeasurement(p2d);&#x2F;&#x2F;设置观测值，其实就是图2 里的匹配特征点的像素位置        edge-&gt;setInformation(Eigen::Matrix2d::Identity());&#x2F;&#x2F;信息矩阵是二维方阵，因为误差是二维        optimizer.addEdge(edge);&#x2F;&#x2F;加入边        index++;&#x2F;&#x2F;边的编号++    &#125;    chrono::steady_clock::time_point t1 &#x3D; chrono::steady_clock::now();    optimizer.setVerbose(true);    optimizer.initializeOptimization();&#x2F;&#x2F;开始  初始化    optimizer.optimize(10);&#x2F;&#x2F;迭代次数    chrono::steady_clock::time_point t2 &#x3D; chrono::steady_clock::now();    chrono::duration&lt;double&gt; time_used &#x3D; chrono::duration_cast&lt;chrono::duration&lt;double&gt;&gt;(t2 - t1);    cout &lt;&lt; &quot;optimization costs time: &quot; &lt;&lt; time_used.count() &lt;&lt; &quot; seconds.&quot; &lt;&lt; endl;    cout &lt;&lt; &quot;pose estimated by g2o &#x3D;\n&quot; &lt;&lt; v-&gt;estimate().matrix() &lt;&lt; endl;    pose &#x3D; v-&gt;estimate();&#125;<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p><a href="https://blog.csdn.net/joun772/article/details/109466153?utm_medium=distribute.pc_relevant.none-task-blog-2~default~baidujs_baidulandingword~default-12-109466153-blog-116243451.235%5Ev28%5Epc_relevant_recovery_v2&spm=1001.2101.3001.4242.7&utm_relevant_index=15">SLAM十四讲-ch7(2)-位姿估计(包含2d-2d、3d-2d、3d-3d、以及三角化实现代码的注释)</a></p><h3 id="ch7-x2F-pose-estimation-3d3d-cpp"><a href="#ch7-x2F-pose-estimation-3d3d-cpp" class="headerlink" title="ch7&#x2F;pose_estimation_3d3d.cpp"></a>ch7&#x2F;pose_estimation_3d3d.cpp</h3><pre class="line-numbers language-c++" data-language="c++"><code class="language-c++">&#x2F;&#x2F;&#x2F;&#x2F; Created by nnz on 2020&#x2F;11&#x2F;5.&#x2F;&#x2F;#include &lt;iostream&gt;#include &lt;opencv2&#x2F;core&#x2F;core.hpp&gt;#include &lt;opencv2&#x2F;features2d&#x2F;features2d.hpp&gt;#include &lt;opencv2&#x2F;highgui&#x2F;highgui.hpp&gt;#include &lt;opencv2&#x2F;calib3d&#x2F;calib3d.hpp&gt;#include &lt;Eigen&#x2F;Core&gt;#include &lt;g2o&#x2F;core&#x2F;base_vertex.h&gt;#include &lt;g2o&#x2F;core&#x2F;base_unary_edge.h&gt;#include &lt;g2o&#x2F;core&#x2F;sparse_optimizer.h&gt;#include &lt;g2o&#x2F;core&#x2F;block_solver.h&gt;#include &lt;g2o&#x2F;core&#x2F;solver.h&gt;#include &lt;g2o&#x2F;core&#x2F;optimization_algorithm_gauss_newton.h&gt;#include &lt;g2o&#x2F;solvers&#x2F;dense&#x2F;linear_solver_dense.h&gt;#include &lt;sophus&#x2F;se3.hpp&gt;#include &lt;chrono&gt;using namespace std;using namespace cv;void find_feature_matches(        const Mat &amp;img_1, const Mat &amp;img_2,        std::vector&lt;KeyPoint&gt; &amp;keypoints_1,        std::vector&lt;KeyPoint&gt; &amp;keypoints_2,        std::vector&lt;DMatch&gt; &amp;matches);&#x2F;&#x2F; 像素坐标转相机归一化坐标Point2d pixel2cam(const Point2d &amp;p, const Mat &amp;K);void pose_estimation_3d3d(        const vector&lt;Point3f&gt; &amp;pts1,        const vector&lt;Point3f&gt; &amp;pts2,        Mat &amp;R, Mat &amp;t);&#x2F;&#x2F;void bundleAdjustment(        const vector&lt;Point3f&gt; &amp;pts1,        const vector&lt;Point3f&gt; &amp;pts2,        Mat &amp;R, Mat &amp;t);int main(int argc,char** argv)&#123;    if(argc!&#x3D;5)    &#123;        cout&lt;&lt;&quot; usage: pose_estimation_3d3d img1 img2 depth1 depth2 &quot;&lt;&lt;endl;        return 1;    &#125;    &#x2F;&#x2F;读取图片    Mat img_1&#x3D;imread(argv[1],CV_LOAD_IMAGE_COLOR);&#x2F;&#x2F;读取彩色图    Mat img_2&#x3D;imread(argv[2],CV_LOAD_IMAGE_COLOR);    vector&lt;KeyPoint&gt; keypoints_1, keypoints_2;&#x2F;&#x2F;容器keypoints_1, keypoints_2分别存放图1和图2的特征点    vector&lt;DMatch&gt; matches;    find_feature_matches(img_1, img_2, keypoints_1, keypoints_2, matches);&#x2F;&#x2F;得到图1与图2的特征匹配点    cout &lt;&lt; &quot;一共找到了&quot; &lt;&lt; matches.size() &lt;&lt; &quot;组匹配点&quot; &lt;&lt; endl;    &#x2F;&#x2F;接下来的是建立3d点 利用深度图可以获取深度信息    &#x2F;&#x2F;depth1是图1对应的深度图 depth2是图2对应的深度图    Mat depth1 &#x3D; imread(argv[3], CV_LOAD_IMAGE_UNCHANGED);       &#x2F;&#x2F; 深度图为16位无符号数，单通道图像    Mat depth2 &#x3D; imread(argv[4], CV_LOAD_IMAGE_UNCHANGED);    &#x2F;&#x2F;内参矩阵    Mat K &#x3D;(Mat_&lt;double&gt;(3, 3) &lt;&lt; 520.9, 0, 325.1, 0, 521.0, 249.7, 0, 0, 1);    vector&lt;Point3f&gt; pts1, pts2;    for(DMatch m:matches)    &#123;        &#x2F;&#x2F;先把两图特征匹配点对应的深度拿出来        ushort d1&#x3D;depth1.ptr&lt;unsigned short &gt;(int(keypoints_1[m.queryIdx].pt.y))[int(keypoints_1[m.queryIdx].pt.x)];        ushort d2&#x3D;depth2.ptr&lt;unsigned short &gt;(int(keypoints_2[m.trainIdx].pt.y))[int(keypoints_2[m.trainIdx].pt.x)];        if(d1&#x3D;&#x3D;0 || d2&#x3D;&#x3D;0)&#x2F;&#x2F;深度无效            continue;        Point2d p1&#x3D;pixel2cam(keypoints_1[m.queryIdx].pt,K);&#x2F;&#x2F;得到图1的特征匹配点在其相机坐标下的x ,y        Point2d p2&#x3D;pixel2cam(keypoints_2[m.trainIdx].pt,K);&#x2F;&#x2F;得到图2的特征匹配点在其相机坐标下的x ,y        &#x2F;&#x2F;对深度进行尺度变化得到真正的深度        float dd1 &#x3D; float(d1) &#x2F; 5000.0;        float dd2 &#x3D; float(d2) &#x2F; 5000.0;        &#x2F;&#x2F;容器 pts_1与pts_2分别存放 图1中的特征匹配点其相机坐标下的3d点 和 图2中的特征匹配点其相机坐标下的3d点        pts1.push_back(Point3f(p1.x * dd1, p1.y * dd1, dd1));        pts2.push_back(Point3f(p2.x * dd2, p2.y * dd2, dd2));    &#125;    &#x2F;&#x2F;这样就可以得到 3d-3d的匹配点    cout &lt;&lt; &quot;3d-3d pairs: &quot; &lt;&lt; pts1.size() &lt;&lt; endl;    Mat R, t;    cout&lt;&lt;&quot; ************************************ICP 3d-3d by SVD**************************************** &quot;&lt;&lt;endl;    pose_estimation_3d3d(pts1, pts2, R, t);    cout &lt;&lt; &quot;ICP via SVD results: &quot; &lt;&lt; endl;    cout &lt;&lt; &quot;R &#x3D; &quot; &lt;&lt; R &lt;&lt; endl;    cout &lt;&lt; &quot;t &#x3D; &quot; &lt;&lt; t &lt;&lt; endl;    cout &lt;&lt; &quot;R^T &#x3D; &quot; &lt;&lt; R.t() &lt;&lt; endl;    cout &lt;&lt; &quot;t^T &#x3D; &quot; &lt;&lt; -R.t() * t &lt;&lt; endl;    cout&lt;&lt;&quot; ************************************ICP 3d-3d by SVD**************************************** &quot;&lt;&lt;endl;    cout&lt;&lt;endl;    cout&lt;&lt;&quot; ************************************ICP 3d-3d by g2o**************************************** &quot;&lt;&lt;endl;    bundleAdjustment(pts1, pts2, R, t);    cout&lt;&lt;&quot;R&#x3D; \n&quot;&lt;&lt;R&lt;&lt;endl;    cout&lt;&lt;&quot;t &#x3D; &quot;&lt;&lt; t.t() &lt;&lt;endl;    cout&lt;&lt;&quot;验证 p2 &#x3D; R*P1 +t &quot;&lt;&lt;endl;    for (int i &#x3D; 0; i &lt; 5; i++) &#123;        cout &lt;&lt; &quot;p1 &#x3D; &quot; &lt;&lt; pts1[i] &lt;&lt; endl;        cout &lt;&lt; &quot;p2 &#x3D; &quot; &lt;&lt; pts2[i] &lt;&lt; endl;        cout &lt;&lt; &quot;(R*p1+t) &#x3D; &quot; &lt;&lt;             R * (Mat_&lt;double&gt;(3, 1) &lt;&lt; pts1[i].x, pts1[i].y, pts1[i].z) + t             &lt;&lt; endl;        cout &lt;&lt; endl;    &#125;    cout&lt;&lt;&quot; ************************************ICP 3d-3d by g2o**************************************** &quot;&lt;&lt;endl;    return 0;&#125;void find_feature_matches(const Mat &amp;img_1, const Mat &amp;img_2,                          std::vector&lt;KeyPoint&gt; &amp;keypoints_1,                          std::vector&lt;KeyPoint&gt; &amp;keypoints_2,                          std::vector&lt;DMatch&gt; &amp;matches) &#123;    &#x2F;&#x2F;-- 初始化    Mat descriptors_1, descriptors_2;    &#x2F;&#x2F; used in OpenCV3    Ptr&lt;FeatureDetector&gt; detector &#x3D; ORB::create();    Ptr&lt;DescriptorExtractor&gt; descriptor &#x3D; ORB::create();    &#x2F;&#x2F; use this if you are in OpenCV2    &#x2F;&#x2F; Ptr&lt;FeatureDetector&gt; detector &#x3D; FeatureDetector::create ( &quot;ORB&quot; );    &#x2F;&#x2F; Ptr&lt;DescriptorExtractor&gt; descriptor &#x3D; DescriptorExtractor::create ( &quot;ORB&quot; );    Ptr&lt;DescriptorMatcher&gt; matcher &#x3D; DescriptorMatcher::create(&quot;BruteForce-Hamming&quot;);    &#x2F;&#x2F;-- 第一步:检测 Oriented FAST 角点位置    detector-&gt;detect(img_1, keypoints_1);    detector-&gt;detect(img_2, keypoints_2);    &#x2F;&#x2F;-- 第二步:根据角点位置计算 BRIEF 描述子    descriptor-&gt;compute(img_1, keypoints_1, descriptors_1);    descriptor-&gt;compute(img_2, keypoints_2, descriptors_2);    &#x2F;&#x2F;-- 第三步:对两幅图像中的BRIEF描述子进行匹配，使用 Hamming 距离    vector&lt;DMatch&gt; match;    &#x2F;&#x2F; BFMatcher matcher ( NORM_HAMMING );    matcher-&gt;match(descriptors_1, descriptors_2, match);    &#x2F;&#x2F;-- 第四步:匹配点对筛选    double min_dist &#x3D; 10000, max_dist &#x3D; 0;    &#x2F;&#x2F;找出所有匹配之间的最小距离和最大距离, 即是最相似的和最不相似的两组点之间的距离    for (int i &#x3D; 0; i &lt; descriptors_1.rows; i++) &#123;        double dist &#x3D; match[i].distance;        if (dist &lt; min_dist) min_dist &#x3D; dist;        if (dist &gt; max_dist) max_dist &#x3D; dist;    &#125;    printf(&quot;-- Max dist : %f \n&quot;, max_dist);    printf(&quot;-- Min dist : %f \n&quot;, min_dist);    &#x2F;&#x2F;当描述子之间的距离大于两倍的最小距离时,即认为匹配有误.但有时候最小距离会非常小,设置一个经验值30作为下限.    for (int i &#x3D; 0; i &lt; descriptors_1.rows; i++) &#123;        if (match[i].distance &lt;&#x3D; max(2 * min_dist, 30.0)) &#123;            matches.push_back(match[i]);        &#125;    &#125;&#125;&#x2F;&#x2F;实现像素坐标到相机坐标的转换（求出来的只是包含相机坐标下的x,y的二维点）Point2d pixel2cam(const Point2d &amp;p, const Mat &amp;K) &#123;    return Point2d            (                    (p.x - K.at&lt;double&gt;(0, 2)) &#x2F; K.at&lt;double&gt;(0, 0),                    (p.y - K.at&lt;double&gt;(1, 2)) &#x2F; K.at&lt;double&gt;(1, 1)            );&#125;&#x2F;&#x2F;参考书上的p197页void pose_estimation_3d3d(        const vector&lt;Point3f&gt; &amp;pts1,        const vector&lt;Point3f&gt; &amp;pts2,        Mat &amp;R, Mat &amp;t)&#123;    int N&#x3D;pts1.size();&#x2F;&#x2F;匹配的3d点个数    Point3f p1,p2;&#x2F;&#x2F;质心    for(int i&#x3D;0;i&lt;N;i++)    &#123;        p1+&#x3D;pts1[i];        p2+&#x3D;pts2[i];    &#125;    p1 &#x3D; Point3f(Vec3f(p1)&#x2F;N);&#x2F;&#x2F;得到质心    p2 &#x3D; Point3f(Vec3f(p2) &#x2F; N);    vector&lt;Point3f&gt; q1(N),q2(N);    for(int i&#x3D;0;i&lt;N;i++)    &#123;        &#x2F;&#x2F;去质心        q1[i]&#x3D;pts1[i]-p1;        q2[i]&#x3D;pts2[i]-p2;    &#125;    &#x2F;&#x2F;计算 W+&#x3D;q1*q2^T(求和)    Eigen::Matrix3d W&#x3D;Eigen::Matrix3d::Zero();&#x2F;&#x2F;初始化    for(int i&#x3D;0;i&lt;N;i++)    &#123;        W+&#x3D; Eigen::Vector3d (q1[i].x,q1[i].y,q1[i].z)*(Eigen::Vector3d (q2[i].x,q2[i].y,q2[i].z).transpose());    &#125;    cout&lt;&lt;&quot;W &#x3D; &quot;&lt;&lt;endl&lt;&lt;W&lt;&lt;endl;    &#x2F;&#x2F;利用svd分解 W&#x3D;U*sigema*V    Eigen::JacobiSVD&lt;Eigen::Matrix3d&gt; svd(W,Eigen::ComputeFullU | Eigen::ComputeFullV);    Eigen::Matrix3d U&#x3D;svd.matrixU();&#x2F;&#x2F;得到U矩阵    Eigen::Matrix3d V&#x3D;svd.matrixV();&#x2F;&#x2F;得到V矩阵    cout &lt;&lt; &quot;U&#x3D;&quot; &lt;&lt; U &lt;&lt; endl;    cout &lt;&lt; &quot;V&#x3D;&quot; &lt;&lt; V &lt;&lt; endl;    Eigen::Matrix3d R_&#x3D;U*(V.transpose());    if (R_.determinant() &lt; 0)&#x2F;&#x2F;若旋转矩阵R_的行列式&lt;0 则取负号    &#123;        R_ &#x3D; -R_;    &#125;    Eigen::Vector3d t_&#x3D;Eigen::Vector3d (p1.x,p1.y,p1.z)-R_*Eigen::Vector3d (p2.x,p2.y,p2.z);&#x2F;&#x2F;得到平移向量    &#x2F;&#x2F;把 Eigen形式的 r 和 t_ 转换为CV 中的Mat格式    R &#x3D; (Mat_&lt;double&gt;(3, 3) &lt;&lt;                            R_(0, 0), R_(0, 1), R_(0, 2),            R_(1, 0), R_(1, 1), R_(1, 2),            R_(2, 0), R_(2, 1), R_(2, 2)    );    t &#x3D; (Mat_&lt;double&gt;(3, 1) &lt;&lt; t_(0, 0), t_(1, 0), t_(2, 0));&#125;&#x2F;&#x2F;对于用g2o来进行优化的话，首先要定义顶点和边的模板&#x2F;&#x2F;顶点，也就是咱们要优化的pose 用李代数表示它 6维class Vertexpose: public g2o::BaseVertex&lt;6,Sophus::SE3d&gt;&#123;public:    EIGEN_MAKE_ALIGNED_OPERATOR_NEW;&#x2F;&#x2F;必须写，我也不知道为什么    &#x2F;&#x2F;重载setToOriginImpl函数 这个应该就是把刚开的待优化的pose放进去    virtual void setToOriginImpl() override    &#123;        _estimate &#x3D; Sophus::SE3d();    &#125;    &#x2F;&#x2F;重载oplusImpl函数，用来更新pose（待优化的系数）    virtual void oplusImpl(const double *update) override    &#123;        Eigen::Matrix&lt;double,6,1&gt; update_eigen;&#x2F;&#x2F;更新的量，就是增量呗，dx        update_eigen &lt;&lt; update[0], update[1], update[2], update[3], update[4], update[5];        _estimate&#x3D;Sophus::SE3d::exp(update_eigen)* _estimate;&#x2F;&#x2F;更新pose 李代数要转换为李群，这样才可以左乘    &#125;    &#x2F;&#x2F;存盘 读盘 ：留空    virtual bool read(istream &amp;in) override &#123;&#125;    virtual bool write(ostream &amp;out) const override &#123;&#125;&#125;;&#x2F;&#x2F;定义边class EdgeProjectXYZRGBD: public g2o::BaseUnaryEdge&lt;3,Eigen::Vector3d,Vertexpose&gt;&#123;public:    EdgeProjectXYZRGBD(const Eigen::Vector3d &amp;point) : _point(point) &#123;&#125;&#x2F;&#x2F;赋值这个是图1坐标下的3d点    &#x2F;&#x2F;计算误差    virtual void computeError() override    &#123;        const Vertexpose *v&#x3D;static_cast&lt;const Vertexpose *&gt;(_vertices[0]);&#x2F;&#x2F;顶点v        _error &#x3D; _measurement - v-&gt;estimate() * _point;    &#125;    &#x2F;&#x2F;计算雅克比    virtual void linearizeOplus() override    &#123;        const Vertexpose *v&#x3D;static_cast&lt;const Vertexpose *&gt;(_vertices[0]);&#x2F;&#x2F;顶点v        Sophus::SE3d T&#x3D;v-&gt;estimate();&#x2F;&#x2F;把顶点的待优化系数拿出来        Eigen::Vector3d xyz_trans&#x3D;T*_point;&#x2F;&#x2F;变换到图2下的坐标点        &#x2F;&#x2F;下面的雅克比没看懂        _jacobianOplusXi.block&lt;3, 3&gt;(0, 0) &#x3D; -Eigen::Matrix3d::Identity();        _jacobianOplusXi.block&lt;3, 3&gt;(0, 3) &#x3D; Sophus::SO3d::hat(xyz_trans);    &#125;    bool read(istream &amp;in) &#123;&#125;    bool write(ostream &amp;out) const &#123;&#125;protected:    Eigen::Vector3d _point;&#125;;&#x2F;&#x2F;利用g2ovoid bundleAdjustment(const vector&lt;Point3f&gt; &amp;pts1,                      const vector&lt;Point3f&gt; &amp;pts2,                      Mat &amp;R, Mat &amp;t)&#123;&#x2F;&#x2F; 构建图优化，先设定g2o    typedef g2o::BlockSolver&lt;g2o::BlockSolverTraits&lt;6, 3&gt;&gt; BlockSolverType;  &#x2F;&#x2F;  优化系数pose is 6, 数据点 landmark is 3    typedef g2o::LinearSolverDense&lt;BlockSolverType::PoseMatrixType&gt; LinearSolverType; &#x2F;&#x2F; 线性求解器类型    &#x2F;&#x2F; 梯度下降方法，可以从GN, LM, DogLeg 中选    auto solver &#x3D; new g2o::OptimizationAlgorithmGaussNewton(            g2o::make_unique&lt;BlockSolverType&gt;                    (g2o::make_unique&lt;LinearSolverType&gt;()));&#x2F;&#x2F;把设定的类型都放进求解器    g2o::SparseOptimizer optimizer;     &#x2F;&#x2F; 图模型    optimizer.setAlgorithm(solver);   &#x2F;&#x2F; 设置求解器 算法g-n    optimizer.setVerbose(true);       &#x2F;&#x2F; 打开调试输出    &#x2F;&#x2F;加入顶点    Vertexpose *v&#x3D;new Vertexpose();    v-&gt;setEstimate(Sophus::SE3d());    v-&gt;setId(0);    optimizer.addVertex(v);    &#x2F;&#x2F;加入边    int index&#x3D;1;    for(size_t i&#x3D;0;i&lt;pts1.size();i++)    &#123;        EdgeProjectXYZRGBD *edge &#x3D; new EdgeProjectXYZRGBD(Eigen::Vector3d(pts1[i].x,pts1[i].y,pts1[i].z));        edge-&gt;setId(index);&#x2F;&#x2F;边的编号        edge-&gt;setVertex(0,v);&#x2F;&#x2F;设置顶点  顶点编号        edge-&gt;setMeasurement(Eigen::Vector3d(pts2[i].x,pts2[i].y,pts2[i].z));        edge-&gt;setInformation(Eigen::Matrix3d::Identity());&#x2F;&#x2F;set信息矩阵为单位矩阵        optimizer.addEdge(edge);&#x2F;&#x2F;加入边        index++;    &#125;    chrono::steady_clock::time_point t1 &#x3D; chrono::steady_clock::now();    optimizer.initializeOptimization();&#x2F;&#x2F;开始    optimizer.optimize(10);&#x2F;&#x2F;迭代次数    chrono::steady_clock::time_point t2 &#x3D; chrono::steady_clock::now();    chrono::duration&lt;double&gt; time_used &#x3D; chrono::duration_cast&lt;chrono::duration&lt;double&gt;&gt;(t2 - t1);    cout &lt;&lt; &quot;optimization costs time: &quot; &lt;&lt; time_used.count() &lt;&lt; &quot; seconds.&quot; &lt;&lt; endl;    cout &lt;&lt; endl &lt;&lt; &quot;after optimization:&quot; &lt;&lt; endl;    cout &lt;&lt; &quot;T&#x3D;\n&quot; &lt;&lt; v-&gt;estimate().matrix() &lt;&lt; endl;    &#x2F;&#x2F; 把位姿转换为Mat类型    Eigen::Matrix3d R_ &#x3D; v-&gt;estimate().rotationMatrix();    Eigen::Vector3d t_ &#x3D; v-&gt;estimate().translation();    R &#x3D; (Mat_&lt;double&gt;(3, 3) &lt;&lt;                            R_(0, 0), R_(0, 1), R_(0, 2),            R_(1, 0), R_(1, 1), R_(1, 2),            R_(2, 0), R_(2, 1), R_(2, 2)    );    t &#x3D; (Mat_&lt;double&gt;(3, 1) &lt;&lt; t_(0, 0), t_(1, 0), t_(2, 0));&#125;<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p><a href="https://blog.csdn.net/joun772/article/details/109466153?utm_medium=distribute.pc_relevant.none-task-blog-2~default~baidujs_baidulandingword~default-12-109466153-blog-116243451.235%5Ev28%5Epc_relevant_recovery_v2&spm=1001.2101.3001.4242.7&utm_relevant_index=15">SLAM十四讲-ch7(2)-位姿估计(包含2d-2d、3d-2d、3d-3d、以及三角化实现代码的注释)</a></p><h2 id="第八讲"><a href="#第八讲" class="headerlink" title="第八讲"></a>第八讲</h2><p>直接法是vo的另一个主要的分支，它与特征点法有很大的不同，理解光流法跟踪特征点的原理，理解直接法估计相机位姿，实现多层直接法的计算</p><h3 id="ch8-x2F-optical-flow-cpp"><a href="#ch8-x2F-optical-flow-cpp" class="headerlink" title="ch8&#x2F;optical_flow.cpp"></a>ch8&#x2F;optical_flow.cpp</h3><p>光流是一种描述像素随时间在图像之间运动的方法：</p><p>光流法有两个假设：（1）灰度不变假设：同一个空间点的像素灰度值，在各个图像中是固定不变的；（2）假设某个窗口内的像素具有相同的运动</p><p>一、本讲的代码使用了三种方法来追踪图像上的特征点</p><ul><li>第一种：使用OpenCV中的LK光流；</li><li>第二种：用高斯牛顿实现光流：单层光流；</li><li>第三种：用高斯牛顿实现光流：多层光流。</li></ul><p>其中高斯牛顿法，即最小化灰度误差估计最优的像素偏移。在具体函数实现中（即calculateOpticalFlow)，求解这样一个问题：</p><p><img src="/pic/%E9%80%89%E5%8C%BA_136.png"></p><p><strong>Opencv中的LK光流</strong>：使用  cv::calculateOpticalFlowPyrLK函数：</p><ul><li>提供前后两张图像及对应的特征点，即可得到追踪后的点，以及各点的状态、误差;</li><li>根据status变量是否为1来确定对应的点是否被正确追踪到。</li><li>cv::calcOpticalFlowPyrLK(img1,img2,pt1,pt2,status,error);</li></ul><p><strong>单层光流</strong>：用高斯牛顿法实现光流，光流也可以看成一个优化问题，通过最小化灰度误差估计最优的像素偏移</p><p><strong>多层光流</strong>：因为单层光流在相机运动较快的情况下，容易达到一个局部极小值，因此引入图像金字塔。当原始图像的像素运动较大时，在金字塔顶层看来，运动仍然是一个小的运动范围</p><p>二、代码注释</p><pre class="line-numbers language-c++" data-language="c++"><code class="language-c++">#include&lt;opencv2&#x2F;opencv.hpp&gt;#include&lt;string&gt;#include&lt;chrono&gt;#include&lt;Eigen&#x2F;Core&gt;#include&lt;Eigen&#x2F;Dense&gt;using namespace std;using namespace cv;string file_1&#x3D;&quot;.&#x2F;LK1.png&quot;;   &#x2F;&#x2F;第一张图像的路径,可能需要写成绝对路径string file_2&#x3D;&quot;.&#x2F;LK2.png&quot;;  &#x2F;&#x2F;第二张图像的路径  &#x2F;&#x2F;使用高斯牛顿法实现光流&#x2F;&#x2F;定义一个光流追踪类class OpticalFlowTracker&#123; public:    OpticalFlowTracker(    &#x2F;&#x2F;带参构造函数，并初始化      const Mat &amp;img1_,      const Mat &amp;img2_,      const vector&lt;KeyPoint&gt;&amp;kp1_,      vector&lt;KeyPoint&gt;&amp;kp2_,      vector&lt;bool&gt;&amp;success_,      bool inverse_&#x3D;true,bool has_initial_&#x3D;false):      img1(img1_),img2(img2_),kp1(kp1_),kp2(kp2_),success(success_),inverse(inverse_),      has_initial(has_initial_) &#123;&#125;            &#x2F;&#x2F;计算光流的函数      void calculateOpticalFlow(const Range &amp;range);  &#x2F;&#x2F;range是一个区间，应该看作一个窗口        private:    const Mat &amp;img1;    const Mat &amp;img2;    const vector&lt;KeyPoint&gt; &amp;kp1;    vector&lt;KeyPoint&gt; &amp;kp2;    vector&lt;bool&gt; &amp;success;    bool inverse&#x3D;true;    bool has_initial&#x3D;false;  &#125;;  &#x2F;&#x2F;单层光流的函数声明  void OpticalFlowSingleLevel(    const Mat &amp;img1,    const Mat &amp;img2,    const vector&lt;KeyPoint&gt;&amp;kp1,    vector&lt;KeyPoint&gt;&amp;kp2,    vector&lt;bool&gt;&amp;success,    bool inverse&#x3D;false,    bool has_initial_guess&#x3D;false  );  &#x2F;&#x2F;多层光流的函数声明  void OpticalFlowMultiLevel(    const Mat &amp;img1,    const Mat &amp;img2,    const vector&lt;KeyPoint&gt;&amp;kp1,    vector&lt;KeyPoint&gt;&amp;kp2,    vector&lt;bool&gt; &amp;success,    bool inverse&#x3D;false       );    &#x2F;&#x2F;从图像中获取一个灰度值  &#x2F;&#x2F;采用双线性内插法，来估计一个点的像素：  &#x2F;&#x2F;f(x,y)&#x3D;f(0,0)(1-x)(1-y)+f(1,0)x(1-y)+f(0,1)(1-x)y+f(1,1)xy  inline float GetPixelValue(const cv::Mat &amp;img,float x,float y)  &#123;    &#x2F;&#x2F;边缘检测    if(x&lt;0)      x&#x3D;0;    if(y&lt;0)      y&#x3D;0;    if(x&gt;&#x3D;img.cols)      x&#x3D;img.cols-1;    if(y&gt;&#x3D;img.rows)      y&#x3D;img.rows-1;    uchar *data&#x3D;&amp;img.data[int(y)*img.step+int(x)];  &#x2F;&#x2F;img.step:表示图像矩阵中每行包含的字节数;int(x)将x转换为int类型        float xx&#x3D;x-floor(x);   &#x2F;&#x2F;floor(x)函数：向下取整函数，即返回一个不大于x的最大整数    float yy&#x3D;y-floor(y);        return float(      (1-xx)*(1-yy)*data[0]+      xx*(1-yy)*data[1]+      (1-xx)*yy*data[img.step]+      xx*yy*data[img.step+1]      );  &#125;    &#x2F;&#x2F;主函数  int main(int argc,char**argv)  &#123;    Mat img1&#x3D;imread(file_1,0);  &#x2F;&#x2F;以灰度读取图像，重点    Mat img2&#x3D;imread(file_2,0);    &#x2F;&#x2F;特征点检测    vector&lt;KeyPoint&gt;kp1;  &#x2F;&#x2F;关键点 存放在容器kp1中    Ptr&lt;GFTTDetector&gt; detector&#x3D;GFTTDetector::create(500,0.01,20); &#x2F;&#x2F;通过GFTTD来获取角点，参数：最大角点数目500;角点可以接受的最小特征值0.01;角点之间的最小距离20    detector-&gt;detect(img1,kp1);&#x2F;&#x2F;类似于ORB特征点的提取过程        &#x2F;&#x2F;接下来实现在第二张图像中追踪这些角点，即追踪 kp1    &#x2F;&#x2F;第一种方法：单层光流    vector&lt;KeyPoint&gt;kp2_single;    vector&lt;bool&gt;success_single;    OpticalFlowSingleLevel(img1,img2,kp1,kp2_single,success_single);        &#x2F;&#x2F;第二种方法：多层光流    vector&lt;KeyPoint&gt;kp2_multi;    vector&lt;bool&gt;success_multi;    chrono::steady_clock::time_point t1&#x3D;chrono::steady_clock::now();    OpticalFlowMultiLevel(img1,img2,kp1,kp2_multi,success_multi,true);    chrono::steady_clock::time_point t2&#x3D;chrono::steady_clock::now();    auto time_used&#x3D;chrono::duration_cast&lt;chrono::duration&lt;double&gt;&gt;(t2-t1);     &#x2F;&#x2F;输出使用高斯牛顿法所花费的时间    cout &lt;&lt; &quot;optical flow by gauss-newton: &quot; &lt;&lt; time_used.count() &lt;&lt; endl;        &#x2F;&#x2F;使用OpenCV中的LK光流    vector&lt;Point2f&gt;pt1,pt2;    for(auto &amp;kp:kp1)     &#x2F;&#x2F;kp1中存放的是第一张图像中的角点，通过遍历，将kp1存放在pt1中      pt1.push_back(kp.pt);    vector&lt;uchar&gt;status;    vector&lt;float&gt;error;    t1&#x3D;chrono::steady_clock::now();    &#x2F;&#x2F;调用cv::calculateOpticalFlowPyrLK函数：    &#x2F;&#x2F;提供前后两张图像及对应的特征点，即可得到追踪后的点，以及各点的状态、误差;    &#x2F;&#x2F;根据status变量是否为1来确定对应的点是否被正确追踪到。    cv::calcOpticalFlowPyrLK(img1,img2,pt1,pt2,status,error);    t2&#x3D;chrono::steady_clock::now();    time_used&#x3D;chrono::duration_cast&lt;chrono::duration&lt;double&gt;&gt;(t2-t1);    cout &lt;&lt; &quot;optical flow by opencv: &quot; &lt;&lt; time_used.count() &lt;&lt; endl;  &#x2F;&#x2F;输出使用opencv所花费的时间        &#x2F;&#x2F;下面一部分代码实现绘图的功能    &#x2F;&#x2F;第一张图像：单层光流的效果图     Mat img2_single;     &#x2F;&#x2F;将输入图像从一个空间转换到另一个色彩空间     cv::cvtColor(img2,img2_single,CV_GRAY2BGR); &#x2F;&#x2F;cvtColor(）函数实现的功能：将img2灰度图转换成彩色图img2_single输出     for(int i&#x3D;0;i&lt;kp2_single.size();i++)     &#123;       if(success_single[i])   &#x2F;&#x2F;判断是否追踪成功       &#123; &#x2F;&#x2F;circle():画圆：参数：源图像，画圆的圆心坐标，圆的半径，圆的颜色，线条的粗细程度 &#x2F;&#x2F;kp2_single[i].pt：用来取第i个角点的坐标；Scalar(0,250,0)：设置颜色，遵循B G R ，所以此图中为绿色 cv::circle(img2_single,kp2_single[i].pt,2,cv::Scalar(0,250,0),2); &#x2F;&#x2F;line():绘制直线：参数：要画的线所在的图像，直线起点，直线终点，直线的颜色（绿色） cv::line(img2_single,kp1[i].pt,kp2_single[i].pt,cv::Scalar(0,250,0));       &#125;     &#125;          &#x2F;&#x2F;第二张图像：多层光流的效果图     Mat img2_multi;     cv::cvtColor(img2,img2_multi,CV_GRAY2BGR);      for(int i&#x3D;0;i&lt;kp2_multi.size();i++)     &#123;       if(success_multi[i])          &#123; cv::circle(img2_multi,kp2_multi[i].pt,2,cv::Scalar(250,0,0),2);  cv::line(img2_multi,kp1[i].pt,kp2_multi[i].pt,cv::Scalar(250,0,0));       &#125;     &#125;          &#x2F;&#x2F;第三张图像：使用OpenCV中的LK光流     Mat img2_CV;     cv::cvtColor(img2,img2_CV,CV_GRAY2BGR);      for(int i&#x3D;0;i&lt;pt2.size();i++)     &#123;       if(status[i])          &#123;  cv::circle(img2_CV,pt2[i],2,cv::Scalar(0,0,250),2);  cv::line(img2_CV,pt1[i],pt2[i],cv::Scalar(0,0,250));       &#125;     &#125;          &#x2F;&#x2F;     cv::imshow(&quot;tracked single level&quot;,img2_single);     cv::imshow(&quot;tracked multi level&quot;,img2_multi);     cv::imshow(&quot;tracked by opencv&quot;,img2_CV);     cv::waitKey(0);     return 0;  &#125;<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p><em><strong>具体功能实现如下：</strong></em></p><pre class="line-numbers language-c++" data-language="c++"><code class="language-c++"> &#x2F;&#x2F;接下来这一部分：具体函数的实现 &#x2F;&#x2F;第一个：单层光流函数的实现 void OpticalFlowSingleLevel(   const Mat &amp;img1,   const Mat &amp;img2,   const vector&lt;KeyPoint&gt;&amp;kp1,   vector&lt;KeyPoint&gt;&amp;kp2,   vector&lt;bool&gt;&amp;success,   bool inverse,   bool has_initial) &#123;   &#x2F;&#x2F;resize()函数：调整图像的大小；size（）函数：获取kp1的长度   &#x2F;&#x2F;初始化   kp2.resize(kp1.size());   success.resize(kp1.size());   &#x2F;&#x2F;是否追踪成功的标志      OpticalFlowTracker tracker(img1,img2,kp1,kp2,success,inverse,has_initial);  &#x2F;&#x2F;创建类的对象tracker   &#x2F;&#x2F;调用parallel_for_并行调用OpticalFlowTracker::calculateOpticalFlow，该函数计算指定范围内特征点的光流   &#x2F;&#x2F;range():从指定的第一个值开始，并在到达指定的第二个值后终止   parallel_for_(Range(0,kp1.size()),std::bind(&amp;OpticalFlowTracker::calculateOpticalFlow,&amp;tracker,placeholders::_1)); &#125;  &#x2F;&#x2F;类外实现成员函数 void OpticalFlowTracker::calculateOpticalFlow(const Range &amp;range) &#123;   &#x2F;&#x2F;定义参数   int half_patch_size&#x3D;4;  &#x2F;&#x2F;窗口的大小8×8   int iterations&#x3D;10;  &#x2F;&#x2F;每个角点迭代10次   for(size_t i&#x3D;range.start;i&lt;range.end;i++)   &#123;     auto kp&#x3D;kp1[i];   &#x2F;&#x2F;将第一张图像中的第i个关键点kp1[i]存放在 kp 中     double dx&#x3D;0,dy&#x3D;0; &#x2F;&#x2F;初始化     if(has_initial)     &#123;dx&#x3D;kp2[i].pt.x-kp.pt.x;   &#x2F;&#x2F;第i个点在第二张图像中的位置与第一张图像中的位置的差值dy&#x3D;kp2[i].pt.y-kp.pt.y;     &#125;          double cost&#x3D;0,lastCost&#x3D;0;     bool succ&#x3D;true;          &#x2F;&#x2F;高斯牛顿方程     &#x2F;&#x2F;高斯牛顿迭代     Eigen::Matrix2d H &#x3D; Eigen::Matrix2d::Zero();   &#x2F;&#x2F;定义H，并进行初始化。     Eigen::Vector2d b &#x3D; Eigen::Vector2d::Zero();   &#x2F;&#x2F;定义b，并初始化.     Eigen::Vector2d J;   &#x2F;&#x2F;定义雅克比矩阵2×1     for(int iter&#x3D;0;iter&lt;iterations;iter++)     &#123;if(inverse&#x3D;&#x3D;false)&#123;  H&#x3D;Eigen::Matrix2d::Zero();  b&#x3D;Eigen::Vector2d::Zero();&#125;else&#123;  b&#x3D;Eigen::Vector2d::Zero();&#125;cost&#x3D;0;&#x2F;&#x2F;假设在这个8×8的窗口内像素具有同样的运动&#x2F;&#x2F;计算cost和Jfor(int x&#x3D;-half_patch_size;x&lt;half_patch_size;x++)  for(int y&#x3D;-half_patch_size;y&lt;half_patch_size;y++)  &#123;    &#x2F;&#x2F;GetPixelValue（）计算某点的灰度值    &#x2F;&#x2F;计算残差：I(x,y)-I(x+dx,y+dy)    double error&#x3D;GetPixelValue(img1,kp.pt.x+x,kp.pt.y+y)-GetPixelValue(img2,kp.pt.x+x+dx,kp.pt.y+y+dy);;    if(inverse&#x3D;&#x3D;false)    &#123;      &#x2F;&#x2F;雅克比矩阵为第二个图像在x+dx,y+dy处的梯度      J&#x3D;-1.0*Eigen::Vector2d(0.5*(GetPixelValue(img2,kp.pt.x+dx+x+1,kp.pt.y+dy+y)-                                 GetPixelValue(img2,kp.pt.x+dx+x-1,kp.pt.y+dy+y)),     0.5*(GetPixelValue(img2,kp.pt.x+dx+x,kp.pt.y+dy+y+1)-         GetPixelValue(img2,kp.pt.x+dx+x,kp.pt.y+dy+y-1)));    &#125;    else if(iter&#x3D;&#x3D;0)  &#x2F;&#x2F;如果是第一次迭代，梯度为第一个图像的梯度，反向光流法      &#x2F;&#x2F;在反向光流中，I(x,y)的梯度是保持不变的，可以在第一次迭代时保留计算的结果，在后续的迭代中使用。      &#x2F;&#x2F;当雅克比矩阵不变时，H矩阵不变，每次迭代只需要计算残差。    &#123;      J&#x3D;-1.0*Eigen::Vector2d(0.5*(GetPixelValue(img1,kp.pt.x+x+1,kp.pt.y+y)-                                 GetPixelValue(img1,kp.pt.x+x-1,kp.pt.y+y)),     0.5*(GetPixelValue(img1,kp.pt.x+x,kp.pt.y+y+1)-         GetPixelValue(img1,kp.pt.x+x,kp.pt.y+y-1)));    &#125;    &#x2F;&#x2F;计算H和b    b+&#x3D;-error*J;    cost+&#x3D;error*error;    if(inverse&#x3D;&#x3D;false||iter&#x3D;&#x3D;0)    &#123;      H+&#x3D;J*J.transpose();    &#125;  &#125;  &#x2F;&#x2F;计算增量update，求解线性方程Hx&#x3D;b  Eigen::Vector2d update&#x3D;H.ldlt().solve(b);  if(std::isnan(update[0]))  &#x2F;&#x2F;判断增量  &#123;    &#x2F;&#x2F;有时当我们遇到一个黑色或白色的方块，H是不可逆的，即高斯牛顿方程无解    cout&lt;&lt;&quot;update is nan&quot;&lt;&lt;endl;    succ&#x3D;false;   &#x2F;&#x2F;追踪失败    break;  &#125;  if(iter&gt;0&amp;&amp;cost&gt;lastCost)  &#123;    break;  &#125;  dx+&#x3D;update[0];  dy+&#x3D;update[1];  lastCost&#x3D;cost;  succ&#x3D;true;  if(update.norm()&lt;1e-2)  &#123;    break;  &#125;     &#125;     success[i]&#x3D;succ;     kp2[i].pt&#x3D;kp.pt+Point2f(dx,dy);   &#125; &#125;&#x2F;&#x2F;迭代完成  &#x2F;&#x2F;第二个：多层光流函数的实现 void OpticalFlowMultiLevel(   const Mat &amp;img1,   const Mat &amp;img2,   const vector&lt;KeyPoint&gt;&amp;kp1,   vector&lt;KeyPoint&gt;&amp;kp2,   vector&lt;bool&gt;&amp;success,   bool inverse) &#123;   int pyramids&#x3D;4; &#x2F;&#x2F;建立4层金字塔   double pyramid_scale&#x3D;0.5;  &#x2F;&#x2F;金字塔每层缩小0.5   double scales[]&#x3D;&#123;1.0,0.5,0.25,0.125&#125;;      &#x2F;&#x2F;建立金字塔   chrono::steady_clock::time_point t1&#x3D;chrono::steady_clock::now();  &#x2F;&#x2F;计算时间   vector&lt;Mat&gt;pyr1,pyr2;   for(int i&#x3D;0;i&lt;pyramids;i++)   &#123;     if(i&#x3D;&#x3D;0)     &#123;&#x2F;&#x2F;将两张图像存放在pyr1,pyr2中pyr1.push_back(img1);    pyr2.push_back(img2);     &#125;     else     &#123;Mat img1_pyr,img2_pyr;&#x2F;&#x2F;对图像进行缩放，参数：原图，输出图像，输出图像大小，Size（宽度，高度）cv::resize(pyr1[i-1],img1_pyr,cv::Size(pyr1[i-1].cols*pyramid_scale,pyr1[i-1].rows*pyramid_scale));cv::resize(pyr2[i-1],img2_pyr,cv::Size(pyr2[i-1].cols*pyramid_scale,pyr2[i-1].rows*pyramid_scale));pyr1.push_back(img1_pyr);pyr2.push_back(img2_pyr);     &#125;   &#125;   chrono::steady_clock::time_point t2&#x3D;chrono::steady_clock::now();   auto time_used&#x3D;chrono::duration_cast&lt;chrono::duration&lt;double&gt;&gt;(t2-t1);   cout&lt;&lt;&quot;build pyramid time:&quot;&lt;&lt;time_used.count()&lt;&lt;endl;      &#x2F;&#x2F;计算光流时，先从顶层的图像开始计算   vector&lt;KeyPoint&gt;kp1_pyr,kp2_pyr;   for(auto &amp;kp:kp1)   &#123;     auto kp_top&#x3D;kp;     kp_top.pt *&#x3D;scales[pyramids-1];   &#x2F;&#x2F;顶层     kp1_pyr.push_back(kp_top);     kp2_pyr.push_back(kp_top);   &#125;      for(int level&#x3D;pyramids-1;level&gt;&#x3D;0;level--)   &#123;     success.clear();     t1&#x3D;chrono::steady_clock::now();     OpticalFlowSingleLevel(pyr1[level],pyr2[level],kp1_pyr,kp2_pyr,success,inverse,true);     t2&#x3D;chrono::steady_clock::now();     auto time_used&#x3D;chrono::duration_cast&lt;chrono::duration&lt;double&gt;&gt;(t2-t1);     cout&lt;&lt;&quot;track pyr&quot;&lt;&lt;level&lt;&lt;&quot;cost time:&quot;&lt;&lt;time_used.count()&lt;&lt;endl;     if(level&gt;0)     &#123;for(auto &amp;kp:kp1_pyr)  kp.pt&#x2F;&#x3D;pyramid_scale;for(auto &amp;kp:kp2_pyr)  kp.pt&#x2F;&#x3D;pyramid_scale;     &#125;   &#125;   for(auto &amp;kp:kp2_pyr)     kp2.push_back(kp); &#125;<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p><a href="https://blog.csdn.net/weixin_51547017/article/details/115359316?utm_medium=distribute.pc_relevant.none-task-blog-2~default~baidujs_baidulandingword~default-0-115359316-blog-126934983.235%5Ev28%5Epc_relevant_recovery_v2&spm=1001.2101.3001.4242.1&utm_relevant_index=3">视觉SLAM第八讲视觉里程计2— LK光流—代码详细讲解</a></p><p><img src="/pic/%E9%80%89%E5%8C%BA_138.png" alt="运行结果"></p><p><img src="/pic/%E9%80%89%E5%8C%BA_137.png" alt="运行结果"></p><h3 id="ch8-x2F-direct-method-cpp"><a href="#ch8-x2F-direct-method-cpp" class="headerlink" title="ch8&#x2F;direct_method.cpp"></a>ch8&#x2F;direct_method.cpp</h3><p>在光流中，我们会首先追踪特征点的位置，再根据这些位置确定相机的运动。这样一种两步走的方案，很难保证全局最优。直接法通过在后一步中调整前一步的结果</p><p><a href="https://blog.csdn.net/pj18862486309/article/details/107829914?spm=1001.2101.3001.6650.6&utm_medium=distribute.pc_relevant.none-task-blog-2~default~BlogCommendFromBaidu~Rate-6-107829914-blog-126993782.235%5Ev28%5Epc_relevant_recovery_v2&depth_1-utm_source=distribute.pc_relevant.none-task-blog-2~default~BlogCommendFromBaidu~Rate-6-107829914-blog-126993782.235%5Ev28%5Epc_relevant_recovery_v2&utm_relevant_index=10">视觉十四讲：第八讲_直接法</a></p><p><img src="/pic/%E9%80%89%E5%8C%BA_139.png" alt="直接法的讨论"></p><p>代码注释待更新~~~~</p>]]></content>
      
      
      <categories>
          
          <category> slam </category>
          
          <category> c++ </category>
          
      </categories>
      
      
        <tags>
            
            <tag> c++ </tag>
            
            <tag> slam </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>slam14讲源代码的记录（前五讲）</title>
      <link href="/2023/04/06/slam14/"/>
      <url>/2023/04/06/slam14/</url>
      
        <content type="html"><![CDATA[<h2 id="第二讲"><a href="#第二讲" class="headerlink" title="第二讲"></a>第二讲</h2><p>本讲主要是cmake的使用以及一些库的链接方式</p><pre class="line-numbers language-cmake" data-language="cmake"><code class="language-cmake"><span class="token comment">#声明cmake的最低版本</span><span class="token keyword">cmake_minimum_required</span><span class="token punctuation">(</span><span class="token property">VERSION</span> <span class="token number">2.8</span><span class="token punctuation">)</span><span class="token comment">#声明一个cmake工程</span><span class="token keyword">project</span><span class="token punctuation">(</span>HelloSLAM<span class="token punctuation">)</span><span class="token comment">#设置编译模式</span><span class="token keyword">set</span><span class="token punctuation">(</span><span class="token variable">CMAKE_BUILD_TYPE</span><span class="token string">"Debug"</span><span class="token punctuation">)</span><span class="token comment">#添加可执行程序</span><span class="token keyword">add_executable</span><span class="token punctuation">(</span>helloSLAM helloSLAM.cpp<span class="token punctuation">)</span><span class="token comment">#下一部分</span><span class="token comment">#添加库文件</span><span class="token keyword">add_library</span><span class="token punctuation">(</span>hello <span class="token namespace">STATIC</span> libHelloSLAM.cpp<span class="token punctuation">)</span>   <span class="token comment">#静态链接库,会生成.a文件,可默认不加STATIC或者SHARED</span><span class="token comment"># add_library(hello_shared SHARED libHelloSLAM.cpp)  #动态链接库，会生成.so文件</span><span class="token keyword">add_executable</span><span class="token punctuation">(</span>useHello useHello.cpp<span class="token punctuation">)</span><span class="token keyword">target_link_libraries</span><span class="token punctuation">(</span>useHello hello<span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p><img src="/pic/11.jpg"></p><p><img src="/pic/12.jpg"></p><p><a href="https://cmake.org/cmake/help/latest/genindex.html#">CMake指令查询网址</a></p><blockquote><p>补充</p></blockquote><p>添加头文件include_directories()可使用具体路径，例如：</p><p>#添加Eigen头文件<br>include_directories(“&#x2F;usr&#x2F;include&#x2F;eigen3”)</p><p>静态库<br>• 原理:在编译时将源代码复制到程序中，运行时不用库文件依旧可以运行。<br>• 优点:运行已有代码，运行时不用再用库;无需加载库，运行更快<br>• 缺点:占用更多的空间和磁盘;静态库升级，需要重新编译程序<br>共享库(常用)<br>• 原理:编译时仅仅是记录用哪一个库里面的哪一个符号，不复制相关代码<br>• 优点:不复制代码，占用空间小;多个程序可以同时调用一个库;升级方便，无需重新编译 • 缺点:程序运行需要加载库，耗费一定时间</p><table><thead><tr><th align="center">操作系统</th><th align="center">静态库</th><th align="center">共享库</th></tr></thead><tbody><tr><td align="center">Windows</td><td align="center">lib</td><td align="center">.dll</td></tr><tr><td align="center">Linux</td><td align="center">.a</td><td align="center">.so</td></tr><tr><td align="center">Mac OS</td><td align="center">.a</td><td align="center">dylib</td></tr></tbody></table><h2 id="第三讲"><a href="#第三讲" class="headerlink" title="第三讲"></a>第三讲</h2><p>第三讲主要是三维空间刚体运动</p><p><strong>Eigen库的使用</strong></p><h3 id="ch3-x2F-useEigen-x2F-eigenMatrix-cpp中的需要记住的"><a href="#ch3-x2F-useEigen-x2F-eigenMatrix-cpp中的需要记住的" class="headerlink" title="ch3&#x2F;useEigen&#x2F;eigenMatrix.cpp中的需要记住的"></a>ch3&#x2F;useEigen&#x2F;eigenMatrix.cpp中的需要记住的</h3><pre class="line-numbers language-c++" data-language="c++"><code class="language-c++">using namespace Eigen;Vector3d v_3d;      &#x2F;&#x2F;实际上是MAtrix&lt;double,3,1&gt;Matrix3d matrix_33&#x3D;Matrix3d::Zero();&#x2F;&#x2F; Eigen::Matrix&lt;double, 3, 3&gt;Matrix&lt;float,2,3&gt; matrix_23;&#x2F;&#x2F;可以直接这样输入数据matrix_23&lt;&lt;1,2,3,4,5,6;&#x2F;&#x2F;相乘不能混用，需要显式转换Matrix&lt;double,2,1&gt; result &#x3D;matrix_23.cast&lt;double&gt;()*v_3d;matrix_33.transpose()；&#x2F;&#x2F;转置matrix_33.trace()&#x2F;&#x2F;迹matrix_33.inverse() ;&#x2F;&#x2F;求逆matrix_33.determinant();&#x2F;&#x2F;求行列式&#x2F;&#x2F;实对称矩阵一定可以对角化SelfAdjointEigenSolver&lt;Matrix3d&gt; eigen_solver(matrix_33.transpose()*matrix_33);eigen_solver.eigenvalues()；&#x2F;&#x2F;特征值eigen_solver.eigenvectors()；&#x2F;&#x2F;特征向量&#x2F;&#x2F;求解 matrix_NN * x &#x3D; v_Nd 这个方程.QR分解x&#x3D;matrix_NN.colPivHouseholderQr().solve(v_Nd);<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h3 id="ch3-x2F-useGeometry-x2F-eigenGeometry-cpp中"><a href="#ch3-x2F-useGeometry-x2F-eigenGeometry-cpp中" class="headerlink" title="ch3&#x2F;useGeometry&#x2F;eigenGeometry.cpp中"></a>ch3&#x2F;useGeometry&#x2F;eigenGeometry.cpp中</h3><pre class="line-numbers language-c++" data-language="c++"><code class="language-c++">Matrix3d rotation_matrix&#x3D;Matrix3d::Identity();&#x2F;&#x2F;单位阵,旋转矩阵AngleAxisd rotation_vector(M_PI&#x2F;4,Vector3d(0,0,1));&#x2F;&#x2F;旋转向量，使用的AngleAxisd&#x2F;&#x2F;旋转角以及旋转轴cout &lt;&lt; &quot;rotation_vector&quot; &lt;&lt; &quot;angle is: &quot; &lt;&lt; rotation_vector.angle() * (180 &#x2F; M_PI)                                 &lt;&lt; &quot; axis is: &quot; &lt;&lt; rotation_vector.axis().transpose() &lt;&lt; endl;&#x2F;&#x2F;旋转向量变旋转矩阵rotation_matrix &#x3D; rotation_vector.toRotationMatrix();&#x2F;&#x2F;旋转矩阵变旋转向量rotation_vector.fromRotationMatrix(rotation_matrix);&#x2F;&#x2F; 欧拉角: 可以将旋转矩阵直接转换成欧拉角Vector3d euler_angles &#x3D; rotation_matrix.eulerAngles(2, 1, 0); &#x2F;&#x2F; ZYX顺序，即yaw-pitch-roll顺序&#x2F;&#x2F;欧式变换使用Isometry,这里注意需要使用头文件 #include&lt;Eigen&#x2F;Geometry&gt;  Isometry3d T &#x3D; Isometry3d::Identity();                &#x2F;&#x2F; 虽然称为3d，实质上是4＊4的矩阵  T.rotate(rotation_vector);                                     &#x2F;&#x2F; 按照rotation_vector进行旋转  T.pretranslate(Vector3d(1, 3, 4));                     &#x2F;&#x2F; 把平移向量设成(1,3,4)  cout &lt;&lt; &quot;Transform matrix &#x3D; \n&quot; &lt;&lt; T.matrix() &lt;&lt; endl;&#x2F;&#x2F;四元数:拥有一个实部，3个虚部，Quaterniond是（s(q0),q1,q2,q3）Quaterniond q&#x3D;Quaterniond(rotation_vector);cout &lt;&lt; &quot;quaternion from rotation vector &#x3D; &quot; &lt;&lt; q.coeffs().transpose()&lt;&lt; endl;   &#x2F;&#x2F; 请注意coeffs的顺序是(x,y,z,w),w为实部，前三者为虚部<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><blockquote><p>补充：pretranslate 是在旋转之前的坐标轴上进行的平移操作，而 translate 是在旋转之后，pretanslate 相当于左乘，translate 相当于右乘。<br><a href="https://www.guyuehome.com/36653">Eigen 中 pretanslate 和 translate 的区别</a></p></blockquote><p>###ch3&#x2F;examples&#x2F;plotTrajectory.cpp中需要注意的</p><p>使用ifstream流来读取文件</p><p>说明：</p><p>1.ifstream类的对象创建成功的时候会返回非空值，借此判断是否创建文件对象成功</p><p>2.ifstream有个函数eof()用来判断文件是否读到尾部,没读到尾部返回false，否则返回true。</p><p>若尾部有回车，那么最后一条记录会读取两次。</p><p>若尾部没有回车，那么最后一条记录只会读取一次</p><p>3.iftream的对象假设为fin，fin在读取数据的时候会根据你的输出对象来选择输出的方式。</p><p>例程</p><pre class="line-numbers language-c++" data-language="c++"><code class="language-c++">#include&lt;iostream&gt;#include&lt;fstream&gt;using namespace std;int main()&#123;ifstream fin(&quot;abc.txt&quot;);&#x2F;&#x2F;读取文件的名字，可以相对或绝对if(!fin) &#123;cout&lt;&lt;&quot;open fail.&quot;&lt;&lt;endl;exit(1);&#125;else&#123;while(!fin.eof())&#123;char a[20],b[20],c[20];fin&gt;&gt;a&gt;&gt;b&gt;&gt;c;&#x2F;&#x2F;读取的时候遇见空格才会跳跃。cout&lt;&lt;a&lt;&lt;&quot;&quot;&lt;&lt;b&lt;&lt;&quot;  &quot;&lt;&lt;c&lt;&lt;&quot;  &quot;&lt;&lt;endl;&#125;fin.close();&#125;&#125;<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>如果STL容器中的元素是Eigen库数据结构，例如这里定义一个vector容器，元素是Matrix4d ，如下所示：</p><blockquote><p>vector<a href="Eigen::Matrix4d">Eigen::Matrix4d</a></p></blockquote><p>这个错误也是和上述一样的提示，编译不会出错，只有在运行的时候出错。解决的方法很简单，定义改成下面的方式：</p><blockquote><p>vector&lt;Eigen::Matrix4d,Eigen::aligned_allocator<a href="Eigen::Matrix4d">Eigen::Matrix4d</a>&gt;;</p></blockquote><p>其实上述的这段代码才是标准的定义容器方法，只是我们一般情况下定义容器的元素都是C++中的类型，所以可以省略，这是因为在C++11标准中，aligned_allocator管理C++中的各种数据类型的内存方法是一样的，可以不需要着重写出来。但是在Eigen管理内存和C++11中的方法是不一样的，所以需要单独强调元素的内存分配和管理。</p><p>对于轨迹文件的处理过程如下：</p><pre class="line-numbers language-c++" data-language="c++"><code class="language-c++">while (!fin.eof()) &#123;  double time, tx, ty, tz, qx, qy, qz, qw;  fin &gt;&gt; time &gt;&gt; tx &gt;&gt; ty &gt;&gt; tz &gt;&gt; qx &gt;&gt; qy &gt;&gt; qz &gt;&gt; qw;  Isometry3d Twr(Quaterniond(qw, qx, qy, qz)); &#x2F;&#x2F; 对比旋转向量的T.rotate(rotation_vector);            &#x2F;&#x2F; 按照rotation_vector进行旋转  Twr.pretranslate(Vector3d(tx, ty, tz));  poses.push_back(Twr);&#125;<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p><img src="/pic/%E9%80%89%E5%8C%BA_132.png" alt="运动轨迹"></p><p><a href="https://blog.csdn.net/weixin_41756645/article/details/122958689">旋转矩阵、变换矩阵、欧式变换</a></p><h2 id="第四讲"><a href="#第四讲" class="headerlink" title="第四讲"></a>第四讲</h2><p>主要是李群以及李代数，Sophus的使用</p><h3 id="ch4-x2F-useSophus-cpp中"><a href="#ch4-x2F-useSophus-cpp中" class="headerlink" title="ch4&#x2F;useSophus.cpp中"></a>ch4&#x2F;useSophus.cpp中</h3><pre class="line-numbers language-c++" data-language="c++"><code class="language-c++">&#x2F;&#x2F;李群的表示  &#x2F;&#x2F; 沿Z轴转90度的旋转矩阵  Matrix3d R &#x3D; AngleAxisd(M_PI &#x2F; 2, Vector3d(0, 0, 1)).toRotationMatrix();  &#x2F;&#x2F; 或者四元数  Quaterniond q(R);  Sophus::SO3d SO3_R(R);              &#x2F;&#x2F; Sophus::SO3d可以直接从旋转矩阵构造  Sophus::SO3d SO3_q(q);              &#x2F;&#x2F; 也可以通过四元数构造    &#x2F;&#x2F; 对SE(3)操作大同小异  Vector3d t(1, 0, 0);           &#x2F;&#x2F; 沿X轴平移1  Sophus::SE3d SE3_Rt(R, t);           &#x2F;&#x2F; 从R,t构造SE(3)  Sophus::SE3d SE3_qt(q, t);            &#x2F;&#x2F; 从q,t构造SE(3)  &#x2F;&#x2F;通过.matrix()查看李群的矩阵   &#x2F;&#x2F;通过对数映射转化为李代数，这里的log直接把李群转化为李代数，不是反对称矩阵  Vector3d so3 &#x3D; SO3_R.log();  cout &lt;&lt; &quot;so3 &#x3D; &quot; &lt;&lt; so3.transpose() &lt;&lt; endl;  &#x2F;&#x2F; hat 为向量到反对称矩阵  cout &lt;&lt; &quot;so3 hat&#x3D;\n&quot; &lt;&lt; Sophus::SO3d::hat(so3) &lt;&lt; endl;    &#x2F;&#x2F; 李代数se(3) 是一个六维向量，方便起见先typedef一下  typedef Eigen::Matrix&lt;double, 6, 1&gt; Vector6d;  Vector6d se3 &#x3D; SE3_Rt.log();  cout &lt;&lt; &quot;se3 &#x3D; &quot; &lt;&lt; se3.transpose() &lt;&lt; endl;  &#x2F;&#x2F; 观察输出，会发现在Sophus中，se(3)的平移在前，旋转在后.需要注意  &#x2F;&#x2F; 同样的，有hat和vee两个算符  cout &lt;&lt; &quot;se3 hat &#x3D; \n&quot; &lt;&lt; Sophus::SE3d::hat(se3) &lt;&lt; endl;  &#x2F;&#x2F; 增量扰动模型的更新，李代数求导    Vector3d update_so3(1e-4, 0, 0); &#x2F;&#x2F;假设更新量为这么多  Sophus::SO3d SO3_updated &#x3D; Sophus::SO3d::exp(update_so3) * SO3_R;  cout &lt;&lt; &quot;SO3 updated &#x3D; \n&quot; &lt;&lt; SO3_updated.matrix() &lt;&lt; endl;    Vector6d update_se3; &#x2F;&#x2F;更新量,这里不是太懂  update_se3.setZero();  update_se3(0, 0) &#x3D; 1e-4;  Sophus::SE3d SE3_updated &#x3D; Sophus::SE3d::exp(update_se3) * SE3_Rt;  cout &lt;&lt; &quot;SE3 updated &#x3D; &quot; &lt;&lt; endl &lt;&lt; SE3_updated.matrix() &lt;&lt; endl;<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p><a href="https://xsin.gitee.io/2019/01/14/SlSophus%E5%87%BD%E6%95%B0%E7%9A%84%E4%BD%BF%E7%94%A8/">Slam中Sophus函数的使用</a></p><h3 id="ch4-x2F-example-x2F-trajectoryError-cpp中比较真实轨迹与估计轨迹之间的误差"><a href="#ch4-x2F-example-x2F-trajectoryError-cpp中比较真实轨迹与估计轨迹之间的误差" class="headerlink" title="ch4&#x2F;example&#x2F;trajectoryError.cpp中比较真实轨迹与估计轨迹之间的误差"></a>ch4&#x2F;example&#x2F;trajectoryError.cpp中比较真实轨迹与估计轨迹之间的误差</h3><pre class="line-numbers language-c++" data-language="c++"><code class="language-c++">&#x2F;&#x2F;vector（动态数组）类型的TrajectoryType存储位姿typedef vector&lt;Sophus::SE3d, Eigen::aligned_allocator&lt;Sophus::SE3d&gt;&gt; TrajectoryType;&#x2F;&#x2F;通过下面这样读取至李群中  while (!fin.eof()) &#123;    double time, tx, ty, tz, qx, qy, qz, qw;    fin &gt;&gt; time &gt;&gt; tx &gt;&gt; ty &gt;&gt; tz &gt;&gt; qx &gt;&gt; qy &gt;&gt; qz &gt;&gt; qw;    Sophus::SE3d p1(Eigen::Quaterniond(qw, qx, qy, qz), Eigen::Vector3d(tx, ty, tz));    trajectory.push_back(p1);  &#125;  &#x2F;&#x2F;轨迹误差的计算方法如下    double rmse &#x3D; 0;  for (size_t i &#x3D; 0; i &lt; estimated.size(); i++) &#123;    Sophus::SE3d p1 &#x3D; estimated[i], p2 &#x3D; groundtruth[i];    double error &#x3D; (p2.inverse() * p1).log().norm();    rmse +&#x3D; error * error;  &#125;  rmse &#x3D; rmse &#x2F; double(estimated.size());  rmse &#x3D; sqrt(rmse);  cout &lt;&lt; &quot;RMSE &#x3D; &quot; &lt;&lt; rmse &lt;&lt; endl;<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p><img src="/pic/%E9%80%89%E5%8C%BA_129.png" alt="轨迹相差图"></p><p><a href="https://blog.csdn.net/qq_28087491/article/details/113540037">Pangolin可视化绘图库的使用</a></p><h2 id="第五讲"><a href="#第五讲" class="headerlink" title="第五讲"></a>第五讲</h2><p>本讲主要是相机与图像,内参外参畸变参数等，Opencv使用以及摄像头标定等</p><h3 id="ch5-x2F-imageBasics-x2F-imageBasics-cpp中操作Opencv图像中需要注意的"><a href="#ch5-x2F-imageBasics-x2F-imageBasics-cpp中操作Opencv图像中需要注意的" class="headerlink" title="ch5&#x2F;imageBasics&#x2F;imageBasics.cpp中操作Opencv图像中需要注意的"></a>ch5&#x2F;imageBasics&#x2F;imageBasics.cpp中操作Opencv图像中需要注意的</h3><pre class="line-numbers language-c++" data-language="c++"><code class="language-c++">  image &#x3D; cv::imread(argv[1]); &#x2F;&#x2F;cv::imread函数读取指定路径下的图像  &#x2F;&#x2F; 文件顺利读取, 首先输出一些基本信息  cout &lt;&lt; &quot;图像宽为&quot; &lt;&lt; image.cols &lt;&lt; &quot;,高为&quot; &lt;&lt; image.rows &lt;&lt; &quot;,通道数为&quot; &lt;&lt; image.channels() &lt;&lt; endl;  cv::imshow(&quot;image&quot;, image);      &#x2F;&#x2F; 用cv::imshow显示图像  cv::waitKey(0);                  &#x2F;&#x2F; 暂停程序,等待一个按键输入  &#x2F;&#x2F; 使用 std::chrono 来给算法计时  chrono::steady_clock::time_point t1 &#x3D; chrono::steady_clock::now();    sleep(1);    chrono::steady_clock::time_point t2 &#x3D; chrono::steady_clock::now();  chrono::duration&lt;double&gt; time_used &#x3D; chrono::duration_cast &lt; chrono::duration &lt; double &gt;&gt; (t2 - t1);  cout &lt;&lt; &quot;遍历图像用时：&quot; &lt;&lt; time_used.count() &lt;&lt; &quot; 秒。&quot; &lt;&lt; endl;&#x2F;&#x2F;time_point表示一个时间点，用来获取1970.1.1以来的秒数和当前的时间。time_point必须要clock来计时，time_point有一个函数time_since_epoch()用来获得1970年1月1日到time_point时间经过的duration&#x2F;&#x2F;时钟间隔Duration&#x2F;&#x2F; 遍历图像, 请注意以下遍历方式亦可使用于随机像素访问for (size_t y &#x3D; 0; y &lt; image.rows; y++) &#123;    &#x2F;&#x2F; 用cv::Mat::ptr获得图像的行指针    unsigned char *row_ptr &#x3D; image.ptr&lt;unsigned char&gt;(y);  &#x2F;&#x2F; row_ptr是第y行的头指针    for (size_t x &#x3D; 0; x &lt; image.cols; x++) &#123;      &#x2F;&#x2F; 访问位于 x,y 处的像素      unsigned char *data_ptr &#x3D; &amp;row_ptr[x * image.channels()]; &#x2F;&#x2F; data_ptr 指向待访问的像素数据      &#x2F;&#x2F; 输出该像素的每个通道,如果是灰度图就只有一个通道      for (int c &#x3D; 0; c !&#x3D; image.channels(); c++) &#123;        unsigned char data &#x3D; data_ptr[c]; &#x2F;&#x2F; data为I(x,y)第c个通道的值      &#125;    &#125;  &#125;    &#x2F;&#x2F; 关于 cv::Mat 的拷贝  &#x2F;&#x2F; 直接赋值并不会拷贝数据  cv::Mat image_another &#x3D; image;  &#x2F;&#x2F; 修改 image_another 会导致 image 发生变化  image_another(cv::Rect(0, 0, 100, 100)).setTo(0); &#x2F;&#x2F; 将左上角100*100的块置零  cv::imshow(&quot;image&quot;, image);  cv::waitKey(0);  &#x2F;&#x2F; 使用clone函数来拷贝数据  cv::Mat image_clone &#x3D; image.clone();  image_clone(cv::Rect(0, 0, 100, 100)).setTo(255);  cv::imshow(&quot;image&quot;, image);  cv::imshow(&quot;image_clone&quot;, image_clone);  cv::waitKey(0);<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><blockquote><p>补充</p></blockquote><p>CMakeLists.txt文件代码</p><pre class="line-numbers language-CMake" data-language="CMake"><code class="language-CMake">cmake_minimum_required( VERSION 2.8 )project(imageBasics) set( CMAKE_CXX_FLAGS &quot;-std&#x3D;c++11 -O3&quot;)find_package(OpenCV)include_directories($&#123;OpenCV&#125;)add_executable(imageBasics imageBasics.cpp)# 链接OpenCV库target_link_libraries(imageBasics $&#123;OpenCV_LIBS&#125;)add_executable(undistortImage undistortImage.cpp)# 链接OpenCV库target_link_libraries(undistortImage $&#123;OpenCV_LIBS&#125;)<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p><a href="https://blog.csdn.net/qq_43428547/article/details/88956911">二维数组与指针(详解)</a></p><p>数组名与指针的关系</p><pre class="line-numbers language-c++" data-language="c++"><code class="language-c++">a;&#x2F;&#x2F;代表数组首行地址，一般用a[0][0]的地址表示&amp;a;&#x2F;&#x2F;代表整个数组的地址，一般用a[0][0]地址表示a[i];代表了第i行起始元素的地址(网上说是代表了第i行的地址，但我觉得不是，在讲数组与指针的关系时我会验证给大家看)&amp;a[i];代表了第i行的地址，一般用a[i][0]的地址表示a[i]+j;&#x2F;&#x2F;代表了第i行第j个元素地址,a[i]就是j&#x3D;&#x3D;0的情况a[i][j];&#x2F;&#x2F;代表了第i行第j个元素&amp;a[i][j];&#x2F;&#x2F;代表了第i行第j个元素的地址*a;&#x2F;&#x2F;代表数组a首元素地址也就是a[0]或者&amp;a[0][0]*(a+i);&#x2F;&#x2F;代表了第i行首元素的地址,*a是i&#x3D;0的情况*(a+i)+j;&#x2F;&#x2F;代表了第i行j个元素的地址**a;&#x2F;&#x2F;代表a的首元素的值也就是a[0][0]*(*(a+i)+j);&#x2F;&#x2F;代表了第i行第j个元素<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p><a href="https://blog.csdn.net/u010420283/article/details/111946293?utm_medium=distribute.pc_relevant.none-task-blog-2~default~baidujs_baidulandingword~default-1-111946293-blog-121178263.235%5Ev27%5Epc_relevant_default_base1&spm=1001.2101.3001.4242.2&utm_relevant_index=4">c++中几种记时函数实例</a></p><p>几种及时函数如下：</p><pre class="line-numbers language-c++" data-language="c++"><code class="language-c++">&#x2F;&#x2F;方法1,标准库#include &lt;sys&#x2F;time.h&gt; struct timeval tv;gettimeofday(&amp;tv,NULL);auto b1&#x3D;(unsigned long long)tv.tv_sec*1000+(unsigned long long)tv.tv_usec&#x2F;1000;sleep(1);gettimeofday(&amp;tv,NULL);auto b2&#x3D;(unsigned long long)tv.tv_sec*1000+(unsigned long long)tv.tv_usec&#x2F;1000;cout&lt;&lt;&quot;method 1, cost time is:&quot;&lt;&lt;(b2-b1)&lt;&lt;endl;  &#x2F;&#x2F;方法2，chrono#include&lt;chrono&gt;using namespace std::chrono; auto t1&#x3D; duration_cast&lt;milliseconds&gt;(steady_clock::now().time_since_epoch()).count();sleep(1);auto t2&#x3D;duration_cast&lt;milliseconds&gt;(steady_clock::now().time_since_epoch()).count();cout&lt;&lt;&quot;method 2, cost time is:&quot;&lt;&lt;(t2-t1)&lt;&lt;endl; &#x2F;&#x2F;方法2副本，该代码只能求两段时间差，不能得到打印出当前时刻。auto begin&#x3D;system_clock::now();sleep(1);auto end&#x3D;system_clock::now();cout&lt;&lt;&quot;method 2-1, cost time is:&quot;&lt;&lt;duration_cast&lt;milliseconds&gt;(end - begin).count()&lt;&lt;endl;   &#x2F;&#x2F;方法3#include &lt;unistd.h&gt;clock_t c1&#x3D;clock();sleep(1);clock_t c2&#x3D;clock();cout&lt;&lt;&quot;method 3, cost time is:&quot;&lt;&lt;(c2-c1)&lt;&lt;endl; <span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h3 id="ch5-x2F-imageBasics-x2F-undistortImage-cpp中图像去畸变"><a href="#ch5-x2F-imageBasics-x2F-undistortImage-cpp中图像去畸变" class="headerlink" title="ch5&#x2F;imageBasics&#x2F;undistortImage.cpp中图像去畸变"></a>ch5&#x2F;imageBasics&#x2F;undistortImage.cpp中图像去畸变</h3><p><img src="/pic/%E9%80%89%E5%8C%BA_102.png" alt="去畸变公式"></p><pre class="line-numbers language-c++" data-language="c++"><code class="language-c++">&#x2F;&#x2F;主要知道内参以及去畸变参数  &#x2F;&#x2F; 畸变参数  double k1 &#x3D; -0.28340811, k2 &#x3D; 0.07395907, p1 &#x3D; 0.00019359, p2 &#x3D; 1.76187114e-05;  &#x2F;&#x2F; 内参  double fx &#x3D; 458.654, fy &#x3D; 457.296, cx &#x3D; 367.215, cy &#x3D; 248.375;  &#x2F;&#x2F; 计算去畸变后图像的内容  for (int v &#x3D; 0; v &lt; rows; v++) &#123;    for (int u &#x3D; 0; u &lt; cols; u++) &#123;      &#x2F;&#x2F; 按照公式，计算点(u,v)对应到畸变图像中的坐标(u_distorted, v_distorted)      double x &#x3D; (u - cx) &#x2F; fx, y &#x3D; (v - cy) &#x2F; fy;      double r &#x3D; sqrt(x * x + y * y);      double x_distorted &#x3D; x * (1 + k1 * r * r + k2 * r * r * r * r) + 2 * p1 * x * y + p2 * (r * r + 2 * x * x);      double y_distorted &#x3D; y * (1 + k1 * r * r + k2 * r * r * r * r) + p1 * (r * r + 2 * y * y) + 2 * p2 * x * y;      double u_distorted &#x3D; fx * x_distorted + cx;      double v_distorted &#x3D; fy * y_distorted + cy;      &#x2F;&#x2F; 赋值 (最近邻插值)      if (u_distorted &gt;&#x3D; 0 &amp;&amp; v_distorted &gt;&#x3D; 0 &amp;&amp; u_distorted &lt; cols &amp;&amp; v_distorted &lt; rows) &#123;        image_undistort.at&lt;uchar&gt;(v, u) &#x3D; image.at&lt;uchar&gt;((int) v_distorted, (int) u_distorted);      &#125; else &#123;        image_undistort.at&lt;uchar&gt;(v, u) &#x3D; 0;      &#125;    &#125;  &#125;<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p><strong>Opencv提供了去畸变函数cv::Undistort().</strong></p><ul><li><p>函数功能：直接对图像进行畸变矫正。</p></li><li><p>其内部调用了initUndistortRectifyMap和remap函数。</p></li></ul><pre class="line-numbers language-c++" data-language="c++"><code class="language-c++">CV_EXPORTS_W void undistort( InputArray src, &#x2F;&#x2F;原始图像                            OutputArray dst,&#x2F;&#x2F;矫正图像                             InputArray cameraMatrix,&#x2F;&#x2F;原相机的内参矩阵                             InputArray distCoeffs,&#x2F;&#x2F;相机矫正参数                             InputArray newCameraMatrix &#x3D; noArray() );&#x2F;&#x2F;新相机内参矩阵<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>直接使用Opencv中的函数主要是对参数写成矩阵形式加进去就可以了</p><pre class="line-numbers language-c++" data-language="c++"><code class="language-c++">#include &lt;opencv2&#x2F;opencv.hpp&gt;#include &lt;string&gt;using namespace std;int main(int argc, char **argv) &#123;    const cv::Mat K &#x3D; ( cv::Mat_&lt;double&gt; ( 3,3 ) &lt;&lt; 458.654, 0.0, 367.215, 0.0, 457.296, 248.375, 0.0, 0.0, 1.0 ); &#x2F;&#x2F; 相机内参矩阵    &#x2F;&#x2F;k1 &#x3D; -0.28340811, k2 &#x3D; 0.07395907, p1 &#x3D; 0.00019359, p2 &#x3D; 1.76187114e-05;    const cv::Mat D &#x3D; (cv::Mat_&lt;double&gt; ( 5,1 ) &lt;&lt;  -0.28340811, 0.07395907, 0.0, 0.00019359, 1.76187114e-05);    &#x2F;&#x2F;double k1 &#x3D; -0.28340811, k2 &#x3D; 0.07395907, k3&#x3D;0,p1 &#x3D; 0.00019359, p2 &#x3D; 1.76187114e-05;    const string str &#x3D; &quot;&#x2F;home&#x2F;chy&#x2F;slambook2&#x2F;ch5&#x2F;imageBasics&#x2F;distorted.png&quot;;  cv::Mat image &#x3D; cv::imread(str, 0);   &#x2F;&#x2F; 图像是灰度图，CV_8UC1  int rows &#x3D; image.rows, cols &#x3D; image.cols;  cv::Mat image_undistort &#x3D; cv::Mat(rows, cols, CV_8UC1);   &#x2F;&#x2F; 去畸变以后的图  cv::undistort(image,image_undistort,K,D,K);  &#x2F;&#x2F; 画图去畸变后图像  cv::imshow(&quot;distorted&quot;, image);  cv::imshow(&quot;undistorted&quot;, image_undistort);  cv::waitKey();  return 0;&#125;<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p><a href="https://blog.csdn.net/cannonfodder9527/article/details/129002265">SLAM14讲第5讲去畸变方法改进</a></p><p><strong>使用 getOptimalNewCameraMatrix + initUndistortRectifyMap + remap 矫正图像</strong></p><p><a href="https://blog.csdn.net/qq_18894441/article/details/122983176">关于OpenCV中的去畸变 c++</a></p><h3 id="ch5-x2F-stereo-x2F-stereoVision-cpp中双目视觉"><a href="#ch5-x2F-stereo-x2F-stereoVision-cpp中双目视觉" class="headerlink" title="ch5&#x2F;stereo&#x2F;stereoVision.cpp中双目视觉"></a>ch5&#x2F;stereo&#x2F;stereoVision.cpp中双目视觉</h3><p><img src="/pic/%E9%80%89%E5%8C%BA_091.png" alt="双目视觉求深度z"></p><p>使用SGBM算法计算左右图像的视差</p><pre class="line-numbers language-c++" data-language="c++"><code class="language-c++">cv::Ptr&lt;cv::StereoSGBM&gt; sgbm &#x3D; cv::StereoSGBM::create(       0, 96, 9, 8 * 9 * 9, 32 * 9 * 9, 1, 63, 10, 100, 32);    &#x2F;&#x2F; sgbm经典参数配置   cv::Mat disparity_sgbm, disparity;   sgbm-&gt;compute(left, right, disparity_sgbm);&#x2F;&#x2F;输入前面两张图，第三个参数是输出   disparity_sgbm.convertTo(disparity, CV_32F, 1.0 &#x2F; 16.0f);  &#x2F;&#x2F;将disparity_sgbm变成32F类型的disparity，这里的disparity才是视差图。  如果Mat类型数据的深度不满足上面的要求，则需要使用convertTo()函数来进行转换。convertTo()函数负责转换数据类型不同的Mat       for (int v &#x3D; 0; v &lt; left.rows; v++)       for (int u &#x3D; 0; u &lt; left.cols; u++) &#123;           if (disparity.at&lt;float&gt;(v, u) &lt;&#x3D; 0.0 || disparity.at&lt;float&gt;(v, u) &gt;&#x3D; 96.0) continue;           Vector4d point(0, 0, 0, left.at&lt;uchar&gt;(v, u) &#x2F; 255.0); &#x2F;&#x2F; 前三维为xyz,第四维为颜色           &#x2F;&#x2F; 根据双目模型计算 point 的位置           double x &#x3D; (u - cx) &#x2F; fx;           double y &#x3D; (v - cy) &#x2F; fy;           double depth &#x3D; fx * b &#x2F; (disparity.at&lt;float&gt;(v, u));           point[0] &#x3D; x * depth;           point[1] &#x3D; y * depth;           point[2] &#x3D; depth;           pointcloud.push_back(point);       &#125;<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p><strong>sgbm参数解释：</strong></p><pre class="line-numbers language-c++" data-language="c++"><code class="language-c++">cv::Ptr&lt;cv::StereoSGBM&gt; sgbm &#x3D; cv::StereoSGBM::create(    0, 96, 9, 8 * 9 * 9, 32 * 9 * 9, 1, 63, 10, 100, 32);    &#x2F;&#x2F; sgbm经典参数配置<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><p><img src="/pic/%E9%80%89%E5%8C%BA_092.png"></p><p><strong>convertTo函数</strong></p><p> 用于计算距离的视差图（CV_32F）和用于肉眼看的视差图（CV_8U）使用的格式不同，并且用于计算的视差图无需进行裁剪和归一化，这些只是为了显示的可读性和美观。所以，在对sgbm进行compute之后得到视差图disparity_sgbm，除以16得到用于计算的视差图disparity（除以16是因为每个像素值由一个16bit表示，其中低位的4位存储的是视差值得小数部分，所以真实视差值应该是该值除以16</p><p><a href="https://blog.csdn.net/weixin_70026476/article/details/127351340">参考:实践部分双目视觉代码讲解</a></p><p>这部分Pangolin的讲解也可以参考上面的链接</p><p><img src="/pic/%E9%80%89%E5%8C%BA_127.png" alt="深度图"></p><p><img src="/pic/%E9%80%89%E5%8C%BA_128.png" alt="点云图"></p><h3 id="ch5-x2F-rgbd-x2F-joinMap-cpp中"><a href="#ch5-x2F-rgbd-x2F-joinMap-cpp中" class="headerlink" title="ch5&#x2F;rgbd&#x2F;joinMap.cpp中"></a>ch5&#x2F;rgbd&#x2F;joinMap.cpp中</h3><p><a href="https://blog.csdn.net/zhiwei121/article/details/95033924">可参考</a></p><p>已知相机内外参，五张RGB图以及他们的深度信息计算任何一个像素的世界坐标系下的位置从而建立一个点云地图</p><pre class="line-numbers language-c++" data-language="c++"><code class="language-c++">&#x2F;&#x2F;起个别名，方便后面使用typedef vector&lt;Sophus::SE3d, Eigen::aligned_allocator&lt;Sophus::SE3d&gt;&gt; TrajectoryType;typedef Eigen::Matrix&lt;double, 6, 1&gt; Vector6d;&#x2F;&#x2F;通过fmt占位符读取图像，具体看下面fmt讲解链接，要先include &lt;boost&#x2F;format.hpp&gt;这个头文件。boost::format fmt(&quot;.&#x2F;%s&#x2F;%d.%s&quot;); &#x2F;&#x2F;图像文件格式colorImgs.push_back(cv::imread((fmt % &quot;color&quot; % (i + 1) % &quot;png&quot;).str()));depthImgs.push_back(cv::imread((fmt % &quot;depth&quot; % (i + 1) % &quot;pgm&quot;).str(), -1)); &#x2F;&#x2F; 使用-1读取原始图像&#x2F;&#x2F; for 在C++11的新特性,具体看下面链接讲解double data[7] &#x3D; &#123;0&#125;;for (auto &amp;d:data)     fin &gt;&gt; d; &#x2F;&#x2F; 计算点云并拼接    &#x2F;&#x2F; 相机内参     double cx &#x3D; 325.5;    double cy &#x3D; 253.5;    double fx &#x3D; 518.0;    double fy &#x3D; 519.0;    double depthScale &#x3D; 1000.0;    vector&lt;Vector6d, Eigen::aligned_allocator&lt;Vector6d&gt;&gt; pointcloud;&#x2F;&#x2F; reserve为容器预先分配内存空间，并未初始化空间元素    pointcloud.reserve(1000000);    for (int i &#x3D; 0; i &lt; 5; i++) &#123;        cout &lt;&lt; &quot;转换图像中: &quot; &lt;&lt; i + 1 &lt;&lt; endl;        cv::Mat color &#x3D; colorImgs[i];        cv::Mat depth &#x3D; depthImgs[i];        Sophus::SE3d T &#x3D; poses[i];        for (int v &#x3D; 0; v &lt; color.rows; v++)            for (int u &#x3D; 0; u &lt; color.cols; u++) &#123;                          &#x2F;*通过用Mat中的ptr模板函数 返回一个unsigned short类型的指针。v表示行 根据内部计算返回data头指针 + 偏移量来计算v行的头指针             * 图像为单通道的   depth.ptr&lt;unsigned short&gt; ( v ) 来获取行指针*&#x2F;                unsigned int d &#x3D; depth.ptr&lt;unsigned short&gt;(v)[u]; &#x2F;&#x2F; 深度值                if (d &#x3D;&#x3D; 0) continue; &#x2F;&#x2F; 为0表示没有测量到                Eigen::Vector3d point;                point[2] &#x3D; double(d) &#x2F; depthScale;&#x2F;&#x2F;实际尺度的一个缩放因子                point[0] &#x3D; (u - cx) * point[2] &#x2F; fx;                point[1] &#x3D; (v - cy) * point[2] &#x2F; fy;                Eigen::Vector3d pointWorld &#x3D; T * point;&#x2F;&#x2F;将相机坐标系转换为世界坐标系                Vector6d p;                &#x2F;&#x2F;head&lt;n&gt;()函数是对于Eigen库中的向量类型而言的，表示提取前n个元素                &#x2F;&#x2F;方法一                p.head&lt;3&gt;() &#x3D; pointWorld;                p[5] &#x3D; color.data[v * color.step + u * color.channels()];   &#x2F;&#x2F; blue                p[4] &#x3D; color.data[v * color.step + u * color.channels() + 1]; &#x2F;&#x2F; green                p[3] &#x3D; color.data[v * color.step + u * color.channels() + 2]; &#x2F;&#x2F; red                &#x2F;&#x2F;方法二：                &#x2F;&#x2F;程序上方读取了一张图片color                    p[5]&#x3D;color.at&lt;cv::Vec3b&gt;(v, u)[0];  &#x2F;&#x2F;B                     p[4]&#x3D;color.at&lt;cv::Vec3b&gt;(v, u)[1];  &#x2F;&#x2F;G                    p[3]&#x3D;color.at&lt;cv::Vec3b&gt;(v, u)[2];  &#x2F;&#x2F;R                                pointcloud.push_back(p);            &#125;    &#125;vector&lt;Vector6d, Eigen::aligned_allocator&lt;Vector6d&gt;&gt; pointcloud;pointcloud.reserve(1000000);<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p><a href="https://blog.csdn.net/hhl317/article/details/118937838">c++中的boost::format,fmt使用方法</a></p><p><a href="https://blog.csdn.net/try_again_later/article/details/81566850">boost::format 以及 for 新特性</a></p><p><a href="https://blog.csdn.net/CxC2333/article/details/107735638?spm=1001.2101.3001.6650.3&utm_medium=distribute.pc_relevant.none-task-blog-2~default~CTRLIST~Rate-3-107735638-blog-107092078.235%5Ev28%5Epc_relevant_recovery_v2&depth_1-utm_source=distribute.pc_relevant.none-task-blog-2~default~CTRLIST~Rate-3-107735638-blog-107092078.235%5Ev28%5Epc_relevant_recovery_v2&utm_relevant_index=6">Opencv关于成员函数data，step，at的使用</a></p><pre class="line-numbers language-c++" data-language="c++"><code class="language-c++">&#x2F;*                                            备注： 3通道的图像的遍历方式总结 * 对于单通道来说 每个像素占8位 3通道则是每个矩阵元素是一个Vec3b 即一个三维的向量 向量内部元素为8位数的unsigned char类型 * 1、使用at遍历图像 * for(v)row *  for(u)col *      image.at&lt;Vec3b&gt;（v,u）[0] 表示第一个通道的像素的值 *      image.at&lt;Vec3b&gt;(v,u)[1] *      image.at&lt;Vec3b&gt;(v,u)[2] * 2、使用迭代器方式 (实际上就是一个指针指向了 cv::Mat矩阵元素) * cv::MatIterator_&lt;Vec3b&gt;begin,end; * for( begin &#x3D; image.begin&lt;Vec3b&gt;(), end &#x3D; image.end&lt;Vec3b&gt;() ; begin !&#x3D; end;  ) *      (*begin)[0] &#x3D; ... *      (*begin)[1] &#x3D; ... *      (*begin)[2] &#x3D; ... * * 3、用指针的方式操作 * for(v) *  for(u) *      image.ptr&lt;Vec3b&gt;(v)[u][0] 表示第一个通道 *      image.ptr&lt;Vec3b&gt;(v)[u][0] 表示第二通道 *              . *              . *              . * *&#x2F;<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p><img src="/pic/%E9%80%89%E5%8C%BA_126.png" alt="点云图"></p>]]></content>
      
      
      <categories>
          
          <category> slam </category>
          
          <category> c++ </category>
          
      </categories>
      
      
        <tags>
            
            <tag> c++ </tag>
            
            <tag> slam </tag>
            
            <tag> CMake </tag>
            
            <tag> Sophus </tag>
            
            <tag> Eigen </tag>
            
            <tag> Pangolin </tag>
            
            <tag> 图像去畸变 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>orb_slam2改进代码的相关汇总</title>
      <link href="/2023/04/05/orb-slam2/"/>
      <url>/2023/04/05/orb-slam2/</url>
      
        <content type="html"><![CDATA[<h1 id="orb-slam2改进代码的相关汇总"><a href="#orb-slam2改进代码的相关汇总" class="headerlink" title="orb_slam2改进代码的相关汇总"></a>orb_slam2改进代码的相关汇总</h1><h2 id="点线特征相关"><a href="#点线特征相关" class="headerlink" title="点线特征相关"></a>点线特征相关</h2><p><strong>添加了线特征。从3D密集SLAM进行表面重建的增量3D线段提取</strong></p><p><a href="https://github.com/atlas-jj/ORB_Line_SLAM">Add line feature based ORB-SLAM2</a></p><p><strong>RGB-D模式下添加了点线融合</strong></p><p><a href="https://github.com/maxee1900/RGBD-PL-SLAM">RGBD-SLAM with Point and Line Features, developed based on ORB_SLAM2</a></p><p><strong>单目线特征</strong></p><p><a href="https://github.com/lanyouzibetty/ORB-SLAM2_with_line">ORB-SLAM2_with_line, Monocular ORB-SLAM with Line Features</a></p><p><strong>双目点线融合,在弱纹理环境中传统点特征方法失效的情况下拥有较高的运行鲁棒性</strong></p><p><a href="https://github.com/rubengooj/pl-slam">PL-SLAM: a Stereo SLAM System through the Combination of Points and Line Segments</a></p><h2 id="用新的特征点替代ORB"><a href="#用新的特征点替代ORB" class="headerlink" title="用新的特征点替代ORB"></a>用新的特征点替代ORB</h2><p><strong>使用了一种更好的特征选择方法</strong></p><p><a href="https://github.com/ivalab/gf_orb_slam2">GF-ORB-SLAM2, Real-Time SLAM for Monocular, Stereo and RGB-D Cameras, with Loop Detectionand Relocalization</a></p><p><strong>用SuperPoint 替代ORB来进行特征提取</strong></p><p><a href="https://github.com/KinglittleQ/SuperPoint_SLAM">SuperPoint-SLAM</a></p><p><strong>设计的GCNv2具有与ORB功能相同的描述符格式,可以替代关键点提取,精度更高,适合在嵌入式低功耗平台上运行</strong></p><p><a href="https://github.com/jiexiong2016/GCNv2_SLAM">GCNv2: Efficient Correspondence Prediction for Real-Time SLAM</a></p><h2 id="直接法代替特征点法"><a href="#直接法代替特征点法" class="headerlink" title="直接法代替特征点法"></a>直接法代替特征点法</h2><p><strong>使用SVO中直接法来跟踪代替耗时的特征点提取匹配,在保持同样精度的情况下,是原始ORB-SLAM2速度的3倍</strong></p><p><a href="https://github.com/gaoxiang12/ORB-YGZ-SLAM">ORB-YGZ-SLAM, average 3x speed up and keep almost same accuracy v.s. ORB-SLAM2, use directtracking in SVO to accelerate the feature matching</a></p><h2 id="融合其他传感器"><a href="#融合其他传感器" class="headerlink" title="融合其他传感器"></a>融合其他传感器</h2><p><strong>双目VIO版本,加入了LK光流和滑动窗口BA优化</strong></p><p><a href="https://github.com/gaoxiang12/ygz-stereo-inertial">YGZ-stereo-inertial SLAM, LK optical flow + sliding window bundle adjustment</a></p><p><strong>京胖实现的VI-ORB-SLAM2</strong></p><p><a href="https://github.com/jingpang/LearnVIORB">VIORB, An implementation of Visual Inertial ORBSLAM based on ORB-SLAM2</a></p><p><strong>支持鱼眼,不需要rectify和裁剪输入图</strong></p><p><a href="https://github.com/lsyads/fisheye-ORB-SLAM">Fisheye-ORB-SLAM, A real-time robust monocular visual SLAM system based on ORB-SLAM for fisheye cameras, without rectifying or cropping the input images</a></p><h2 id="地图相关"><a href="#地图相关" class="headerlink" title="地图相关"></a>地图相关</h2><p><strong>添加保存和导入地图功能</strong></p><p><a href="https://github.com/AlejandroSilvestri/osmap">Osmap, Save and load orb-slam2 maps</a></p><p><a href="https://github.com/Jiankai-Sun/ORB_SLAM2_Enhanced">ORB_SLAM2 with map load&#x2F;save function</a></p><p><strong>添加了地图可视化</strong></p><p><a href="https://github.com/AlejandroSilvestri/Osmap-viewer">Viewer for maps from ORB-SLAM2 Osmap</a></p><p><strong>高翔实现的添加稠密点云地图</strong></p><p><a href="https://github.com/gaoxiang12/ORBSLAM2_with_pointcloud_map">ORBSLAM2_with_pointcloud_map</a></p><p><strong>在高翔基础上添加了稠密闭环地图</strong></p><p><a href="https://github.com/tiantiandabaojian/ORB-SLAM2_RGBD_DENSE_MAP">ORB-SLAM2_RGBD_DENSE_MAP, modified from Xiang Gao’s “ORB_SLAM2_modified”. It is added a dense loopclosing map model</a></p><h2 id="动态环境"><a href="#动态环境" class="headerlink" title="动态环境"></a>动态环境</h2><p><strong>适合动态环境,增加了动态物体检测和背景修复的能力</strong></p><p><a href="https://github.com/BertaBescos/DynaSLAM">DynaSLAM, is a SLAM system robust in dynamic environments for monocular, stereo and RGB-Dsetups(Mask-Rcnn)</a></p><p><a href="https://github.com/bijustin/YOLO-DynaSLAM">YOLO版本的DynaSLAM</a></p><p><strong>使用语义分割网络和运动一致性检查的方法（光流法）相结合的方法减少视觉SLAM中动态物体造成的影响</strong></p><p><a href="https://github.com/ivipsourcecode/DS-SLAM">DS-SLAM: A Semantic Visual SLAM towards Dynamic Environments</a></p><p><strong>基于语义的实时动态vSLAM算法RDS-SLAM，跟踪线程不再需要等待语义结果</strong></p><p><a href="https://github.com/yubaoliu/RDS-SLAM">RDS-SLAM: Real-Time Dynamic SLAM Using Semantic Segmentation Methods</a></p><p><strong>一种用于动态环境下资源受限机器人的实时RGB-D惯性里程计系统-Dynamic-VINS</strong></p><p><a href="https://github.com/HITSZ-NRSL/Dynamic-VINS">RGB-D Inertial Odometry for a Resource-Restricted Robot in Dynamic Environments</a></p><p><em><strong>动态slam汇总大全</strong></em></p><p><a href="https://github.com/oceanechy/Awesome_Dynamic_SLAM">动态slam汇总大全</a></p><h2 id="语义相关"><a href="#语义相关" class="headerlink" title="语义相关"></a>语义相关</h2><p><strong>动态语义SLAM 目标检测+VSLAM+光流&#x2F;多视角几何动态物体检测+octomap地图+目标数据库</strong></p><p><a href="https://github.com/Ewenwan/ORB_SLAM2_SSD_Semantic">ORB_SLAM2_SSD_Semantic, 动态语义SLAM 目标检测+VSLAM+光流&#x2F;多视角几何动态物体检测+octomap地图+目标数据库</a></p><p><strong>用YOLO v3的语义信息来增加跟踪性能</strong></p><p><a href="https://github.com/Eralien/TE-ORB_SLAM2">TE-ORB_SLAM2, Tracking Enhanced ORB-SLAM2</a></p><p><strong>通过手持RGB-D相机进行SLAM,ORB-SLAM2作为后端,用PSPNet做语义预测并将语义融入octomap</strong></p><p><a href="https://github.com/floatlazer/semantic_slam">Semantic SLAM,Real time semantic slam in ROS with a hand held RGB-D camera orb-slam2 with semantic labelling</a></p><p><strong>用深度学习的场景理解来增强传统特征检测方法,基于贝叶斯SegNet 和ORB-SLAM2,用于长时间定位</strong></p><p><a href="https://github.com/navganti/SIVO">SIVO: Semantically Informed Visual Odometry and Mapping</a></p><blockquote><p>参考计算机视觉life内容，仅供学习使用</p></blockquote>]]></content>
      
      
      <categories>
          
          <category> slam </category>
          
      </categories>
      
      
        <tags>
            
            <tag> slam </tag>
            
            <tag> orb_slam2 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>slam14简单总结捋思路</title>
      <link href="/2023/04/05/slam14-2/"/>
      <url>/2023/04/05/slam14-2/</url>
      
        <content type="html"><![CDATA[<p><img src="/pic/%E9%80%89%E5%8C%BA_093.png"></p><p><img src="/pic/%E9%80%89%E5%8C%BA_094.png"></p><p><img src="/pic/%E9%80%89%E5%8C%BA_095.png"></p><p>实际操作 图像去畸变 双目模型生成点云 RGB-D生成点云</p><p><img src="/pic/%E9%80%89%E5%8C%BA_096.png"></p><p><img src="/pic/%E9%80%89%E5%8C%BA_097.png"></p><p><img src="/pic/%E9%80%89%E5%8C%BA_098.png"></p><p><img src="/pic/%E9%80%89%E5%8C%BA_099.png"></p><p><img src="/pic/%E9%80%89%E5%8C%BA_100.png"></p><p><img src="/pic/%E9%80%89%E5%8C%BA_101.png"></p>]]></content>
      
      
      <categories>
          
          <category> slam </category>
          
          <category> c++ </category>
          
      </categories>
      
      
        <tags>
            
            <tag> c++ </tag>
            
            <tag> slam </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Opencv系列（五）</title>
      <link href="/2023/04/04/opencv5/"/>
      <url>/2023/04/04/opencv5/</url>
      
        <content type="html"><![CDATA[<h2 id="3D相关HEF矩阵计算"><a href="#3D相关HEF矩阵计算" class="headerlink" title="3D相关HEF矩阵计算"></a>3D相关HEF矩阵计算</h2><h3 id="F矩阵含义"><a href="#F矩阵含义" class="headerlink" title="F矩阵含义"></a>F矩阵含义</h3><ul><li>相邻两帧一组对应像素点的约束关系</li><li>像素点在下一帧极线上的位置</li></ul><p><img src="/pic/%E9%80%89%E5%8C%BA_056.png" alt="运动示意图"></p><p><img src="/pic/%E9%80%89%E5%8C%BA_057.png" alt="约束关系"></p><p><img src="/pic/%E9%80%89%E5%8C%BA_058.png" alt="极线位置"></p><p><img src="/pic/%E9%80%89%E5%8C%BA_059.png" alt="八点法前的归一化"></p><p><img src="/pic/%E9%80%89%E5%8C%BA_060.png" alt="八点法的讨论"></p><p><img src="/pic/%E9%80%89%E5%8C%BA_061.png" alt="最小二乘求解(≥8点)"></p><p><img src="/pic/%E9%80%89%E5%8C%BA_062.png" alt="最小二乘求解的讨论"></p><p><img src="/pic/%E9%80%89%E5%8C%BA_063.png" alt="无敌的RANSAC(≥8点)"></p><p><img src="/pic/%E9%80%89%E5%8C%BA_064.png" alt="无敌的RANSAC的讨论"></p><p><img src="/pic/%E9%80%89%E5%8C%BA_065.png" alt="无敌的RANSAC的讨论"></p><p><img src="/pic/%E9%80%89%E5%8C%BA_066.png" alt="无敌的RANSAC的讨论"></p><p><img src="/pic/%E9%80%89%E5%8C%BA_067.png" alt="Opencv中的函数"></p><h3 id="E矩阵含义"><a href="#E矩阵含义" class="headerlink" title="E矩阵含义"></a>E矩阵含义</h3><p>本质矩阵是归一化平面下的基本矩阵的特殊形式</p><p><img src="/pic/%E9%80%89%E5%8C%BA_068.png" alt="约束关系"></p><p><img src="/pic/%E9%80%89%E5%8C%BA_069.png" alt="具体求解方法(不展开)"></p><p><img src="/pic/%E9%80%89%E5%8C%BA_070.png" alt="关于E矩阵的讨论"></p><p><img src="/pic/%E9%80%89%E5%8C%BA_071.png" alt="Opencv中的函数"></p><p><img src="/pic/%E9%80%89%E5%8C%BA_072.png" alt="根据Essential Matrix恢复位姿信息R与t)"></p><p><img src="/pic/%E9%80%89%E5%8C%BA_073.png" alt="根据Essential Matrix恢复位姿信息R与t)"></p><p><img src="/pic/%E9%80%89%E5%8C%BA_074.png" alt="Opencv中的函数"></p><p><img src="/pic/%E9%80%89%E5%8C%BA_075.png" alt="根据E估计深度(三角化)"></p><h3 id="H矩阵意义"><a href="#H矩阵意义" class="headerlink" title="H矩阵意义"></a>H矩阵意义</h3><p><img src="/pic/%E9%80%89%E5%8C%BA_076.png"></p><p><img src="/pic/%E9%80%89%E5%8C%BA_077.png" alt="Opencv中的函数"></p><blockquote><p>注：以上截图内容参考计算机视觉life</p></blockquote><h2 id="位姿求取"><a href="#位姿求取" class="headerlink" title="位姿求取"></a>位姿求取</h2><h3 id="求取位姿的方法"><a href="#求取位姿的方法" class="headerlink" title="求取位姿的方法"></a>求取位姿的方法</h3><ol><li>对极约束：2D-2D，通过二维图像点的对应关系，恢复出在两帧之间摄像机的运动。</li><li>PnP：3D-2D,求解3D到2D点对运动的方法。描述了当知道n个3D空间点及其投影位置时，如何估计相机的位姿。</li><li>ICP：3D-3D，配对好的3D点,已知世界参考系下的3D点和相机参考系下的3D点</li></ol><h3 id="PnP概念"><a href="#PnP概念" class="headerlink" title="PnP概念"></a>PnP概念</h3><p>如果场景的三维结构已知，利用多个控制点在三维场景中的坐标及其在图像中的透视投影坐标即可<br>求解出相机坐标系与世界坐标系之间的绝对位姿关系，包括绝对平移向量t以及旋转矩阵R，该类求<br>解方法统称为N点透视位姿求解（Perspective-N-Point，PNP问题）。这里的控制点是指准确知道三<br>维空间坐标位置，同时也知道对应图像平面坐标的点。<br><strong>已知条件：</strong></p><ul><li>n3D参考点(3D reference points)坐标; </li><li>与这n个3D点对应的、投影在图像上的2D参考点(2D reference points)坐标; </li><li>相机的的内参K;</li></ul><p><strong>求解：</strong></p><ul><li>相机的位姿。</li></ul><h3 id="PnP应用场景"><a href="#PnP应用场景" class="headerlink" title="PnP应用场景"></a>PnP应用场景</h3><p><strong>场景主要有两个</strong></p><ol><li>求解相机的位姿，一般应用于AR，人脸跟踪等；<br>通常输入的是物体在世界坐标系下的3D点以及这些3D点在图像上投影的2D点，因此求得的是相机（相机坐标系）相对于真实物体（世界坐标系）的位姿</li></ol><p><img src="/pic/%E9%80%89%E5%8C%BA_078.png"></p><ol start="2"><li>求取前一帧到当前帧的相机位姿变化，一般用于slam中；</li></ol><p>通常输入的是上一帧中的3D点（在上一帧的相机坐标系下表示的点）和这些3D点在当前帧中的投影得到的2D点，所以它求得的是当前帧相对于上一帧的位姿变换</p><p><img src="/pic/%E9%80%89%E5%8C%BA_079.png"></p><h3 id="Opencv中的函数"><a href="#Opencv中的函数" class="headerlink" title="Opencv中的函数"></a>Opencv中的函数</h3><p><strong>solvePnP</strong></p><p><img src="/pic/%E9%80%89%E5%8C%BA_080.png"></p><p>flags有如下的求解方法：</p><pre class="line-numbers language-c++" data-language="c++"><code class="language-c++">enum &#123; SOLVEPNP_ITERATIVE &#x3D; 0,       SOLVEPNP_EPNP      &#x3D; 1, &#x2F;&#x2F;!&lt; EPnP: Efficient Perspective-n-Point Camera Pose Estimation @cite lepetit2009epnp       SOLVEPNP_P3P       &#x3D; 2, &#x2F;&#x2F;!&lt; Complete Solution Classification for the Perspective-Three-Point Problem @cite gao2003complete       SOLVEPNP_DLS       &#x3D; 3, &#x2F;&#x2F;!&lt; A Direct Least-Squares (DLS) Method for PnP  @cite hesch2011direct       SOLVEPNP_UPNP      &#x3D; 4, &#x2F;&#x2F;!&lt; Exhaustive Linearization for Robust Camera Pose and Focal Length Estimation @cite penate2013exhaustive       SOLVEPNP_AP3P      &#x3D; 5, &#x2F;&#x2F;!&lt; An Efficient Algebraic Solution to the Perspective-Three-Point Problem @cite Ke17       SOLVEPNP_MAX_COUNT      &#x2F;&#x2F;!&lt; Used for count&#125;;<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><blockquote><p>提示</p></blockquote><ol><li>SOLVEPNP_ITERATIVE，默认值，它通过迭代求出重投影误差最小的解作为问题的最优解。该方法<br>实质是迭代求出重投影误差最小的解，这个解显然不一定是正解。</li></ol><p><strong>使用范围：</strong>4对共面匹配点，小于4组点不准确</p><ol start="2"><li>SOLVEPNP_P3P，是使用非常经典的Gao的P3P问题求解算法。P3P算法要求输入的控制点个数只能是4对。3对点求出4组可能的解，通过对第4对点进行重投影，返回重投影误差最小的，确定最优解。可以使用任意4对匹配点求解，不要共面，特征点数量不为4时报错.</li></ol><p><strong>使用范围：</strong>4对不共面匹配点</p><ol start="3"><li>SOLVEPNP_EPNP，该方法使用EfficientPNP方法，求解问题EPnP使用大于等于3组点，是目前最有效的PnP求解方法。</li></ol><p><strong>使用范围：</strong>最少需要4对不共面的匹配点（对于共面的情况只需要3对）,点太少能运行，但不准确</p><p><strong>注：</strong>方法 SOLVEPNP_DLS 和 SOLVEPNP_UPNP 不能使用，因为当前的实现是不稳定的，有时会给出完全错误的结果。如果传递这两个标志之一，则将使用 SOLVEPNP_EPNP 方法。<br>建议：小于4组匹配点时，用SOLVEPNP_ITERATIVE；在4组匹配点时，用SOLVEPNP_P3P；大于4组匹配点时，用SOLVEPNP_EPNP。</p><p><strong>solvePnPRansac</strong></p><p>solvePnP的一个缺点是对异常值不够鲁棒，当我们用相机定位真实世界的点，可能存在错配，对误<br>匹配进行Ransac过滤，RANSAC是“Random Sample Consensus（随机抽样一致）”的缩写</p><p><img src="/pic/%E9%80%89%E5%8C%BA_081.png"></p><p><a href="https://docs.opencv.org/4.0.1/d9/d0c/group__calib3d.html#ga549c2075fac14829ff4a58bc931c033d">可查看官方文档</a></p><h2 id="扩展-PnP问题求解原理"><a href="#扩展-PnP问题求解原理" class="headerlink" title="扩展-PnP问题求解原理"></a>扩展-PnP问题求解原理</h2><p>目前主要有直接线性变换（DLT），P3P，EPnP，UPnP以及非线性优化方法。</p><p>关于PnP的求解问题</p><p><img src="/pic/%E9%80%89%E5%8C%BA_082.png"></p><p>把（3）式带入（1）式和（2）式，整理得：</p><p><img src="/pic/%E9%80%89%E5%8C%BA_083.png"></p><p>我们可以看出，fx fy u0 v0是相机内参，上一节中已经求出，Xw Yw x y是一组3D&#x2F;2D点的坐标，所以未知数有R11 R12 R13 R21 R22 R23 R31 R32 R33 T1 T2 T3一共12个，由于旋转矩阵是正交矩阵，每行每列都是单位向量且两两正交，所以R的自由度为3，秩也是3，比如知道R11 R12 R21就能求出剩下的Rxx。加上平移向量的3个未知数，一共6个未知数，而每一组2D&#x2F;3D点提供的x y Xw Yw Zw可以确立两个方程，所以3组2D&#x2F;3D点的坐标能确立6个方程从而解出6个未知数。故PnP需要知道至少3组2D&#x2F;3D点。</p><h3 id="DLT"><a href="#DLT" class="headerlink" title="DLT"></a>DLT</h3><p>通过2D-3D的关系直接构建方程，直接求解：<br>DLT主要是通过构建一个12维的增广矩阵（R|t），然后通过投影矩阵构建一个方程：</p><p>通过最后一行，消去s，最后构建一个12维的线性方程组，通过6对匹配点（一对点两个方程）来求解中间的矩阵。 最后将[R|t]左侧3<em>3矩阵块进行QR分解，用一个旋转矩阵去近似（将3</em>3矩阵空间投影到SE(3)流形上）。</p><p><img src="/pic/%E9%80%89%E5%8C%BA_085.png"></p><p>优点：求解简单<br>缺点：直接将 T 矩阵看成了 12 个未知数，忽略了它们之间的联系。因为旋转矩阵 R ∈ SO(3)，用 DLT 求出的解不一定满足该约束，它是一个一般矩阵。寻找一个最好的旋转矩阵对它进行近似</p><h3 id="P3P（ICP-3D-3D）"><a href="#P3P（ICP-3D-3D）" class="headerlink" title="P3P（ICP:3D-3D）"></a>P3P（ICP:3D-3D）</h3><p>采用一种变换的形式，把2D-3D匹配关系，变换成3D-3D点的匹配关系：<br>将世界坐标系下的ABC三点和图像坐标系下的abc三点匹配，其中AB，BC，AC的⻓度已知，<br>&lt;a,b&gt;，&lt;b,c&gt;，&lt;a,c&gt;也是已知，通过余弦定理得出方程组</p><p><img src="/pic/%E9%80%89%E5%8C%BA_086.png"></p><p>类似于分解 E 的情况，该方程最多可能得到四个解，但我们可以用验证点来计算最可能的解，得到<br>A, B, C 在相机坐标系下的 3D 坐标。然后，根据 3D-3D 的点对，使用类似ICP的坐标系对，计算相<br>机的运动 R, t。</p><p>优点：需要的匹配点少</p><p>缺点：P3P 只利用三个点的信息。当给定的配对点多于 3 组时，难以利用更多的信息；如果 3D 点<br>或 2D 点受噪声影响，或者存在误匹配，则算法失效。</p><p>可参考论文《<a href="https://cmp.felk.cvut.cz/~zimmerk/track3D/papers/p3p.pdf">CompleteSolution Classification for the Perspective-Three-Point Problem</a>》</p><h3 id="EPnP"><a href="#EPnP" class="headerlink" title="EPnP"></a>EPnP</h3><p>将世界坐标系中的3D坐标表示为一组虚拟的控制点的加权和。</p><p><img src="/pic/%E9%80%89%E5%8C%BA_087.png"></p><p>可参考论文《<a href="https://github.com/cvlab-epfl/EPnP">EPnP: Efficient Perspective-n-Point Camera Pose Estimation</a>》</p>]]></content>
      
      
      <categories>
          
          <category> Opencv </category>
          
          <category> PnP </category>
          
          <category> HEF </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Opencv </tag>
            
            <tag> HEF矩阵 </tag>
            
            <tag> PnP </tag>
            
            <tag> EPnP </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Opencv系列（四）</title>
      <link href="/2023/04/03/opencv4/"/>
      <url>/2023/04/03/opencv4/</url>
      
        <content type="html"><![CDATA[<h1 id="Opencv相机标定"><a href="#Opencv相机标定" class="headerlink" title="Opencv相机标定"></a>Opencv相机标定</h1><p>基本理论知识请查看如下链接：</p><p><a href="https://zhuanlan.zhihu.com/p/520357612">相机模型</a></p><h2 id="张正友标定法"><a href="#张正友标定法" class="headerlink" title="张正友标定法"></a>张正友标定法</h2><p><img src="/pic/%E9%80%89%E5%8C%BA_055.png"></p><p><strong>张正友标定法假设</strong></p><ul><li>标定板的角点在一个平面上</li><li>世界坐标系的xy平面在标定板平面上，Z&#x3D;0</li><li>相机模型不考虑畸变</li></ul><h2 id="Opencv现有函数"><a href="#Opencv现有函数" class="headerlink" title="Opencv现有函数"></a>Opencv现有函数</h2> <pre class="line-numbers language-c++" data-language="c++"><code class="language-c++">cv::calibrateCamera(object_points, &#x2F;&#x2F;三维点坐标                        image_points_seq, &#x2F;&#x2F;像素坐标                        image_size, &#x2F;&#x2F;图像尺寸                        cameraMatrix, &#x2F;&#x2F;输出相机矩阵                        distCoeffs, &#x2F;&#x2F;输出畸变矩阵                        rvecsMat, tvecsMat, &#x2F;&#x2F;r,t                        0);cv::findChessboardCorners(imageInput,&#x2F;&#x2F;输入图像                          board_size,&#x2F;&#x2F;标定板上角点的行列                          image_points_buf&#x2F;&#x2F;输出角点的像素坐标                         )cv::cornerSubPix(view_gray,&#x2F;&#x2F;输入图像，最好是灰度图                 image_points_buf,&#x2F;&#x2F;输入输出角点的像素坐标                 cv::Size(5,5),&#x2F;&#x2F;搜索窗口的半径                 cv::Size(-1,-1),&#x2F;&#x2F;-1表示忽略                 cv::TermCriteria(cv::TermCriteria::MAX_ITER + cv::TermCriteria::EPS,30,&#x2F;&#x2F;最大迭代次数                    0.1)&#x2F;&#x2F;最小精度                );cv::drawChessboardCorners(view_gray, board_size, image_points_buf,                           true);&#x2F;&#x2F;如果角点全部找到，返回true<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p><a href="https://blog.csdn.net/dcrmg/article/details/52929669?ops_request_misc=&request_id=&biz_id=102&utm_term=boardsize%20%20opencv&utm_medium=distribute.pc_search_result.none-task-blog-2~all~sobaiduweb~default-4-.first_rank_v2_pc_rank_v29&spm=1018.2226.3001.4187">Opencv 张正友相机标定傻瓜教程</a></p>]]></content>
      
      
      <categories>
          
          <category> Opencv </category>
          
          <category> Camera </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Opencv </tag>
            
            <tag> 相机模型 </tag>
            
            <tag> 相机标定 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Opencv系列（三）</title>
      <link href="/2023/04/03/opencv3/"/>
      <url>/2023/04/03/opencv3/</url>
      
        <content type="html"><![CDATA[<h1 id="Opencv显示与绘制"><a href="#Opencv显示与绘制" class="headerlink" title="Opencv显示与绘制"></a>Opencv显示与绘制</h1><h2 id="OpenCV-的基础绘制功能"><a href="#OpenCV-的基础绘制功能" class="headerlink" title="OpenCV 的基础绘制功能"></a>OpenCV 的基础绘制功能</h2><p>OpenCV 中提供直线、椭圆、矩形、圆以及多边形的绘制功能。 在OpenCV 的图形绘制中我们会经常使用以下两种结构：</p><ul><li>cv::Point 和cv::Scalar</li></ul><blockquote><p>cv::Point 表示2D 平面上的点，通过指定其在图像上的坐标位置x 和y 来实现。<br>语法结构如下：<br>Point pt; pt.x &#x3D; 10； pt.y &#x3D; 8; 或者 Point pt &#x3D; Point(10, 8);</p></blockquote><blockquote><p>cv::Scalar 表示一个含有4 个元素的向量，OpenCV 中用来传递像素值。<br>语法结构如下：<br>Scalar( a, b, c )<br>其中Blue &#x3D; a, Green &#x3D; b, Red &#x3D; c。这里由于我们只有BGR 三个颜色值，所以我们只需要定义三个变量，最后一个元素可以省略。 </p></blockquote><p>下面开始介绍直线、椭圆、矩形、圆以及多边形分别的语法结构:</p><p><strong>直线</strong></p><pre class="line-numbers language-c++" data-language="c++"><code class="language-c++">CV_EXPORTS_W void line(InputOutputArray img, Point pt1, Point pt2, const Scalar&amp; color,                     int thickness &#x3D; 1, int lineType &#x3D; LINE_8, int shift &#x3D; 0);<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><p>参数：</p><ul><li>img:  图像 </li><li>pt:  线段的起点 </li><li>pt2:  线段的终点 </li><li>color:  直线的颜色 </li><li>thickness:  直线的粗细 </li><li>lineType:  直线类型，具体可以参考下表 </li><li>shift:  点坐标中的小数点位数</li></ul><p><img src="/pic/%E9%80%89%E5%8C%BA_047.png"></p><p><img src="/pic/%E9%80%89%E5%8C%BA_048.png"></p><p>8 联通线是指下一个点连接上一个点的边或者角，4 联通线是指下一个点与上一个点边相连。4 联通线消除了8 联通线的断裂瑕疵</p><p><strong>椭圆</strong></p><pre class="line-numbers language-c++" data-language="c++"><code class="language-c++">CV_EXPORTS_W void ellipse(InputOutputArray img, Point center, Size axes,                        double angle, double startAngle, double endAngle,                        const Scalar&amp; color, int thickness &#x3D; 1,                        int lineType &#x3D; LINE_8, int shift &#x3D; 0);<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span></span></code></pre><p>参数： </p><ul><li>img:  图像 </li><li>center:  椭圆中心 </li><li>axes:  椭圆主轴尺寸的一半 </li><li>angle:  椭圆旋转角度 </li><li>startAngle:  椭圆弧的起始角度 </li><li>engAngle:  椭圆弧的终止角度 </li><li>color:  椭圆的颜色 </li><li>thickness:  椭圆轮廓的粗细，如果不设置默认填充</li><li>lineType:  椭圆边界线类型 shift:  中心坐标和轴值的小数点位数</li></ul><p>cv::ellipse 可以用来绘制椭圆线、实心椭圆、椭圆弧、椭圆扇面。如果想要绘制一个完整的椭圆，startAngle&#x3D;0、endAngle&#x3D;360。如果起始角度大于终止角度，他们会进行交换。下图 在绘制蓝色弧时各个参数的含义。</p><p><img src="/pic/%E9%80%89%E5%8C%BA_049.png" alt="椭圆弧绘制中各参数的意义"></p><p><strong>矩形</strong></p><pre class="line-numbers language-c++" data-language="c++"><code class="language-c++">CV_EXPORTS_W void rectangle(InputOutputArray img, Point pt1, Point pt2,                          const Scalar&amp; color, int thickness &#x3D; 1,                          int lineType &#x3D; LINE_8, int shift &#x3D; 0);<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre><p>参数： </p><ul><li>img:  图像</li><li>pt1:  矩形的顶点 </li><li>pt2:与pt1 相对的矩形顶点 </li><li>color:  矩形颜色或者亮度 </li><li>thickness:  矩形轮廓的粗细 </li><li>lineType:  线条类型 </li><li>shift:  点坐标中的小数点位数</li></ul><p><strong>多线段</strong></p><pre class="line-numbers language-c++" data-language="c++"><code class="language-c++">CV_EXPORTS void polylines(Mat&amp; img, const Point* const* pts, const int* npts,                          int ncontours, bool isClosed, const Scalar&amp; color,                          int thickness &#x3D; 1, int lineType &#x3D; LINE_8, int shift &#x3D; 0 );<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre><p>参数： </p><ul><li>img:  图像 </li><li>pts:  多边形曲线阵列 </li><li>isClosed:  所绘制多段线是否闭合 </li><li>color:  多段线颜色 </li><li>thickness:  多段线粗细 </li><li>lineType:  多段线种类 </li><li>shift:  点坐标中的小数点位数</li></ul><p><strong>多边形填充</strong></p><pre class="line-numbers language-c++" data-language="c++"><code class="language-c++">CV_EXPORTS void fillPoly(Mat&amp; img, const Point** pts,                         const int* npts, int ncontours,                         const Scalar&amp; color, int lineType &#x3D; LINE_8, int shift &#x3D; 0,                         Point offset &#x3D; Point() );<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span></span></code></pre><p>cv::fillPoly 可以填充由多边形包围的区域，可用于填充复杂区域。 </p><p>参数： </p><ul><li>img:  图像 </li><li>pts:  多边形数组，每个多边形都是一组点 </li><li>npts:  顶点个数 </li><li>ncontours:  多边形轮廓个数 </li><li>color:  多边形颜色 </li><li>lineType:  多边形边界粗细 </li><li>shift:  点坐标的小数点位数 </li><li>offset:  可选择轮廓点偏移量</li></ul><p><strong>实例</strong></p><pre class="line-numbers language-c++" data-language="c++"><code class="language-c++">#include &lt;opencv2&#x2F;core.hpp&gt; #include &lt;opencv2&#x2F;imgproc.hpp&gt; #include &lt;opencv2&#x2F;highgui.hpp&gt;#define w 400 using namespace cv; void MyEllipse( Mat img, double angle ); void MyFilledCircle( Mat img, Point center ); void MyPolygon( Mat img ); void MyLine( Mat img, Point start, Point end ); int main( void )&#123;&#x2F;&#x2F;创建两个窗口和两个图像来进行图形绘制    char atom_window[] &#x3D; &quot;Drawing 1: Atom&quot;;     char rook_window[] &#x3D; &quot;Drawing 2: Rook&quot;;     Mat atom_image &#x3D; Mat::zeros( w, w, CV_8UC3 );     Mat rook_image &#x3D; Mat::zeros( w, w, CV_8UC3 ); &#x2F;&#x2F;原子图像绘制，创建了MyEllipse 和MyFilledCircle 两个函数     MyEllipse( atom_image, 90 );     MyEllipse( atom_image, 0 );     MyEllipse( atom_image, 45 );     MyEllipse( atom_image, -45 );     MyFilledCircle( atom_image, Point( w&#x2F;2, w&#x2F;2) );    &#x2F;&#x2F;绘制国际象棋的车创建了MyPolygon 和MyLine 函数，且应用了矩形的绘制函数     MyPolygon( rook_image );     rectangle( rook_image,                Point( 0, 7*w&#x2F;8 ),                Point( w, w),                Scalar( 0, 255, 255 ),                FILLED,                LINE_8 );     MyLine( rook_image, Point( 0, 15*w&#x2F;16 ), Point( w, 15*w&#x2F;16 ) );     MyLine( rook_image, Point( w&#x2F;4, 7*w&#x2F;8 ), Point( w&#x2F;4, w ) );     MyLine( rook_image, Point( w&#x2F;2, 7*w&#x2F;8 ), Point( w&#x2F;2, w ) );     MyLine( rook_image, Point( 3*w&#x2F;4, 7*w&#x2F;8 ), Point( 3*w&#x2F;4, w ) );     imshow( atom_window, atom_image );     moveWindow( atom_window, 0, 200 );     imshow( rook_window, rook_image );     moveWindow( rook_window, w, 200 );     waitKey( 0 );     return(0); &#125; void MyEllipse( Mat img, double angle ) &#123;     int thickness &#x3D; 2;     int lineType &#x3D; 8;     ellipse( img,              Point( w&#x2F;2, w&#x2F;2 ),              Size( w&#x2F;4, w&#x2F;16 ),              angle,              0,              360,              Scalar( 255, 0, 0 ),              thickness,              lineType ); &#125; void MyFilledCircle( Mat img, Point center ) &#123;     circle( img,             center,             w&#x2F;32,             Scalar( 0, 0, 255 ),             FILLED,             LINE_8 ); &#125; void MyPolygon( Mat img ) &#123;     int lineType &#x3D; LINE_8;     Point rook_points[1][20];     rook_points[0][0]  &#x3D; Point(    w&#x2F;4,   7*w&#x2F;8 );     rook_points[0][1]  &#x3D; Point(  3*w&#x2F;4,   7*w&#x2F;8 );     rook_points[0][2]  &#x3D; Point(  3*w&#x2F;4,  13*w&#x2F;16 );     rook_points[0][3]  &#x3D; Point( 11*w&#x2F;16, 13*w&#x2F;16 );     rook_points[0][4]  &#x3D; Point( 19*w&#x2F;32,  3*w&#x2F;8 );     rook_points[0][5]  &#x3D; Point(  3*w&#x2F;4,   3*w&#x2F;8 );     rook_points[0][6]  &#x3D; Point(  3*w&#x2F;4,     w&#x2F;8 );     rook_points[0][7]  &#x3D; Point( 26*w&#x2F;40,    w&#x2F;8 );     rook_points[0][8]  &#x3D; Point( 26*w&#x2F;40,    w&#x2F;4 );     rook_points[0][9]  &#x3D; Point( 22*w&#x2F;40,    w&#x2F;4 );     rook_points[0][10] &#x3D; Point( 22*w&#x2F;40,    w&#x2F;8 );     rook_points[0][11] &#x3D; Point( 18*w&#x2F;40,    w&#x2F;8 );     rook_points[0][12] &#x3D; Point( 18*w&#x2F;40,    w&#x2F;4 );     rook_points[0][13] &#x3D; Point( 14*w&#x2F;40,    w&#x2F;4 );     rook_points[0][14] &#x3D; Point( 14*w&#x2F;40,    w&#x2F;8 );     rook_points[0][15] &#x3D; Point(    w&#x2F;4,     w&#x2F;8 );     rook_points[0][16] &#x3D; Point(    w&#x2F;4,   3*w&#x2F;8 );     rook_points[0][17] &#x3D; Point( 13*w&#x2F;32,  3*w&#x2F;8 );     rook_points[0][18] &#x3D; Point(  5*w&#x2F;16, 13*w&#x2F;16 );     rook_points[0][19] &#x3D; Point(    w&#x2F;4,  13*w&#x2F;16 );     const Point* ppt[1] &#x3D; &#123; rook_points[0] &#125;;     int npt[] &#x3D; &#123; 20 &#125;;     fillPoly( img,               ppt,               npt,               1,               Scalar( 255, 255, 255 ),               lineType ); &#125; void MyLine( Mat img, Point start, Point end ) &#123;     int thickness &#x3D; 2;     int lineType &#x3D; LINE_8;     line( img,           start,           end,           Scalar( 0, 0, 0 ),           thickness,           lineType ); &#125;<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p><img src="/pic/%E9%80%89%E5%8C%BA_050.png" alt="绘制结果"></p><h2 id="轮廓"><a href="#轮廓" class="headerlink" title="轮廓"></a>轮廓</h2><h3 id="如何在图像中寻找物体轮廓"><a href="#如何在图像中寻找物体轮廓" class="headerlink" title="如何在图像中寻找物体轮廓"></a>如何在图像中寻找物体轮廓</h3><p><strong>在二值图像中寻找轮廓</strong></p><pre class="line-numbers language-c++" data-language="c++"><code class="language-c++">CV_EXPORTS_W void findContours( InputOutputArray image, OutputArrayOfArrays contours,                              OutputArray hierarchy, int mode,                              int method, Point offset &#x3D; Point());<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre><p>参数： </p><ul><li>image:  8 位单通道图像，非零像素视为 1，零值像素视为 0，因此所有的输入图像均被视为二值图像。因此在应用时我们一般会利用 Canny,  compare,  InRange,  threshold, adaptiveThreshold 等函数处理得到二值图像。如果模式为RETR_CCOMP 或者RETR_FLOODFILL，输入也可以是32 位的灰度图像。 </li><li>contours: 检测到的轮廓，每个轮廓都储存为一组点向量，定义为std::vector&lt;std::vector<a href="cv::Point">cv::Point</a>&gt;。 </li><li>hierarchy:  可选输出向量，定义为 std::vector<a href="cv::Vec4i">cv::Vec4i</a>，包含有图像拓扑信息。其内部元素的个数等同于所检测轮廓的个数。Vec 4i 是Vec &lt;int,  4&gt;的别名，定义为向量内每一个元素包含4 个int 型变量。hierarchy 向量内第i 个轮廓的4 个int型变量（hierarchy[i][0], hierarchy[i][1], hierarchy[i][2], hierarchy[i][3]）分别表示其同一层级下的后一个轮廓，前一个轮廓，子轮廓以及父轮廓的索引编号。如果轮廓 i 没有对应的上述轮廓，则 hierarchy[i][0],  hierarchy[i][1],  hierarchy[i][2],  hierarchy[i][3]被置为-1。 </li><li>mode:  轮廓的检索模式。具体见下表二。 </li><li>method:  轮廓的近似方法。具体见表三。</li><li>offset: Point 偏移量</li></ul><p><img src="/pic/%E9%80%89%E5%8C%BA_051.png"></p><p><strong>轮廓的绘制</strong></p><pre class="line-numbers language-c++" data-language="c++"><code class="language-c++">CV_EXPORTS_W void drawContours( InputOutputArray image, InputArrayOfArrays contours,                              int contourIdx, const Scalar&amp; color,                              int thickness &#x3D; 1, int lineType &#x3D; LINE_8,                              InputArray hierarchy &#x3D; noArray(),                              int maxLevel &#x3D; INT_MAX, Point offset &#x3D; Point() );<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>参数： </p><ul><li>image:  目的图像。 </li><li>contours:  所有输入轮廓，每个轮廓储存为一组点向量。 </li><li>contourIdx:  指示要绘制的轮廓线参数，如果为负，则绘制所有轮廓线。 </li><li>color:  轮廓线颜色。 </li><li>thickness:  绘制轮廓线的粗细，如果为负（thickness&#x3D;FILLED）,则轮廓内部也会被绘制。当thickness&#x3D;FILLED 时，即使没有提供层级数据，也能成功绘制有孔的轮廓。 </li><li>lineType:  直线绘制算法。 </li><li>hierarchy:  可选层级信息，只有当想绘制多条轮廓时才需要用到。 </li><li>mexLevel:  所绘制轮廓的最大层级。如果是0，则只绘制指定的轮廓；如果是1，则绘制指定轮廓以及嵌套轮廓；如果是2，则绘制指定轮廓、嵌套轮廓以及嵌套轮廓的嵌套轮廓以此类推。该参数只在层级参数可用时可以使用。 </li><li>offset: Point 的偏移</li></ul><p> <strong>实例</strong></p> <pre class="line-numbers language-c++" data-language="c++"><code class="language-c++">#include &quot;opencv2&#x2F;imgcodecs.hpp&quot; #include &quot;opencv2&#x2F;highgui.hpp&quot; #include &quot;opencv2&#x2F;imgproc.hpp&quot; #include &lt;iostream&gt; using namespace cv; using namespace std; Mat src_gray; int thresh &#x3D; 100; RNG rng(12345); void thresh_callback(int, void* ); int main( int argc, char** argv ) &#123;    &#x2F;&#x2F;读取图像     Mat src &#x3D; imread( &quot;1.jpg&quot;, IMREAD_COLOR );     if( src.empty() )     &#123;         cout &lt;&lt; &quot;Could not open or find the image!\n&quot; &lt;&lt; endl;         cout &lt;&lt; &quot;Usage: &quot; &lt;&lt; argv[0] &lt;&lt; &quot; &lt;Input image&gt;&quot; &lt;&lt; endl;         return -1;&#125;     &#x2F;&#x2F;转化为灰度图像，模糊处理以去除噪声     cvtColor( src, src_gray, COLOR_BGR2GRAY );     blur( src_gray, src_gray, Size(3,3) );     &#x2F;&#x2F;创建名为Source 的窗口并在其中显示输入图像     const char* source_window &#x3D; &quot;Source&quot;;     namedWindow( source_window );     imshow( source_window, src );     const int max_thresh &#x3D; 255;     createTrackbar( &quot;Canny thresh:&quot;, source_window, &amp;thresh, max_thresh, thresh_callback );     thresh_callback( 0, 0 );     waitKey();     return 0; &#125; void thresh_callback(int, void* ) &#123;     Mat canny_output;     &#x2F;&#x2F;Canny 边缘检测     Canny( src_gray, canny_output, thresh, thresh*2 );     vector&lt;vector&lt;Point&gt; &gt; contours;     vector&lt;Vec4i&gt; hierarchy;     &#x2F;&#x2F;寻找轮廓的函数     findContours( canny_output, contours, hierarchy, RETR_TREE, CHAIN_APPROX_SIMPLE );     Mat drawing &#x3D; Mat::zeros( canny_output.size(), CV_8UC3 );     for( size_t i &#x3D; 0; i&lt; contours.size(); i++ )     &#123;         Scalar color &#x3D; Scalar( rng.uniform(0, 256), rng.uniform(0,256), rng.uniform(0,256) );     &#x2F;&#x2F;轮廓绘制         drawContours( drawing, contours, (int)i, color, 2, LINE_8, hierarchy, 0 );     &#125;     imshow( &quot;Contours&quot;, drawing ); &#125;<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p><img src="/pic/403.gif"></p><h3 id="凸包函数"><a href="#凸包函数" class="headerlink" title="凸包函数"></a>凸包函数</h3><pre class="line-numbers language-c++" data-language="c++"><code class="language-c++">CV_EXPORTS_W void convexHull( InputArray points, OutputArray hull,                              bool clockwise &#x3D; false, bool returnPoints &#x3D; true );<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><p>参数： </p><ul><li>points:  输入的二维点集，存储在std::vector 或者Mat 中 </li><li>hull:  输出找到的凸包。 </li><li>clockwise:  操作方向，当 clockwise&#x3D;true 时，输出凸包为顺时针方向。否则输出凸包方向为逆时针方向。 </li><li>returnPoints:  操作标识符，默认值为 true，此时返回各凸包的各点，否则返回凸包各点的索引。当输出数组为std::vector 时，此标识被忽略</li></ul> <pre class="line-numbers language-c++" data-language="c++"><code class="language-c++">#include &quot;opencv2&#x2F;imgcodecs.hpp&quot; #include &quot;opencv2&#x2F;highgui.hpp&quot; #include &quot;opencv2&#x2F;imgproc.hpp&quot; #include &lt;iostream&gt; using namespace cv; using namespace std; Mat src_gray; int thresh &#x3D; 100; RNG rng(12345); void thresh_callback(int, void* ); int main( int argc, char** argv ) &#123;     Mat src &#x3D; imread( &quot;11.png&quot; , IMREAD_COLOR);     if( src.empty() )     &#123;         cout &lt;&lt; &quot;Could not open or find the image!\n&quot; &lt;&lt; endl;         cout &lt;&lt; &quot;Usage: &quot; &lt;&lt; argv[0] &lt;&lt; &quot; &lt;Input image&gt;&quot; &lt;&lt; endl;         return -1;     &#125;     cvtColor( src, src_gray, COLOR_BGR2GRAY );     blur( src_gray, src_gray, Size(3,3) );     const char* source_window &#x3D; &quot;Source&quot;;     namedWindow( source_window );     imshow( source_window, src );     const int max_thresh &#x3D; 255;     createTrackbar( &quot;Canny thresh:&quot;, source_window, &amp;thresh, max_thresh, thresh_callback );     thresh_callback( 0, 0 );     waitKey();     return 0; &#125; void thresh_callback(int, void* ) &#123;     Mat canny_output;     Canny( src_gray, canny_output, thresh, thresh*2 );     vector&lt;vector&lt;Point&gt; &gt; contours;     findContours( canny_output, contours, RETR_TREE, CHAIN_APPROX_SIMPLE );     &#x2F;&#x2F;利用凸包函数找到图形中的凸包     vector&lt;vector&lt;Point&gt; &gt;hull( contours.size() );     for( size_t i &#x3D; 0; i &lt; contours.size(); i++ )     &#123;         convexHull( contours[i], hull[i] );     &#125;     &#x2F;&#x2F;绘制轮廓与凸包     Mat drawing &#x3D; Mat::zeros( canny_output.size(), CV_8UC3 );     for( size_t i &#x3D; 0; i&lt; contours.size(); i++ )     &#123;         Scalar color &#x3D; Scalar( rng.uniform(0, 256), rng.uniform(0,256), rng.uniform(0,256) );         drawContours( drawing, contours, (int)i, color );         drawContours( drawing, hull, (int)i, color );     &#125;     imshow( &quot;Hull demo&quot;, drawing ); &#125;<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>]]></content>
      
      
      <categories>
          
          <category> Opencv </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Opencv </tag>
            
            <tag> 显示与绘制 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Opencv系列（二）</title>
      <link href="/2023/04/02/opencv2/"/>
      <url>/2023/04/02/opencv2/</url>
      
        <content type="html"><![CDATA[<h1 id="Opencv图像滤波"><a href="#Opencv图像滤波" class="headerlink" title="Opencv图像滤波"></a>Opencv图像滤波</h1><h2 id="连通域-amp-直方图"><a href="#连通域-amp-直方图" class="headerlink" title="连通域&amp;直方图"></a>连通域&amp;直方图</h2><p><img src="/pic/%E9%80%89%E5%8C%BA_042.png"></p><p><strong>计算图像直方图</strong></p><pre class="line-numbers language-c++" data-language="c++"><code class="language-c++">void cv::calcHist(       const Mat *     images,           &#x2F;&#x2F;输入图像    int     nimages,                  &#x2F;&#x2F;源图像的个数（通常为1）    const int *     channels,         &#x2F;&#x2F;列出通道    InputArray  mask,                 &#x2F;&#x2F;输入掩码（需处理的像素）    OutputArray     hist,             &#x2F;&#x2F;输出直方图    int     dims,                     &#x2F;&#x2F;直方图的维度（通道数量）    const int *     histSize,         &#x2F;&#x2F;每个维度位数    const float **  ranges,           &#x2F;&#x2F;每个维度的范围    bool    uniform &#x3D; true,           &#x2F;&#x2F;true表示箱子间距相同    bool    accumulate &#x3D; false        &#x2F;&#x2F;是否在多次调用时进行累积    )<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p><strong>二值函数</strong></p><p>cv::threshold()</p><blockquote><p>〖原理〗：通过将所有像素与某个阈值（第三个参数）进行比较赋值，将图像表示为只有两种像素值的图像（例子：学生排队）。</p></blockquote><p>阈值是图像分割的标尺。</p><pre class="line-numbers language-c++" data-language="c++"><code class="language-c++">#include&lt;opencv2&#x2F;imgproc.hpp&gt;&#x2F;&#x2F;创建二值图像double cv::threshold(       InputArray  src,          &#x2F;&#x2F;输入图像（多通道、8位或32位浮点类型）       OutputArray     dst,      &#x2F;&#x2F;输出图像    double  thresh,           &#x2F;&#x2F;指定阈值    double  maxval,           &#x2F;&#x2F;设定最大值（常取255）    int     type              &#x2F;&#x2F;阈值类型    )&#x2F;*type，详见参数介绍1、THRESH_BINARY：将所有大于thresh的像素赋值为maxval，将其他像素赋值为0；2、THRESH_BINARY_INY：将所有大于thresh的像素赋值为0，将其他像素赋值为maxval；3、THRESH_TRUNC：截断，将所有大于thresh的像素赋值为thresh，其他像素值不变；4、THRESH_TOZERO：所有大于thresh的像素值保持不变，将其他像素赋值为0；5、THRESH_TOZERO_INV：所有大于thresh的像素值赋值为0，其他像素值保持不变.6、THRESH_OTSU:使用Otsu算法去寻找到最优的阈值7、THRESH_TRIANGLE：使用三角化方法寻找到最有的阈值*&#x2F;<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><blockquote><p>【应用】：可用于区分图像的前景和背景（通常前景的像素值大于背景的像素值）；</p></blockquote><h2 id="形态学运算变换图像"><a href="#形态学运算变换图像" class="headerlink" title="形态学运算变换图像"></a>形态学运算变换图像</h2><p><strong>概念</strong></p><ul><li>形态学是一种滤波器，用结构元素探测图像中每个像素的操作过程称为形态学滤波器的应用过程；</li><li>结构元素是一堆像素的组合，原则上可以是任何形状，通常是正方形、圆形或菱形，中心点为原点（锚点）。<br> <strong>作用</strong></li><li>可用于强化或消除特殊形状</li></ul><p><img src="/pic/%E9%80%89%E5%8C%BA_043.png"></p><p><strong>腐蚀与膨胀</strong></p><pre class="line-numbers language-c++" data-language="c++"><code class="language-c++">&#x2F;&#x2F;腐蚀图像&#x2F;&#x2F;原理：在某个像素上应用结构元素时，结构元素的锚点与该像素对齐，腐蚀就是把当前像素替换成所定义像素集合中的最小像素值。void cv::erode  (       InputArray  src,           &#x2F;&#x2F;输入图像：灰度图像&amp;彩色图像    OutputArray     dst,       &#x2F;&#x2F;输出图像    InputArray  kernel,           &#x2F;&#x2F;结构元素，默认cv::Mat()，3x3的正方形    Point   anchor &#x3D; Point(-1,-1),  &#x2F;&#x2F;结构元素的锚点位置，默认为中心    int     iterations &#x3D; 1,         &#x2F;&#x2F;腐蚀次数    int     borderType &#x3D; BORDER_CONSTANT,  &#x2F;&#x2F;边界类型（像素外推的方法）    const Scalar &amp;  borderValue &#x3D; morphologyDefaultBorderValue()   &#x2F;&#x2F;连续边界的边界值)&#x2F;&#x2F;膨胀图像&#x2F;&#x2F;原理：把当前像素替换成所定义像素集合中的最大像素值。void cv::dilate (       InputArray  src,    OutputArray     dst,    InputArray  kernel,    Point   anchor &#x3D; Point(-1,-1),    int     iterations &#x3D; 1,    int     borderType &#x3D; BORDER_CONSTANT,    const Scalar &amp;  borderValue &#x3D; morphologyDefaultBorderValue() )       &#x2F;&#x2F;增加腐蚀&#x2F;膨胀次数或者使用更大的结构元素，都会增加腐蚀&#x2F;膨胀的效果。&#x2F;&#x2F;以腐蚀和膨胀操作作为基础，执行高级的形态变换void cv::morphologyEx   (       InputArray  src,    OutputArray     dst,    int     op,             &#x2F;&#x2F;形态学操作类型    InputArray  kernel,    Point   anchor &#x3D; Point(-1,-1),    int     iterations &#x3D; 1,    int     borderType &#x3D; BORDER_CONSTANT,         const Scalar &amp;  borderValue &#x3D; morphologyDefaultBorderValue() )   <span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p><strong>形态学梯度运算提取图像边缘</strong></p><pre class="line-numbers language-c++" data-language="c++"><code class="language-c++">&#x2F;&#x2F;以腐蚀和膨胀操作作为基础，执行高级的形态变换void cv::morphologyEx   (       InputArray  src,    OutputArray     dst,    int     op,             &#x2F;&#x2F;形态学操作类型    InputArray  kernel,    Point   anchor &#x3D; Point(-1,-1),    int     iterations &#x3D; 1,    int     borderType &#x3D; BORDER_CONSTANT,         const Scalar &amp;  borderValue &#x3D; morphologyDefaultBorderValue() )   <span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p><strong>图形分割</strong></p><pre class="line-numbers language-c++" data-language="c++"><code class="language-c++">CV_EXPORTS_W void watershed( InputArray image, InputOutputArray markers );&#x2F;&#x2F; cv::InputArray image：待分割的源图像；&#x2F;&#x2F; cv::InputOutputArray markers：标记图像；即这个参数用于存放函数调后的输出结果，需和源图片有一样的尺寸和类型。<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre><p><a href="https://blog.csdn.net/weixin_43863869/article/details/128534217">利用分水岭算法实现图像分割</a></p><h2 id="图像滤波"><a href="#图像滤波" class="headerlink" title="图像滤波"></a>图像滤波</h2><ul><li>概念：即选择性地提取图像中某些方面的内容，这些内容通常在特定的应用环境下传达了重要信息。</li><li>作用：滤波器是一种放大（也可以不改变）图像中某些频段，同时滤掉（或减弱）其他频段的算子，分为低通滤波器&amp;高通滤波器。</li><li>示例：去噪（噪声点）、重采样</li></ul><h3 id="频域分析"><a href="#频域分析" class="headerlink" title="频域分析"></a>频域分析</h3><p>描述图像的两种形式：</p><ul><li>频域：观察图像内容强度值（灰度值）变化的频率（蓝天 VS 杂货间），图像中精致的细节对应着高频；</li></ul><p><img src="/pic/%E9%80%89%E5%8C%BA_044.png"></p><ul><li>空域：观察图像内容灰度分布来描述图像特征（直方图）</li></ul><p><strong>频域分析</strong>：把图像分解成从低频到高频的频率成分。图像强度值变化慢的区域只包含低频率，强度值变化快的区域产生高频率。</p><p>二维图像的频率分为垂直频率和水平频率。</p><h3 id="低通滤波器"><a href="#低通滤波器" class="headerlink" title="低通滤波器"></a>低通滤波器</h3><p>目的：消除图像中的高频部分，减少图像变化的幅度（把前景变得光滑；把前景和背景之间的差异变小）。</p><p>常用方法：把每个像素的值替换成它周围像素的平均值，线性滤波。</p><p><strong>块滤波器（box filter）——&gt;卷积核（掩膜）</strong></p><pre class="line-numbers language-c++" data-language="c++"><code class="language-c++">&#x2F;&#x2F;典型示例一：均值滤波器void cv::blur   (       InputArray  src,                  &#x2F;&#x2F;输入图像    OutputArray     dst,              &#x2F;&#x2F;输出图像    Size    ksize,                    &#x2F;&#x2F;卷积核大小（值为系统默认指定？）    Point   anchor &#x3D; Point(-1,-1),     &#x2F;&#x2F;锚点位置    int     borderType &#x3D; BORDER_DEFAULT &#x2F;&#x2F;边界类型)   void cv::boxFilter  (       InputArray  src,    OutputArray     dst,    int     ddepth,    Size    ksize,    Point   anchor &#x3D; Point(-1,-1),    bool    normalize &#x3D; true,    int     borderType &#x3D; BORDER_DEFAULT )   void cv::filter2D   (       InputArray  src,    OutputArray     dst,    int     ddepth,    InputArray  kernel,                        &#x2F;&#x2F;卷积核值    Point   anchor &#x3D; Point(-1,-1),    double  delta &#x3D; 0,    int     borderType &#x3D; BORDER_DEFAULT )   &#x2F;&#x2F;典型示例二：高斯滤波器void cv::GaussianBlur   (       InputArray  src,    OutputArray     dst,    Size    ksize,                   &#x2F;&#x2F;滤波器尺寸,必须为奇数，否则会引发错误    double  sigmaX,                  &#x2F;&#x2F;控制高斯曲线水平方向形状的参数    double  sigmaY &#x3D; 0,              &#x2F;&#x2F;控制高斯曲线垂直方向形状的参数    int     borderType &#x3D; BORDER_DEFAULT     )  <span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p><strong>图像降采样</strong></p><ul><li>降低图像精度的过程称为缩减像素采样（downsampling）；</li><li>提升图像精度的过程称为提升像素采样（upsampling）</li></ul><p>难点：重采样的过程需要尽可能地保持图像质量</p><p><strong>中值滤波器</strong></p><pre class="line-numbers language-c++" data-language="c++"><code class="language-c++">&#x2F;&#x2F;原理：非线性滤波，把当前像素和它的邻域组成一个集合，然后计算出这个集合的中间值，以此作为当前像素的值（用邻域内集合的中位数代替当前像素值）void cv::medianBlur (       InputArray  src,    OutputArray     dst,    int     ksize                    &#x2F;&#x2F;注意这里的ksize类型是 int类型)<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h3 id="高通滤波器"><a href="#高通滤波器" class="headerlink" title="高通滤波器"></a>高通滤波器</h3><p><strong>定向滤波器（边缘检测）</strong></p><ul><li>二维图像分为水平方向和垂直方向；</li><li>比较经典的卷积核称为算子</li></ul><p><img src="/pic/%E9%80%89%E5%8C%BA_045.png"></p><p><img src="/pic/%E9%80%89%E5%8C%BA_046.png"></p><blockquote><p>Q:如何辨别x方向和y方向的滤波？（分别沿x&#x2F;y方向去找灰度值发生急剧变化的边缘处）</p></blockquote><pre class="line-numbers language-c++" data-language="c++"><code class="language-c++">&#x2F;&#x2F;Sobel滤波器：只对垂直或水平方向的图像频率起作用void cv::Sobel  (       InputArray  src,    OutputArray     dst,    int     ddepth,             &#x2F;&#x2F;位深，-1代表输出图像和源图像的位深相同    int     dx,                 &#x2F;&#x2F;x方向的微分，几阶导数    int     dy,                 &#x2F;&#x2F;y方向的微分，几阶导数    int     ksize &#x3D; 3,          &#x2F;&#x2F;sobel内核尺寸，只能为 1,3,5或7    double  scale &#x3D; 1,    double  delta &#x3D; 0,    int     borderType &#x3D; BORDER_DEFAULT )   &#x2F;&#x2F;Schar滤波器void cv::Scharr (       InputArray  src,    OutputArray     dst,    int     ddepth,           &#x2F;&#x2F;注意，此处位深最好选用CV_16S,否则会丢失很多信息    int     dx,    int     dy,    double  scale &#x3D; 1,    double  delta &#x3D; 0,    int     borderType &#x3D; BORDER_DEFAULT )      &#x2F;&#x2F;Laplacian算子    void cv::Laplacian  (       InputArray  src,    OutputArray     dst,    int     ddepth,    int     ksize &#x3D; 1,    double  scale &#x3D; 1,    double  delta &#x3D; 0,    int     borderType &#x3D; BORDER_DEFAULT )       <span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h3 id="Canny边缘检测算子"><a href="#Canny边缘检测算子" class="headerlink" title="Canny边缘检测算子"></a>Canny边缘检测算子</h3><pre class="line-numbers language-c++" data-language="c++"><code class="language-c++">&#x2F;&#x2F;Canny边缘检测算法void cv::Canny  (       InputArray  image,           &#x2F;&#x2F;8位的输入图像    OutputArray     edges,       &#x2F;&#x2F;输出图像，一般是二值图像    double  threshold1,          &#x2F;&#x2F;低阈值，常取高阈值的1&#x2F;2或1&#x2F;3    double  threshold2,          &#x2F;&#x2F;高阈值    int     apertureSize &#x3D; 3,    &#x2F;&#x2F;sobel算子的size，通常取值3    bool    L2gradient &#x3D; false   &#x2F;&#x2F;选择true表示用L2归一化，选择false表示用L1来归一化)<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>]]></content>
      
      
      <categories>
          
          <category> Opencv </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Opencv </tag>
            
            <tag> 直方图 </tag>
            
            <tag> 腐蚀膨胀 </tag>
            
            <tag> 图像滤波 </tag>
            
            <tag> Canny </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Opencv系列（一）</title>
      <link href="/2023/04/01/opencv/"/>
      <url>/2023/04/01/opencv/</url>
      
        <content type="html"><![CDATA[<h1 id="Opencv"><a href="#Opencv" class="headerlink" title="Opencv"></a>Opencv</h1><p><strong>main modules</strong></p><ul><li><p>core</p><p> 定义基本数据结构的紧凑模块，包括稠密的多维数组 Mat 和所有其他模块使用的基本功能。</p></li><li><p>imgproc</p><p> 一个图像处理模块，包括线性和非线性图像滤波、几何图像变换（调整大小、仿射和透视变换、通用的基于表的重映射）、色彩空间转换、直方图等。</p></li><li><p>imgcodecs</p><p> 图像文件的读取和写入</p></li><li><p>videoio</p><p> 视频输入和输出</p></li><li><p>highgui</p><p> GUI界面</p></li><li><p>calib3d</p><p> 相机标定和三维重建</p></li><li><p>features2d</p><p> 2d特征的检测，描述，匹配以及在图像上绘制2d特征点和匹配对</p></li><li><p>objdetect</p><p> 用于目标检测的基于Haar 特征的级联分类器  </p></li><li><p>dnn</p><p> 用于构建深度神经网络，主要是测试网络的输出，不支持网络训练</p></li><li><p>ml</p><p> 一组用于统计分类、回归和数据聚类的类和函数。</p><ul><li>flann<br>  FLANN库的opencv接口（功能不完整）</li></ul></li><li><p>photo</p><p> 照片处理算法，包括修补，去噪，HDR成像等</p></li><li><p>stitching</p><p> 图像拼接</p></li><li><p>gapi</p><p> OpenCV Graph API（或 G-API）是一个新的 OpenCV 模块，旨在使常规图像处理快速且便携。这两个目标是通过引入新的基于图的执行模型来实现的</p></li></ul><p><strong>contrib modules</strong></p><ul><li><p>alphamat</p><p> 从背景图像中提取具有软边界的前景</p></li><li><p>aruco</p><p> ArUco 标记是二进制方形基准标记，可用于相机姿态估计。他们的主要好处是他们的检测是鲁棒、快速和简单的。</p><p> aruco 模块包括这些类型的标记的检测以及使用它们进行姿势估计和相机校准的工具。 </p></li><li><p>bgsegm</p><p> 背景分割</p></li><li><p>bioinspired</p><p> 视网膜模型及其在图像处理中的应用</p></li><li><p>ccalib</p><p> 多相机和广角相机标定</p></li><li><p>cnn_3dobj</p><p> 用于3D物体分类和位姿估计的卷积神经网络 </p></li><li><p>cvv</p><p> 应用于计算机视觉类应用的交互式Debug </p></li><li><p>dnn_objdetect</p><p> 使用卷积神经网络进行目标检测</p></li><li><p>dnn_superres</p><p> 使用卷积神经网络进行图像放大（提高分辨率）</p></li><li><p>face</p><p>  人脸识别的相关算法</p></li><li><p>fuzzy</p><p> 模糊数学理论在图像处理中的应用，主要是F变换 </p></li><li><p>hdf</p><p> hdf5文件的输入和输出</p></li><li><p>Julia</p><p> OpenCV的Julia语言封装</p></li><li><p>line_descriptor</p><p> 从图像中检测直线</p></li><li><p>mcc</p><p> 图像色彩校正</p></li><li><p>phase_unwrapping</p><p> 二维相位展开</p></li><li><p>sfm</p><p> 运动结构恢复</p></li><li><p>stereo</p><p> 稠密立体匹配</p></li><li><p>structured_light</p><p> 结构光反射图案的解析</p></li><li><p>Text</p><p> Tesseract文字识别框架</p></li><li><p>tracking</p><p> 图像中的物体追踪</p></li><li><p>viz</p><p> 可视化窗口（类似Qt）</p></li><li><p>ximgproc</p><p> 拓展图像处理模块。包含结构森林，变化域滤波器，导向滤波，自适应流行滤波器，联合双边滤波器和超像素。</p></li><li><p>xphoto</p><p> 白平衡调整</p></li></ul><h1 id="Opencv如何对像素进行操作"><a href="#Opencv如何对像素进行操作" class="headerlink" title="Opencv如何对像素进行操作"></a>Opencv如何对像素进行操作</h1><h2 id="取"><a href="#取" class="headerlink" title="取"></a>取</h2><pre class="line-numbers language-c++" data-language="c++"><code class="language-c++">Mat cv::imread  (   const String &amp;  filename,                    int     flags &#x3D; IMREAD_COLOR                 )   <span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre><p>filename （必须的）：需要读取的图像名，你也可以写成读取的路径：绝对路劲和相对路径都可</p><p>flag （可选）：flag时读取图像的格式。<br>如果你没有flag选项就按照原始的图像格式，如果有flag选项就按照flag格式读取<br>flag可以是数字，也可以是具体的类型（枚举)</p><pre class="line-numbers language-c++" data-language="c++"><code class="language-c++">enum ImreadModes &#123;       IMREAD_UNCHANGED            &#x3D; -1, &#x2F;&#x2F;!&lt; If set, return the loaded image as is (with alpha channel, otherwise it gets cropped).       IMREAD_GRAYSCALE            &#x3D; 0,  &#x2F;&#x2F;!&lt; If set, always convert image to the single channel grayscale image.       IMREAD_COLOR                &#x3D; 1,  &#x2F;&#x2F;!&lt; If set, always convert image to the 3 channel BGR color image.       IMREAD_ANYDEPTH             &#x3D; 2,  &#x2F;&#x2F;!&lt; If set, return 16-bit&#x2F;32-bit image when the input has the corresponding depth, otherwise convert it to 8-bit.       IMREAD_ANYCOLOR             &#x3D; 4,  &#x2F;&#x2F;!&lt; If set, the image is read in any possible color format.       IMREAD_LOAD_GDAL            &#x3D; 8,  &#x2F;&#x2F;!&lt; If set, use the gdal driver for loading the image.       IMREAD_REDUCED_GRAYSCALE_2  &#x3D; 16, &#x2F;&#x2F;!&lt; If set, always convert image to the single channel grayscale image and the image size reduced 1&#x2F;2.       IMREAD_REDUCED_COLOR_2      &#x3D; 17, &#x2F;&#x2F;!&lt; If set, always convert image to the 3 channel BGR color image and the image size reduced 1&#x2F;2.       IMREAD_REDUCED_GRAYSCALE_4  &#x3D; 32, &#x2F;&#x2F;!&lt; If set, always convert image to the single channel grayscale image and the image size reduced 1&#x2F;4.       IMREAD_REDUCED_COLOR_4      &#x3D; 33, &#x2F;&#x2F;!&lt; If set, always convert image to the 3 channel BGR color image and the image size reduced 1&#x2F;4.       IMREAD_REDUCED_GRAYSCALE_8  &#x3D; 64, &#x2F;&#x2F;!&lt; If set, always convert image to the single channel grayscale image and the image size reduced 1&#x2F;8.       IMREAD_REDUCED_COLOR_8      &#x3D; 65, &#x2F;&#x2F;!&lt; If set, always convert image to the 3 channel BGR color image and the image size reduced 1&#x2F;8.       IMREAD_IGNORE_ORIENTATION   &#x3D; 128 &#x2F;&#x2F;!&lt; If set, do not rotate the image according to EXIF&#39;s orientation flag.     &#125;;     <span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h2 id="存"><a href="#存" class="headerlink" title="存"></a>存</h2><pre class="line-numbers language-c++" data-language="c++"><code class="language-c++">bool cv::imwrite    (   const String &amp;  filename,                        InputArray  img,                        const std::vector&lt; int &gt; &amp;params&#x3D;std::vector&lt; int &gt;()                     )   <span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span></span></code></pre><p>filename （必须）同上<br>img  （必须）表示需要保存的Mat类型的图像数据<br>通常，使用此功能只能保存 8 位单通道或 3 通道（具有“BGR”通道顺序）图像，除去一些特殊情况。</p><pre class="line-numbers language-c++" data-language="c++"><code class="language-c++">enum ImwriteFlags &#123;       IMWRITE_JPEG_QUALITY        &#x3D; 1,  &#x2F;&#x2F;!&lt; For JPEG, it can be a quality from 0 to 100 (the higher is the better). Default value is 95.       IMWRITE_JPEG_PROGRESSIVE    &#x3D; 2,  &#x2F;&#x2F;!&lt; Enable JPEG features, 0 or 1, default is False.       IMWRITE_JPEG_OPTIMIZE       &#x3D; 3,  &#x2F;&#x2F;!&lt; Enable JPEG features, 0 or 1, default is False.       IMWRITE_JPEG_RST_INTERVAL   &#x3D; 4,  &#x2F;&#x2F;!&lt; JPEG restart interval, 0 - 65535, default is 0 - no restart.       IMWRITE_JPEG_LUMA_QUALITY   &#x3D; 5,  &#x2F;&#x2F;!&lt; Separate luma quality level, 0 - 100, default is 0 - don&#39;t use.       IMWRITE_JPEG_CHROMA_QUALITY &#x3D; 6,  &#x2F;&#x2F;!&lt; Separate chroma quality level, 0 - 100, default is 0 - don&#39;t use.       IMWRITE_PNG_COMPRESSION     &#x3D; 16, &#x2F;&#x2F;!&lt; For PNG, it can be the compression level from 0 to 9. A higher value means a smaller size and longer compression time. If specified, strategy is changed to IMWRITE_PNG_STRATEGY_DEFAULT (Z_DEFAULT_STRATEGY). Default value is 1 (best speed setting).       IMWRITE_PNG_STRATEGY        &#x3D; 17, &#x2F;&#x2F;!&lt; One of cv::ImwritePNGFlags, default is IMWRITE_PNG_STRATEGY_RLE.       IMWRITE_PNG_BILEVEL         &#x3D; 18, &#x2F;&#x2F;!&lt; Binary level PNG, 0 or 1, default is 0.       IMWRITE_PXM_BINARY          &#x3D; 32, &#x2F;&#x2F;!&lt; For PPM, PGM, or PBM, it can be a binary format flag, 0 or 1. Default value is 1.       IMWRITE_EXR_TYPE            &#x3D; (3 &lt;&lt; 4) + 0, &#x2F;* 48 *&#x2F; &#x2F;&#x2F;!&lt; override EXR storage type (FLOAT (FP32) is default)       IMWRITE_WEBP_QUALITY        &#x3D; 64, &#x2F;&#x2F;!&lt; For WEBP, it can be a quality from 1 to 100 (the higher is the better). By default (without any parameter) and for quality above 100 the lossless compression is used.       IMWRITE_PAM_TUPLETYPE       &#x3D; 128,&#x2F;&#x2F;!&lt; For PAM, sets the TUPLETYPE field to the corresponding string value that is defined for the format       IMWRITE_TIFF_RESUNIT &#x3D; 256,&#x2F;&#x2F;!&lt; For TIFF, use to specify which DPI resolution unit to set; see libtiff documentation for valid values       IMWRITE_TIFF_XDPI &#x3D; 257,&#x2F;&#x2F;!&lt; For TIFF, use to specify the X direction DPI       IMWRITE_TIFF_YDPI &#x3D; 258 &#x2F;&#x2F;!&lt; For TIFF, use to specify the Y direction DPI     &#125;;<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h2 id="ROI区域"><a href="#ROI区域" class="headerlink" title="ROI区域"></a>ROI区域</h2><blockquote><p>定义<br>有事需要让一个处理函数只在图像的某个部分起作用，所以需要定义图像的子区域，也就是ROI区域（region of interest）感兴趣区域</p></blockquote><pre class="line-numbers language-c++" data-language="c++"><code class="language-c++">int  main()&#123;cv::Mat image&#x3D; cv::imread(&quot;3.png&quot;);cv::Mat logo&#x3D; cv::imread(&quot;2.jpg&quot;);cv::Mat imageROI(image,                                    cv::Rect(0,                                    0,                                    logo.cols,                                    logo.rows));imshow(&quot;1&quot;,imageROI);&#x2F;&#x2F;将logo替换image中的感兴区域imageROIlogo.copyTo(imageROI);imshow(&quot;2&quot;,logo);imshow(&quot;3&quot;,image);cv::waitKey(0);return 0;&#125;<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><blockquote><p>扩展：除了使用起点和终点位置，还可以通过列数和行数实现ROI区域定义：</p></blockquote><pre class="line-numbers language-c++" data-language="c++"><code class="language-c++">cv:: Mat imageROI &#x3D; image(cv::Range(0,logo.rows),                                  cv::Range(0,logo.cols));<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><h2 id="指针遍历"><a href="#指针遍历" class="headerlink" title="指针遍历"></a>指针遍历</h2><p>像素遍历就是将图像的所有像素都访问一次。由于图像像素数量非常庞大，高效遍历就十分必要</p><pre class="line-numbers language-c++" data-language="c++"><code class="language-c++">int nl&#x3D; image.rows; &#x2F;&#x2F; 行数 &#x2F;&#x2F; 每行的元素数量 int nc&#x3D; image.cols * image.channels();  for (int j&#x3D;0; j&lt;nl; j++) &#123;  &#x2F;&#x2F; 取得行 j 的地址,这里以uchar图像类型作为例子 uchar* data&#x3D; image.ptr&lt;uchar&gt;(j);  for (int i&#x3D;0; i&lt;nc; i++) &#123; &#x2F;&#x2F; 处理每个像素 data[i] &#125; &#x2F;&#x2F; 一行结束 &#125;<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>例子：减色算法，对每个像素做减色，对于8为无符号字符类型的彩色图有256x256x256中颜色，减色就是减色颜色的种类</p><pre class="line-numbers language-c++" data-language="c++"><code class="language-c++">&#x2F;*减色算法：假设 N 是减色因子，将图像中每个像素的值除以 N（这里假定使用整数除法，不保留余数）。然后将结果乘以 N，得到 N 的倍数，并且刚好不超过原始像素 值。加上 N &#x2F; 2，就得到相邻的 N 倍数之间的中间值。对所有 8 位通道值重复这个过程，就会得到  (256 &#x2F; N) × (256 &#x2F; N) × (256 &#x2F; N)种可能的颜色值*&#x2F;void colorReduce(cv::Mat image, int div &#x3D;64)&#123;    int nl &#x3D;image.rows;    int nc &#x3D;image.cols;    for(int j&#x3D;0;j&lt;nl;j++)&#123;        uchar* data&#x3D;image.ptr&lt;uchar&gt;(j);        for(int i&#x3D;0;i&lt;nc;i++)&#123;            data[i]&#x3D; data[i]&#x2F;div*div+div&#x2F;2;                    &#125;    &#125;&#125;int  main()&#123;    cv::Mat image&#x3D;cv::imread(&quot;1.jpg&quot;);    colorReduce(image);    cv::imshow(&quot;1&quot;,image);    cv::waitKey();    return 0;&#125;<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h2 id="迭代器遍历"><a href="#迭代器遍历" class="headerlink" title="迭代器遍历"></a>迭代器遍历</h2><pre class="line-numbers language-c++" data-language="c++"><code class="language-c++">&#x2F;&#x2F;创建迭代器bagin和end，注意要给出图像的数据类型，此处以cv::Vec3b为例&#x2F;&#x2F;在opencv中cv::Vec3b向量包含三个无符号字符类型的数据，可以用于描述彩色图的三通道。cv::Mat_&lt;cv::Vec3b&gt;::iterator it&#x3D; image.begin&lt;cv::Vec3b&gt;(); cv::Mat_&lt;cv::Vec3b&gt;::iterator itend&#x3D; image.end&lt;cv::Vec3b&gt;();  &#x2F;&#x2F; 扫描全部像素 for ( ; it!&#x3D; itend; ++it) &#123; &#x2F;&#x2F;处理每个像素 (*it)[0] (*it)[1] (*it)[2] &#125; &#x2F;&#x2F;或者while (it!&#x3D; itend) &#123;  &#x2F;&#x2F; 处理每个像素 ---------------------  ... &#x2F;&#x2F; 像素处理结束 ---------------------  ++it; &#125; <span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h1 id="Opencv点检测"><a href="#Opencv点检测" class="headerlink" title="Opencv点检测"></a>Opencv点检测</h1><h2 id="Harris角点检测"><a href="#Harris角点检测" class="headerlink" title="Harris角点检测"></a>Harris角点检测</h2><h3 id="角点定义"><a href="#角点定义" class="headerlink" title="角点定义"></a>角点定义</h3><p>角点是图像中某些属性较为突出的像素点，例如像素值最大或者最小的点、线段的顶点、孤立的边缘点等，图中圆圈包围的线段的拐点就是一些常见的角点。常用的角点有以下几种。  </p><ul><li>灰度梯度的最大值对应的像素点；  </li><li>两条直线或者曲线的交点；  </li><li>一阶梯度的导数最大值和梯度方向变化率最大的像素点；  </li><li>一阶导数值最大，但是二阶导数值为0的像素点；</li></ul><h3 id="Harris算法原理"><a href="#Harris算法原理" class="headerlink" title="Harris算法原理"></a>Harris算法原理</h3><p>Harris角点是最经典的角点之一，其从像素值变化度对角点进行定义，像素值的局部最大峰值即为Harris角点。Harris角点的检测过程如图9-3所示，首先以某个像素为中心构建一个矩形滑动窗口，滑动窗口覆盖图像像素值通过线性叠加得到得到滑动窗口所有像素值的衡量系数，该系数与滑动窗口范围内的像素值成正比，当滑动窗口范围内像素值整体变大时，该衡量系数也变大。在图像中以每个像素为中心向各个方向移动滑动窗口，当滑动窗口无论向哪个方向移动像素值衡量系数都缩小时，滑动窗口中心点对应的像素点即为Haris角点</p><p><img src="/pic/%E9%80%89%E5%8C%BA_032.png"></p><p>角点检测最原始的想法就是取某个像素的一个邻域窗口，当这个窗口在各个方向上进行小范围移动时，观察窗口内平均的像素灰度值的变化（即E(u,v)，Window-averaged change of intensity）。从上图可知，我们可以将一幅图像大致分为三个区域（‘flat’，‘edge’，‘corner’），这三个区域变化是不一样的。</p><p><img src="/pic/%E9%80%89%E5%8C%BA_035.png"></p><p>其中：  </p><ul><li>u、v是窗口在水平，竖直方向的偏移；  </li><li>w(x,y)表示滑动窗口权重函数，可以是常数，也可以是高斯函数；</li></ul><p><img src="/pic/%E9%80%89%E5%8C%BA_034.png"></p><p>图中蓝线圈出的地方我们称之为Harris角点的梯度协方差矩阵，记为M。其中，Ix和Iy分别为X方向和Y方向的梯度。  由于E(x,y)取值与M相关，进一步对其进行简化，定义Harris角点评价系数R为：</p><blockquote><p>R&#x3D;det(M)-k(tr(M))^2</p></blockquote><p>其中k为常值权重系数，det(M)&#x3D;λ1λ2,tr(M)&#x3D;λ1+λ2,λ1和λ2是梯度协方差矩阵M的特征向量，将特征向量代入得:</p><blockquote><p>R&#x3D;λ1λ2-k(λ1+λ2)^2</p></blockquote><p>当R较大时，说明两个特征向量较相似或者接近，则该点为角点；当R&lt;0时，说明两个特征向量相差较大，则该点位于直线上；当|R|较小，说明两个特征值较小，则该点位于平面。</p><h3 id="Opencv实现"><a href="#Opencv实现" class="headerlink" title="Opencv实现"></a>Opencv实现</h3><p><strong>cornerHarris()——计算角点Harris评价系数R</strong></p><pre class="line-numbers language-c++" data-language="c++"><code class="language-c++">void cv::cornerHarris( InputArray src, OutputArray dst, int blockSize, int ksize, double k, int borderType &#x3D; BORDER_DEFAULT)<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><ul><li>src:待检测Harris角点的输入图像，图像必须是CV_8U或者CV_32F的单通道灰度图像；</li><li>dst:存放Harris评价系数R的矩阵，数据类型为CV_32F的单通道图像，与输入图像具有相同的尺寸</li><li>blockSize:邻域大小（窗口大小），通常取2；</li><li>ksize：Sobel算子的半径，用于得到图像梯度信息，该参数需要是奇数，多使用3或者5；</li><li>k:计算Harris评价系数R的权重系数，一般取值为0.02~0.04;</li><li>borderType：像素外推算法标志，这里使用默认。</li></ul><p><strong>drawKeypoints()——一次性绘制所有的角点（关键词）</strong></p><blockquote><p>绘制关键点</p></blockquote><pre class="line-numbers language-c++" data-language="c++"><code class="language-c++">void drawKeypoints(  InputArray image,  const std::vector&lt;KeyPoint&gt;&amp; keypoints, InputOutputArray outImage, const Scalar&amp; color&#x3D;Scalar::all(-1), int flags&#x3D;DrawMatchesFlags::DEFAULT )   &#x2F;&#x2F;KeyPoint类数据 class KeyPoint&#123; float angle      &#x2F;&#x2F;关键点的角度 int class_id     &#x2F;&#x2F;关键点的分类号 int octave       &#x2F;&#x2F;特征点来源（“金字塔”） Point2f pt       &#x2F;&#x2F;关键点坐标 float response   &#x2F;&#x2F;最强关键点的响应 float size       &#x2F;&#x2F;关键点邻域的直径 <span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><ul><li>image:绘制关键点的原图像，图像可以是单通道的灰度图像和三通道的彩色图像；</li><li>keypoints:来自原图像中的关键点向量，vector向量中存放着表示关键点的KeyPoint类型的数据；</li><li>outImage：绘制关键点后的输出图像；</li><li>color：关键点空心圆的颜色，默认使用随机颜色绘制空心圆；</li><li>flag:绘制功能选择标志，其实就是设置特征点的那些信息需要绘制，那些不需要绘制，有以下几种模式可选：</li></ul><table><thead><tr><th align="center">标志参数</th><th align="center">简记</th><th align="center">含义</th></tr></thead><tbody><tr><td align="center">DEFAULT</td><td align="center">0</td><td align="center">只绘制特征点的坐标点,显示在图像上就是一个个小圆点,每个小圆点的圆心坐标都是特征点的坐标。</td></tr><tr><td align="center">DRAW_OVER_OUTIMG</td><td align="center">1</td><td align="center">函数不创建输出的图像,而是直接在输出图像变量空间绘制,要求本身输出图像变量就是一个初始化好了的,size与type都是已经初始化好的变量</td></tr><tr><td align="center">NOT_DRAW_SINGLE_POINTS</td><td align="center">2</td><td align="center">单点的特征点不被绘制</td></tr><tr><td align="center">DRAW_RICH_KEYPOINTS</td><td align="center">4</td><td align="center">绘制特征点的时候绘制的是一个个带有方向的圆,这种方法同时显示图像的坐标,size，和方向,是最能显示特征的一种绘制方式</td></tr></tbody></table><blockquote><p>Harris 算法实现步骤：<br>（1）计算图像在两个方向上的梯度<br>（2）计算两个方向梯度乘积<br>（3）使用高斯函数进行加权平均，生成矩阵元素和<br>（4）计算每个像素Harris响应值，并对小于某一个阈值的像素置0<br>（5）在阈值的邻域内进行非最大值抑制，局部最大值即为Harris角点Harris算法优劣：  </p></blockquote><blockquote><p>（1）优点：计算简单，提取的特征点均匀且合理稳定（对图像旋转、亮度变化、噪声影响和视点变换不敏感）；<br>（2）缺点：a.对尺度很敏感，不具有尺度不变性；b.提取的角点精度是像素级的；c.需要设计对应的描述子和匹配算法；</p></blockquote><pre class="line-numbers language-c++" data-language="c++"><code class="language-c++">int  main()&#123;    cv::Mat image&#x3D;cv::imread(&quot;1.png&quot;);    cv::Mat grayImage;    cv::cvtColor(image,grayImage,CV_BGR2GRAY);    cv::Mat dstImage;    cv::cornerHarris(grayImage,dstImage,2,3,0.01);    cv::imshow(&quot;直接显示&quot;,dstImage);    cv::Mat thredImage;threshold(dstImage, thredImage, 0.0001, 255, CV_THRESH_BINARY);imshow(&quot;【阀值后显示】&quot;, thredImage);    cv::waitKey();    return 0;&#125;<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><blockquote><p>绘制匹配点</p></blockquote><pre class="line-numbers language-c++" data-language="c++"><code class="language-c++">void cv::drawMatches (InputArray img1,const std::vector&lt; KeyPoint &gt; &amp; keypoints1,InputArray img2,const std::vector&lt; KeyPoint &gt; &amp; keypoints2,const std::vector&lt; DMatch &gt; &amp; matches1to2,InputOutputArray outImg,const Scalar &amp; matchColor &#x3D; Scalar::all(-1),const Scalar &amp; singlePointColor &#x3D; Scalar::all(-1),const std::vector&lt; char &gt; &amp; matchesMask &#x3D; std::vector&lt; char &gt;(),DrawMatchesFlags flags &#x3D; DrawMatchesFlags::DEFAULT)<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><ul><li>img1:第一个源图像，</li><li>keypoints1:第一个源图像的关键点，</li><li>img2:第二个源图像，</li><li>keypoints2:第二个源图像的关键点，</li><li>matches1to2:从第一张图像匹配到第二张图像，</li><li>outimg: 输出图像。它的内容取决于定义在输出图像中绘制的内容的标志值，</li><li>matchColor:匹配的颜色（线和连接的关键点），</li><li>singlePointColor:单个关键点（圆圈）的颜色，表示关键点不匹配，</li><li>matchesMask:确定绘制哪些匹配项的掩码。如果掩码为空，则绘制所有匹配项。</li><li>flags:标志设置绘图功能</li></ul><h2 id="SIFT特征点检测"><a href="#SIFT特征点检测" class="headerlink" title="SIFT特征点检测"></a>SIFT特征点检测</h2><h3 id="SIFT综述"><a href="#SIFT综述" class="headerlink" title="SIFT综述"></a>SIFT综述</h3><p>尺度不变特征转换(SIFT)是一种电脑视觉的算法用来侦测与描述影像中的局部性特征，它在空间尺度中寻找极值点，并提取出其位置、尺度、旋转不变量，此算法由 David Lowe在1999年所发表，2004年完善总结。</p><p>Lowe将SIFT算法分解为如下四步：  </p><ul><li>尺度空间极值检测：搜索所有尺度上的图像位置。通过高斯微分函数来识别潜在的对于尺度和旋转不变的兴趣点。  </li><li>关键点定位：在每个候选的位置上，通过一个拟合精细的模型来确定位置和尺度。关键点的选择依据于它们的稳定程度。  </li><li>方向确定：基于图像局部的梯度方向，分配给每个关键点位置一个或多个方向。所有后面的对图像数据的操作都相对于关键点的方向、尺度和位置进行变换，从而提供对于这些变换的不变性。  </li><li>关键点描述：在每个关键点周围的邻域内，在选定的尺度上测量图像局部的梯度。这些梯度被变换成一种表示，这种表示允许比较大的局部形状的变形和光照变化。</li></ul><h3 id="SIFT算法的OpenCV实现"><a href="#SIFT算法的OpenCV实现" class="headerlink" title="SIFT算法的OpenCV实现"></a>SIFT算法的OpenCV实现</h3><p>OpenCV中的SIFT函数主要有两个接口。</p><p>构造函数：</p><pre class="line-numbers language-c++" data-language="c++"><code class="language-c++">SIFT::SIFT(int nfeatures&#x3D;0, int nOctaveLayers&#x3D;3, double contrastThreshold&#x3D;0.04, double edgeThreshold&#x3D;10, double sigma&#x3D;1.6) <span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><ul><li>nfeatures：特征点数目（算法对检测出的特征点排名，返回最好的nfeatures个特征点）。</li><li>nOctaveLayers：金字塔中每组的层数（算法中会自己计算这个值，后面会介绍）。</li><li>contrastThreshold：过滤掉较差的特征点的对阈值。contrastThreshold越大，返回的特征点越少。</li><li>edgeThreshold：过滤掉边缘效应的阈值。edgeThreshold越大，特征点越多（被多滤掉的越少）。</li><li>sigma：金字塔第0层图像高斯滤波系数，也就是σ。</li></ul><p>重载操作符：</p><pre class="line-numbers language-c++" data-language="c++"><code class="language-c++">void SIFT::operator()(InputArray img, InputArray mask, vector&lt;KeyPoint&gt;&amp; keypoints, OutputArray descriptors, bool useProvidedKeypoints&#x3D;false) <span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre><ul><li>img：8bit灰度图像 mask：图像检测区域（可选）</li><li>keypoints：特征向量矩阵</li><li>descipotors：特征点描述的输出向量（如果不需要输出，需要传cv::noArray()）。</li><li>useProvidedKeypoints：是否进行特征点检测。ture，则检测特征点；false，只计算图像特征描述。</li></ul><h3 id="SURF特征"><a href="#SURF特征" class="headerlink" title="SURF特征"></a>SURF特征</h3><p>SURF（Speeded Up Robust Features）是对SIFT的一种改进，主要特点是快速。SURF与SIFT主要有以下几点不同处理：</p><blockquote><p>1、SIFT在构造DOG金字塔以及求DOG局部空间极值比较耗时，SURF的改进是使用Hessian矩阵变换图像，极值的检测只需计算Hessian矩阵行列式，作为进一步优化，使用一个简单的方程可以求出Hessian行列式近似值，使用盒状模糊滤波（box  blur）求高斯模糊近似值。<br>2、 SURF不使用降采样，通过保持图像大小不变，但改变盒状滤波器的大小来构建尺度金字塔。<br>3、在计算关键点主方向以及关键点周边像素方向的方法上，SURF不使用直方图统计，而是使用哈尔(haar)小波转换。SIFT的KPD达到128维，导致KPD的比较耗时，SURF使用哈尔(haar)小波转换得到的方向，让SURF的KPD降到64维，减少了一半，提高了匹配速度</p></blockquote><h2 id="ORB特征"><a href="#ORB特征" class="headerlink" title="ORB特征"></a>ORB特征</h2><p>ORB特征由关键点和描述子两部分组成，关键点称为“Oriented FAST”，是一种改进的FAST 角点。它的描述子称为BRIEF(Binary Robust Independent Elementary Feature)。</p><p>提取 ORB 特征分为如下两个步骤：  </p><ul><li>FAST 角点提取：找出图像中的“角点”。相较于原始的 FAST，ORB 中计算了特征点的主方向，为BRIEF 描述子增加了旋转不变特性。  </li><li>BRIEF 描述子的计算：对前一步提取出特征点的周围图像区域进行描述。ORB 对 BRIEF 进行了改进，主要是在BRIEF 中使用了先前计算的方向信息。</li></ul><h3 id="FAST关键点"><a href="#FAST关键点" class="headerlink" title="FAST关键点"></a>FAST关键点</h3><p>FAST 是一种角点，主要检测局部像素灰度变化明显的地方，以速度快著称。它的思想是：如果一个像素与邻域的像素差别较大（过亮或过暗），那么它可能是角点。检测步骤如下：</p><ul><li>在图像中选取像素 p，假设它的亮度为Ip。</li><li>设置一个阈值 T（比如，Ip的20%）。</li><li>以像素 p 为中心，选取半径为3的圆上的16个像素点。</li><li>假如选取的圆上有连续的 N 个点的亮度大于 Ip+T 或小于 Ip−T，那么像素p 可以被认为是特征点（N通常取12，即为 FAST-12。其他常用的N取值为9和11，它们分别被称为FAST-9和FAST-11）。</li><li>循环以上四步，对每一个像素执行相同的操作。</li></ul><p>在FAST-12算法中，可以进行预测试操作，以快速地排除绝大多数不是角点的像素。</p><blockquote><p>具体操作为，对于每个像素，直接检测邻域圆上的第 1, 5, 9, 13 个像素的亮度。只有当这 4个像素中有 3 个同时大于 Ip+T或小于 Ip−T 时，当前像素才有可能是一个角点，否则应该直接排除。这大大加速了角点检测。</p></blockquote><p>还需要用非极大值抑制(Non-maximal suppression)，在一定区域内仅保留响应极大值的角点，避免角点集中的问题。  </p><p><img src="/pic/%E9%80%89%E5%8C%BA_037.png" alt="FAST特征点"></p><p>FAST特征点的计算仅仅是比较像素间亮度的差异，所以速度非常快。它的缺点是重复性不强，分布不均匀，不具有方向信息。同时，由于它固定取半径为3的圆，存在尺度问题：远处看着像是角点的地方，接近后看可能就不是角点了。</p><blockquote><p>针对 FAST 角点不具有方向性和尺度的弱点，ORB添加了尺度和旋转的描述。尺度不变性由构建图像金字塔解决，在金字塔的每一层上检测角点。特征的旋转是由灰度质心法(Intensity Centroid)实现。</p></blockquote><p><img src="/pic/%E9%80%89%E5%8C%BA_038.png" alt="使用金字塔可以匹配不同缩放倍率下的图像"></p><p>图像金字塔如上图，金字塔底层是原始图像，每往上一层，就对图像进行一个固定倍率的缩放，这样就有了不同分辨率的图像。较小的图像可以看成是远处看过来的场景。在特征匹配算法中，我们可以匹配不同层上的图像，从而实现尺度不变性。例如，如果相机在后退，那么我们应该能够在上一个图像金字塔的上层和下一个图像的下层中找到匹配。</p><p><img src="/pic/%E9%80%89%E5%8C%BA_039.png"></p><p>通过以上方法，FAST 角点便具有了尺度与旋转的描述，从而大大提升了其在不同图像之间表述的鲁棒性。所以在 ORB中，把这种改进后的 FAST 称为 Oriented FAST。</p><h3 id="BRIEF描述子"><a href="#BRIEF描述子" class="headerlink" title="BRIEF描述子"></a>BRIEF描述子</h3><p>在提取 Oriented FAST 关键点后，对每个点计算其描述子，ORB 使用改进的BRIEF特征描述。BRIEF 是一种二进制描述子，其描述向量由许多个 0 和 1 组成，这里的 0 和 1 编码了关键点附近两个随机像素（比如p和q）的大小关系：如果p 比 q 大，则取 1，反之就取 0。如果我们取了 128个这样的 p, q，最后就得到 128 维由 0、1 组成的向量。关于一对随机点的选择方法，ORB论文原作者测试了以下5种方法，发现方法（2）比较好：</p><p><img src="/pic/%E9%80%89%E5%8C%BA_040.png"></p><p>原始的 BRIEF 描述子不具有旋转不变性，因此在图像发生旋转时容易丢失。而 ORB 在 FAST 特征点提取阶段计算了关键点的方向，所以可以利用方向信息，计算了旋转之后的“Steer BRIEF”特征使 ORB 的描述子具有较好的旋转不变性。</p><p><strong>ORB类定义</strong></p><pre class="line-numbers language-c++" data-language="c++"><code class="language-c++">CV_WRAP static Ptr&lt;ORB&gt; create(int nfeatures&#x3D;500, float scaleFactor&#x3D;1.2f, int nlevels&#x3D;8, int edgeThreshold&#x3D;31,    int firstLevel&#x3D;0, int WTA_K&#x3D;2, int scoreType&#x3D;ORB::HARRIS_SCORE, int patchSize&#x3D;31, int fastThreshold&#x3D;20);<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><p>其中：</p><ul><li>nfeatures:需要的特征点总数；</li><li>scaleFactor:尺度因子；</li><li>nlevels:金字塔层数；</li><li>edgeThreshold:边界阈值；</li><li>firstLevel:起始层； </li><li>WTA_K：描述子形成方法,WTA_K&#x3D;2表示，采用两两比较；</li><li>scoreType:角点响应函数，可以选择Harris或者Fast的方法；</li><li>patchSize:特征点邻域大小</li></ul><pre class="line-numbers language-c++" data-language="c++"><code class="language-c++">int  main()&#123;    cv::Mat image&#x3D;cv::imread(&quot;1.jpg&quot;);    cv::Mat grayImage;    cv::cvtColor(image,grayImage,CV_BGR2GRAY);    vector&lt;KeyPoint&gt; keypoints_1;    Mat descriptors_1;    Ptr&lt;FeatureDetector&gt; detector &#x3D; ORB::create();    Ptr&lt;DescriptorExtractor&gt; descriptor &#x3D; ORB::create();     &#x2F;&#x2F;-- 第一步:检测 Oriented FAST 角点位置    detector-&gt;detect ( grayImage,keypoints_1 );    &#x2F;&#x2F;-- 第二步:根据角点位置计算 BRIEF 描述子    descriptor-&gt;compute ( grayImage, keypoints_1, descriptors_1 );    Mat outimg1;    Mat outimg2;    &#x2F;&#x2F;-- 第三步:显示特征点    drawKeypoints( grayImage, keypoints_1, outimg1, Scalar::all(-1), DrawMatchesFlags::DEFAULT );    drawKeypoints( grayImage, keypoints_1, outimg2, Scalar::all(-1), DrawMatchesFlags::DRAW_RICH_KEYPOINTS );    imshow(&quot;ORB特征点&quot;,outimg1);    imshow(&quot;ORB特征方向圆&quot;,outimg2);    cv::waitKey();    return 0;&#125;<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p><img src="/pic/%E9%80%89%E5%8C%BA_041.png"></p><p><strong>特征提取及形成描述子</strong></p><pre class="line-numbers language-c++" data-language="c++"><code class="language-c++">void ORB::operator()( InputArray _image, InputArray _mask, vector&lt;KeyPoint&gt;&amp; _keypoints,                        OutputArray _descriptors, bool useProvidedKeypoints)<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><ul><li>_image:输入图像；</li><li>_mask:掩码图像;</li><li>_keypoints:输入角点；</li><li>_descriptors:如果为空，只寻找特征点，不计算特征描述子；</li><li>_useProvidedKeypoints:如果为true,函数只计算特征描述子</li></ul><pre class="line-numbers language-c++" data-language="c++"><code class="language-c++">#include &lt;iostream&gt;#include &lt;chrono&gt;using namespace std;#include &lt;opencv2&#x2F;core&#x2F;core.hpp&gt;#include &lt;opencv2&#x2F;highgui&#x2F;highgui.hpp&gt;int main ( int argc, char** argv )&#123;    &#x2F;&#x2F; 读取argv[1]指定的图像    cv::Mat image;    image &#x3D; cv::imread ( argv[1] );     &#x2F;&#x2F; 判断图像文件是否正确读取    if ( image.data &#x3D;&#x3D; nullptr ) &#x2F;&#x2F;数据不存在,可能是文件不存在    &#123;        cerr&lt;&lt;&quot;文件&quot;&lt;&lt;argv[1]&lt;&lt;&quot;不存在.&quot;&lt;&lt;endl;        return 0;    &#125;    &#x2F;&#x2F; 文件顺利读取, 首先输出一些基本信息 H*W  rows*cols    cout&lt;&lt;&quot;图像宽为&quot;&lt;&lt;image.cols&lt;&lt;&quot;,高为&quot;&lt;&lt;image.rows&lt;&lt;&quot;,通道数为&quot;&lt;&lt;image.channels()&lt;&lt;endl;    cv::imshow ( &quot;image&quot;, image );      &#x2F;&#x2F; 用cv::imshow显示图像    cv::waitKey ( 0 );                  &#x2F;&#x2F; 暂停程序,等待一个按键输入    &#x2F;&#x2F; 判断image的类型    if ( image.type() !&#x3D; CV_8UC1 &amp;&amp; image.type() !&#x3D; CV_8UC3 )    &#123;        &#x2F;&#x2F; 图像类型不符合要求        cout&lt;&lt;&quot;请输入一张彩色图或灰度图.&quot;&lt;&lt;endl;        return 0;    &#125;    &#x2F;&#x2F; 遍历图像, 请注意以下遍历方式亦可使用于随机像素访问    &#x2F;&#x2F; 使用 std::chrono 来给算法计时    chrono::steady_clock::time_point t1 &#x3D; chrono::steady_clock::now();    for ( size_t y&#x3D;0; y&lt;image.rows; y++ )    &#123;        &#x2F;&#x2F; 用cv::Mat::ptr获得图像的行指针        unsigned char* row_ptr &#x3D; image.ptr&lt;unsigned char&gt; ( y );  &#x2F;&#x2F; row_ptr是第y行的头指针        for ( size_t x&#x3D;0; x&lt;image.cols; x++ )        &#123;            &#x2F;&#x2F; 访问位于 x,y 处的像素            unsigned char* data_ptr &#x3D; &amp;row_ptr[ x*image.channels() ]; &#x2F;&#x2F; data_ptr 指向待访问的像素数据            &#x2F;&#x2F; 输出该像素的每个通道,如果是灰度图就只有一个通道            for ( int c &#x3D; 0; c !&#x3D; image.channels(); c++ )            &#123;                unsigned char data &#x3D; data_ptr[c]; &#x2F;&#x2F; data为I(x,y)第c个通道的值            &#125;        &#125;    &#125;    chrono::steady_clock::time_point t2 &#x3D; chrono::steady_clock::now();    chrono::duration&lt;double&gt; time_used &#x3D; chrono::duration_cast&lt;chrono::duration&lt;double&gt;&gt;( t2-t1 );    cout&lt;&lt;&quot;遍历图像用时：&quot;&lt;&lt;time_used.count()&lt;&lt;&quot; 秒。&quot;&lt;&lt;endl;    &#x2F;&#x2F; 关于 cv::Mat 的拷贝    &#x2F;&#x2F; 直接赋值并不会拷贝数据    cv::Mat image_another &#x3D; image;    &#x2F;&#x2F; 修改 image_another 会导致 image 发生变化    image_another ( cv::Rect ( 0,0,100,100 ) ).setTo ( 0 ); &#x2F;&#x2F; 将左上角100*100的块置零    cv::imshow ( &quot;image&quot;, image );    cv::waitKey ( 0 );    &#x2F;&#x2F; 使用clone函数来拷贝数据    cv::Mat image_clone &#x3D; image.clone();    image_clone ( cv::Rect ( 0,0,100,100 ) ).setTo ( 255 );    cv::imshow ( &quot;image&quot;, image );    cv::imshow ( &quot;image_clone&quot;, image_clone );    cv::waitKey ( 0 );    &#x2F;&#x2F; 对于图像还有很多基本的操作,如剪切,旋转,缩放等,限于篇幅就不一一介绍了,请参看OpenCV官方文档查询每个函数的调用方法.    cv::destroyAllWindows();    return 0;&#125;cmake_minimum_required( VERSION 2.8 )project( imageBasics )# 添加c++ 11标准支持set( CMAKE_CXX_FLAGS &quot;-std&#x3D;c++11&quot; )# 寻找OpenCV库set(OpenCV_DIR  ~&#x2F;ssd&#x2F;software&#x2F;opencv3.3.1&#x2F;build)   #添加OpenCVConfig.cmake的搜索路径find_package( OpenCV 3 REQUIRED )# 添加头文件include_directories( $&#123;OpenCV_INCLUDE_DIRS&#125; )add_executable( imageBasics imageBasics.cpp )# 链接OpenCV库target_link_libraries( imageBasics $&#123;OpenCV_LIBS&#125; )         <span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>]]></content>
      
      
      <categories>
          
          <category> Opencv </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Opencv </tag>
            
            <tag> Harris </tag>
            
            <tag> SIFT </tag>
            
            <tag> SURF </tag>
            
            <tag> ORB </tag>
            
            <tag> FAST </tag>
            
            <tag> BRIEF描述子 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>服务器开发与工具使用（vscode,anaconda,DL等）</title>
      <link href="/2023/03/31/net/"/>
      <url>/2023/03/31/net/</url>
      
        <content type="html"><![CDATA[<h1 id="服务器"><a href="#服务器" class="headerlink" title="服务器"></a>服务器</h1><p>首先拥有一个属于自己的服务器账号，包括服务器ip，端口，用户名以及密码等。</p><p>接着，需要安装ssh,一般linux系统都自带ssh，windows一般自带ssh客户端。如未安装，可查看以下教程安装</p><p><a href="https://blog.csdn.net/weixin_50964512/article/details/123588745">ssh安装与配置，详解版</a></p><p><a href="https://blog.csdn.net/qq_33594636/article/details/128849482">Windows安装和启动SSH服务</a></p><h2 id="软件安装"><a href="#软件安装" class="headerlink" title="软件安装"></a>软件安装</h2><p><strong>xftp7以及xshell7的安装</strong></p><ul><li><p>Xftp7是一个功能强大但轻量级的SFTP&#x2F;FTP客户机，用于需要在网络上安全地传输文件的用户。通过使用 拖放、直接编辑、增强的同步、传输调度和更直观的选项卡界面等特性，文件传输得到了简化。</p></li><li><p>Xshell 是一个强大的安全终端模拟软件，它支持SSH1、SHH2、以及 Microsoft Windows 平台的TELNET协议。Xshell 通过互联网到远程主机的安全连接以及它创新性的设计和特色帮助用户在复杂的网络环境中享受他们的工作。</p></li><li><p>Xshell 可以在 Windows 界面下用来访问远端不同系统下的服务器，从而比较好的达到远程控制终端的目的。除此之外，其还有丰富的外观配色方案以及样式选择。</p></li></ul><p>下载需要填个邮箱，之后它会给你的邮箱发个下载链接，点击下载链接安装即可。</p><p><a href="https://www.xshell.com/zh/free-for-home-school/">学生版下载网址</a></p><p><a href="https://www.xshell.com/">官网网址</a></p><p><strong>vscode软件安装</strong></p><p>Visual Studio Code 简称 VSCode ，2015 年由微软公司发布。</p><p>可用于 Windows，macOS 和 Linux。它具有对 JavaScript，TypeScript 和 Node.js 的内置支持，并具有丰富的其他语言（例如 C++，C＃，Java，Python，PHP，Go</p><p><a href="https://code.visualstudio.com/">vscode下载官网</a></p><p><a href="https://blog.csdn.net/weixin_44950987/article/details/128129613">vscode安装教程</a></p><p><strong>vscode链接远程服务器</strong></p><p>在vscode软件里安装插件<em><strong>Remote-SSH</strong></em>，安装完成后需要添加服务器连接配置，具体操作如下链接：</p><p><a href="https://blog.csdn.net/zhaxun/article/details/120568402">vscode连接远程服务器</a></p><blockquote><p>总结：xftp7用来传数据，xshell7与vscode都可以通过命令行形式对远程服务器进行操控，其中vscode依靠它强大的性能可以直接对服务器中个人程序进行图形化编写，运行，调试等</p></blockquote><h1 id="服务器中开发环境配置"><a href="#服务器中开发环境配置" class="headerlink" title="服务器中开发环境配置"></a>服务器中开发环境配置</h1><h2 id="Anaconda-x2F-miniconda安装"><a href="#Anaconda-x2F-miniconda安装" class="headerlink" title="Anaconda&#x2F;miniconda安装"></a>Anaconda&#x2F;miniconda安装</h2><p>对于需要进行深度学习的同学，需要安装Anaconda以管理自己的服务器环境，对于本实验室的服务器，可直接访问&#x2F;share&#x2F;software来获得此前下载过的Anaconda各版本</p><p>如果想自己安装不同版本的anaconda，请查看如下教程：</p><p><a href="https://zhuanlan.zhihu.com/p/32925500">Anaconda介绍，安装及使用教程</a></p><p><a href="https://blog.csdn.net/qq_42257666/article/details/121383450">anaconda安装配置教程</a></p><p><strong>conda是Anaconda提供的一个管理版本和Python环境的工具，我们一般都使用它创建虚拟环境，能够隔离不同的Python软件包环境，也不会影响到其他用户。强烈建议每个用户单独将它安装在用户目录用于Python环境管理。不建议使用系统自带的Python环境。</strong></p><p><a href="https://zhuanlan.zhihu.com/p/537439636">conda的使用,创建虚拟环境</a></p><p>安装完成之后，可以替换镜像源为清华源，安装软件包时速度一般能更快，换源教程如下：</p><p><a href="https://blog.csdn.net/qq_43115981/article/details/129064657">anaconda换源</a></p><h2 id="在自己创建的虚拟环境中安装PyTorch"><a href="#在自己创建的虚拟环境中安装PyTorch" class="headerlink" title="在自己创建的虚拟环境中安装PyTorch"></a>在自己创建的虚拟环境中安装PyTorch</h2><p>参照官网，选择好对应的cuda版本和python语言以及linux系统即可。</p><p>注意使用conda安装时，会自动安装cudatoolkit与cudnn，</p><p><a href="https://pytorch.org/">pytorch官网</a></p><p><a href="https://pytorch.org/get-started/previous-versions/">pytorch以往版本</a></p><p>安装好pytorch后，就可以开始自己的深度学习编程之旅了~~~</p><h1 id="深度学习教程"><a href="#深度学习教程" class="headerlink" title="深度学习教程"></a>深度学习教程</h1><p>这里推荐一些深度学习教程</p><p><a href="https://zh-v2.d2l.ai/">动手学深度学习</a></p><p><a href="hhttps://www.bilibili.com/video/BV1FT4y1E74V/?p=3&vd_source=c3ce2553ecbf3f37d2f4be6f466233fa">吴恩达深度学习</a></p><p><a href="https://www.bilibili.com/video/BV1nJ411z7fe/?spm_id_from=333.337.search-card.all.click&vd_source=c3ce2553ecbf3f37d2f4be6f466233fa">斯坦福李飞飞cs231n计算机视觉课程【附中文字幕】</a></p><p>强烈推荐B站UP主：<a href="https://space.bilibili.com/18161609/channel/series">霹雳吧啦Wz</a></p><p>他对深度学习基础的网络进行了详细的讲解并附上源码，包括图像分类、语义分割、关键点检测、目标检测，实例分割等，强烈推荐学习</p><p><a href="https://github.com/WZMIAOMIAO/deep-learning-for-image-processing">源码地址</a></p><p>另外推荐一些开源的深度学习开源工具箱，零基础上手就可以跑起来</p><p><a href="https://github.com/open-mmlab">OpenMMLab 平台</a></p><p><a href="https://github.com/open-mmlab/mmdetection">MMDetection基于 PyTorch 的目标检测开源工具箱</a></p><p><a href="https://github.com/open-mmlab/mmyolo">mmyolo基于 PyTorch 和 MMDetection 的 YOLO 系列算法开源工具箱</a></p>]]></content>
      
      
      <categories>
          
          <category> net </category>
          
          <category> DL </category>
          
      </categories>
      
      
        <tags>
            
            <tag> net </tag>
            
            <tag> ssh </tag>
            
            <tag> DL </tag>
            
            <tag> vscode </tag>
            
            <tag> anaconda </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>ROS介绍及快速体验</title>
      <link href="/2023/03/30/ros/"/>
      <url>/2023/03/30/ros/</url>
      
        <content type="html"><![CDATA[<h1 id="ROS概念"><a href="#ROS概念" class="headerlink" title="ROS概念"></a>ROS概念</h1><p><strong>ROS全称Robot Operating System(机器人操作系统)</strong></p><ul><li><p>ROS是适用于机器人的开源元操作系统</p></li><li><p>ROS集成了大量的工具，库，协议，提供类似OS所提供的功能，简化对机器人的控制</p></li><li><p>还提供了用于在多台计算机上获取，构建，编写和运行代码的工具和库，ROS在某些方面类似于“机器人框架”</p></li><li><p>ROS设计者将ROS表述为“ROS &#x3D; Plumbing + Tools + Capabilities + Ecosystem”，即ROS是通讯机制、工具软件包、机器人高层技能以及机器人生态系统的集合体</p></li></ul><p><img src="/pic/%E9%80%89%E5%8C%BA_028.png"></p><h1 id="ROS框架"><a href="#ROS框架" class="headerlink" title="ROS框架"></a>ROS框架</h1><p>ROS 框架主要分成三个层级，分别是 ROS 文件系统、ROS 计算图和 ROS 社区。</p><h2 id="ROS文件系统"><a href="#ROS文件系统" class="headerlink" title="ROS文件系统"></a>ROS文件系统</h2><p>ROS 的文件系统主要介绍了硬盘上 ROS 文件的组织形式。其中，我们必须了解的主要有以下几个方面：</p><ul><li>软件包（Package）：ROS 软件包是 ROS 软件框架的独立单元。ROS 软件包可能包含源代码、第三方软件库、配置文件等。ROS 软件包可以复用和共享。</li><li>软件包清单（Package Manifest）：清单文件（package.xml）列出了软件包的所有详细信息，包括名称、描述、许可信息以及最重要的依赖关系。</li><li>消息（msg）类型：消息的描述存储在软件包的 msg 文件夹下。ROS 消息是一组通过 ROS 的消息传递系统进行数据发送的数据结构。消息的定义存储在扩展名为 .msg 的文件里。</li><li>服务（srv）类型：服务的描述使用扩展名 .srv 存储在 srv 文件夹下。该文件定义了 ROS 内服务请求和响应的数据结构。</li></ul><p><img src="/pic/%E9%80%89%E5%8C%BA_029.png"></p><pre class="line-numbers language-none"><code class="language-none">WorkSpace --- 自定义的工作空间    |--- build:编译空间，用于存放CMake和catkin的缓存信息、配置信息和其他中间文件。    |--- devel:开发空间，用于存放编译后生成的目标文件，包括头文件、动态&amp;静态链接库、可执行文件等。    |--- src: 源码        |-- package：功能包(ROS基本单元)包含多个节点、库与配置文件，包名所有字母小写，只能由字母、数字与下划线组成            |-- CMakeLists.txt 配置编译规则，比如源文件、依赖项、目标文件            |-- package.xml 包信息，比如:包名、版本、作者、依赖项...(以前版本是 manifest.xml)            |-- scripts 存储python文件            |-- src 存储C++源文件            |-- include 头文件            |-- msg 消息通信格式文件            |-- srv 服务通信格式文件            |-- action 动作格式文件            |-- launch 可一次性运行多个节点             |-- config 配置信息        |-- CMakeLists.txt: 编译的基本配置<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h2 id="ROS计算图"><a href="#ROS计算图" class="headerlink" title="ROS计算图"></a>ROS计算图</h2><p>ros 程序运行之后，不同的节点之间是错综复杂的，ROS 中提供了一个实用的工具:rqt_graph。</p><p>rqt_graph能够创建一个显示当前系统运行情况的动态图形。ROS 分布式系统中不同进程需要进行数据交互，计算图可以以点对点的网络形式表现数据交互过程。rqt_graph是rqt程序包中的一部分。</p><p>ROS 计算图中的基本功能包括节点、ROS 控制器、参数服务器、消息和服务：</p><ul><li><strong>节点（Node）</strong>：ROS 节点是使用 ROS 功能处理数据的进程。节点的基本功能是计算。例如，节点可以对激光扫描仪数据进行处理，以检查是否存在碰撞。ROS 节点的编写需要 ROS 客户端库文件（如roscpp和rospy）的支持。</li><li><strong>ROS 控制器（Master）</strong>：ROS 节点可以通过名为 ROS 控制器的程序相互连接。此程序提供计算图其他节点的名称、注册和查找信息。如果不运行这个控制器，节点之间将无法相互连接和发送消息。</li><li><strong>参数服务器（Parameter server）</strong>：ROS 参数是静态值，存储在叫作参数服务器的全局位置。所有节点都可以从参数服务器访问这些值。我们甚至可以将参数服务器的范围设置为 private 以访问单个节点，或者设置为 public 以访问所有节点。</li><li><strong>ROS主题（Topic）</strong>：ROS 节点使用命名总线（叫作 ROS 主题）彼此通信。数据以消息的形式流经主题。通过主题发送消息称为发布，通过主题接收数据称为订阅。</li><li><strong>消息（Message）</strong>：ROS 消息是一种数据类型，可以由基本数据类型（如整型、浮点型、布尔类型等）组成。ROS 消息流经 ROS 主题。一个主题一次只能发送&#x2F;接收一种类型的消息。我们可以创建自己的消息定义并通过主题发送它。</li><li><strong>服务（Service）</strong>：我们看到使用 ROS 主题的发布&#x2F;订阅模型是一种非常灵活的通信模式，这是一种一对多的通信模式，意味着一个主题可以被任意数量的节点订阅。在某些情况下，可能还需要一种<strong>请求&#x2F;应答</strong>类型的交互方式，它可以用于分布式系统。这种交互方式可以使用 ROS 服务实现。ROS 服务的工作方式与 ROS 主题类似，因为它们都有消息类型定义。使用该消息定义可以将服务请求发送到另一个提供该服务的节点。服务的结果将作为应答发送。该节点必须等待，直到从另一个节点接收到结果。</li><li><strong>ROS 消息记录包（Bag）</strong>：这是一种用于保存和回放 ROS 主题的文件格式。ROS 消息记录包是记录传感器数据和处理数据的重要工具。这些包之后可以用于离线测试算法</li></ul><blockquote><p>演示</p></blockquote><p>首先，按照前面所示，运行案例</p><p>然后，启动新终端，键入: rqt_graph 或 rosrun rqt_graph rqt_graph，可以看到类似下图的网络拓扑图，该图可以显示不同节点之间的关系。</p><p><img src="/pic/%E9%80%89%E5%8C%BA_030.png"></p><h2 id="ROS快速体验"><a href="#ROS快速体验" class="headerlink" title="ROS快速体验"></a>ROS快速体验</h2><h3 id="HelloWorld实现简介"><a href="#HelloWorld实现简介" class="headerlink" title="HelloWorld实现简介"></a>HelloWorld实现简介</h3><p>ROS中涉及的编程语言以C++和Python为主，ROS中的大多数程序两者都可以实现,ROS中的程序即便使用不同的编程语言，实现流程也大致类似，以当前HelloWorld程序为例，实现流程大致如下：</p><ul><li>先创建一个工作空间；</li><li>再创建一个功能包；</li><li>编辑源文件；</li><li>编辑配置文件；</li><li>编译并执行。</li></ul><p><strong>1.创建工作空间并初始化</strong></p><pre class="line-numbers language-none"><code class="language-none">mkdir -p 自定义空间名称&#x2F;srccd 自定义空间名称catkin_make<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre><p><strong>2.进入 src 创建 ros 包并添加依赖</strong></p><pre class="line-numbers language-none"><code class="language-none">cd srccatkin_create_pkg 自定义ROS包名 roscpp rospy std_msgs<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><p>上述命令，会在工作空间下生成一个功能包，该功能包依赖于** <em>roscpp、rospy 与 std_msgs</em>**，其中roscpp是使用C++实现的库，而rospy则是使用python实现的库，std_msgs是标准消息库，创建ROS功能包时，一般都会依赖这三个库实现。</p><p><em><strong>补充</strong></em> ：在ROS中，虽然实现同一功能时，C++和Python可以互换，但是具体选择哪种语言，需要视需求而定，因为两种语言相较而言:C++运行效率高但是编码效率低，而Python则反之，基于二者互补的特点，ROS设计者分别设计了roscpp与rospy库，前者旨在成为ROS的高性能库，而后者则一般用于对性能无要求的场景，旨在提高开发效率。</p><h3 id="HelloWorld-C-版"><a href="#HelloWorld-C-版" class="headerlink" title="HelloWorld(C++版)"></a>HelloWorld(C++版)</h3><p><strong>1.进入 ros 包的 src 目录编辑源文件</strong></p><pre class="line-numbers language-none"><code class="language-none">cd 自定义的包<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>C++源码实现(文件名自定义)</p><pre class="line-numbers language-c++" data-language="c++"><code class="language-c++">#include &quot;ros&#x2F;ros.h&quot;int main(int argc, char *argv[])&#123;    &#x2F;&#x2F;执行 ros 节点初始化    ros::init(argc,argv,&quot;hello&quot;);    &#x2F;&#x2F;创建 ros 节点句柄(非必须)    ros::NodeHandle n;    &#x2F;&#x2F;控制台输出 hello world    ROS_INFO(&quot;hello world!&quot;);    return 0;&#125;<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p><strong>2.编辑 ros 包下的 Cmakelist.txt文件</strong></p><pre class="line-numbers language-c++" data-language="c++"><code class="language-c++">add_executable(步骤3的源文件名  src&#x2F;步骤3的源文件名.cpp)target_link_libraries(步骤3的源文件名  $&#123;catkin_LIBRARIES&#125;)<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p><strong>3.进入工作空间目录并编译</strong></p><pre class="line-numbers language-none"><code class="language-none">cd 自定义空间名称catkin_make<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><p>生成 build devel ….</p><p><strong>4.执行</strong></p><p><em>先启动命令行1：</em></p><pre class="line-numbers language-none"><code class="language-none">roscore<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p><em>再启动命令行2：</em></p><pre class="line-numbers language-none"><code class="language-none">cd 工作空间source .&#x2F;devel&#x2F;setup.bashrosrun 包名 C++节点<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre><p>命令行输出: HelloWorld!</p><p><em><strong>补充</strong></em>：source ~&#x2F;工作空间&#x2F;devel&#x2F;setup.bash可以添加进.bashrc文件，使用上更方便</p><p><strong>添加方式1: 直接使用 gedit 或 vi 编辑 .bashrc 文件，最后添加该内容</strong></p><p><strong>添加方式2:echo “source ~&#x2F;工作空间&#x2F;devel&#x2F;setup.bash” &gt;&gt; ~&#x2F;.bashrc</strong></p><blockquote><p>参考文献</p></blockquote><p><a href="http://c.biancheng.net/view/9853.html">ROS机器人操作系统简介</a></p><p><a href="http://www.autolabor.com.cn/book/ROSTutorials/chapter1/15-ben-zhang-xiao-jie/153-rosji-suan-tu.html">机器人入门教程</a></p>]]></content>
      
      
      <categories>
          
          <category> ROS </category>
          
          <category> c++ </category>
          
      </categories>
      
      
        <tags>
            
            <tag> c++ </tag>
            
            <tag> ros </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>RANSAC、LMEDS以及Hough变换原理</title>
      <link href="/2023/03/28/ransac/"/>
      <url>/2023/03/28/ransac/</url>
      
        <content type="html"><![CDATA[<h1 id="RANSAC算法简介"><a href="#RANSAC算法简介" class="headerlink" title="RANSAC算法简介"></a>RANSAC算法简介</h1><p>RANSAC是”RANdom SAmple Consensus”（随机采样一致）的缩写。它可以从一组包含“局外点”的观测数据集中，通过迭代方式估计数学模型的参数，它是一种不确定的算法—-它有一定的概率得出一个合理的结果；为了提高概率必须提高迭代次数。</p><blockquote><p>RANSAC基本假设</p></blockquote><ul><li>数据由<strong>局内点</strong>组成， 例如，数据的分布可以用一些模型（比如直线方程）参数来解释；</li><li><strong>局外点</strong>是不能适应该模型的参数；</li><li>除此之外的数据属于噪声；</li></ul><p>局外点产生的原因有：噪声的极值；错误的测量方法；对数据的错误假设等；</p><blockquote><p>RANSAC概述</p></blockquote><p>RANSAC算法的输入时一组观测数据，一个可以解释或者适应于观测数据的参数化模型，一些可行的参数。</p><p>RANSAC通过反复选择数据中的一组随机子集来达成目标。被选取的子集被假设为局内点，并用下述方法进行验证：</p><ul><li>有一个模型适应于假定的局内点，即所有的未知参数都能从假设的局内点计算得出；</li><li>用1中得到的模型去测试所有的其它数据，如果某个点适用于估计的模型，认为他也是局内点；</li><li>如果有足够多的点呗归类为假设的局内点，那么估计的模型就足够合理；</li><li>然后，用所有假设的局内点去重新估计模型，因为它仅仅被初始的假设局内点估计过；</li><li>最后，通过估计局内点与模型的错误率来评估模型；</li></ul><p>这个过程被重复执行固定的次数，每次产生的模型要么因为局内点太少而被抛弃，要么因为比现有的模型更好而被选用。</p><p>关于模型好坏算法实现上有两种方式：</p><ul><li>规定一个点数，达到这个点数后，算这些点与模型间的误差，找误差最小的模型。 对应下面算法一</li><li>规定一个误差，找匹配模型并小于这个误差的所有点，匹配的点最多的模型，就是最好模型。 对应下面算法二</li></ul><p>算法伪代码一：</p><pre class="line-numbers language-伪代码" data-language="伪代码"><code class="language-伪代码">输入：data ---- 一组观测数据model ---- 适应于数据的模型n ---- 适用于模型的最少数据个数k ---- 算法的迭代次数t ---- 用于决定数据是否适应于模型的阈值d ---- 判定模型是否适用于数据集的数据数目输出：best_model —— 跟数据最匹配的模型参数（如果没有找到，返回null）best_consensus_set —— 估计出模型的数据点best_error —— 跟数据相关的估计出的模型的错误iterations &#x3D; 0best_model &#x3D; nullbest_consensus_set &#x3D; nullbest_error &#x3D; 无穷大while( iterations &lt; k )    maybe_inliers &#x3D;  从数据集中随机选择n个点    maybe_model &#x3D; 适合于maybe_inliers的模型参数    consensus_set &#x3D; maybe_inliers    for (每个数据集中不属于maybe_inliers的点)        if （如果点适合于maybe_model，并且错误小于t）           将该点添加到consensus_set    if (consensus_set中的点数大于d)        已经找到了好的模型， 现在测试该模型到底有多好       better_model &#x3D; 适用于consensus_set中所有点的模型参数       this_error &#x3D;  better_model 究竟如何适合这些点的度量        if （this_error &lt; best_error）        发现比以前好的模型，保存该模型直到更好的模型出现        best_model &#x3D; better_model        best_consensus_set &#x3D; consensus_set        best_error &#x3D; this_error    iterations ++函数返回best_model, best_consensus_set, best_error<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>RANSAC算法的可能变化包括以下几种：</p><ul><li>如果发现一种足够好的模型（该模型有足够下的错误率）， 则跳出主循环，这样节约不必要的计算；设置一个错误率的阈值，小于这个值就跳出循环；</li><li>可以直接从maybe_model计算this_error，而不从consensus_set重新估计模型，这样可能会节约时间，但是可能会对噪音敏感。</li></ul><p>算法伪代码二：</p><pre class="line-numbers language-伪代码" data-language="伪代码"><code class="language-伪代码">输入：data ---- 一组观测数据numForEstimate ----- 初始模型需要的点数delta ------ 判定点符合模型的误差probability ----- 表示迭代过程中从数据集内随机选取出的点均为局内点的概率输出：best_model —— 跟数据最匹配的模型参数（如果没有找到，返回null）best_consensus_set —— 估计出模型的数据点k &#x3D; 1000&#x2F;&#x2F;设置初始值iterations &#x3D; 0best_model &#x3D; nullbest_consensus_set &#x3D; nullwhile( iterations &lt; k )    maybe_inliers &#x3D;  从数据集中随机选择numForEstimate个点    maybe_model &#x3D; 适合于maybe_inliers的模型参数，比如直线，取两个点，得直线方程    for (每个数据集中不属于maybe_inliers的点)        if （如果点适合于maybe_model，并且错误小于delta）            将该点添加到maybe_inliers    if(maybe_inliers的点数 &gt; best_consensus_set 的点数）&#x2F;&#x2F;找到更好的模型        best_model &#x3D; maybe_model        best_consensus_set  &#x3D; maybe_inliers        根据公式k&#x3D;log(1-p)&#x2F;log(1-pow(w,n))重新计算k    iterations ++函数返回best_model, best_consensus_set,<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h2 id="RANSAC参数"><a href="#RANSAC参数" class="headerlink" title="RANSAC参数"></a>RANSAC参数</h2><p>我们不得不根据特定的问题和数据集通过实验来确定参数t和d。然而参数k（迭代次数）可以从理论结果推断。当我们从估计模型参数时，用p表示一些迭代过程中从数据集内随机选取出的点均为局内点的概率；此时，结果模型很可能有用，因此p也表征了算法产生有用结果的概率。用w表示每次从数据集中选取一个局内点的概率，如下式所示： w &#x3D; 局内点的数目 &#x2F; 数据集的数目 通常情况下，我们事先并不知道w的值，但是可以给出一些鲁棒的值。假设估计模型需要选定n个点，wn是所有n个点均为局内点的概率；1 − wn是n个点中至少有一个点为局外点的概率，此时表明我们从数据集中估计出了一个不好的模型。 (1 − wn)k表示算法永远都不会选择到n个点均为局内点的概率，它和1-p相同。因此，<br>$$<br>1-p&#x3D;\left(1-w^n\right)^k<br>$$</p><p>其中</p><ul><li><p>p 表示置信度confidence</p></li><li><p>w 表示数据集中inlier占的比例</p></li><li><p>n 表示采样点数</p></li><li><p>k 表示需要迭代采样的最少次数</p></li><li><p>1 − wn 表示采样一次，n个点中至少有一个outlier的概率</p></li><li><p>(1 − wn)k 表示采样k次，n个点中至少有一个outlier的概率</p></li><li><p>因为p为采样k次，能有至少一次n个点都是inlier的概率</p></li><li><p>所以(1 − p)和(1 − wn)k相等时，k 为需要迭代采样的最少次数</p></li></ul><p>下面是k的解析解：</p><p>$$<br>k&#x3D;\frac{\log (1-p)}{\log \left(1-w^n\right)}<br>$$</p><p>值得注意的是，这个结果假设n个点都是独立选择的；也就是说，某个点被选定之后，它可能会被后续的迭代过程重复选定到。这种方法通常都不合理，由此推导出的k值被看作是选取不重复点的上限。例如，要从上图中的数据集寻找适合的直线，RANSAC算法通常在每次迭代时选取2个点，计算通过这两点的直线maybe_model，要求这两点必须唯一。</p><p>为了得到更可信的参数，标准偏差或它的乘积可以被加到k上。k的标准偏差定义为：</p><p>$$<br>S D(k)&#x3D;\frac{\sqrt{1-w^n}}{w^n}<br>$$</p><blockquote><p>RANSAC的函数接口 参照opencv来说主要需要3-4个参数（第四个不是必须的）</p></blockquote><ul><li>误差阈值ransacThreshold：区分inlier和outliner的依据</li><li>置信度confidence：设置之后代表RANSAC采样n次过程中会出现（至少一次）采样点数据集中的点都为内点的概率。这个值设置的太大，会增加采样次数。太小，会使结果不太理想。一般取0.95-0.99</li><li>最大采样迭代次数maxIters：为了防止一直在采样计算</li><li>最大精细迭代次数refineIters：在采样之后，选取最优解。可以增加精确优化，比如使用LM算法获得更优解。</li></ul><p>以上4个参数，有三个是经验值。其中最大采样迭代次数maxIters是可以有数值解的。</p><h2 id="RANSAC优点与缺点"><a href="#RANSAC优点与缺点" class="headerlink" title="RANSAC优点与缺点"></a>RANSAC优点与缺点</h2><p>RANSAC的优点是它能鲁棒的估计模型参数。例如，它能从包含大量局外点的数据集中估计出高精度的参数。</p><p>RANSAC的缺点是它计算参数的迭代次数没有上限；如果设置迭代次数的上限，得到的结果可能不是最优的结果，甚至可能得到错误的结果。RANSAC只有一定的概率得到可信的模型，概率与迭代次数成正比。RANSAC的另一个缺点是它要求设置跟问题相关的阀值。</p><p>RANSAC只能从特定的数据集中估计出一个模型，如果存在两个（或多个）模型，RANSAC不能找到别的模型。如果有多个模型，可以先估算出一个，然后用剩余的数据重新运算，重复这个过程，直到没有模型。</p><h2 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h2><p><a href="https://www.cnblogs.com/xrwang/archive/2011/03/09/ransac-1.html">王先荣RANSAC介绍</a></p><p><a href="https://zhuanlan.zhihu.com/p/402727549">RANSAC详解，保姆级教程</a></p><p><a href="https://zhuanlan.zhihu.com/p/45532306">计算机视觉基本原理</a></p><h2 id="RANSAC应用"><a href="#RANSAC应用" class="headerlink" title="RANSAC应用"></a>RANSAC应用</h2><p>OpenCV中使用RANSAC算法实现多张图像拼接思路：</p><ul><li><p>获取图像的特征点，将每张图片的特征点保存到一个vector中；</p></li><li><p>通过特征点匹配的方法，找到每张图片的共有特征点，并将其保存到一个vector中；</p></li><li><p>通过RANSAC算法求解出拼接的变换矩阵；</p></li><li><p>根据变换矩阵对每张图片进行仿射变换；</p></li><li><p>将拼接后的图片进行裁剪；</p></li><li><p>将裁剪后的图片拼接起来，最终得到拼接后的图片。</p></li></ul><p><a href="https://blog.csdn.net/qq_39312146/article/details/129053592">OpenCV中使用RANSAC算法实现多张图像拼接</a></p><p>OpenCV中的solvePnPRansac函数和findHomography函数都具有RANSAC特性，该特性使算法对少量的错误数据鲁棒。<br>这两个函数利用RANSACPointSetRegistrator类实现RANSAC算法，但这个类并没有对外开放，因此只能通过阅读OpenCV源代码学习RANSAC算法的实现和使用。</p><p>类的实现在ptsetreg.cpp中，可通过调用precomp.hpp文件中的createRANSACPointSetRegistrator函数使用。此外，该文件还提供了createLMeDSPointSetRegistrator函数调用最小中值算法。</p><p><a href="https://blog.csdn.net/HopefulLight/article/details/78775974">在OpenCV中使用RANSAC</a></p><h1 id="LMEDS算法概述（最小中值法：Least-Median-of-Squares）"><a href="#LMEDS算法概述（最小中值法：Least-Median-of-Squares）" class="headerlink" title="LMEDS算法概述（最小中值法：Least Median of Squares）"></a>LMEDS算法概述（最小中值法：Least Median of Squares）</h1><blockquote><p>经典步骤</p></blockquote><ul><li><p>随机采样</p></li><li><p>计算模型参数</p></li><li><p>计算相对模型的点集偏差err，并求出偏差中值Med(err)</p></li><li><p>迭代2. 3.步直至获得符合阈值的最优解：Med(err)最小</p></li><li><p>精确优化模型参数（LM算法迭代优化）</p></li></ul><blockquote><p>LMedS的函数接口 参照opencv来说主要需要2-3个参数（第三个不是必须的）</p></blockquote><ul><li><p>置信度confidence：设置之后代表RANSAC采样n次过程中会出现（至少一次）采样点数据集中的点都为内点的概率，这个值设置的太大，会增加采样次数。太小，会使结果不太理想。一般取0.95-0.99</p></li><li><p>最大采样迭代次数maxIters：为了防止一直在采样计算</p></li><li><p>最大精细迭代次数refineIters：在采样之后，选取最优解。可以增加精确优化，比如使用LM算法获得更优解。</p></li></ul><p>注意： 相对于RANSAC，LMedS有一个优点：不需要指定 - 误差阈值ransacThreshold：区分inlier和outliner的依据</p><blockquote><p>RANSAC与LMEDS两者的区别</p></blockquote><p><em><strong>RANSAC的阈值在具有物理意义或者几何意义的时候比较容易确定，但是当阈值不具有这些特征的时候，就成了一个不太好调整的参数了。这时LMedS可以自适应迭代获得最优解。</strong></em></p><blockquote><p>此外，LMedS也能自适应获得inlier和outliner,公式如下：</p></blockquote><p>$$<br>\hat{\sigma}&#x3D;1.4826\left(1+\frac{5}{n-p}\right) \operatorname{med}_i \sqrt{r_i^2}<br>$$</p><p>其中</p><ul><li><p>n 表示点集的个数</p></li><li><p>p 表示计算模型一次采样的点个数</p></li><li><p>ri2  表示误差</p></li><li><p>med(ri2 ) 表示误差中值</p></li></ul><p> 筛选条件为：</p> <!-- $$w_i=\left\{\begin{array}{cc}1 & \frac{\left|r_i\right|}{\hat{\sigma}} \leq 2.5 \\ 0 & \frac{\left|r_i\right|}{\hat{\sigma}}>2.5\end{array}\right.$$ --><p><img src="/pic/%E9%80%89%E5%8C%BA_027.png"></p><p><strong>由于LMedS会需要对整个点集的err求中值，当点集很大的时候，求中值的过程会很消耗时间</strong></p><p><a href="https://blog.csdn.net/billbliss/article/details/78592216">RANSAC LMedS 详细分析</a></p><h1 id="霍夫变换-Hough"><a href="#霍夫变换-Hough" class="headerlink" title="霍夫变换(Hough)"></a>霍夫变换(Hough)</h1><p><strong>霍夫变换</strong>是一种特征提取(feature extraction)，被广泛应用在图像分析（image analysis）、计算机视觉(computer vision)以及数位影像处理(digital image processing)。霍夫变换是用来辨别找出物件中的特征，例如：线条。他的算法流程大致如下，给定一个物件、要辨别的形状的种类，算法会在参数空间(parameter space)中执行投票来决定物体的形状，而这是由累加空间(accumulator space)里的局部最大值(local maximum)来决定。</p><p>经典的霍夫变换是侦测图片中的直线，之后，霍夫变换不仅能识别直线，也能够识别任何形状，常见的有圆形、椭圆形。1981年，因为DanaH.Ballard的一篇期刊论文”Generalizing the Hough transform to detect arbitrary shapes”，让霍夫变换开始流行于计算机视觉界。</p><p><a href="https://zhuanlan.zhihu.com/p/47649796">霍夫变换-神奇的特征提取方法</a></p><p><a href="https://zhuanlan.zhihu.com/p/203292567">通俗易懂-霍夫变换原理</a></p><p><a href="https://zhuanlan.zhihu.com/p/386048978">霍夫变换及代码实现</a></p>]]></content>
      
      
      <categories>
          
          <category> Opencv </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Opencv </tag>
            
            <tag> RANSAC </tag>
            
            <tag> LMEDS </tag>
            
            <tag> Hough </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>视觉及其融合惯性SLAM技术发展综述</title>
      <link href="/2023/03/27/slam/"/>
      <url>/2023/03/27/slam/</url>
      
        <content type="html"><![CDATA[<h1 id="SLAM技术"><a href="#SLAM技术" class="headerlink" title="SLAM技术"></a>SLAM技术</h1><p>同步定位与建图（Simultaneous Localization and Mapping，简称SLAM）问题可以描述为：机器人在未知环境中从一个未知位置开始移动,在移动过程中根据位置和地图进行自身定位，同时在自身定位的基础上建造增量式地图，实现机器人的自主定位和导航。</p><blockquote><p>SLAM 技术主要呈现以下 3 点发展趋势。</p></blockquote><ul><li>理论优化改进：由于应用场景需求的多样化，结合惯性、异源图像等多传感器的信息融合模式成为 SLAM 主流，促进了以紧耦合为主的信息融合理论发展，而随着大场景 SLAM 应用需求及图优化理论的推进，逐步形成了<strong>基于扩展卡尔曼滤波</strong>框架的改进滤波器优化架构，和以<strong>光束法平差（BA）</strong>为主的非线性优化架构两种研究趋势。</li><li>新型技术引入：随着<strong>深度学习</strong>技术在计算机视觉中的广泛应用，视觉 SLAM 呈现出由传统几何变换方式逐步转向结合深度学习的智能融合趋势。一方面<strong>视觉图像与语义信息</strong>的紧密联系，使得集成语义信息的视觉 SLAM 得到更多探索；另一方面为减少对传统方式依赖，利用<strong>神经网络架构</strong>替代 SLAM 的部分模块或端到端<strong>强化学习</strong>的模式得以广泛研究。</li><li>应用领域推广：视觉 SLAM 目前在<em>智能家居、自动驾驶、无人机</em>等领域得到了不同层次的应用，随着硬件性能的提升，视觉 SLAM</li></ul><p><img src="/pic/%E5%9B%BE%E7%89%871.png" alt="视觉、惯性SLAM系统框架结构"></p><p><img src="/pic/%E5%9B%BE%E7%89%872.png" alt="视觉SLAM构建地图类型"></p><p><img src="/pic/%E5%9B%BE%E7%89%873.png"></p><p><img src="/pic/%E5%9B%BE%E7%89%874.png"></p><p><img src="/pic/%E5%9B%BE%E7%89%875.png"></p><p><img src="/pic/%E5%9B%BE%E7%89%876.png"></p><p><img src="/pic/%E5%9B%BE%E7%89%877.png" alt="标准视觉SLAM Pipeline"></p><p><img src="/pic/%E5%9B%BE%E7%89%878.png" alt="极具影响力的视觉SLAM方法"></p><blockquote><p>VSLAM 常用数据集：表内的GT 是指真值的可用性</p></blockquote><p><img src="/pic/%E5%9B%BE%E7%89%879.png"></p><p><img src="/pic/%E5%9B%BE%E7%89%8710.png" alt="各种论文中用于评估的一些主流视觉SLAM数据集的实例"></p>]]></content>
      
      
      
        <tags>
            
            <tag> slam </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>图像变换(image_transformer)</title>
      <link href="/2023/03/27/image-transformer/"/>
      <url>/2023/03/27/image-transformer/</url>
      
        <content type="html"><![CDATA[<h1 id="图像变换"><a href="#图像变换" class="headerlink" title="图像变换"></a>图像变换</h1><p>基本的图像变换有<strong>刚性变换(等距变换、欧式变换)、相似变换、仿射变换、射影变换(透视变换、投影变换)</strong></p><blockquote><p>刚性变化：只对图像进行平移与旋转，形状保持不变</p></blockquote><p>欧式变换（等距变换）保持了向量的<strong>长度和夹角</strong>，相当于我们把一个刚体原封不动地进行移动或旋转，不改变它自身的样子</p><p><img src="/pic/%E9%80%89%E5%8C%BA_020.png" alt="刚体变换矩阵"></p><blockquote><p>相似变换： 等距变换与一个均匀缩放的复合；等距变换+ 均匀缩放，类似相似三角形，比例不变</p></blockquote><p>相似变换比欧氏变换多了一个自由度，它允许物体进行均匀的放缩，其矩阵表示形式为：</p><p><img src="/pic/%E9%80%89%E5%8C%BA_021.png" alt="相似变换矩阵"></p><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">import</span> cv2<span class="token keyword">import</span> matplotlib<span class="token punctuation">.</span>pyplot <span class="token keyword">as</span> plt<span class="token keyword">import</span> numpy <span class="token keyword">as</span> npimg <span class="token operator">=</span> cv2<span class="token punctuation">.</span>imread<span class="token punctuation">(</span><span class="token string">'/hone/chy/pic/git.png'</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span>img <span class="token operator">=</span> cv2<span class="token punctuation">.</span>cvtColor<span class="token punctuation">(</span>img<span class="token punctuation">,</span> cv2<span class="token punctuation">.</span>COLOR_BGR2RGB<span class="token punctuation">)</span><span class="token comment"># print(img.shape)</span><span class="token comment"># 得到相似变换的矩阵  # center：旋转中心 angle：旋转角度   scale：缩放比例</span>M <span class="token operator">=</span> cv2<span class="token punctuation">.</span>getRotationMatrix2D<span class="token punctuation">(</span>center <span class="token operator">=</span> <span class="token punctuation">(</span>img<span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token operator">/</span><span class="token number">2</span><span class="token punctuation">,</span>img<span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token operator">/</span><span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">,</span>                              angle <span class="token operator">=</span> <span class="token number">30</span><span class="token punctuation">,</span>                              scale <span class="token operator">=</span> <span class="token number">0.5</span><span class="token punctuation">)</span> <span class="token comment"># 原图像按照相似矩阵进行相似变换  三个参数：原图像，相似矩阵，画布面积</span>img_rotate <span class="token operator">=</span> cv2<span class="token punctuation">.</span>warpAffine<span class="token punctuation">(</span>img<span class="token punctuation">,</span> M<span class="token punctuation">,</span> <span class="token punctuation">(</span>img<span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">,</span> img<span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">)</span>plt<span class="token punctuation">.</span>figure<span class="token punctuation">(</span>figsize<span class="token operator">=</span><span class="token punctuation">(</span><span class="token number">8</span><span class="token punctuation">,</span><span class="token number">8</span><span class="token punctuation">)</span><span class="token punctuation">)</span>plt<span class="token punctuation">.</span>subplot<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span><span class="token number">2</span><span class="token punctuation">,</span><span class="token number">1</span><span class="token punctuation">)</span>plt<span class="token punctuation">.</span>imshow<span class="token punctuation">(</span>img<span class="token punctuation">)</span>plt<span class="token punctuation">.</span>subplot<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span><span class="token number">2</span><span class="token punctuation">,</span><span class="token number">2</span><span class="token punctuation">)</span>plt<span class="token punctuation">.</span>imshow<span class="token punctuation">(</span>img_rotate<span class="token punctuation">)</span>plt<span class="token punctuation">.</span>show<span class="token punctuation">(</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p><img src="/pic/%E9%80%89%E5%8C%BA_019.png" alt="相似变换结果"></p><blockquote><p>仿射变换：非齐次坐标下的一个非奇异线性变换与一个平移变换的复合，（即第三行是0,0,1）; 旋转+平移+缩放+切变，保持平行性</p></blockquote><p>仿射变换只要求 A 是一个可逆矩阵，而不必是正交矩阵。仿射变换也叫正交投影。经过仿射变换之后，立方体就不再是方的了，但是各个面仍是平行四边形 </p><ul><li>性质：Parallel lines are still parallel lines（不再具有保角性，具有保平行性）</li><li>三个非共线的点对（6 parameters）确定一个仿射变换。</li></ul><p><img src="/pic/%E9%80%89%E5%8C%BA_022.png" alt="仿射变换矩阵"></p><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">import</span> cv2<span class="token keyword">import</span> matplotlib<span class="token punctuation">.</span>pyplot <span class="token keyword">as</span> plt<span class="token keyword">import</span> numpy <span class="token keyword">as</span> np<span class="token comment"># 3 Src(原始) Points + 3 Dst(目标) Points</span><span class="token comment"># cols：列/长  rows：行/宽</span>img <span class="token operator">=</span> cv2<span class="token punctuation">.</span>imread<span class="token punctuation">(</span><span class="token string">'lenna.jpg'</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span>img <span class="token operator">=</span> cv2<span class="token punctuation">.</span>cvtColor<span class="token punctuation">(</span>img<span class="token punctuation">,</span> cv2<span class="token punctuation">.</span>COLOR_BGR2RGB<span class="token punctuation">)</span><span class="token comment"># print(img.shape)</span>cols <span class="token operator">=</span> img<span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span>rows <span class="token operator">=</span> img<span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span>pt1 <span class="token operator">=</span> np<span class="token punctuation">.</span>float32<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">,</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token punctuation">[</span>cols<span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">,</span> rows<span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">)</span>pt2 <span class="token operator">=</span> np<span class="token punctuation">.</span>float32<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token punctuation">[</span>cols<span class="token operator">*</span><span class="token number">0.3</span><span class="token punctuation">,</span> rows<span class="token operator">*</span><span class="token number">0.3</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token punctuation">[</span>cols<span class="token operator">*</span><span class="token number">0.8</span><span class="token punctuation">,</span> rows<span class="token operator">*</span><span class="token number">0.2</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token punctuation">[</span>cols<span class="token operator">*</span><span class="token number">0.1</span><span class="token punctuation">,</span> rows<span class="token operator">*</span><span class="token number">0.9</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token comment"># [[0,0], [cols, 0], [0, rows]] --> [[cols*0.3, rows*0.3], [cols*0.8, rows*0.2], [cols*0.1, rows*0.9]]</span>M <span class="token operator">=</span> cv2<span class="token punctuation">.</span>getAffineTransform<span class="token punctuation">(</span>pt1<span class="token punctuation">,</span> pt2<span class="token punctuation">)</span>       <span class="token comment"># 仿射变换矩阵</span>dst <span class="token operator">=</span> cv2<span class="token punctuation">.</span>warpAffine<span class="token punctuation">(</span>img<span class="token punctuation">,</span> M<span class="token punctuation">,</span> <span class="token punctuation">(</span>cols<span class="token punctuation">,</span> rows<span class="token punctuation">)</span><span class="token punctuation">)</span>plt<span class="token punctuation">.</span>figure<span class="token punctuation">(</span>figsize<span class="token operator">=</span><span class="token punctuation">(</span><span class="token number">8</span><span class="token punctuation">,</span><span class="token number">8</span><span class="token punctuation">)</span><span class="token punctuation">)</span>plt<span class="token punctuation">.</span>subplot<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span><span class="token number">2</span><span class="token punctuation">,</span><span class="token number">1</span><span class="token punctuation">)</span>plt<span class="token punctuation">.</span>imshow<span class="token punctuation">(</span>img<span class="token punctuation">)</span>plt<span class="token punctuation">.</span>subplot<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span><span class="token number">2</span><span class="token punctuation">,</span><span class="token number">2</span><span class="token punctuation">)</span>plt<span class="token punctuation">.</span>imshow<span class="token punctuation">(</span>dst<span class="token punctuation">)</span>plt<span class="token punctuation">.</span>show<span class="token punctuation">(</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p><img src="/pic/%E9%80%89%E5%8C%BA_024.png" alt="仿射变换结果"></p><blockquote><p>射影变换:次坐标的一般非奇异线性变换 。射影变换可以分解为相似变换，仿射变换，射影变换的复合，不保留平行性，保留重合关系、长度的交比</p></blockquote><p><img src="/pic/%E9%80%89%E5%8C%BA_026.png" alt="射影变换矩阵"></p><p>它左上角为可逆矩阵 A，右上为平移 t，左下缩放 a^{T} 。由于采用齐坐标，当 v \neq 0 时吗我们可以对整个矩阵除于 v得到一个右下角为 1 的矩阵；否则，则得到右下角为 0 的矩阵。因此，2D 的射影变换一共有8个自由度，3D则共有15个自由度。</p><ul><li>性质：Lines are still lines（不保角，不保平行，保直线性）</li><li>四个非共线的点对（8 parameters）确定一个透视变换</li></ul><p><img src="/pic/%E9%80%89%E5%8C%BA_023.png" alt="射影变换矩阵"></p><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">import</span> cv2<span class="token keyword">import</span> matplotlib<span class="token punctuation">.</span>pyplot <span class="token keyword">as</span> plt<span class="token keyword">import</span> numpy <span class="token keyword">as</span> npimg <span class="token operator">=</span> cv2<span class="token punctuation">.</span>imread<span class="token punctuation">(</span><span class="token string">'lenna.jpg'</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span>img <span class="token operator">=</span> cv2<span class="token punctuation">.</span>cvtColor<span class="token punctuation">(</span>img<span class="token punctuation">,</span> cv2<span class="token punctuation">.</span>COLOR_BGR2RGB<span class="token punctuation">)</span>width <span class="token operator">=</span> img<span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span>height <span class="token operator">=</span> img<span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span>pts1 <span class="token operator">=</span> np<span class="token punctuation">.</span>float32<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">,</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token punctuation">[</span>width<span class="token punctuation">,</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">,</span>height<span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token punctuation">[</span>width<span class="token punctuation">,</span>height<span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">)</span>pts2 <span class="token operator">=</span> np<span class="token punctuation">.</span>float32<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token punctuation">[</span>width<span class="token operator">*</span><span class="token number">0.1</span><span class="token punctuation">,</span>height<span class="token operator">*</span><span class="token number">0.1</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token punctuation">[</span>width<span class="token operator">*</span><span class="token number">0.9</span><span class="token punctuation">,</span> width<span class="token operator">*</span><span class="token number">0.1</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token punctuation">[</span>height<span class="token operator">*</span><span class="token number">0.2</span><span class="token punctuation">,</span>height<span class="token operator">*</span><span class="token number">0.8</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token punctuation">[</span>width<span class="token operator">*</span><span class="token number">0.7</span><span class="token punctuation">,</span>height<span class="token operator">*</span><span class="token number">0.7</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">)</span>M_warp <span class="token operator">=</span> cv2<span class="token punctuation">.</span>getPerspectiveTransform<span class="token punctuation">(</span>pts1<span class="token punctuation">,</span> pts2<span class="token punctuation">)</span>     <span class="token comment"># 单应性矩阵</span>img_warp <span class="token operator">=</span> cv2<span class="token punctuation">.</span>warpPerspective<span class="token punctuation">(</span>img<span class="token punctuation">,</span> M_warp<span class="token punctuation">,</span> <span class="token punctuation">(</span>width<span class="token punctuation">,</span> height<span class="token punctuation">)</span><span class="token punctuation">)</span>plt<span class="token punctuation">.</span>figure<span class="token punctuation">(</span>figsize<span class="token operator">=</span><span class="token punctuation">(</span><span class="token number">8</span><span class="token punctuation">,</span><span class="token number">8</span><span class="token punctuation">)</span><span class="token punctuation">)</span>plt<span class="token punctuation">.</span>subplot<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span><span class="token number">2</span><span class="token punctuation">,</span><span class="token number">1</span><span class="token punctuation">)</span>plt<span class="token punctuation">.</span>imshow<span class="token punctuation">(</span>img<span class="token punctuation">)</span>plt<span class="token punctuation">.</span>subplot<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span><span class="token number">2</span><span class="token punctuation">,</span><span class="token number">2</span><span class="token punctuation">)</span>plt<span class="token punctuation">.</span>imshow<span class="token punctuation">(</span>img_warp<span class="token punctuation">)</span>plt<span class="token punctuation">.</span>show<span class="token punctuation">(</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p><img src="/pic/%E9%80%89%E5%8C%BA_025.png" alt="射影变换结果"></p>]]></content>
      
      
      <categories>
          
          <category> cs231A </category>
          
      </categories>
      
      
        <tags>
            
            <tag> transformer </tag>
            
            <tag> opencv </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>对极几何(Epipolar_Geometry)</title>
      <link href="/2023/03/27/epipolar-geometry/"/>
      <url>/2023/03/27/epipolar-geometry/</url>
      
        <content type="html"><![CDATA[<h1 id="对极几何基本用处"><a href="#对极几何基本用处" class="headerlink" title="对极几何基本用处"></a>对极几何基本用处</h1><h2 id="立体匹配"><a href="#立体匹配" class="headerlink" title="立体匹配"></a>立体匹配</h2><p>对于已知两视角空间位置关系的情况下，由于对极几何这个几何模型限定的约束条件，使得在立体图像对上搜索空间上的分别在两个图像中的位置只需要相应的对极线上找，把原来的二维搜搜问题，直接简化为一维搜索，双目测距就是这方面得应用之一。</p><h2 id="确定两个摄像点的相对位置与姿态问题"><a href="#确定两个摄像点的相对位置与姿态问题" class="headerlink" title="确定两个摄像点的相对位置与姿态问题"></a>确定两个摄像点的相对位置与姿态问题</h2><p>在未知视角位置的情况下，通过搜索图像对中的匹配点，可以求得两个位置和姿态得相对关系，这一点常用在机器人导航、地图得生成、三维重建等方面。</p><blockquote><p>基本概念</p></blockquote><ul><li>极点（Epipoles）：两个相机得基线与两个成像平面得交点</li><li>极线（Epipolar Lines）：空间中点在成像平面上的投影点与极点的连线</li><li>极平面（Epipolar Plane）：空间中的点与两个相机的光轴中心点所组成的平面</li></ul><blockquote><p>本质矩阵</p></blockquote><p><img src="/pic/duiji2.png"></p><blockquote><p>对极约束(E)</p></blockquote><p><img src="/pic/duiji.png"></p><blockquote><p>基础矩阵</p></blockquote><p><img src="/pic/duiji3.png"></p><blockquote><p>对极约束(F)</p></blockquote><p><img src="/pic/duiji4.png"></p>]]></content>
      
      
      <categories>
          
          <category> cs231A </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 对极几何 </tag>
            
            <tag> 对极约束 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Git 基础用法</title>
      <link href="/2023/03/26/git/"/>
      <url>/2023/03/26/git/</url>
      
        <content type="html"><![CDATA[<h1 id="Git介绍"><a href="#Git介绍" class="headerlink" title="Git介绍"></a>Git介绍</h1><p>Git是一个开源的<strong>分布式版本控制系统</strong>，可以有效、高速地处理从很小到非常大的项目版本管理。也是Linus Torvalds为了帮助管理Linux内核开发而开发的一个开放源码的版本控制软件。与常用的版本控制工具<strong>CVS, Subversion</strong>等不同，它采用了<strong>分布式版本库</strong>的方式，不必服务器端软件支持（wingeddevil注：这得分是用什么样的服务端，使用http协议或者git协议等不太一样。并且在push和pull的时候和服务器端还是有交互的。），使源代码的发布和交流极其方便。 Git 的速度很快，这对于诸如 Linux kernel 这样的大项目来说自然很重要。 Git 最为出色的是它的合并跟踪（merge tracing）能力。</p><p>Torvalds 开始着手开发 Git 是为了作为一种过渡方案来替代 BitKeeper</p><p><em>分布式相比于集中式的最大区别在于开发者可以提交到本地，每个开发者通过克隆（git clone），在本地机器上拷贝一个完整的Git仓库</em></p><h2 id="Git基本工作过程"><a href="#Git基本工作过程" class="headerlink" title="Git基本工作过程"></a>Git基本工作过程</h2><blockquote><p>9个常见操作，具体如下</p></blockquote><ul><li><p>1.新建项目文件夹（只做一次）</p></li><li><p>2.进入文件夹 (重要)</p></li><li><p>3.初始化仓库：git init（只做一次）</p></li><li><p>4.编码</p></li><li><p>5.添加文件信息： git add .</p></li><li><p>6.确认添加信息：git commit -m”描述信息”</p></li><li><p>7.查看详细日志信息：git log</p></li><li><p>8.查看简略日志信息：git log –oneline</p></li><li><p>9.版本回滚:git reset –hard 版本号</p></li></ul><img src="/pic/git.png"><h2 id="其他的常用操作"><a href="#其他的常用操作" class="headerlink" title="其他的常用操作"></a>其他的常用操作</h2><h3 id="git-init"><a href="#git-init" class="headerlink" title="git init"></a>git init</h3><blockquote><p>用法：git clone [url]</p></blockquote><p>该命令可用于通过指定的URL获取一个代码库。</p><h3 id="git-config"><a href="#git-config" class="headerlink" title="git config"></a>git config</h3><blockquote><p>用法：git config –global user.name “[name]”</p></blockquote><blockquote><p>用法：git config –global user.email “[email address]”</p></blockquote><p>该命令将分别设置提交代码的用户名和电子邮件地址。</p><h3 id="git-add"><a href="#git-add" class="headerlink" title="git add"></a>git add</h3><blockquote><p>用法：git add [file]</p></blockquote><p>该命令可以将一个文件添加至stage(暂存区)。</p><blockquote><p>用法：git add *</p></blockquote><p>该命令可以将多个文件添加至stage(暂存区)。</p><h3 id="git-commit"><a href="#git-commit" class="headerlink" title="git commit"></a>git commit</h3><blockquote><p>用法：git commit -m “[ Type in the commit message]”</p></blockquote><p>该命令可以在版本历史记录中永久记录文件。</p><blockquote><p>用法：git commit -a</p></blockquote><p>该命令将提交git add命令添加的所有文件，并提交git add命令之后更改的所有文件。 </p><h3 id="git-diff"><a href="#git-diff" class="headerlink" title="git diff"></a>git diff</h3><blockquote><p>用法：git diff</p></blockquote><p>该命令可以显示尚未添加到stage的文件的变更。</p><blockquote><p>用法：git diff –staged</p></blockquote><p>该命令可以显示添加到stage的文件与当前最新版本之间的差异。</p><blockquote><p>用法：git diff [first branch] [second branch]</p></blockquote><p>该命令可以显示两个分支之间的差异。</p><h3 id="git-reset"><a href="#git-reset" class="headerlink" title="git reset"></a>git reset</h3><blockquote><p>用法：git reset [file]</p></blockquote><p>该命令将从stage中撤出指定的文件，但可以保留文件的内容。</p><blockquote><p>用法：git reset [commit]</p></blockquote><p>该命令可以撤销指定提交之后的所有提交，并在本地保留变更。</p><h3 id="git-status"><a href="#git-status" class="headerlink" title="git status"></a>git status</h3><blockquote><p>用法：git status</p></blockquote><p>该命令将显示所有需要提交的文件。</p><h3 id="git-rm"><a href="#git-rm" class="headerlink" title="git rm"></a>git rm</h3><blockquote><p>用法：git rm [file]</p></blockquote><p>该命令将删除工作目录中的文件，并将删除动作添加到stage。</p><h3 id="git-log"><a href="#git-log" class="headerlink" title="git log"></a>git log</h3><blockquote><p>用法：git log</p></blockquote><p>该命令可用于显示当前分支的版本历史记录。</p><blockquote><p>用法：git log –follow[file]</p></blockquote><p>该命令可用于显示某个文件的版本历史记录，包括文件的重命名。</p><h3 id="git-show"><a href="#git-show" class="headerlink" title="git show"></a>git show</h3><blockquote><p>用法：git show [commit]</p></blockquote><p>该命令经显示指定提交的元数据以及内容变更。</p><h3 id="git-tag"><a href="#git-tag" class="headerlink" title="git tag"></a>git tag</h3><blockquote><p>用法：git tag [commitID]</p></blockquote><p>该命令可以给指定的提交添加标签。</p><h3 id="git-branch"><a href="#git-branch" class="headerlink" title="git branch"></a>git branch</h3><blockquote><p>用法：git branch</p></blockquote><p>该命令将显示当前代码库中所有的本地分支。</p><blockquote><p>用法：git branch [branch name]</p></blockquote><p>该命令将创建一个分支。</p><blockquote><p>用法：git branch -d [branch name]</p></blockquote><p>该命令将删除指定的分支。</p><h3 id="git-checkout"><a href="#git-checkout" class="headerlink" title="git checkout"></a>git checkout</h3><blockquote><p>用法：git checkout [branch name]</p></blockquote><p>你可以通过该命令切换分支。</p><blockquote><p>用法：git checkout -b [branch name]</p></blockquote><p>你可以通过该命令创建一个分支，并切换到新分支上。</p><h3 id="git-merge"><a href="#git-merge" class="headerlink" title="git merge"></a>git merge</h3><blockquote><p>用法：git merge [branch name]</p></blockquote><p>该命令可以将指定分支的历史记录合并到当前分支。</p><h3 id="git-remote"><a href="#git-remote" class="headerlink" title="git remote"></a>git remote</h3><blockquote><p>用法：git remote add [variable name] [Remote Server Link]</p></blockquote><p>你可以通过该命令将本地的代码库连接到远程服务器。</p><h3 id="git-push"><a href="#git-push" class="headerlink" title="git push"></a>git push</h3><blockquote><p>用法：git push [variable name] master</p></blockquote><p>该命令可以将主分支上提交的变更发送到远程代码库。</p><blockquote><p>用法：git push [variable name] [branch]</p></blockquote><p>该命令可以将指定分支上的提交发送到远程代码库。</p><blockquote><p>用法：git push –all [variable name]</p></blockquote><p>该命令可以将所有分支发送到远程代码库。</p><blockquote><p>用法：git pull [Repository Link]</p></blockquote><p>该命令将获取远程服务器上的变更，并合并到你的工作目录。</p><h3 id="git-stash"><a href="#git-stash" class="headerlink" title="git stash"></a>git stash</h3><blockquote><p>用法：git stash save</p></blockquote><p>该命令将临时保存所有修改的文件。</p><blockquote><p>用法：git stash pop</p></blockquote><p>该命令将恢复最近一次stash（储藏）的文件。</p><blockquote><p>用法：git stash list</p></blockquote><p>该命令将显示stash的所有变更。</p><blockquote><p>用法：git stash drop</p></blockquote><p>该命令将丢弃最近一次stash的变更。</p><h2 id="Git小游戏"><a href="#Git小游戏" class="headerlink" title="Git小游戏"></a>Git小游戏</h2><p>游戏链接为<a href="https://oschina.gitee.io/learn-git-branching/">GIt小游戏</a></p><p>推荐一个图解答案<a href="https://blog.csdn.net/GDUT_xin/article/details/125537967">答案</a></p><pre class="line-numbers language-none"><code class="language-none">---基础篇1.git commitgit commitgit commit2.git branchgit branch bugFixgit checkout bugFix3.git mergegit branch bugFixgit checkout bugFixgit commit -m “commit bugFix”git checkout maingit commit -m “commit main”git merge bugFix4.git rebasegit branch bugFixgit checkout bugFixgit commit -m “bugFix”git checkout maingit commit -m “main commit”git checkout bugFixgit rebase main高级篇1.分离HEADgit checkout c42.相对引用(^)git checkout main^git checkout c33.相对引用2(~)git branch -f main c6git checkout HEAD~1git branch -f bugFix HEAD~14.撤销变更git reset HEAD~1git chekout pushedgit revert HEAD移动提交记录1.git cherry-pickgit cherry-pick c3 c4 c72.交互式rebasegit rebase -i overHere （打开控制面板）omit c2 （点击c2）c4 c5交换位置 （拉取）杂项1.只取一个提交记录git checkout maingit cherry-pick c42.提交的技巧#1git rebase -i HEAD~2 交换2和3的位置git commit --amendgit rebase -i HEAD~2 恢复2和3的位置git checkout miangit rebase caption main3.提交的技巧#2git checkout maingit cherry-pick c2git commit --amendgit cherry-pick c34.git taggit tag v0 c1git tag v1 c2git checkout c25. git descridegit commit高级话题1.多次rebasegit rebase main bugFixgit rebase bugFix sidegit rebase side anothergit rebase another main2.两个父节点git branch bugWork main^ ^ 2^3.纠缠不清的分支git checkout onegit cherry-pick c4 c3 c2git checkout twogit cherry-pick c5 c4 c3 c2git branch -f three c2Push &amp; Pull —— Git 远程仓库！1.git clonegit clone2.远程分支git commitgit checkout o&#x2F;maingit commit3.git fetchgit fetch4.git pullgit pull5.模拟团队合作git clonegit fakeTeamwork 2git commitgit pull6.git pushgit commitgit commitgit push7.偏离的提及历史git clonegit fakeTeamworkgit commitgit pull --rebasegit push8.锁定的main（locked main）git reset --hard o&#x2F;maingit checkout -b feature c2git push origin feature关于 origin 和它的周边 —— Git 远程仓库高级操作1.推送主分支git fetchgit rebase o&#x2F;main side1git rebase side1 side2git rebase side2 side3git rebase side3 maingit push2.合并远程仓库git checkout maingit pullgit merge side1git merge side2git merge side3git push3.远程追踪git checkout -b side o&#x2F;maingit commitgit pull --rebasegit push4.git push的参数git push origin maingit push origin foo5.git push的参数2git push origin main^:foogit push origin foo:main6.git fetch的参数git fetch origin main~1:foogit fetch origin foo:maingit checkout foogit merge main7.没有source的sourcegit fetch origin :bargit push origin :foo8.git pull的参数git pull origin bar:foogit pull origin main:side---<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>]]></content>
      
      
      <categories>
          
          <category> git </category>
          
      </categories>
      
      
        <tags>
            
            <tag> git基础 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>你好</title>
      <link href="/2023/03/25/chy/"/>
      <url>/2023/03/25/chy/</url>
      
        <content type="html"><![CDATA[<h1 id="hexo-theme-matery"><a href="#hexo-theme-matery" class="headerlink" title="hexo-theme-matery"></a>hexo-theme-matery</h1><p><a href="http://hits.dwyl.io/blinkfox/hexo-theme-matery"><img src="http://hits.dwyl.io/blinkfox/hexo-theme-matery.svg" alt="HitCount"></a> <a href="https://gitter.im/hexo-theme-matery/Lobby?utm_source=badge"><img src="https://img.shields.io/gitter/room/blinkfox/hexo-theme-matery.svg" alt="Gitter"></a> <a href="https://github.com/blinkfox/hexo-theme-matery/issues"><img src="https://img.shields.io/github/issues/blinkfox/hexo-theme-matery.svg" alt="GitHub issues"></a> <a href="https://github.com/blinkfox/hexo-theme-matery/blob/master/LICENSE"><img src="https://img.shields.io/github/license/blinkfox/hexo-theme-matery.svg" alt="GitHub license"></a> <a href="https://codeload.github.com/blinkfox/hexo-theme-matery/zip/master"><img src="https://img.shields.io/badge/downloads-master-green.svg" alt="Download"></a> <a href="http://hexo.io/"><img src="https://img.shields.io/badge/hexo-%3E%3D%205.0.0-blue.svg" alt="Hexo Version"></a> <a href="https://github.com/blinkfox/hexo-theme-matery/network"><img src="https://img.shields.io/github/forks/blinkfox/hexo-theme-matery.svg" alt="GitHub forks"></a> <a href="https://github.com/blinkfox/hexo-theme-matery/stargazers"><img src="https://img.shields.io/github/stars/blinkfox/hexo-theme-matery.svg" alt="GitHub stars"></a></p><p><a href="README.md">🇺🇸English Document</a> | <a href="http://blinkfox.com/">国内访问示例 (http://blinkfox.com)</a> | <a href="https://blinkfox.github.io/">Github 部署演示示例 (https://blinkfox.github.io)</a> </p><p>QQ 交流群1（已满）: <a href="https://jq.qq.com/?_wv=1027&k=5zMDYHT"><code>926552981</code></a> | QQ 交流群2（已满）: <a href="https://jq.qq.com/?_wv=1027&k=53q2Ayp"><code>971887688</code></a> | QQ 交流群3（推荐）: <a href="https://qm.qq.com/cgi-bin/qm/qr?k=fC1-kU-_aTn4q-JQq4GsYKr4WcKdgfGa&jump_from=webapi"><code>670694035</code></a></p><blockquote><p>这是一个采用 <code>Material Design</code> 和响应式设计的 Hexo 博客主题。</p></blockquote><h2 id="特性"><a href="#特性" class="headerlink" title="特性"></a>特性</h2><ul><li>简单漂亮，文章内容美观易读</li><li><a href="https://material.io/">Material Design</a> 设计</li><li>响应式设计，博客在桌面端、平板、手机等设备上均能很好的展现</li><li>首页轮播文章及每天动态切换 <code>Banner</code> 图片</li><li>瀑布流式的博客文章列表（文章无特色图片时会有 <code>24</code> 张漂亮的图片代替）</li><li>时间轴式的归档页</li><li><strong>词云</strong>的标签页和<strong>雷达图</strong>的分类页</li><li>丰富的关于我页面（包括关于我、文章统计图、我的项目、我的技能、相册等）</li><li>可自定义的数据的友情链接页面</li><li>支持文章置顶和文章打赏</li><li>支持 <code>MathJax</code></li><li>支持中文繁简转换</li><li><code>TOC</code> 目录</li><li>可设置复制文章内容时追加版权信息</li><li>可设置阅读文章时做密码验证</li><li><a href="https://gitalk.github.io/">Gitalk</a>、<a href="https://imsun.github.io/gitment/">Gitment</a>、<a href="https://valine.js.org/">Valine</a> 和 <a href="https://disqus.com/">Disqus</a> 评论模块（推荐使用 <code>Gitalk</code>）</li><li>集成了<a href="http://busuanzi.ibruce.info/">不蒜子统计</a>、谷歌分析（<code>Google Analytics</code>）和文章字数统计等功能</li><li>支持在首页的音乐播放和视频播放功能</li><li>支持<code>emoji</code>表情，用<code>markdown emoji</code>语法书写直接生成对应的能<strong>跳跃</strong>的表情。</li><li>支持 <a href="http://www.daovoice.io/">DaoVoice</a>、<a href="https://www.tidio.com/">Tidio</a> 在线聊天功能。</li></ul><table><thead><tr><th>表头</th><th>表头</th></tr></thead><tbody><tr><td>单元格</td><td>单元格</td></tr><tr><td>单元格</td><td>单元格</td></tr></tbody></table><table><thead><tr><th align="left">左对齐</th><th align="right">右对齐</th><th align="center">居中对齐</th></tr></thead><tbody><tr><td align="left">单元格</td><td align="right">单元格</td><td align="center">单元格</td></tr><tr><td align="left">单元格</td><td align="right">单元格</td><td align="center">单元格</td></tr></tbody></table><h2 id="贡献者"><a href="#贡献者" class="headerlink" title="贡献者"></a>贡献者</h2><p>感谢下面列出的贡献者，没有他们，hexo-theme-matery 不会这么完美。</p><ul><li><a href="https://github.com/HarborZeng">@HarborZeng</a></li><li><a href="https://github.com/shw2018">@shw2018</a></li><li><a href="https://github.com/L1cardo">@L1cardo</a></li><li><a href="https://github.com/Five-great">@Five-great</a></li></ul><h2 id="下载"><a href="#下载" class="headerlink" title="下载"></a>下载</h2><p>本主题<strong>推荐你使用 Hexo 5.0.0 及以上的版本</strong>。如果，你已经有一个自己的 <a href="https://hexo.io/zh-cn/">Hexo</a> 博客了，建议你将 Hexo 升级到最新稳定的版本。</p><p>点击 <a href="https://codeload.github.com/blinkfox/hexo-theme-matery/zip/master">这里</a> 下载 <code>master</code> 分支的最新稳定版的代码，解压缩后，将 <code>hexo-theme-matery</code> 的文件夹复制到你 Hexo 的 <code>themes</code> 文件夹中即可。</p><p>当然你也可以在你的 <code>themes</code> 文件夹下使用 <code>git clone</code> 命令来下载:</p><pre class="line-numbers language-bash" data-language="bash"><code class="language-bash"><span class="token function">git</span> clone https://github.com/blinkfox/hexo-theme-matery.git<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><h2 id="配置"><a href="#配置" class="headerlink" title="配置"></a>配置</h2><h3 id="切换主题"><a href="#切换主题" class="headerlink" title="切换主题"></a>切换主题</h3><p>修改 Hexo 根目录下的 <code>_config.yml</code> 的  <code>theme</code> 的值：<code>theme: hexo-theme-matery</code></p><h4 id="config-yml-文件的其它修改建议"><a href="#config-yml-文件的其它修改建议" class="headerlink" title="_config.yml 文件的其它修改建议:"></a><code>_config.yml</code> 文件的其它修改建议:</h4><ul><li>请修改 <code>_config.yml</code> 的 <code>url</code> 的值为你的网站主 <code>URL</code>（如：<code>http://xxx.github.io</code>）。</li><li>建议修改两个 <code>per_page</code> 的分页条数值为 <code>6</code> 的倍数，如：<code>12</code>、<code>18</code> 等，这样文章列表在各个屏幕下都能较好的显示。</li><li>如果你是中文用户，则建议修改 <code>language</code> 的值为 <code>zh-CN</code>。</li></ul><h3 id="新建分类-categories-页"><a href="#新建分类-categories-页" class="headerlink" title="新建分类 categories 页"></a>新建分类 categories 页</h3><p><code>categories</code> 页是用来展示所有分类的页面，如果在你的博客 <code>source</code> 目录下还没有 <code>categories/index.md</code> 文件，那么你就需要新建一个，命令如下：</p><pre class="line-numbers language-bash" data-language="bash"><code class="language-bash">hexo new page <span class="token string">"categories"</span><span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>编辑你刚刚新建的页面文件 <code>/source/categories/index.md</code>，至少需要以下内容：</p><pre class="line-numbers language-yaml" data-language="yaml"><code class="language-yaml"><span class="token punctuation">---</span><span class="token key atrule">title</span><span class="token punctuation">:</span> categories<span class="token key atrule">date</span><span class="token punctuation">:</span> <span class="token datetime number">2018-09-30 17:25:30</span><span class="token key atrule">type</span><span class="token punctuation">:</span> <span class="token string">"categories"</span><span class="token key atrule">layout</span><span class="token punctuation">:</span> <span class="token string">"categories"</span><span class="token punctuation">---</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>Hello World</title>
      <link href="/2023/03/23/hello-world/"/>
      <url>/2023/03/23/hello-world/</url>
      
        <content type="html"><![CDATA[<p>Welcome to <a href="https://hexo.io/">Hexo</a>! This is your very first post. Check <a href="https://hexo.io/docs/">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href="https://hexo.io/docs/troubleshooting.html">troubleshooting</a> or you can ask me on <a href="https://github.com/hexojs/hexo/issues">GitHub</a>.</p><h2 id="Quick-Start"><a href="#Quick-Start" class="headerlink" title="Quick Start"></a>Quick Start</h2><h3 id="Create-a-new-post"><a href="#Create-a-new-post" class="headerlink" title="Create a new post"></a>Create a new post</h3><pre class="line-numbers language-bash" data-language="bash"><code class="language-bash">$ hexo new <span class="token string">"My New Post"</span><span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>More info: <a href="https://hexo.io/docs/writing.html">Writing</a></p><h3 id="Run-server"><a href="#Run-server" class="headerlink" title="Run server"></a>Run server</h3><pre class="line-numbers language-bash" data-language="bash"><code class="language-bash">$ hexo server<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>More info: <a href="https://hexo.io/docs/server.html">Server</a></p><h3 id="Generate-static-files"><a href="#Generate-static-files" class="headerlink" title="Generate static files"></a>Generate static files</h3><pre class="line-numbers language-bash" data-language="bash"><code class="language-bash">$ hexo generate<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>More info: <a href="https://hexo.io/docs/generating.html">Generating</a></p><h3 id="Deploy-to-remote-sites"><a href="#Deploy-to-remote-sites" class="headerlink" title="Deploy to remote sites"></a>Deploy to remote sites</h3><pre class="line-numbers language-bash" data-language="bash"><code class="language-bash">$ hexo deploy<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>More info: <a href="https://hexo.io/docs/one-command-deployment.html">Deployment</a></p>]]></content>
      
      
      
    </entry>
    
    
  
  
</search>
